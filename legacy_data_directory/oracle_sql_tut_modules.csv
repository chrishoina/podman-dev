"ID","TUTORIAL_ID","NAME","DISPLAY_SEQ","CONTENT","CREATED","CREATED_BY","UPDATED","UPDATED_BY"
182256270815113871510434945310548021,182256270813904945690820316135841845,"Introduction to Anonymous Blocks",10,"<p>
PL/SQL, the Oracle procedural extension of SQL, is a portable, high-performance transaction-processing language that is tightly integrated with SQL.  The basic unit of a PL/SQL source program is the block, which groups related declarations and statements.
</p>
<p>
A PL/SQL block is defined by the keywords DECLARE, BEGIN, EXCEPTION, and END. These keywords divide the block into a declarative part, an executable part, and an exception-handling part. Only the executable part is required.
<code>
begin
   dbms_output.put_line('hello world');
end;
/
</code>
<p>
Because an anonymous block can have its own declaration and exception sections, developers can use anonymous blocks to provide scope for identifiers and exception handling within a larger program.
</p>
<code>
declare
    l_today date := sysdate;
begin
    dbms_output.put_line(
           'today is '||to_char(l_today,'Day'));
exception when others then
    dbms_output.put_line(sqlerrm);
end;
/
</code>",16-SEP-14 09.33.36.635541000 PM,"LIVESQL",16-SEP-14 09.33.36.635598000 PM,"LIVESQL"
166413577001589559637988793287051708,66403703756642340425947956952870068,"Querying Data",40,"<p>To select data from a single table it is reasonably easy, simply use the SELECT ... FROM ... WHERE ... ORDER BY ... syntax.</p>
<code>
select * from employees;
</code>
<p>To query data from two related tables you can join the data</p>
<code>
select e.name employee,
           d.name department,
           e.job,
           d.location
from departments d, employees e
where d.deptno = e.deptno(+)
order by e.name;
</code>
<p>As an alternative to a join you can use an inline select to query data.</p>
<code>
select e.name employee,
          (select name 
           from departments d 
           where d.deptno = e.deptno) department,
           e.job
from employees e
order by e.name;
</code>
",16-SEP-14 09.33.36.635860000 PM,"LIVESQL",18-DEC-14 10.28.20.501220000 PM,"MICHAEL.HICHWA@ORACLE.COM"
790055919507112484820594005908262491,182256270813904945690820316135841845,"Conditional Logic",20,"<p>Lets add some conditional logic using if ... then ... else syntax.  The TO_CHAR function can be used to format dates and number data types.  The <strong>D</strong> format mask is the day of the week, a number between 1 and 7.</p>
<code>
declare
    l_today date := sysdate;
begin
    if to_char(l_today,'D') < 4 then
        dbms_output.put_line(
           'Have a wonderful week');
    else
         dbms_output.put_line(
            'Enjoy the rest of the week');
     end if;
    dbms_output.put_line('today is '||
         to_char(l_today,'Day')||
         ' day '||to_char(l_today,'D')||
         ' of the week.');
end;
/
</code>",16-SEP-14 09.33.36.636268000 PM,"LIVESQL",16-SEP-14 09.33.36.636320000 PM,"LIVESQL"
1230817056766999134007418837776647148,182256270813904945690820316135841845,"Static SQL",30,"<p>With PL/SQL it is easy to integrate SQL statements, the example below shows a an example of using SELECT INTO with an Oracle data dictionary table.</p>

<code>
DECLARE
  howmany     INTEGER;
  num_tables  INTEGER;
BEGIN
  -- Begin processing
  SELECT COUNT(*) INTO howmany
  FROM USER_OBJECTS
  WHERE OBJECT_TYPE = 'TABLE'; -- Check number of tables
  num_tables := howmany;       -- Compute another value
  dbms_output.put_line (to_char(num_tables,'999G999G990')||' tables');
END;
</code>
<p>Here is an example of using a cursor for loop</p>
<code>
DECLARE
  l_table_count integer := 0;
BEGIN
  for c1 in (
  SELECT table_name
  FROM USER_TABLES
  order by 1) loop
      l_table_count := l_table_count + 1;
      dbms_output.put_line(c1.table_name);
  end loop;
  if l_table_count = 0 then
     dbms_output.put_line('You have no tables in your schema');
  end if;
END;
</code>",16-SEP-14 09.33.36.637819000 PM,"LIVESQL",16-SEP-14 09.33.36.637871000 PM,"LIVESQL"
1230817056877011383592350092674909164,182256270813904945690820316135841845,"Dynamic SQL",40,"<p>Lets extend our static SQL example to include some dynamic content.  To facilitate dynamic SQL passing a value back to the PL/SQL block we will create a package named ""PKG"".</p>
<code>
CREATE OR REPLACE PACKAGE pkg AUTHID DEFINER AS
  table_row_count integer;
END;
/

DECLARE
  l_table_count integer := 0;
  l_sql         varchar2(32767);
BEGIN
  for c1 in (
  SELECT table_name
  FROM USER_TABLES
  order by 1) loop
      l_table_count := l_table_count + 1;
      l_sql := 'begin select count(*) into pkg.table_row_count from ""'||c1.table_name||'""; end;';
      execute immediate l_sql;
      dbms_output.put_line(c1.table_name||' - '||to_char(pkg.table_row_count,'999G999G990')||' rows');
  end loop;
  if l_table_count = 0 then
     dbms_output.put_line('You have no tables in your schema');
  end if;
END;
/
</code>",16-SEP-14 09.33.36.638040000 PM,"LIVESQL",16-SEP-14 09.33.36.638092000 PM,"LIVESQL"
1311432585570323190912306356255595342,66403703756642340425947956952870068,"Compressing Data",85,"<p>As your database grows in size to gigabytes or terabytes and beyond, consider using table compression. Table compression saves disk space and reduces memory use in the buffer cache. Table compression can also speed up query execution during reads. There is, however, a cost in CPU overhead for data loading and DML. Table compression is completely transparent to applications. It is especially useful in online analytical processing (OLAP) systems, where there are lengthy read-only operations, but can also be used in online transaction processing (OLTP) systems.
</p>
<p>
You specify table compression with the COMPRESS clause of the CREATE TABLE statement. You can enable compression for an existing table by using this clause in an ALTER TABLE statement. In this case, the only data that is compressed is the data inserted or updated after compression is enabled. Similarly, you can disable table compression for an existing compressed table with the ALTER TABLE...NOCOMPRESS statement. In this case, all data the was already compressed remains compressed, and new data is inserted uncompressed.
</p>
<p>To enable compression for future data use the following syntax.</p>
<code>
alter table EMPLOYEES compress for oltp; 
alter table DEPARTMENTS compress for oltp; 
</code>
",16-SEP-14 09.33.36.638399000 PM,"LIVESQL",16-SEP-14 09.33.36.638450000 PM,"LIVESQL"
86364095479139315200687718653368146,66403703756642340425947956952870068,"Creating Tables",10,"<p>Tables are the basic unit of data storage in an Oracle Database. Data is stored in rows and columns. You define a table with a table name, such as employees, and a set of columns. You give each column a column name, such as employee_id, last_name, and job_id; a datatype, such as VARCHAR2, DATE, or NUMBER; and a width. The width can be predetermined by the datatype, as in DATE. If columns are of the NUMBER datatype, define precision and scale instead of width. A row is a collection of column information corresponding to a single record.
</p>

<p>
You can specify rules for each column of a table. These rules are called integrity constraints. One example is a NOT NULL integrity constraint. This constraint forces the column to contain a value in every row.
</p>

<p>For example:</p>
<code>
create table DEPARTMENTS (  
  deptno        number,  
  name          varchar2(50) not null,  
  location      varchar2(50),  
  constraint pk_departments primary key (deptno)  
);
</code>

<p>Tables can declarative specify relationships between tables, typically referred to as referential integrity.  To see how this works we can create a ""child"" table of the DEPARTMENTS table by including a foreign key in the EMPLOYEES table that references the DEPARTMENTS table.  For example:</p>

<code>
create table EMPLOYEES (  
  empno             number,  
  name              varchar2(50) not null,  
  job               varchar2(50),  
  manager           number,  
  hiredate          date,  
  salary            number(7,2),  
  commission        number(7,2),  
  deptno           number,  
  constraint pk_employees primary key (empno),  
  constraint fk_employees_deptno foreign key (deptno) 
      references DEPARTMENTS (deptno)  
);
</code>
<p>Foreign keys must reference primary keys, so to create a ""child"" table the ""parent"" table must have a primary key for the foreign key to reference.</p>",16-SEP-14 09.33.36.591832000 PM,"LIVESQL",16-SEP-14 09.33.36.591899000 PM,"LIVESQL"
166413576938725417018028076202330556,66403703756642340425947956952870068,"Creating Triggers",20,"<p>Triggers are procedures that are stored in the database and are implicitly run, or fired, when something happens.  Traditionally, triggers supported the execution of a procedural code, in Oracle procedural SQL is called a PL/SQL block.  PL stands for procedural language.  When an INSERT, UPDATE, or DELETE occurred on a table or view. Triggers support system and other data events on DATABASE and SCHEMA. </p>

<p>Triggers are frequently used to automatically populate table primary keys, the trigger examples below show an example trigger to do just this. We will use a built in function to obtain a globallally unique identifier or GUID.</p>
<code>
create or replace trigger  DEPARTMENTS_BIU
    before insert or update on DEPARTMENTS
    for each row
begin
    if inserting and :new.deptno is null then
        :new.deptno := to_number(sys_guid(), 
          'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX');
    end if;
end;
/

create or replace trigger EMPLOYEES_BIU
    before insert or update on EMPLOYEES
    for each row
begin
    if inserting and :new.empno is null then
        :new.empno := to_number(sys_guid(), 
            'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX');
    end if;
end;
/
</code>",16-SEP-14 09.33.36.592681000 PM,"LIVESQL",16-SEP-14 09.33.36.592734000 PM,"LIVESQL"
166413576942352194476871963726449084,92094449129961689585393288404949059,"About Sequences",10,"<p>Use the CREATE SEQUENCE statement to create a sequence, which is a database object from which multiple users may generate unique integers. You can use sequences to automatically generate primary key values.

When a sequence number is generated, the sequence is incremented, independent of the transaction committing or rolling back. If two users concurrently increment the same sequence, then the sequence numbers each user acquires may have gaps, because sequence numbers are being generated by the other user. One user can never acquire the sequence number generated by another user. After a sequence value is generated by one user, that user can continue to access that value regardless of whether the sequence is incremented by another user.</p>

<p>Sequence numbers are generated independently of tables, so the same sequence can be used for one or for multiple tables. It is possible that individual sequence numbers will appear to be skipped, because they were generated and used in a transaction that ultimately rolled back. Additionally, a single user may not realize that other users are drawing from the same sequence.</p>

",16-SEP-14 09.33.36.592905000 PM,"LIVESQL",16-SEP-14 09.33.36.592958000 PM,"LIVESQL"
166413576948396823574945109599979964,66403703756642340425947956952870068,"Inserting Data",30,"<p>Now that we have tables created, and we have triggers to automatically populate our primary keys, we can add data to our tables.  Because we have a parent child relationship, with the DEPARTMENTS table as the parent table, and the EMPLOYEES table as the child we will first INSERT a row into the DEPARTMENTS table.</p>

<code>
insert into departments (name, location) values
   ('Finance','New York');

insert into departments (name, location) values
   ('Development','San Jose');
</code>

<p>Lets verify that the insert was successful by running a SQL SELECT statement to query all columns and all rows of our table.</p>

<code>
select * from departments;
</code>

<p>You can see that an ID will have been automatically generated.  You can now insert into the EMPLOYEES table a new row but you will need to put the generated DEPTID value into your SQL INSERT statement.  The examples below show how we can do this using a SQL query, but you could simply enter the department number directly.</p>

<code>
insert into EMPLOYEES 
   (name, job, salary, deptno) 
   values
   ('Sam Smith','Programmer', 
    5000, 
  (select deptno 
  from departments 
  where name = 'Development'));

insert into EMPLOYEES 
   (name, job, salary, deptno) 
   values
   ('Mara Martin','Analyst', 
   6000, 
   (select deptno 
   from departments 
   where name = 'Finance'));

insert into EMPLOYEES 
   (name, job, salary, deptno) 
   values
   ('Yun Yates','Analyst', 
   5500, 
   (select deptno 
   from departments 
   where name = 'Development'));
</code>",16-SEP-14 09.33.36.593135000 PM,"LIVESQL",16-SEP-14 09.33.36.593187000 PM,"LIVESQL"
185746517254397992293351837919740996,66403703756642340425947956952870068,"Adding Columns",50,"<p>You can add additional columns after you have created your table using the ALTER TABLE ... ADD ... syntax.  For example:</p>
<code>
alter table EMPLOYEES 
add country_code varchar2(2);
</code>",16-SEP-14 09.33.36.593345000 PM,"LIVESQL",16-SEP-14 09.33.36.593396000 PM,"LIVESQL"
185746517293083618521019971510338628,66403703756642340425947956952870068,"Querying the Oracle Data Dictionary",60,"<p>Table meta data is accessible from the Oracle data dictionary.  The following queries show how you can query the data dictionary tables.</p>
<code>
select table_name, tablespace_name, status
from user_tables
where table_Name = 'EMPLOYEES';

select column_id, column_name , data_type
from user_tab_columns
where table_Name = 'EMPLOYEES'
order by column_id;
</code>
",16-SEP-14 09.33.36.593544000 PM,"LIVESQL",16-SEP-14 09.33.36.593596000 PM,"LIVESQL"
185746517424856532859014551553311812,66403703756642340425947956952870068,"Updating Data",70,"<p>You can use SQL to update values in your table, to do this we will use the update clause</p>

<code>
update employees
set country_code = 'US';
</code>

<p>The query above will update all rows of the employee table and set the value of country code to US.  You can also selectively update just a specific row.</p>

<code>
update employees
set commission = 2000
where  name = 'Sam Smith';
</code>

<p>Lets run a Query to see what our data looks like</p>
<code>
select name, country_code, salary, commission
from employees
order by name;
</code>",16-SEP-14 09.33.36.593772000 PM,"LIVESQL",16-SEP-14 09.33.36.593824000 PM,"LIVESQL"
185746517554211595557779873246872644,66403703756642340425947956952870068,"Aggregate Queries",80,"<p>You can sum data in tables using aggregate functions.  We will use column aliases to rename columns for readability, we will also use the null value function (NVL) to allow us to properly sum columns with null values.  </p>
<code>
select 
      count(*) employee_count,
      sum(salary) total_salary,
      sum(commission) total_commission,
      min(salary + nvl(commission,0)) min_compensation,
      max(salary + nvl(commission,0)) max_compensation
from employees;
</code>",16-SEP-14 09.33.36.594027000 PM,"LIVESQL",16-SEP-14 09.33.36.594079000 PM,"LIVESQL"
485616852624673161369871175234808724,66403703756642340425947956952870068,"Deleting Data",90,"<p>You can delete one or more rows from a table using the DELETE syntax.  For example to delete a specific row:</p>
<code>
delete from employees 
where name = 'Sam Smith';
</code>",16-SEP-14 09.33.36.594239000 PM,"LIVESQL",16-SEP-14 09.33.36.594290000 PM,"LIVESQL"
485616852804803108492450922266028948,66403703756642340425947956952870068,"Dropping Tables",100,"<p>You can drop tables using the SQL DROP command.  Dropping a table will remove all of the rows and drop sub-objects including indexes and triggers.  The following DROP statements will drop the departments and employees tables.  The optional cascade constraints clause will drop remove constraints thus allowing you to drop database tables in any order.</p>
<code>
drop table departments cascade constraints;
drop table employees cascade constraints;
</code>",16-SEP-14 09.33.36.594449000 PM,"LIVESQL",16-SEP-14 09.33.36.594501000 PM,"LIVESQL"
485616852806012034312065551440735124,66403703756642340425947956952870068,"Un-dropping Tables",110,"<p>If the RECYCLEBIN initialization parameter is set to ON (the default in 10g), then dropping this table will place it in the recycle bin.  To see if you can undrop a table run the following data dictionary query:</p>
<code>
select object_name, 
       original_name, 
       type, 
       can_undrop, 
       can_purge
from recyclebin;
</code>
<p>To undrop tables we use the flashback command, for example:</p>
<code>
flashback table DEPARTMENTS to before drop;
flashback table EMPLOYEES to before drop;
select count(*) departments 
from departments;
select count(*) employees
from employees;
</code>",16-SEP-14 09.33.36.594640000 PM,"LIVESQL",16-SEP-14 09.33.36.594691000 PM,"LIVESQL"
1311432585226988258141751670639041358,66403703756642340425947956952870068,"Indexing Columns",35,"<p>Typically developers index columns for three major reasons:</p>
<ol><li>To enforce unique values within a column</li>
<li>To improve data access performance</li>
<li>To prevent lock escalation when updating rows of tables that use declarative referential integrity</li>
</ol>
<p>When a table is created and a PRIMARY KEY is specified an index is automatically created to enforce the primary key constraint.  If you specific UNIQUE for a column when creating a column a unique index is also created.  To see the indexes that already exist for a given table you can run the following dictionary query.</p>

<code>
select table_name ""Table"", 
       index_name ""Index"", 
       column_name ""Column"", 
       column_position ""Position""
from  user_ind_columns 
where table_name = 'EMPLOYEES' or 
      table_name = 'DEPARTMENTS'
order by table_name, column_name, column_position
</code>

<p>It is typically good form to index foreign keys, foreign keys are columns in a table that reference another table.  In our EMPLOYEES and DEPARTMENTS table example the DEPTNO column in the EMPLOYEE table references the primary key of the DEPARTMENTS table.</p>
<code>
create index employee_dept_no_fk_idx 
on employees (deptno)
</code>
<p>We may also determine that the EMPLOYEE table will be frequently searched by the NAME column.  To improve the performance searches and to ensure uniqueness we can create a unique index on the EMPLOYEE table NAME column.</p>

<code>
create unique index employee_ename_idx
on employees (name)
</code>

<p>Oracle provides many other indexing technologies including function based indexes which can index expressions, such as an upper function, text indexes which can index free form text, bitmapped indexes useful in data warehousing.  You can also create indexed organized tables, you can use partition indexes and more.  Sometimes it is best to have fewer indexes and take advantage of in memory capabilities.  All of these topics are beyond the scope of this basic introduction.</p>",16-SEP-14 09.33.36.594848000 PM,"LIVESQL",16-SEP-14 09.33.36.594900000 PM,"LIVESQL"
1321744041406742030396375039270866462,92094449129961689585393288404949059,"Creating Sequences",20,"<p>After a sequence is created, you can access its values in SQL statements with the CURRVAL pseudocolumn, which returns the current value of the sequence, or the NEXTVAL pseudocolumn, which increments the sequence and returns the new value.</p>

<p>For example:</p>
<code>
create sequence my_sequence start with 1;
</code>

<p>Now lets run some code to see how we can use this sequence.  The DUAL table is a pseudo table in Oracle you can select from that has one row.  Now lets repeat the above, but first lets request the next value from the sequence.</p>
 
<code>select my_sequence.NEXTVAL
from dual;
</code>


<code>select my_sequence.CURRVAL
from dual;
</code>


<p>Now repeat the first query</p>

<code>select my_sequence.NEXTVAL
from dual;
</code>",16-SEP-14 09.33.36.638608000 PM,"LIVESQL",16-SEP-14 09.33.36.638660000 PM,"LIVESQL"
1321744041407950956215989668445572638,92094449129961689585393288404949059,"Referencing Sequences in Triggers",30,"<p>Perhaps the most common use of sequences is to populate table primary key values from within triggers.  The example below creates a table and uses a trigger to populate the primary key.</p>

<code>
create sequence simple_employees_seq;

create table SIMPLE_EMPLOYEES (  
  empno             number primary key,  
  name              varchar2(50) not null,  
  job               varchar2(50)
  );

create or replace trigger SIMPLE_EMPLOYEES_BIU_TRIG
    before insert or update on SIMPLE_EMPLOYEES
    for each row
begin
    if inserting and :new.empno is null then
        :new.empno := simple_employees_seq.nextval;
    end if;
end;
/
</code>",16-SEP-14 09.33.36.639489000 PM,"LIVESQL",16-SEP-14 09.33.36.639542000 PM,"LIVESQL"
1321744041451472285722116318734994974,92094449129961689585393288404949059,"Testing Triggered Sequence",40,"<p>Lets populate some data and test the trigger based generation of the primary key.</p>

<code>
insert into simple_employees (name, job) values ('Mike', 'Programmer');
insert into simple_employees (name, job) values ('Taj', 'Analyst');
insert into simple_employees (name, job) values ('Jill', 'Finance');
insert into simple_employees (name, job) values ('Fred', 'Facilities');
insert into simple_employees (name, job) values ('Sabra', 'Programmer');
commit;
</code>
<p>Lets query our table to see how the EMPNO column was populated</p>
<code>
select empno, name, job
from simple_employees
order by 1
</code>

",16-SEP-14 09.33.36.639697000 PM,"LIVESQL",16-SEP-14 09.33.36.639749000 PM,"LIVESQL"
35019070928713586442449453253988615109,35011252049457208546674439406966472602,"Insert data into the FLIGHTS table",15,"<p>Insert data into the FLIGHTS table. The following statements insert seven rows into the FLIGHTS table and commit the changes. </p>

<code>
insert into flights  
values (1, 'BA123', 'LHR', 'JFK', timestamp'2015-01-01 10:00:00 -00:00', interval '5' hour, 'BA');
insert into flights  
values (2, 'BA123', 'LHR', 'JFK', timestamp'2015-01-02 10:00:00 -00:00', interval '5' hour, 'BA');
insert into flights  
values (3, 'AA567', 'LHR', 'JFK', timestamp'2015-01-01 13:00:00 -00:00', interval '6' hour, 'AA');
insert into flights  
values (4, 'BA124', 'JFK', 'LHR', timestamp'2015-01-01 22:00:00 -05:00', interval '5' hour, 'BA');
insert into flights  
values (5, 'BA124', 'JFK', 'LHR', timestamp'2015-01-02 22:00:00 -05:00', interval '5' hour, 'BA');
insert into flights  
values (6, 'AA567', 'JFK', 'LHR', timestamp'2015-01-01 20:00:00 -05:00', interval '6' hour, 'AA');
insert into flights  
values (7, 'AA987', 'JFK', 'SFO', timestamp'2015-01-02 12:00:00 -05:00', interval '4' hour, 'AA');

commit;
</code>",08-JUL-15 07.14.00.581152000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 07.14.00.581217000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35029548269919685321568679000021828835,35011252049457208546674439406966472602,"Update single column (self update)",30,"<p> This statement updates a single column, FLIGHT_DURATION, in the FLIGHTS table by setting its value as a factor of its existing value.</p>
<p>The SELECT statement that follows the update operation displays the data in the FLIGHTS table, thus verifying that the update operation completed successfully.</p>
<code>
update flights 
set    flight_duration = flight_duration + interval '30' minute 
where  flight_id = 7;

select * from flights;
</code>",08-JUL-15 09.12.49.142141000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 09.13.46.759348000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35029548269920894247388293629196535011,35011252049457208546674439406966472602,"Update using correlated subquery",50,"<p>This statement uses a correlated subquery to update a single column across multiple rows in the FLIGHTS table. It increases the duration of all BA flights by thirty minutes.</p>
<p>The subquery returns all flights with OPERATING_CARRIER_CODE set to 'BA'. This result is used as the predicate value in the UPDATE statement. Thus, the value of the FLIGHT_DURATION column is updated for all rows FLIGHT_ID matches those returned by the query.</p>
<p>The SELECT statement displays the data in the FLIGHTS table and is used to verify that the update operation completed successfully.</p>
<code>
update flights f1 
set    f1.flight_duration = f1.flight_duration + interval '30' minute 
where  f1.flight_id in (select f2.flight_id  
                        from   flights f2 
                        where  f2.operating_carrier_code = 'BA');
                        
select * from flights;

</code>",08-JUL-15 09.24.14.766652000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 09.27.02.100685000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35032224292201313640716490770364384378,35011252049457208546674439406966472602,"Correlated update",60,"<p>This statement performs a correlated update of the rows in the FLIGHTS table. For all flights with OPERATING_CARRIER_CODE='BA' that depart on 1 Jan 2015 , it appends zero to the flight number and sets the flight duration to the duration of the AA flight on the same day and same route. It also sets the departure times for these flights to 11AM GMT.</p>
<p>The SELECT statement displays the updated contents of the FLIGHTS table and enables you to verify that the data has been successfully updated.</p>

<code>
update flights f1 
set    f1.departure_datetime = timestamp'2015-01-01 11:00:00 -00:00', 
       (f1.flight_number, f1.flight_duration) = ( 
          select f2.flight_number || '0', f2.flight_duration 
          from   flights f2 
          where  f2.departure_airport_code = f1.departure_airport_code 
          and    f2.destination_airport_code = f1.destination_airport_code 
          and    trunc(f2.departure_datetime) = trunc(f2.departure_datetime) 
          and    f2.operating_carrier_code = 'AA' 
        ) 
where  f1.operating_carrier_code = 'BA' 
and    trunc(f1.departure_datetime) = date'2015-01-01';

select * from flights;
</code>",08-JUL-15 09.42.45.335568000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 09.42.45.335641000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35015523015721407127288641813299767665,35011252049457208546674439406966472602,"Create FLIGHTS table",10,"<p> The first step is to create the FLIGHTS table that will be used in this tutorial. This table contains details about scheduled flights including the source and destination city, flight departure time, flight duration, and operating carrier code. It is a range-partitioned table that is partitioned based on the airport code of the city from which the flights depart. 
</p>
<code>
create table flights ( 
  flight_id                integer not null primary key, 
  flight_number            varchar2(6) not null, 
  departure_airport_code   varchar2(3) not null, 
  destination_airport_code varchar2(3) not null, 
  departure_datetime       timestamp with time zone not null, 
  flight_duration          interval day to second(0) not null, 
  operating_carrier_code   varchar2(2) not null 
) partition by range (departure_airport_code) ( 
  partition pA_E values less than ('F'), 
  partition pF_J values less than ('K'), 
  partition pK_O values less than ('P'), 
  partition pP_T values less than ('U'), 
  partition pU_Z values less than ('a') 
);

</code>
",08-JUL-15 05.52.30.363763000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 07.13.19.765552000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35015523015722616053108256442474473841,35011252049457208546674439406966472602,"Update single column in a row",20,"<p> This statement updates the value of a single column, FLIGHT_NUMBER, in a particular row. The row to be updated is specified using a WHERE clause. </p>
<p>After the update operation, a SELECT statement displays the data in the FLIGHTS table. Verify that the specified column was updated.</p>
<code> 
update flights set flight_number = 'AA986'  where  flight_id = 7;

select * from flights;
</code>",08-JUL-15 05.53.03.194970000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 09.01.21.941284000 AM,"PADMAJA.POTINENI@ORACLE.COM"
35015523015752839198598622171842128241,35011252049457208546674439406966472602,"Update multiple columns in a row",40,"<p>The following statement updates three columns, FLIGHT_NUMBER, FLIGHT_DURATION, and OPERATING_CARRIER_CODE, in a single row of the FLIGHTS table. The row to be updated is identified using the WEHERE clause.</p>
<p>A SELECT statement displays the updated content for the FLIGHTS table for verification.</p>
<code> 
update flights 
set    flight_number = 'BA986', flight_duration = interval '5' hour, operating_carrier_code = 'BA' 
where  flight_id = 7;

select * from flights;
</code>",08-JUL-15 05.57.19.133001000 AM,"PADMAJA.POTINENI@ORACLE.COM",08-JUL-15 09.13.35.212754000 AM,"PADMAJA.POTINENI@ORACLE.COM"
67318924947213193595261095545164938864,66677775709556657797196623725084693197,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE clickdata PURGE;
</code>",12-MAY-16 12.26.24.889121000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.09.35.775245000 PM,"SHARON.KENNEDY@ORACLE.COM"
67318924947309907660830265879141432944,66677775709556657797196623725084693197,"Overview of SQL Pattern Matching",10,"<h3>Finding patterns with SQL</h3>
<p>
Recognizing patterns in a sequence of rows has been a capability that was widely desired, but not really possible with SQL until now. There were many workarounds, but these were difficult to write, hard to understand, and inefficient to execute.</p> 
<p>
With Oracle Database 12c you can use the MATCH_RECOGNIZE clause to perform pattern matching in SQL to do the following:</p>
<ol>
<li>Logically group and order the data that is used in the MATCH_RECOGNIZE clause using the PARTITION BY and ORDER BY clauses.</li>
<li>Define business rules/patterns using the PATTERN clause. These patterns use regular expressions syntax, a powerful and expressive feature and applied to the pattern variables.</li>
<li>Specify the logical conditions required to map a row to a row pattern variable using the DEFINE clause.</li>
<li>Define output measures, which are expressions within the MEASURES clause.</li>
<li>Control the output (summary vs. detailed) from the pattern matching process.</li>
</ol>
",12-MAY-16 12.37.05.665699000 PM,"KEITH.LAKER@ORACLE.COM",13-MAY-16 08.47.53.611842000 AM,"KEITH.LAKER@ORACLE.COM"
67318924948007457858747906912946896496,66677775709556657797196623725084693197,"More about PATTERN and DEFINE  keywords",45,"<h3>How do we build the PATTERN and DEFINE clauses?</h3>
<p>
As we mentioned earlier, the <strong>PATTERN</strong> defines a regular expression, which is a highly expressive way to search for patterns. <strong>PATTERN (b s+)</strong> says that the pattern we are searching for has two pattern variables: <strong>b</strong> and <strong>s</strong>. The star sign <strong>(*)</strong> after <strong>s</strong> means that zero or more rows must be mapped to confirm a match.
</p>
<p>The plus sign <strong>(+)</strong> is part of a library of quantifiers that can be used to describe your pattern. We will explore these in a separate tutorial</p>

<p><strong>DEFINE</strong> gives us the conditions that must be met for a row to map to your row pattern variables <strong>b</strong> and <strong>s</strong>. Because there is no condition for b, any row can be mapped to b. Why have a pattern variable with no condition? You use it as a starting point for testing for matches. S takes advantage of the PREV() function, which lets them compare the timestamp in the current row to the timestamp in the prior row. S is matched when a row has a timestamp than is within 10 minutes of the row that preceded it.
</p>",12-MAY-16 01.16.16.491375000 PM,"KEITH.LAKER@ORACLE.COM",28-APR-17 10.49.31.264257000 AM,"KEITH.LAKER@ORACLE.COM"
83205801604506548639706235386764899722,83205148911047716319241478701553687837,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>
<p>You can view the full data set using the following code:</p>
<code>SELECT * FROM ticker ORDER BY symbol, tstamp;</code>
<br>",11-OCT-16 03.01.38.505294000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.58.58.373763000 PM,"SHARON.KENNEDY@ORACLE.COM"
83205801604597218076177332574867862922,83205148911047716319241478701553687837,"Managing output from MATCH_RECOGNIZE",11,"<p>When determining the type of output you want MATCH_RECOGNIZE to return most developers will opt for one of the following:
<blockquote>
ONE ROW PER MATCH - each match produces one summary row. This is the default.
ALL ROWS PER MATCH - a match spanning multiple rows will produce one output row for each row in the match.
</blockquote>
The default behaviour for MATCH_RECOGNIZE is to return one summary row for each match. In the majority of use cases this is probably the ideal solution. However, there are also many use cases that require more detailed information to be returned. If you are degugging your MATCH_RECOGNIZE statement then a little more information can help show how the pattern is being matched to your data set. 
<br><br>
In some cases you may find it useful, or even necessary, to use the extended syntax of the ALL ROWS PER MATCH keywords. 
<br><br>
There are three sub options:
<ul>
<li>ALL ROWS PER MATCH SHOW EMPTY MATCHES <- note that this is the default</li>
<li>ALL ROWS PER MATCH OMIT EMPTY MATCHES</li>
<li>ALL ROWS PER MATCH WITH UNMATCHED ROWS</li>
</ul>
Letâ€™s look at these sub-options in more detail but first a quick point of reference: all the examples shown below use the default <strong>AFTER MATCH SKIP PAST LAST ROW</strong> syntax. This will be covered in a separate tutorial.
</p>",11-OCT-16 03.06.03.128316000 PM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 09.57.36.839979000 AM,"KEITH.LAKER@ORACLE.COM"
83205801604772512320021453805200258442,83205148911047716319241478701553687837,"MATCH_NUMBER and EMPTY MATCHES",50,"<h3>What's the effect on MATCH_MUMBER?</h3>
<p>
The MATCH_NUMBER() function counts the empty matches and assigns a number to them - as you can see in the output from the previous code. Therefore, if we omit the empty matches from the results the MATCH_NUMBER() column no longer contains a contiguous set of numbers:</p>
<code>SELECT
  symbol,
  tstamp,
  price,
  start_tstamp,
  end_tstamp,
  match_num,
  classifier
FROM ticker 
MATCH_RECOGNIZE ( 
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(down.tstamp) AS start_tstamp,
           LAST(down.tstamp) AS end_tstamp,
           match_number() AS match_num,
           classifier() AS classifier
  ALL ROWS PER MATCH OMIT EMPTY MATCHES
  PATTERN (DOWN*) 
  DEFINE <br>
    DOWN AS (price <= PREV(price))
)
WHERE symbol = 'GLOBEX';
</code>
<p>
As you can see from the previous output the MATCH_NUMBER() column starts with match number 4 followed by match 6 followed by match 10. Therefore, you need to be very careful if you decide to test for a specific match number within the MATCH_RECOGNIZE section and/or the result set because you might get caught out if you are expecting a contiguous series of numbers.
</p>",11-OCT-16 03.18.15.752298000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.00.09.984555000 PM,"SHARON.KENNEDY@ORACLE.COM"
67313423838119783157836524038423990479,66677775709556657797196623725084693197,"Create my dataset",20,"<h3>Creating our log data</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE clickdata (tstamp integer, userid varchar2(15));

INSERT INTO clickdata VALUES(1, 'Mary');
INSERT INTO clickdata VALUES(2, 'Sam');
INSERT INTO clickdata VALUES(11, 'Mary');
INSERT INTO clickdata VALUES(12, 'Sam');
INSERT INTO clickdata VALUES(22, 'Sam');
INSERT INTO clickdata VALUES(23, 'Mary');
INSERT INTO clickdata VALUES(32, 'Sam');
INSERT INTO clickdata VALUES(34, 'Mary');
INSERT INTO clickdata VALUES(43, 'Sam');
INSERT INTO clickdata VALUES(44, 'Mary');
INSERT INTO clickdata VALUES(47, 'Sam');
INSERT INTO clickdata VALUES(48, 'Sam');
INSERT INTO clickdata VALUES(53, 'Mary');
INSERT INTO clickdata VALUES(59, 'Sam');
INSERT INTO clickdata VALUES(60, 'Sam');
INSERT INTO clickdata VALUES(63, 'Mary');
INSERT INTO clickdata VALUES(68, 'Sam');
commit;
</code>",12-MAY-16 11.31.21.986483000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.10.25.149519000 PM,"SHARON.KENNEDY@ORACLE.COM"
67320961604881843878753901716980809037,66677775709556657797196623725084693197,"Partitioning and ordering data",60,"<h3>Some more important keywords</h3>
<p>What we need to do is use the PARTITION BY and ORDER BY key phrases to correctly divide up our data set by each user and then within each user make sure the data is ordered by our timestamp so we can determine if the clicks recorded in our log by each user are within 10 minutes of their previous click.
</p>
<p>We need to expand our MATCH_RECOGNIZE() clause as shown here:</p>
<code>SELECT 
 tstamp,
 userid,
 session_id
FROM clickdata MATCH_RECOGNIZE(         
   PARTITION BY userid ORDER BY tstamp
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   PATTERN (b s+)
   DEFINE
       s as (tstamp - prev(tstamp) <=10)
 );
</code>
<p>Now you should see that Mary has logged 3 sessions and Sam has logged three sessions. Which means that have now correctly specified our MATCH_RECOGNIZE clause to create our sessionization report.</p>",12-MAY-16 12.46.28.347393000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.12.17.223634000 PM,"SHARON.KENNEDY@ORACLE.COM"
67313423838197154410291860305605185743,66677775709556657797196623725084693197,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<br>
<code>SELECT count(*) FROM clickdata;</code>
<p>You should have 17 rows of data and you can view the rows using the following code: </p>
<br>
<code>SELECT * FROM clickdata;</code>
",12-MAY-16 11.32.47.084487000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.10.54.117110000 PM,"SHARON.KENNEDY@ORACLE.COM"
67320961605443994384874704283219180877,66677775709556657797196623725084693197,"Counting the number of events",70,"<h3>Adding more measures</h3>
<p>The next stage is to count the number of events within each session and calculate the duration of each session</p>
<p>How do we do that? We can use some of the other built-in measures such as <strong>FIRST()</strong> and <strong>LAST()</strong> to extract values from our resultset and we can calculate new values such as the duration of a session.</p>
<p>In our code we will add some new measures</p>
<ul>
<li>count(*) returns the number of events within a session</li>
<li>first(tstamp) returns the start time of each session</li>
<li>last(tstamp) returns the end time of each session</li>
<li>last(tstamp) - first(tstamp) calculates the duration of each session</li>    
</ul>
<br>
<code>SELECT
 tstamp,
 userid,
 session_id,
 no_of_events,
 start_time,
 session_duration
FROM clickdata MATCH_RECOGNIZE(
   PARTITION BY userid ORDER BY tstamp
   MEASURES match_number() as session_id,
            COUNT(*) as no_of_events,
            FIRST(b.tstamp) start_time,
            LAST(s.tstamp) end_time,
            LAST(s.tstamp) - FIRST(s.tstamp) session_duration   
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (tstamp - prev(tstamp) <= 10)
 ); 
</code>
<p>This is looking good but it would it would be better if we could create a summary report</p>",12-MAY-16 01.05.55.733708000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.12.57.775842000 PM,"SHARON.KENNEDY@ORACLE.COM"
67302079613871847387396790661962944846,66677775709556657797196623725084693197,"Business Problem",11,"<h3>Sessionization analysis</h3>
<p>
This tutorial is designed to help show you how you can run sessionization analysis on application logs, web logs, etc.</p>
<p>
For this tutorial we are going to define a session as a sequence of one or more events where the inter-timestamp gap is less than 10 minutes between events.
</p>
<p>
Using our simplified table of click data, which will create in the next step, we will create a sessionization data set which tracks each session, the duration of the session and the number of clicks/events using the new 12c MATCH_RECOGNIZE feature.
</p> ",12-MAY-16 08.48.03.868839000 AM,"KEITH.LAKER@ORACLE.COM",13-MAY-16 08.47.16.767160000 AM,"KEITH.LAKER@ORACLE.COM"
83205148911073103761453385914222517533,83205148911047716319241478701553687837,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE ticker PURGE;
</code>",11-OCT-16 02.55.30.654842000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.58.32.713657000 PM,"SHARON.KENNEDY@ORACLE.COM"
67315813151755509426825791870767603956,66677775709556657797196623725084693197,"Building my first sessionization report",40,"<h3>Some important keywords</h3>
<p>Running our first sessionization report using only the main keywords within MATCH_RECOGNIZE clause</p>
<p>The keywords that we are going to use are:</p>
<blockquote>
<p>
<strong>ALL ROWS PER MATCH</strong><br>
You will sometimes want summary data about the matches and other times need details. For this report we are going to return a detailed report.
</p>
<p>
<strong>PATTERN</strong><br>
The PATTERN clause lets you define which pattern variables must be matched, the sequence in which they must be matched, and the quantity of rows which must be matched. The PATTERN clause specifies a regular expression for the match search.
<p>
<strong>DEFINE</strong><br>
The PATTERN clause depends on pattern variables, therefore, you must have a clause to define these variables. They are specified in the DEFINE clause. DEFINE is a required clause, used to specify the conditions that a row must meet to be mapped to a specific pattern variable.
</p>
</blockquote>
<p>
So let's write our first MATCH_RECOGNIZE statement based on our business requirements:</p>
<code>SELECT 
 tstamp,
 userid
FROM clickdata MATCH_RECOGNIZE(         
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (tstamp - prev(tstamp) <= 10)
 );
</code>",12-MAY-16 11.59.01.657701000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.11.20.281593000 PM,"SHARON.KENNEDY@ORACLE.COM"
67315813152495372028429944925687783668,66677775709556657797196623725084693197,"Calculating the session number",50,"<h3>Finding the session number</h3>
<p>Congratulations your report runs! We have a list of events where each event is within 10 minutes of the previous event</p>
<p>But we don't actually know how many sessions each user logged, so how can we find out this information?</p>
<p>We can use the internal <strong>MATCH_NUMBER()</strong> function to return the session id for each user - assuming that a session contains clicks that are within a 10 minute interval of previous click</p>

<p>What does <strong>MATCH_NUMBER()</strong> actually do?<br>
You might have a large number of matches for your pattern inside a given row partition. How do you tell these matches apart? This is done with the <strong>MATCH_NUMBER()</strong> function. Matches within a row pattern partition are numbered sequentially starting with 1 in the order they are found. This numbering starts at 1 within each row pattern partition, because there is no linked ordering across row pattern partitions.
</p>

<code>SELECT 
 tstamp,
 userid,
 session_id
FROM clickdata MATCH_RECOGNIZE(         
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (tstamp - prev(tstamp) <= 10)
 );
</code>
<h5>That does not look correct...</h5>
<p>What you will notice is that each row has a session id of 1 which means that we are only finding one instance of our pattern within our data set.</p>
<p>Obviously this is not correct because just looking at the raw data we can see that Mary has logged more than one session</p>
<code>SELECT * 
FROM clickdata
WHERE userid = 'Mary'
</code>",12-MAY-16 12.17.47.127595000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.11.58.521484000 PM,"SHARON.KENNEDY@ORACLE.COM"
83205148911070685909814156655873105181,83205148911047716319241478701553687837,"Overview of SQL Pattern Matching",10,"<h3>Finding patterns with SQL</h3>
<p>
Recognizing patterns in a sequence of rows has been a capability that was widely desired, but not really possible with SQL until now. There were many workarounds, but these were difficult to write, hard to understand, and inefficient to execute.</p> 
<p>
The aim of this tutorial is to explain the difference between the various row output options within MATC_RECOGNIZE, specifically the EMPTY MATCHES and UNMATCHED ROWS keywords within the ALL ROWS PER MATCH syntax.
</p>",11-OCT-16 02.54.40.509297000 PM,"KEITH.LAKER@ORACLE.COM",11-OCT-16 02.54.40.509356000 PM,"KEITH.LAKER@ORACLE.COM"
67324399808992633113235521234681908684,66677775709556657797196623725084693197,"Building a summary report",80,"<h3>Summarizing our results</h3>
<p>What we really want is a report that shows one line for each session, shows the number of events, the start time and the session duration.</p>
<p>To return a summary report we only need to change one line of code!</p>
<p>We just need to change the output type from <strong>ALL ROWS PER MATCH</strong> to <strong>ONE ROW PER MATCH</strong> and remove the reference to <strong>tstamp</strong> within the SELECT clause.</p>
<code>SELECT
 userid,
 session_id,
 no_of_events,
 start_time,
 session_duration
FROM clickdata MATCH_RECOGNIZE(
   PARTITION BY userid ORDER BY tstamp
   MEASURES match_number() as session_id,
            COUNT(*) as no_of_events,
            FIRST(b.tstamp) start_time,
            LAST(s.tstamp) end_time,
            LAST(s.tstamp) - FIRST(b.tstamp) session_duration    
   ONE ROW PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (tstamp - prev(tstamp) <= 10)
 );
</code>
<p>...and there is our finished sessionization report.</p>",12-MAY-16 01.37.17.378940000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.13.16.812141000 PM,"SHARON.KENNEDY@ORACLE.COM"
83205148911074312687273000543397223709,83205148911047716319241478701553687837,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);

INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);

INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);

commit;

END;

</code>",11-OCT-16 02.55.47.594808000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.58.44.215211000 PM,"SHARON.KENNEDY@ORACLE.COM"
83205801604745915951989931963356722570,83205148911047716319241478701553687837,"Empty matches",40,"<p>
<h3>What is the difference between an â€œempty matchâ€ and an â€œunmatched row""?</h3>
<br>
This is largely determined by the type of quantifier used as part of the pattern definition. By changing the quantifier it is possible to produce the similar result using both sets of keywords.
<br><br>
To help explore the subtleties of these keywords here is a simple pattern that looks for price decreases - note that we are using the * (star) quantifier to indicate that we are looking for zero or more matches of the DOWN pattern. Therefore, if we run the following code we will in fact return a result set containing all 20 rows for the symbol ""GLOBEX"":
<br><br>
<code>SELECT 
  symbol, 
  tstamp,
  price,
  start_tstamp,
  end_tstamp,
  match_num,
  classifier
FROM ticker 
MATCH_RECOGNIZE ( 
  PARTITION BY symbol ORDER BY tstamp 
  MEASURES FIRST(down.tstamp) AS start_tstamp,
           LAST(down.tstamp) AS end_tstamp,
           match_number() AS match_num,
           classifier() AS classifier
  ALL ROWS PER MATCH SHOW EMPTY MATCHES
  PATTERN (DOWN*)
  DEFINE 
    DOWN AS (price <= PREV(price))
)
WHERE symbol = 'GLOBEX';
</code>
</p>
<p>
You can see that the result set contains all 20 rows that make up the data for the symbol â€œGLOBEX"". Rows 1- 3, 9, and 13-15 are identified as unmatched rows - the classifier returns null. These rows appear because we have defined the search requirements for pattern DOWN as being zero or more occurrences.<p>
<p>
<h3>So what is an empty match?</h3>
Based on this we can state that an empty match is a row that does not map explicitly to a pattern variable (in this case DOWN). However, it is worth noting that an empty match does in fact have a starting row and it is assigned a sequential match number, based on the ordinal position of its starting row. The above situation is largely the result of the specific quantifier that we are using: * (asterisk).",11-OCT-16 03.16.44.327111000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.59.50.570110000 PM,"SHARON.KENNEDY@ORACLE.COM"
83285133915740633892152853025520276733,83285133915637875197485609545670251773,"Overview",10,"<h3>Finding patterns with SQL</h3>
<p>
Recognizing patterns in a sequence of rows has been a capability that was widely desired, but not really possible with SQL until now. There were many workarounds, but these were difficult to write, hard to understand, and inefficient to execute.</p> 
<p>
The aim of this tutorial is to explain the difference greedy and reluctant quantifiers. Pattern quantifiers that attempt to match as many instances as possible of the token to which they are applied are referred to as greedy, i.e. they will gobble up as many rows as possible. This is the default mode of operation.
</p>",12-OCT-16 09.56.44.650804000 AM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 09.56.44.650863000 AM,"KEITH.LAKER@ORACLE.COM"
83301768980983980076912527312977283765,83301768980930787340849483629290212021,"Overview",10,"<p>In this tutorial we will review the two built-in measures that are part of MATCH_RECOGNIZE:</p>
<ul>
<li>MATCH_NMUBER()</li>
<li>CLASSIFIER()</li>
</ul>
<p>These measures are designed to help you understand how your data set is mapped to the pattern that you have defined.
</p>
<p>These two functions will typically only be used within the MEASURE clause. Interestingly, it is possible to use both these functions within the DEFINE clause but it is doubtful, at least in my mind, as to whether this would ever be useful.
</p>",12-OCT-16 01.03.30.632080000 PM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 02.09.03.406134000 PM,"KEITH.LAKER@ORACLE.COM"
92046253613455359167476750387465844566,92046253613454150241657135758291138390,"Introduction",10,"<p>The <strong>LISTAGG</strong> function was introduced in Oracle RDBMS 11gR2 and based on the number of posts across various forums and blogs, it is widely used by developers.</p>
<p>When using LISTAGG on very large data sets it is possible to create a list that is too long and this causes the following overflow error to be generated:</p>
<br>
<blockquote><strong>ORA-01489</strong>: result of string concatenation is too long.</blockquote>
<br>
<p>The primary objective of this tutorial is to explain the changes that have been made to LISTAGG so that developers, within application and SQL code, can control the process by which overflows (ORA-01489 errors) are managed.</p>",04-JAN-17 05.57.54.419314000 AM,"SYS",04-JAN-17 05.57.54.419379000 AM,"SYS"
92046253613457777019115979645815256918,92046253613454150241657135758291138390,"Creating employee table",20,"<p>First step is to create our employee table which we will use to demonstrate the new LISTAGG syntax.<p>
<code>CREATE TABLE ""EMP"" 
(""EMPNO"" NUMBER(4,0), 
	""ENAME"" VARCHAR2(10 BYTE), 
	""JOB"" VARCHAR2(9 BYTE), 
	""MGR"" NUMBER(4,0), 
	""HIREDATE"" DATE, 
	""SAL"" NUMBER(7,2), 
	""COMM"" NUMBER(7,2), 
	""DEPTNO"" NUMBER(2,0)
);
</code>",04-JAN-17 05.57.54.422841000 AM,"SYS",18-DEC-18 03.17.26.387857000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613458985944935594274989963094,92046253613454150241657135758291138390,"Add employee data ",30,"<p>Next step is to add data to our employee table</p>
<code>Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7369,'SMITH','CLERK',7902,to_date('17-DEC-80','DD-MON-RR'),800,null,20);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7499,'ALLEN','SALESMAN',7698,to_date('20-FEB-81','DD-MON-RR'),1600,300,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7521,'WARD','SALESMAN',7698,to_date('22-FEB-81','DD-MON-RR'),1250,500,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7566,'JONES','MANAGER',7839,to_date('02-APR-81','DD-MON-RR'),2975,null,20);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7654,'MARTIN','SALESMAN',7698,to_date('28-SEP-81','DD-MON-RR'),1250,1400,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7698,'BLAKE','MANAGER',7839,to_date('01-MAY-81','DD-MON-RR'),2850,null,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7782,'CLARK','MANAGER',7839,to_date('09-JUN-81','DD-MON-RR'),2450,null,10);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7788,'SCOTT','ANALYST',7566,to_date('19-APR-87','DD-MON-RR'),3000,null,20);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7839,'KING','PRESIDENT',null,to_date('17-NOV-81','DD-MON-RR'),5000,null,10);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7844,'TURNER','SALESMAN',7698,to_date('08-SEP-81','DD-MON-RR'),1500,0,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7876,'ADAMS','CLERK',7788,to_date('23-MAY-87','DD-MON-RR'),1100,null,20);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7900,'JAMES','CLERK',7698,to_date('03-DEC-81','DD-MON-RR'),950,null,30);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7902,'FORD','ANALYST',7566,to_date('03-DEC-81','DD-MON-RR'),3000,null,20);
Insert into EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values (7934,'MILLER','CLERK',7782,to_date('23-JAN-82','DD-MON-RR'),1300,null,10);
commit;

</code>",04-JAN-17 05.57.54.424245000 AM,"SYS",18-DEC-18 03.17.57.100439000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613460194870755208904164669270,92046253613454150241657135758291138390,"Building a long list of values",40,"<p>To demonstrate the key new features of the 12.2 <strong>LISTAGG</strong> syntax we are going to use the SQL <strong>MODEL</strong> clause, which was introduced in Oracle 10g, to build a new column containing a very long string - essentially we are going to repeatedly concatenate the employee name.</p>
<CODE>SELECT * 
FROM EMP
MODEL 
 PARTITION BY (deptno) DIMENSION BY (empno) 
 MEASURES (ename en, CAST('' AS VARCHAR2(32000)) AS longname)
 RULES ITERATE (1200) (longname[ANY] = longname[cv()]||en[cv()]||'-');</CODE>
<p>You can see that we have created a new column within our resultset called ""longname"" and this is based on the data from the ename column within our employee table. Note that we have defined the datatype of this new column to use the new <strong>32K</strong> varchar2 feature.</p>
<p>To simplify our example let's convert the above example into a view</p>
<code>CREATE OR REPLACE VIEW empln AS
(SELECT * 
FROM EMP
 MODEL 
   PARTITION BY (deptno) DIMENSION BY (empno) 
   MEASURES (ename en, CAST('' AS VARCHAR2(32000)) AS longname)
   RULES ITERATE (1200) (longname[ANY] = longname[cv()]||en[cv()]||'-')
);</code>
<p>Now we are ready to explore the new 12.2 <strong>LISTAGG</strong> syntax...</p>",04-JAN-17 05.57.54.428146000 AM,"SYS",17-MAR-17 09.04.31.531525000 AM,"KEITH.LAKER@ORACLE.COM"
83205148911233890895462131594458438941,83205148911047716319241478701553687837,"Quick Recap",60,"<h3>Summary of EMPTY MATCHES</h3>
<p>
Some patterns permit empty matches such as those using the asterisk quantifier, as shown above. Three mains points to remember when your pattern permits this type of matching:
<ol>
<li>The value of MATCH_NUMBER() is the sequential match number of the empty match.</li>
<li>Any COUNT is 0.</li>
<li>Any other aggregate, row pattern navigation operation, or ordinary row pattern column reference is null.</li>
</ol>
The default is always to return empty matches, therefore, it is always a good idea to determine from the start if your pattern is capable of returning an empty match and how you want to manage those rows: include them (SHOW EMPTY MATCHES) or exclude them (OMIT EMPTY MATCHES).</p>
<p>Be careful if you are using MATCH_NUMBER() within the DEFINE section as part of a formula because empty matches increment the MATCH_NUMBER() counter.</p>",11-OCT-16 03.32.16.146499000 PM,"KEITH.LAKER@ORACLE.COM",11-OCT-16 03.32.16.146555000 PM,"KEITH.LAKER@ORACLE.COM"
83209904976976290338832398263167671007,83205148911047716319241478701553687837,"Unmatched Rows",70,"<h3>Reporting unmatched rows?</h3>
<p>Always useful to view the complete result set - at least when you are running your code against test data sets. Getting all the input rows into your output is relatively easy because you just need to include the phrase ALL ROWS PER MATCH WITH UNMATCHED ROWS. Other than for testing purposes I canâ€™t think of a good use case for using this in production so make sure you check your code before submit your production-ready code to your DBA.
<code>SELECT
  symbol,
  tstamp,
  price,
  start_tstamp,
  end_tstamp,
  match_num,
  classifier
FROM ticker 
MATCH_RECOGNIZE ( 
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(down.tstamp) AS start_tstamp,
           LAST(down.tstamp) AS end_tstamp,
           match_number() AS match_num,
           classifier() AS classifier
  ALL ROWS PER MATCH WITH UNMATCHED ROWS
  PATTERN (DOWN*) 
  DEFINE 
    DOWN AS (price <= PREV(price))
)
WHERE symbol = 'GLOBEX';
</code>
</p>
<h3>What about skipping?</h3>
<p>
Note that if ALL ROWS PER MATCH WITH UNMATCHED ROWS is used with the default skipping behaviour (AFTER MATCH SKIP PAST LAST ROW), then there is exactly one row in the output for every row in the input.</p>
<p>This statement will lead us nicely into yet another tutorial on MATCH_RECOGNIZE which explores the concept of SKIPPING. Taking a quick peak into this other tutorial. . .obviously there are many different types of skipping behaviours that are permitted when using WITH UNMATCHED ROWS. It does, in fact, become possible for a row to be mapped by more than one match and appear in the row pattern output table multiple times. Unmatched rows will appear in the output only once.</p>",11-OCT-16 03.36.48.755851000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.00.41.065413000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321307462647443373517032784704,83285133915637875197485609545670251773,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE ticker PURGE;
</code>",12-OCT-16 09.58.08.806011000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.45.39.163653000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321331641163835666100526908224,83285133915637875197485609545670251773,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);

INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);

INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);

commit;

END;
</code>",12-OCT-16 09.59.05.822494000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.45.57.655533000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321379998196620251267515155264,83285133915637875197485609545670251773,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>
<p>You can view the full data set using the following code:</p>
<code>SELECT * FROM ticker ORDER BY symbol, tstamp;</code>
<p> Compared to previous MATCH_RECOGNIZE tutorials, here there is a small change to the data for ticker symbol ACME. The price on Apr-16 has been changed to <strong>14</strong> so that there are now three consecutive rows (15-Apr, 16-Apr and 17-Apr) with the same value (14) for price.</p>

<code>SELECT * FROM ticker 
WHERE symbol = 'ACME' 
ORDER BY tstamp;
</code>",12-OCT-16 10.02.13.384034000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.46.20.874833000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321382416048259480525864567616,83285133915637875197485609545670251773,"Using Greedy Quantifiers",40,"<h3>Now we have ties in our ticker data - watch out for greedy quantifiers</h3>

<p>If we now search for V-shaped patterns and define the <strong>down</strong> and <strong>up</strong> variables so that there is potential for ties by using <strong><=</strong> and <strong>>=</strong> in their respective definitions then we can explore this idea of greedy quantifiers by using the following code:</p>

<code>SELECT symbol, tstamp, price, mn, pattern, 
first_down, first_price, last_up, last_price
FROM ticker MATCH_RECOGNIZE ( 
  PARTITION BY symbol ORDER BY tstamp 
  MEASURES MATCH_NUMBER() AS mn,
           CLASSIFIER() as pattern,
           FIRST(strt.tstamp) AS first_down,
           FIRST(strt.price) as first_price,
           LAST(up.tstamp) AS last_up,
           LAST(up.price) as last_price
  ALL ROWS PER MATCH 
  PATTERN (strt down+ up+) 
  DEFINE 
        down AS (price <= PREV(price)),
        up AS (price >= PREV(price))
)
WHERE symbol = 'ACME'
ORDER BY symbol, tstamp;
</code>
<p>Note that on April 17 we match the row to the <strong>down</strong> variable because that variable is being <strong>greedy</strong>. It could have been matched to the 'up' variable because the price is actually equal to the previous row but 'down' took precedence.</p>",12-OCT-16 10.06.16.719798000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.46.37.203127000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321383624974079095155039273792,83285133915637875197485609545670251773,"Using Reluctant Quantifiers",50,"<p>What if we change down and make it reluctant by using the ? quantifier:</p>

<code>SELECT symbol, tstamp, price, mn, pattern, 
first_down, first_price, last_up, last_price
FROM ticker MATCH_RECOGNIZE ( 
  PARTITION BY symbol ORDER BY tstamp 
  MEASURES MATCH_NUMBER() AS mn,
           CLASSIFIER() as pattern,
           FIRST(strt.tstamp) AS first_down,
           FIRST(strt.price) as first_price,
           LAST(up.tstamp) AS last_up,
           LAST(up.price) as last_price
  ALL ROWS PER MATCH 
  PATTERN (strt down+? up+) 
  DEFINE 
        down AS (price <= PREV(price)),
        up AS (price >= PREV(price))
)
WHERE symbol = 'ACME'
ORDER BY symbol, tstamp;
</code>
<p>Now you can see that because down is reluctant it matches as few occurrences as possible and where a row could be mapped to either down or up it is up that takes precedence. </p>",12-OCT-16 10.07.36.692079000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.46.50.121597000 PM,"SHARON.KENNEDY@ORACLE.COM"
83289635321468249781452119197268706112,83285133915637875197485609545670251773,"Why is this important?",60,"<p>Obviously this can impact the way that rows are matched to your pattern. Therefore, you need to think carefully about how you are going to manage the situation where rows could be matched to more than one variable - do you have preference for which variable wins?
</p> 
<p>If you have measures that are tied to specific variables then these will be impacted by whether the variable is greedy or reluctant. Obvious examples are where you are performing some sort of calculation such as averages, sums or counts.
</p> 
<p>Therefore, when you are constructing your pattern please think very carefully about how greedy you want your matching process to be as it processes your data set.</p>",12-OCT-16 10.10.58.609150000 AM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 10.10.58.609209000 AM,"KEITH.LAKER@ORACLE.COM"
83304431452970960267213143752397441262,83301768980930787340849483629290212021,"What does MATCH_NUMBER() do?",11,"<p>To get you started here is a quick definition of MATCH_NUMBER()</p>
<p>
<blockquote>
You might have a large number of matches for your pattern inside a given row partition. How do you tell these matches apart? This is done with the MATCH_NUMBER() function. Matches within a row pattern partition are numbered sequentially starting with 1 in the order they are found. This numbering starts at 1 within each row pattern partition, because there is no linked ordering across row pattern partitions.
</blockquote>
</p>",12-OCT-16 01.19.59.224182000 PM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 01.06.00.069338000 PM,"KEITH.LAKER@ORACLE.COM"
83396736889890322960661662482939123216,83392487823947840787102120821717075963,"Missing rows",50,"<p>In this example we have enhanced our pattern by slightly changing our always-true event: this time we are going to test if the current tstamp is greater than the previous timestamp, which of course if will be!
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
  ORDER BY tstamp 
  ALL ROWS PER MATCH 
  PATTERN (e+) 
  DEFINE 
    e AS tstamp > prev(tstamp));
</code>
<p>Something is incorrect with the output because we are only getting 19 rows returned even though we are using the keywords ALL ROWS PER MATCH. Why is this?
</p>
<p>The answer is because like earlier, we have multiple stock ticker entries per tstamp
</p>
<code>SELECT * 
FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 ALL ROWS PER MATCH 
 PATTERN (e+) 
 DEFINE 
   e AS tstamp > prev(tstamp));
</code>
<p>But we still seem to have a incorrect result because there are only 57 rows output and not the 60 rows that we were expecting. Why is this?
</p>
<p>This is because the very first record in each data stream has no previous timestamp to compare with, therefore, the first record is excluded from our output!
</p>
<p>This can easily be corrected by incorporating a dummy event
</p>
<code>SELECT *
FROM ticker 
 MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp 
  ALL ROWS PER MATCH 
  PATTERN (strt e+) 
  DEFINE 
   e AS tstamp > prev(tstamp));
</code>",13-OCT-16 10.37.45.158382000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.03.47.350472000 PM,"SHARON.KENNEDY@ORACLE.COM"
83396736890110347459831524992735647248,83392487823947840787102120821717075963,"Same result is not same result",60,"<p>Now let's our simple always-true pattern again
</p>
<code>SELECT
 esymbol,
 etstamp,
 eprice 
FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES 
    e.symbol as esymbol, 
    e.tstamp as etstamp, 
    e.price as eprice 
 ALL ROWS PER MATCH 
 PATTERN (e) 
 DEFINE 
   e AS price=price)
WHERE symbol='ACME';
</code>
<p>Notice that we get 20 rows back. Now let's run the almost same query again but first let's make a subtle alteration to the pattern definition of the always true event:
</p>
<code>SELECT
 esymbol,
 etstamp,
 eprice 
FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES 
    e.symbol as esymbol, 
    e.tstamp as etstamp, 
    e.price as eprice 
 ALL ROWS PER MATCH 
 PATTERN (e+)
 DEFINE 
  e AS price=price)
WHERE symbol='ACME';
</code>
<p>Is this the same result? We have still returned 20 rows, so it must be the same result, correct?
</p>
<p>The answer to the question lies in the match of individual records to a pattern. Let's run the same statements again, using some built-in functions:
<ul>
<li>classifier()</li>
<li>match_number()</li>
</ul>
<p>Please note that if you want more information about these two built-in measures there is a separate tutorial available. 
</p>
<p>Now, Let's run the last example again:
</p>
<code>SELECT
 esymbol,
 etstamp,
 eprice,
 event,
 match 
FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES 
    e.symbol as esymbol, 
    e.tstamp as etstamp, 
    e.price as eprice, 
    classifier() event, 
    match_number() match  
 ALL ROWS PER MATCH 
 PATTERN (e+) 
 DEFINE 
   e AS price=price)
WHERE symbol='ACME';
</code>
<p>Now let's run the first example again:
</p>
<code>SELECT
 esymbol,
 etstamp,
 eprice,
 event,
 match 
FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES 
    e.symbol as esymbol, 
    e.tstamp as etstamp, 
    e.price as eprice, 
    classifier() event, 
    match_number() match  
 ALL ROWS PER MATCH 
 PATTERN (e) 
 DEFINE 
   e AS price=price)
WHERE symbol='ACME';
</code>
<br>

<p>You can immediately see that the two results are not in fact the same result!
</p>
<p>The e+ pattern searches for one or more instances of our <strong>price = price</strong> pattern which results in a single match across all 20 rows. The first example, e, searches for a single occurrence of <strong>price = price</strong> and it is able to match 20 individual occurrences where price equals price. 
</p>
<p>BUT, the number for each match is not contiguous. Why is that? Why does our match number column not start at 1?
</p>
<p>The answer is because the match number function is evaluated prior to filtering the resultset to using the predicate <strong>WHERE symbol='ACME'</strong>.
</p>
<p>How do we solve this? There are two options: 1) we can reintroduce the PARTITION BY clause - did you spot that it was removed! or 2) we can move the predicate inside the pattern, as shown here:
</p>
<code>SELECT
 esymbol,
 etstamp,
 eprice,
 event,
 match 
FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 MEASURES 
    e.symbol as esymbol, 
    e.tstamp as etstamp, 
    e.price as eprice,
    classifier() event, 
    match_number() match  
 ALL ROWS PER MATCH 
 PATTERN (e) 
 DEFINE 
  e AS price=price and symbol='ACME');
</code>
<p>Now we have a sequential number for our match number column.
</p>",13-OCT-16 10.54.31.986692000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.04.43.462209000 PM,"SHARON.KENNEDY@ORACLE.COM"
85405696532564023328179364227785063072,85402068267675499952938542872037871601,"Looking for shapes and controlling skipping",60,"<p>As I previously stated, you might think the obvious position to start searching for the next occurrence of a pattern is the next record after the last row of the current match. But what if there are overlapping patterns where the middle of an earlier match overlaps with the start of the next match? For example if we are looking for a w-shaped pattern within our ticker data set then it is quite possible to have overlapping w-shapes where the next â€œWâ€ starts within the second down phase of the previous â€Wâ€.
</p>
<p>Fortunately MATCH_RECOGNIZE  provides great flexibility in terms of being able to specify the restart point. If we look at the source data for the ACME symbol within our ticker data set then we can see that there are overlapping W-shapes (assuming we allow for the flat-top in the middle of the 2nd w-shape by using the <= and >= tests for each pattern variable!).
</p>
<img src=""https://lh3.googleusercontent.com/-kyaXE4UDAn4/WBiVl1u6ZUI/AAAAAAAAC54/9XZ5kwAxjqA/double-w.jpg"" alt="""" width=""200px"" />
<p>
Letâ€™s use this example to explore the various AFTER MATCH SKIP TO optionsâ€¦starting with the default behaviour:
</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp
 MEASURES STRT.tstamp AS start_w,
          LAST(z.tstamp) AS end_w
 ONE ROW PER MATCH
 AFTER MATCH SKIP PAST LAST ROW<
 PATTERN (STRT x+ y+ w+ z+)
DEFINE
  x AS x.price <= PREV(x.price),
  y AS y.price >= PREV(y.price),
  w AS w.price <= PREV(w.price),
  z AS z.price >= PREV(z.price) 
) MR
WHERE symbol='ACME'
ORDER BY symbol, MR.start_w;
</code>
<p>returns only one match within the ACME data set.</p>",01-NOV-16 04.09.41.651799000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.56.28.631319000 PM,"SHARON.KENNEDY@ORACLE.COM"
85405696533506985467478774984055880352,85402068267675499952938542872037871601,"Restarting from end of last Y",70,"<p>Now letâ€™s change the above code sample so that after the first pattern has been found we begin searching at the row after the end of the matching process for the Y variable - i.e. row 6, 10-Apr-11.
</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp
 MEASURES STRT.tstamp AS start_w,
          LAST(z.tstamp) AS end_w
 ONE ROW PER MATCH
 AFTER MATCH SKIP TO LAST Y
 PATTERN (STRT x+ y+ w+ z+)
 DEFINE
     x AS x.price <= PREV(x.price),
     y AS y.price >= PREV(y.price),
     w AS w.price <= PREV(w.price),
     z AS z.price >= PREV(z.price) 
) MR
WHERE symbol='ACME'
ORDER BY symbol,start_w;
</code>
<p>which now finds two w-shapes with the second W starting on 10-Apr-11 and ending on 18-Apr-11.</p>
<p>but what is going on under-the-covers?
</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp
 MEASURES STRT.tstamp AS start_w,
          LAST(z.tstamp) AS end_w,
          classifier() AS pv,
          match_number() AS mn,
          count(*) as row_count
 ALL ROWS PER MATCH
 AFTER MATCH SKIP TO LAST Y
 PATTERN (STRT x+ y+ w+ z+)
 DEFINE
      x AS x.price <= PREV(x.price),
      y AS y.price >= PREV(y.price),
      w AS w.price <= PREV(w.price), 
      z AS z.price >= PREV(z.price) 
) MR
WHERE symbol='ACME'
ORDER BY symbol, mn, tstamp;
</code>
<p>now shows us that the records for 10-Apr-11 to 14-Apr-11 were actually processed twice.</p>",01-NOV-16 04.16.48.434202000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.56.54.836587000 PM,"SHARON.KENNEDY@ORACLE.COM"
85406520008930332875263407857120994653,85402068267675499952938542872037871601,"Skip to next row?",80,"<p>What about using the SKIP TO NEXT ROW syntax? How does that affect our results? It is important to remember that this will force MATCH_RECOGNIZE to resume pattern matching at the row after the first row of the current match. Using our ticker data we can see that this would actually increase the number of W-shapes to three!
</p>
<p><img src=""https://lh3.googleusercontent.com/-Dlq7oU3w_1w/WBipxUFXkwI/AAAAAAAAC7I/lCaE3ue0iGk/3-w-matches.jpg?imgmax=1600"" alt=""3 W shapes in our ticker data stream"" width=""250"" border=""0"" />
</p>
<p>In match 2 we have two occurrences of pattern variable x, therefore, once the second W-shape has been matched the search process restarts on row 12, i.e. the first row of the current match, which is row 12 mapped to STRT.</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp
 MEASURES STRT.tstamp AS start_w,
          LAST(z.tstamp) AS end_w
 ONE ROW PER MATCH
 AFTER MATCH SKIP TO NEXT ROW
 PATTERN (STRT x+ y+ w+ z+)
 DEFINE
      x AS x.price <= PREV(x.price),
      y AS y.price >= PREV(y.price),
      w AS w.price <= PREV(w.price),
      z AS z.price >= PREV(z.price) 
 ) MR
WHERE symbol='ACME'
ORDER BY symbol, mr.start_w;
</code>
<p>and if we change our code to return for this statement to show the more detailed report we can see how the pattern is actually being matched:</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp
 MEASURES STRT.tstamp AS start_w,
          LAST(z.tstamp) AS end_w,
          classifier() AS pv,
          match_number() AS mn,
          count(*) as row_count
 ALL ROWS PER MATCH
 AFTER MATCH SKIP TO NEXT ROW
 PATTERN (STRT x+ y+ w+ z+)
 DEFINE
      x AS x.price <= PREV(x.price),
      y AS y.price >= PREV(y.price),
      w AS w.price <= PREV(w.price),
      z AS z.price >= PREV(z.price) 
) MR
WHERE symbol='ACME'
ORDER BY symbol, mn, tstamp;
</code>
<p> Note that match two, the 2nd W-shape, starts on line 11 but we began the search for this second match on row 2, i.e. the next row after the first start variable. Similarly, the search for the third W-shape started on row 12 after the second STRT variable.</p>
<p>Given that our original data set for ACME only contained 20 rows you can see from this example that it is in fact possible to do a lot more pattern discovery when you start to fully exploit the power of the AFTER MATCH SKIP syntax.</p>",01-NOV-16 04.20.37.345855000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.57.10.837551000 PM,"SHARON.KENNEDY@ORACLE.COM"
85406520008950884614196856553090999645,85402068267675499952938542872037871601,"Just accept the default?",90,"<p>The AFTER MATCH SKIP clause determines the point at which we will resume searching for the next match after a non-empty match has been found. The default for the clause is AFTER MATCH SKIP PAST LAST ROW: resume pattern matching at the next row after the last row of the current match. 
</p>
<p>In most examples of using MATCH_RECOGNIZE you will notice that the AFTER MATCH clause is not present and the developer blindly assumes that the AFTER MATCH SKIP PAST LAST ROW clause is applied. This obviously does not help the next developer who has to amend the code to fit new business requirements. 
</p>
<p>Therefore, my recommendation is that you should always clearly state where you want the matching process to start searching for the next match. Never assume the default will behaviour will be good enough!
</p>",01-NOV-16 04.21.19.873738000 PM,"KEITH.LAKER@ORACLE.COM",08-NOV-16 10.23.54.633922000 AM,"KEITH.LAKER@ORACLE.COM"
85406520008971436353130305249061004637,85402068267675499952938542872037871601,"Summary",100,"<p>Hopefully this tutorial has explained the ways in which you can use the AFTER MATCH SKIPâ€¦ clause to ensure that you capture all of the required patterns/shapes within your data set.
</p>
<p>Itâ€™s always a good idea to explicitly include this clause because it is very important - if you donâ€™t want to allow for overlapping matches then clearly state this in your code by using AFTER MATCH SKIP PAST LAST ROW clause.
</p> 
<p>Donâ€™t assume the default will kick-in and that the next developer will have time to read all your detailed documentation when making the next round of changes to the code.
</p>",01-NOV-16 04.22.18.138574000 PM,"KEITH.LAKER@ORACLE.COM",01-NOV-16 04.22.18.138629000 PM,"KEITH.LAKER@ORACLE.COM"
92046253613391286099037175041206417238,92046253613390077173217560412031711062,"Introduction",10,"<p>
Welcome to the Oracle Partitioning Tutorial. You can find tons of basic core examples for Oracle Partitioning in LiveSQL, so we want to focus on somewhat more enhanced topics in this tutorial in individual
modules. You can go through all the modules in order, you can do it in
random order, or you can only look at individual ones.</p>
<p>All you should do is run the first module to clean up any
remnants you might have lingering around. It's easier to do the
cleanup once and be done with it rather than ""littering"" the individual
modules with DROP TABLE or other cleanup commands.</p>
<p>This tutorial is only a first starting point. There is always
more to functionality than we can possibly introduce in an overview
tutorial, so we encourage you to consult the documentation for details.
Especially the ""VLDB and Partitioning Guide"" and the ""SQL Language
Reference Manual"" are your best friends here.</p>
<p>We also tried hard to conceptualize the examples and focus on introducing the technology and some related important details. So please bear
with us if you see meaningless table and column names. Business scenarios
and use cases are manifold and are or will be discussed in blogs.</p>
<p>Oracle hopes you enjoy this short tutorial for Oracle Partitioning. For
suggestions and comments, please contact <a href=""mailto:hermann.baer@oracle.com"">us</a>.
We love to hear from you.</p>",04-JAN-17 05.57.45.063782000 AM,"SYS",23-JAN-24 01.05.19.630061000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613392495024856789670381123414,92046253613390077173217560412031711062,"Cleanup",20,"<p>To not litter the individual tutorials with DROP TABLE commands, let's
remove all the objects that will be created. Simply execute the following
code snippet and be done with it .. . You can safely ignore any 'table
does not exist' error message. If a table does not exist there is nothing
wrong with not being able to drop it ..<br><br>
<code>rem cleanup of all objects
drop table mc purge;
drop table alp purge;
drop table soon2bpart purge;
drop table part4filter purge;
drop table ropt purge;
drop table part4xchange purge;
drop table np4xchange purge;
drop table compart4xchange purge;
drop table p4xchange purge;
</code>
<p>That's all you needed to do. You're ready to go.</p>",04-JAN-17 05.57.45.110072000 AM,"SYS",13-APR-20 05.16.38.760900000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613393703950676404299555829590,92046253613390077173217560412031711062,"Multi-column list Partitioning",30,"<p>Partitioning allows the break down of tables and indexes into smaller
physical pieces while keeping the logical view of a single object.
Partitioning improves the performance, manageability, and availability of
large objects in the database by enabling processing and data management
to happen on the smaller physical pieces called partitions.</p>
<p>List Partitioning allows to specify lists of discrete
values of the partitioning columns that qualify rows to be included in a
partition. You can create a list partitioned table not only on one, but multiple columns, which we will demonstrate in this example.</p>
<p>To create a simple multi-column list partitioned table you can issue the
following command:<br><br>
<code>rem simple multi-column list partitioned table
create table mc (col1 number, col2 number)
partition by list (col1, col2)
(partition p1 values ((1,2),(3,4)),
 partition p2 values ((4,4)),
 partition p3 values (default));</code>
<p>The metadata of this table is as follows. You can also identify
multi-column list partitioning by looking at the number of partition keys
in the table metadata<br><br>
<code>rem table metadata - number of partition keys
select table_name, partitioning_type, partitioning_key_count 
from user_part_tables where table_name='MC';</code>
</p>
<p>Looking at the partition level you will see how the multi-column key
values are represented. You will see that the high value of the
partitioning metadata indicates that we have multiple columns as
partitioning key. Value pairs for the multi-column key are enclosed with
parenthesis and multiple value pairs are separated through a comma.<br><br>
<code>rem metadata of individual partitions
select partition_name, high_value
from user_tab_partitions where table_name='MC';</code>
</p>
Let us now insert some data into our previously created table and see where
the data is actually stored, We intentionally insert some data that is
explicitly aligned with the partition key values and some other data that
isn't. We expect all records that do not match a specific partition to end
up in the DEFAULT partition of the table.<br><br>
<code>rem insert some sample data
insert into mc values (1,2);
insert into mc values (1,3);
insert into mc values (99,99);
commit;</code><br>
<p>Let's now check where the data ended up. We will use first the partition
extended syntax to point specifically to partition p1. The only valid
records we expect to see are records that either have (1,2) or (3,4) as
partition key.<br><br>
<code>rem content of partition p1 using partition extended syntax
select * from mc partition (p1);</code><br>
With multi-column partitioning you can also use the partition extended
syntax with the FOR () clause. Simply point to a fully qualified record,
meaning you have to specify a complete partitioning key criteria, which is
a value pair in our case. Using the FOR () clause will show you the
complete content of the partition that contains the specified partition
keys. In the following example we chose a value pair that is not
explicitly defined for any of the partitions, so it points to the DEFAULT
partition.<br><br>
<code>rem content of DEFAULT partition using the partitioned extended syntax PARTITION FOR ()
select * from mc partition for (1,3);</code></p>
<p>Note that DEFAULT is not a value, so if you were to try to use it as
""value"" with the partitioned extended syntax you will get an error:<br><br>
<code>rem wrong usage of partition extended syntax: DEFAULT is not a valid partition key value
select * from mc partition for (default);</code><br>
After having introduced the basic working of a multi-column partitioned
table, let's do a standard partition maintenance operation. You will see
that it behaves exactly as it does for any other partitioning method; the
only difference is that a fully qualified partition key now obviously
consists 
of value pairs with values for all partition k
ey columns.<br><br>
<code>rem simple partition maintenance operation, demonstrating split
alter table mc split partition p3 into (partition p3 values (1,3),partition p4) online;</code></p>
<p>After the split we expect all records for the newly split partition p3 to
contain records with partition key (1,3), and partition p4 to contain the
rest .. since this partition is now the new DEFAULT partition. Just like
with a single column partitioned table, you can only add a partition to a
list partitioned table with a DEFAULT partition by splitting the DEFAULT
partition. Oracle cannot simply create a new partition in this case since
conceptually all possible partition keys are contained in this
catch-it-all partition.</p>
<p>Let's check the content of our ""new"" partition that we created by
splitting the DEFAULT partition:<br><br>
<code>rem content of partition p3 after split
select * from mc partition (p3);</code></p>
Let's quickly check the metadata of the table again to see what the split
did:<br><br>
<code>rem partition information for our table
select partition_name, high_value
from user_tab_partitions
where table_name='MC'
order by partition_position;</code></p>
<p>The new DEFAULT partition has all the rest of the records:<br><br>
<code>rem content of partition p4 after the split, our new DEFAULT partition
select * from mc partition (p4);</code></p>
<p> You successfully made it to the end of module <span style=""font-weight: bold;"">'multi-column
list partitioning'</span>. For further details please consult the
documentation. For suggestions and comments, please contact <a href=""mailto:hermann.baer@oracle.com"">us</a>.
We love to hear from you.</p>
<p>",04-JAN-17 05.57.45.112055000 AM,"SYS",23-JAN-24 01.10.45.711010000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613394912876496018928730535766,92046253613390077173217560412031711062,"Auto-list Partitioning",40,"<p>Another enhanced technique with list partitioned tables is  auto-list partitioning. Similar to interval
partitioning, auto-list automatically creates a new partition as soon as a
new partitioning key value is seen. As data is loaded into the table, the
database automatically creates a new partition if the loaded partitioning
key value does not correspond to any of the existing partitions. </p>
<p>Let's create a simple auto-list partitioned table:<br><br>
<code>rem create a simple auto-list partitioned table
create table alp (col1 number, col2 number)
partition by list (col1) automatic
(partition p1 values (1,2,3,4,100),
 partition p2 values (5));</code></p>
<p>Just by using the new keyword AUTOMATIC you created an auto-list
partitioned table. Note that you <span style=""font-weight: bold;"">always</span>
have to specify at minimum one partition. Oracle does not allow the
creation of a table with at least one partition. That was easy, wasn't it?</p>
<p>There are some subtle differences between the way an interval partitioned
table works versus an auto-list partitioned table. The first difference
can be seen when looking at the data dictionary information of our newly
created auto-list partitioned table:<br><br>
<code>rem metadata information for an auto-list partitioned table
select table_name, partitioning_type, autolist, partition_count 
from user_part_tables where table_name='ALP';</code></p>
<p>As you can see, there is a new metadata column that classifies a table as
an auto-list partitioned table. You also see that, unlike an interval
partitioned table, an auto-list partitioned table does not show a
partition count of one million. The reason is that an auto-list
partitioned table does not require any metadata to be auto-list
partitioned. It just has to know that whenever a new partition key value
arrives 
it has to create a new partition. Interval partitioned table must
have an INTERVAL specified; with this specification all partitions of an
interval partitioned table are know in advance, and are semantically
created (albeit not persistent on disk or as individual partition
metadata.</p>
<p>Let's now insert some data that has a partition key that is not defined
yet.<br><br>
<code>rem insert some data w/o matching partition
insert into alp values (999,999);
commit;</code></p>
<p>Since we do not have a partition yet that has 999 as partition key value
we expect a new partition to being created:<br><br>
<code>rem metadata, newly created partition
select partition_name, high_value 
from user_tab_partitions where table_name='ALP' order by partition_position;</code></p>
<p>Voila, here we go. We have a new partition with a system generated name.
Let's quickly change the name to something more meaningful using the
partition extended syntax (this is deterministic all the time, so if you
want to have your own names all the time, just do this programmatically.<br><br>
<code>rem change auto-generated name
alter table alp rename partition for (999) to p999;
</code></p>
Another subtle difference between an auto-list partitioned table and an
interval partitioned table is that you can do all partition maintenance
operations without limitation. Since no metadata is needed other than
""create new partition for new partition key value"", you do not have any
limitation when it comes to merging, splitting, or even manually adding
partitions.<br>
<br>
Let's quickly add a partition manually to our auto-list partitioned table:<br><br>
<code>rem standard PMOP that would not be allowed on interval partitioned table
alter table alp add partition pnew values (10,11,12);</code><br>
<p>Note that the partition position of an partition in an auto-list
partitioned table is derived by the order of the creation of a partition,
not the order of the partition key values .. if that was even possible.
The same is true for a normal list partitione
d table as well.<br><br>
<code>rem see the newly created partition
select partition_name, high_value 
from user_tab_partitions where table_name='ALP' order by partition_position;</code></p>
<p>One thing that auto-list partitioned tables have in common with interval
partitioning is that auto-list is extension to list partitioning as
interval is an extension to range partitioning. Consequently you can
evolve a list partitioned table into an auto-list partitioned one.</p>
<p>Let's quickly do this for our earlier created multi-column list
partitioned table MC. In case you have not gone through the module
multi-column list partitioning, here is the DDL for the table again:<br><br>
<code>rem simple multi-column list partitioned table
create table mc (col1 number, col2 number)
partition by list (col1, col2)
(partition p1 values ((1,2),(3,4)),
 partition p2 values ((4,4)),
 partition p3 values ((9,9)),
 partition p4 values (default));</code> </p>
One pre-requirement to evolve a list partitioned table to an auto-list one
is that you must not have a DEFAULT partition. Kind of makes sense when you
think of it: a DEFAULT partition is the catch-it-all partition for a list
partitioned table, meaning that all partition key values that are not
explicitly defined as partition key for other partitions will be stored in
it. Guess how many new partitions you would create with auto-list if Oracle
was to allow a DEFAULT partition with it .. exactly, not a single one.<br>
<br>
So let's remove the DEFAULT partition:<br><br>
<code>rem remove the default partition
alter table mc drop partition p4;</code><br>
<p>To ensure that we are having a normal list partitioned table without a
DEFAULT partition, let's try to insert a record that does not have a
matching partition. It will fail.<br><br>
<code>rem try to insert out--of-bound value, will fail
insert into mc values (1234,5678);</code></p>
<p>Let's now evolve the table to an auto-list partitioned table. Whoever did
the similar operation to evolve a range partitioned table to an interval
one will see the similarity:<br><br>
<code>rem evolve table
alter table mc set automatic;</code></p>
<p>Now we are trying the insert of the same record again:<br><br>
<code>rem try to insert again .. will work
insert into mc values (1234,5678);
commit;</code></p>
<p>As expected, it succeeded, and we now have a newly created partition in
our evolved multi-column list partitioned table.<br><br>
<code>rem metadata after creation of first automated list partition
select partition_name, high_value 
from user_tab_partitions where table_name='MC';</code><br>
</p>
<p> You successfully made it to the end of module <span style=""font-weight: bold;"">'auto-list
partitioning'</span>. For further details please consult the
documentation. For suggestions and comments, please contact <a href=""mailto:hermann.baer@oracle.com"">us</a>. We love to hear from you.</p>",04-JAN-17 05.57.45.287671000 AM,"SYS",23-JAN-24 01.11.49.614397000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613396121802315633557905241942,92046253613390077173217560412031711062,"Conversion of a nonpartitioned table to a partitioned table",50,"A non-partitioned
heap table can be converted to a partitioned table with a MODIFY clause
added to the ALTER TABLE SQL statement. In addition, the keyword ONLINE can
be specified, enabling concurrent DML operations while the conversion is
ongoing.<br>
<br>
Let's quickly demonstrate this with a simple example and introduce the rules
what such a conversion means to indexes. The full set of details can be
found in the documentation.<br>
<br>
The following SQL creates a nonpartitioned table with 500 records:<br><br>
rem sample nonpartitioned table<br>
<code>create table soon2bpart (col1 number primary key, col2 number, col3 number not null, col4 number);
insert /*+ append */ into soon2bpart 
select rownum, mod(rownum,100), mod(rownum,1000), dbms_random.normal from dual connect by level <=10;
commit;</code><br>
<p> Our sample table needs a couple of indexes, so let's create them:<br><br>
<code>rem create a bunch of different indexes on it
rem some indexes, different shape and type
create index i1_prefix_soon2bpart on soon2bpart(col2);
create index i2_nonprefix_soon2bpart on soon2bpart(col4);
create index i3_prefix_but_ovrd_soon2bpart on soon2bpart(col3, col2);
create index i4_global_part_soon2bpart on soon2bpart(col3) global partition by hash(col3) partitions 4;
create bitmap index i5_bix_soon2bpart on soon2bpart (col2,col3);
</code><br>
<br>
Let's see the index metadata for our table as it exists prior to the
conversion:<br><br>
<code>rem indexes in general
select index_name, index_type, uniqueness, partitioned, status 
from user_indexes
where table_name='SOON2BPART'
order by 1;

rem partitioned index
select index_name, partitioning_type, partition_count, locality 
from user_part_indexes
where table_name='SOON2BPART'
order by 1;</code></p>
<p>The conversion is not an in-place conversion: one of the key concepts of
Oracle Partitioning is that data of individual partitions is, well, stored
in individual physical segments. The nonpartitioned table has data stored
""wherever"" in the table. So for the duration of the conversion you will
need the extra space for the new table partition and index segments. After
the successful conversion the space for the old nonpart
itioned table and
its indexes will be freed.<br>
</p>
<p>Let's kick off the conversion of the table. Note that we are doing an
online conversion, so if you were able to spawn a second session that does
DML against our table while the conversion is in place you'd experience
that all your DML will go through without being blocked. We will also rely
on the default index conversion rules that are defined, with the exception
of one index. This helps to demonstrate the default behavior and to give
you a glimpse insight into what you can do for indexes as part of the
online conversion:<br><br>
<code>rem do an online conversion
rem - only one index will not use default conversion
alter table soon2bpart modify
partition by list (col2) automatic
(partition p1 values (1)) online
update indexes (i3_prefix_but_ovrd_soon2bpart global);
</code><br>
</p>
<p>OK, the table is successfully converted. Let' see the table partitioning
metadata:<br><br>
<code>rem partitioning metadata
select table_name, partitioning_type, partition_count 
from user_part_tables where table_name='SOON2BPART';
select partition_name, high_value 
from user_tab_partitions where table_name='SOON2BPART'
order by partition_position asc;</code></p>
<p>What happened to the indexes? Oracle is smart enough to have a couple of
default index conversion rules, that can be overwritten as we demonstrate
with one index. The rules are:</p>
<ul>
<li>Global partitioned indexes are untouched and re
tain their shape.</li>
<li>Non-prefixed indexes will become global nonpartitioned tables.</li>
<li>Prefixed indexes will become local partitioned indexes.</li>
<li>Bitmap indexes will become local partitioned indexes.</li>
</ul>
<p>So let's check the index shape and their status:<br><br>
<code>rem indexes general
select index_name, index_type, uniqueness, partitioned, status 
from user_indexes
where table_name='SOON2BPART'
order by 1;</code><br>
</p>
<p>You see that the conversion rules were applied as discussed, with the
exception of index I1B_SOON2BPART which was defined as becoming a global
nonpartitioned index as part of the conversion.<br><br>
<code>rem partitioned indexes
select index_name, partitioning_type, partition_count, locality 
from user_part_indexes
where table_name='SOON2BPART'
order by 1;</code><br>
</p>
<p>All the index partitions are also in a valid state:<br><br>
<code>rem status of partitioned index
select ip.index_name, ip.status, count(*) cnt
from user_ind_partitions ip, user_indexes i
where i.index_name=ip.index_name and table_name='SOON2BPART'
group by ip.index_name, ip.status
order by 1;</code></p>
<p> You successfully made it to the end of module <span style=""font-weight: bold;"">'conversion
to partitioned table'</span>. For further details please consult the
documentation. For suggestions and comments, please contact <a href=""mailto:hermann.baer@oracle.com"">us</a>.We love to hear from you.</p>",04-JAN-17 05.57.45.306301000 AM,"SYS",23-JAN-24 01.12.32.971768000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613397330728135248187079948118,92046253613390077173217560412031711062,"Read only partitions and subpartitions",60,"<p>You can set not only tables
but partitions and subpartitions to read-only status to protect data from
unintentional DML operations by any user or trigger. Any attempt to update
data in a partition or subpartition that is set to read only results in an
error, while updating data in partitions or subpartitions that are set to
read write succeeds.</p>
<p>We will demonstrate this functionality using a single level range
partitioned table.<br><br>
<code>rem simple interval partitioned table with one read only partition
create table ropt (col1, col2, col3) nocompress
partition by range (col1) interval (10)
(partition p1 values less than (1) read only,
 partition p2 values less than (11))
as select rownum, rownum*10, rpad('a',rownum,'b') 
from dual connect by level <= 100;</code><br>
</p>
<p>As you can see, we did specify read only for partition P1 but nowhere
else, neither on table nor partition level. Let's see what we ended up
with:<br><br>
<code>rem metadata
select table_name, def_read_only from user_part_tables where table_name='ROPT';
rem currently existent partitions<br>
select partition_name, high_value, read_only
from user_tab_partitions
where table_name='ROPT';</code></p>
<p>As probably expected, we only have one partition that is set to read only
in this example. That means that:</p>
<ul>
<li>The table level default is (and will stay) read write.</li>
<li>Only partition p1 is defined as read only where it was explicitly
defined.</li>
</ul>
<p>You can change the read only/read write attribute for existing
partitions. <br><br>
<code>rem change the status of a partition to read only
alter table ropt modify partition for (5) read only;</code></p>
<p>As partition level attribute, read only can obviously be used in
conjunction with other partition maintenance operations. The question now
begs what does it really mean for partition maintenance operations, and
especially when these PMOPs are executed in an online mode?</p>
The answer is simple: Oracle made the conscious design decision that we do
not allow the combination of an online partition maintenance operations and
a scenario where either one (or multiple) of the origin partitions are read
only or where one (or multiple) of the target partitions (after the PMOP)
are set to read only.    <br>
<br>
So any attempt to combine issue an online PMOP with read only partitions you
will get a runtime error, just like in the following split partition
example:<br><br>
<code>rem online PMOP will not work when one of the target partitions is read only
alter table ropt split partition for (5) into 
(partition pa values less than (7), partition pb read only) online;</code><br>
<p>Read only is considered a guaranteed state for the time when a PMOP is
started. It would be also ambiguous when to change the state if a change from read write to read only is taking place or vice versa. So for the statement to work you have to run it in offline mode,
meaning that no data changes are allowed as soon as the PMOP starts:<br><br>
<code>rem offline PMOP
alter table ropt split partition for (5) into 
(partition pa values less than (7), partition pb read only);</code><br>
</p>
<p>You can also set a whole table to read only. This will change the state
for all existing partitions as well as the default of the table.Note that
this is in line with other attributes. <br><br>
<code>rem set everything read only, including the default property
alter table ropt read only;</code><br>
</p>
<p>Let's now have a closer look what it means to have a partition set to
read only and how Oracle describes data immutability in this context. The
fundamental data immutability rule for read only tables and partitions is
that only operations are allowed that must not change the data content of
the partition at the point in time when a partition was set to read only.
Or, in more sql-like words, the result of SELECT &lt;column list at read
only setting time&gt; FROM &lt;table&gt; PARTITION &lt;partition set to
read only&gt;    within the partitioned tables must not change. </p>
<p><b>So what works?</b></p>
<p>Any operation that does not change the content, but only the physical
representation on disk. A classical example is moving a partition to
introduce compression. Let's demonstrate this using a partition of our now
fully read only table:<br><br>
<code>rem partition pb
select partition_name, high_value, read_only, compression
from user_tab_partitions
where table_name='ROPT' and partition_name='PB';</code><br>
</p>
<p>Let's move and compress this partition:<br><br>
<code>rem do the move and compress
alter table ropt move partition pb compress for oltp;</code><br>
</p>
<p>The partition move on the read only partition succeeded without raising
any error. Checking the partition attributes again you now will see that
the partition is compressed.<br><br>
<code>rem partition pb
select partition_name, high_value, read_only, compression
from user_tab_partitions
where table_name='ROPT' and partition_name='PB';</code><br>
</p>
<p>Another operation that works on a table with read only partitions is
adding a column. Such an operation works irrespective of whether the new
column is nullable or not and whether the column has a default value.<br><br>
<code>rem add a column to the table
alter table ropt add (newcol number default 99);</code></p>
<p>This might be surprising to you at first glance, but if you go back to
how we defined data immutability you will see that adding a column does
not violate our rule. Why did Oracle choose this approach? The answer is
simple: if we had decided to use a SELECT * FROM &lt;read only
partition&gt; as data immutability we had ruled out any schema evolution
like adding a column for a partitioned table as soon as one partition was
set to read only. This was considered an unacceptable limitation for
partitioned tables.</p>
<p><b>Now, what operations are not allowed?</b> Anything that is considered
changing the data immutability as defined earlier. Examples are</p>
<p>Any form of DML on a read only partition:<br><br>
<code>rem no DML on read only partitions
update ropt set col2=col2 where col1=88;</code><br>
</p>
<p>Dropping or truncating a read only partition - since this is semantically
equivalent to a DELETE FROM &lt;table&gt; WHERE &lt;partitioning
criteria&gt;:<br><br>
<code>rem no drop or truncate partition
alter table ropt drop partition for (56);</code></p>
<p>Dropping a column (or setting a column to unused):<br><br>
<code>rem drop column is not allowed
alter table ropt drop column col2;</code></p>
<p>Last but not least, and this is no different to existing read only
tables, you can drop a table with one, multiple or all partitions in a
read only state:<br><br>
<code>rem drop everything succeeds
drop table ropt purge;</code></p>
<p>Semantically you are not violating any data immutability when you remove
a complete object. If you want to preserve this case you should address
this with proper privilege management or, under some circumstances, by
disabling the table level lock. The latter one prevents a drop table, but
also all other operations that require an exclusive table level lock.</p>
<p> You successfully made it to the end of module <span style=""font-weight: bold;"">'read
only partitions and subpartitions'</span>. For further details please
consult the documentation. For suggestions and comments, please contact <a
href=""mailto:hermann.baer@oracle.com"">us</a>.We love to hear from you.</p>",04-JAN-17 05.57.45.317014000 AM,"SYS",23-JAN-24 01.13.10.086363000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613398539653954862816254654294,92046253613390077173217560412031711062,"Create a table for Exchange",70,"<p>Partition exchange is a common operation to load or 'unload' data from a
partitioned table by exchanging a standalone table the exactly matches the
shape of a partitioned table. However, over the years, matching the exact
shape of a partitioned table became harder and more complicated. In fact,
under some circumstances it is not even possible to create a table for a
successful exchange without knowing exactly what DDL operations have taken
place on the partitioned table and in what order.</p>
<p>Oracle offers an explicit DDL command to address the creation of table for an exchange:
tables can be created with the FOR EXCHANGE WITH clause to exactly match
the shape of a partitioned table and be eligible for a partition exchange
command. </p>
<p>Let's first create a partitioned table that we are going to exchange
with:<br><br>
<code>rem partitioned table as target for exchange
create table part4xchange (col1, col2, col3) nocompress
partition by range (col1) interval (1000)
(partition p1 values less than (1),
 partition p2 values less than (11))
as select rownum, rownum*10, rpad('a',rownum,'b') 
from dual connect by level <=100;</code></p>
<p>To add a little bit to the fun, let's do some DDL on this table that
changes the shape of it:<br><br>
<code>rem add a column
alter table part4xchange add (colnew1 number default 99);
rem set col unused
alter table part4xchange set unused (col2);</code></p>
<br>
Without knowing the history of what lead to the shape of our partitioned
table we are now simply using the new syntax to create a table for exchange
and let Oracle do the job.<br><br>
<code>rem create table 4 xchange
create table np4xchange for exchange with table part4xchange; </code><br>
<br>
You can specify all table level attributes as you like, e.g. the location,
compression, cache attributes, etc. Note that auxiliary data structures,
such as indexes are not created as part of this command. This newly created
table for exchange is empty and could be populated with data if the exchange
operation is used for fast data loading.<br>
<br>
We now use this newly created table for exchange and exchange it with a
partition of my partitioned table.<br><br>
<code>rem exchange
alter table part4xchange exchange partition for (99) with table np4xchange;</code><br>
<p>We can also use this new functionality to exchange a partition of a
composite partitioned table. <br><br>
<code>rem composite partitioned table as target for exchange
create table compart4xchange (col1, col2, col3) nocompress
partition by range (col1) interval (100)
subpartition by list (col2)
(partition p1 values less than (1)
  (subpartition sp1_1 values (0), subpartition sp1_2 values (1)),
 partition p2 values less than (11)
  (subpartition sp2_d values (DEFAULT)))
as select rownum, mod(rownum,2), rpad('a',rownum,'b') 
from dual connect
by level <=100;</code><br>
</p>
<p>An exchange of a partition of a composite table is slightly different
than the exchange of a partition of a single-level partitioned table. In
our example, a partition of interval-list composite partitioned table
compart4xchange is further subpartitioned by list. We therefore need a
list-partitioned table to exchange any partition of this table with and
the table to exchange has to match exactly the subpartitioning setup of
the partition. </p>
<p>As you see in this example, the subpartitioning layout is different for
partitions p1 and p2, so it makes a difference which partition you are
exchanging with. Currently there is no syntax support for the new CREATE
TABLE FOR EXCHANGE command to copy the metadata of exactly one   
target partition so when we now create a table for exchange we have to
specify the list partitioning strategy of the target partition we want to
exchange with.   </p>
<p>We want to exchange with partition p1, so we add the identical first subpartitioning layout of partition p1 as list partitioning layout for our
table to be exchanged:<br><br>
<code>rem table for exchange with composite partitioned table
create table p4xchange
  partition by list (col2)
  (partition p1 values (0), partition p2 values (1))
for exchange with table compart4xchange;</code><br>
</p>
<p>We now can exchange this table with partition p1 of our composite
partitioned table (if you were to try an exchange with partition p2 you
would get an error because the layout does not match):<br><br>
<code>rem exchange
alter table compart4xchange exchange partition p1 with table p4xchange;</code><br>
</p>
<P> You successfully made it to the end of module <span style=""font-weight: bold;"">'create
a table for exchange'</span>. For further details please consult the
documentation. For suggestions and comments, please contact <a href=""mailto:hermann.baer@oracle.com"">us</a>. We love to hear from you.</p>",04-JAN-17 05.57.45.319724000 AM,"SYS",23-JAN-24 01.14.38.312175000 PM,"HERMANN.BAER@ORACLE.COM"
92046253613400957505594092074604066646,92046253613399748579774477445429360470,"Setup",1,"<p>Please run the following statements to set up the Tutorial. They can also be used to reset the tutorial if you want to start again at any point</p>

<code>
create or replace view EMPLOYEE_KEY_VALUE
as
select EMPLOYEE_ID as ID, 'EmployeeId' as KEY, to_char(EMPLOYEE_ID) as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'FirstName' as KEY, FIRST_NAME as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'LastName' as KEY, LAST_NAME as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'EmailAddress' as KEY, EMAIL as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'TelephoneNumber' as KEY, PHONE_NUMBER as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID , 'HireDate' as KEY, to_char(HIRE_DATE) as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'JobId' as KEY, JOB_ID as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'Salary' as KEY, to_char(SALARY) as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'Commision' as KEY, to_char(COMMISSION_PCT) as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'ManagerId' as KEY, to_char(MANAGER_ID) as VALUE
  from HR.EMPLOYEES
union all
select EMPLOYEE_ID as ID, 'DepartmentId' as KEY, to_char(DEPARTMENT_ID) as VALUE
  from HR.EMPLOYEES
/
declare
  cursor getTable
  is
  select TABLE_NAME
    from ALL_TABLES
   where TABLE_NAME in ( 'J_PURCHASEORDER', 'JSON_DUMP_CONTENTS','CITY_LOT_FEATURES')
     and OWNER = SYS_CONTEXT('USERENV','CURRENT_USER');
begin
  for t in getTable() loop
    execute immediate 'DROP TABLE ""' || t.TABLE_NAME || '"" PURGE';
  end loop;
end;
/
</code>",04-JAN-17 05.57.47.100878000 AM,"SYS",04-JAN-17 05.57.47.100935000 AM,"SYS"
92046253613402166431413706703778772822,92046253613399748579774477445429360470,"Storing JSON Documents in Oracle Database",20,"<h3>Create a simple table to store JSON documents</h3>
<p>In Oracle there is no dedicated JSON data type. JSON documents are stored in the database using standard Oracle data types such as VARCHAR2, CLOB and BLOB. VARCHAR2 can be used where the size of the JSON document will never exceed 4K (32K in database where when <a href=""http://docs.oracle.com/database/121/LNPLS/release_changes.htm#LNPLS105"">extended</A> VARCHAR support has been enabled.) Larger documents are stored using CLOB or BLOB data types.</p>

<p>In order to ensure that the content of the column is valid JSON data, a new constraint IS JSON, is provided that can be applied to a column. This constraint returns TRUE if the content of the column is well-formed, valid JSON and FALSE otherwise.</p>

<p>This first statement in this module creates a table which will be used to contain JSON documents.</p>

<code>
create table J_PURCHASEORDER (
  ID            RAW(16) NOT NULL,
  DATE_LOADED   TIMESTAMP(6) WITH TIME ZONE,
  PO_DOCUMENT CLOB CHECK (PO_DOCUMENT IS JSON)
)
</code>		

<p>This statement creates a very simple table, J_PURCHASEORDER. The table has a column PO_DOCUMENT of type CLOB. The <strong>IS JSON</strong> constraint is applied to the column PO_DOCUMENT, ensuring that the column can store only well formed JSON documents.</p>

<h3>Loading JSON Documents into the database</h3>
<p>JSON documents can come from a number of different sources. Since Oracle stores JSON data using standard SQL data types, all of the popular Oracle APIs can be used to load JSON documents into the database. JSON documents contained in files can be loaded directly into the database using External Tables.</p>

<p>This statement creates a simple external table that can read JSON documents from a dump file generated by a typical No-SQL style database. In this case, the documents are contained in the file purchaseOrders.json. The SQL directory object ORDER_ENTRY points to the folder containing the dump file, and the SQL directory object JSON_LOADER_OUTPUT points to the databaseâ€™s trace folder which will contain any â€˜logâ€™ or â€˜badâ€™ files generated when the table is processed.<p>


<p><strong>The following code is for reference only. External Tables are not supported in the LiveSQL environment.</strong></p>
<pre>
CREATE TABLE DUMP_FILE_CONTENTS(
  PO_DOCUMENT CLOB
)
ORGANIZATION EXTERNAL(
   TYPE ORACLE_LOADER
   DEFAULT DIRECTORY ORDER_ENTRY
   ACCESS PARAMETERS (
      RECORDS DELIMITED BY 0x'0A' 
      BADFILE JSON_LOADER_OUTPUT: 'JSON_DUMPFILE_CONTENTS.bad'
      LOGFILE JSON_LOADER_OUTPUT: 'JSON_DUMPFILE_CONTENTS.log'
      FIELDS(
        JSON_DOCUMENT CHAR(5000)
      ) 
   )
   LOCATION (
     ORDER_ENTRY:'purchaseOrders.json'
   )
)
PARALLEL
REJECT LIMIT UNLIMITED
</pre>

<p>The following code creates a simulation of an external table that can read a dump file that has been stored in a location that is accessible when using LiveSQL</p>
<code>
create or replace view JSON_DUMP_CONTENTS (
PO_DOCUMENT 
)
as
select JSON_DOCUMENT
  from JSON_TABLE(
         xdburitype('/public/tutorials/json/testdata/purchaseOrders.json').getClob(),
         '$[*]'
         columns (
           JSON_DOCUMENT VARCHAR2(32000) FORMAT JSON PATH '$'
         )
       )
</code>

<p>The following statement copies the JSON documents from the dump file into the J_PURCHASEORDER table.</p>

<code>
insert into J_PURCHASEORDER
select SYS_GUID(), SYSTIMESTAMP, PO_DOCUMENT 
  from JSON_DUMP_CONTENTS 
 where PO_DOCUMENT IS JSON
   and rownum < 1001
/
commit
/
</code>

<p>The IS JSON condition is used to ensure that the insert operation takes place only for well formed documents. Make sure that the commit statement is executed after the insert statement has completed. SYS_GUID and SYSTIMESTAMP are used to populate columns ID and DATE_LOADED.</p>",04-JAN-17 05.57.47.101615000 AM,"SYS",02-FEB-23 01.44.44.683294000 PM,"ROGER.FORD@ORACLE.COM"
92046253613403375357233321332953478998,92046253613399748579774477445429360470,"Accessing JSON content using Oracle's Simplified Syntax For JSON",30,"<p>Oracle allows a simple â€˜dottedâ€™ notation to be used to perform a limited set of operations on columns containing JSON. It also introduces a set of SQL operators that allow the full power of the JSON path language to be used with JSON. This is similar to the way in which the database allows XQuery to be used to access the contents of XML documents stored in the database.</p>

<h3>Accessing JSON using simplified syntax</h3>

<p>The dotted notation can be used to perform basic operations on JSON stored in Oracle Database. Using the dotted notation you can access the value of any keys contained in the JSON document. In order to use the dotted notation, a table alias must be assigned to the table in the FROM clause, and any reference to the JSON column must be prefixed with the assigned alias. All data is returned as VARCHAR2(4000). Also the simplified syntax can only be used with columns that have had an IS JSON constraint applied to them.</p>

<p>The following examples demonstrate how to use the simplified syntax to extract values from an JSON document and how to filter a result set based on the content of a JSON document. The first query shows the count of PurchaseOrder documents by cost center</p>

<code>
select j.PO_DOCUMENT.CostCenter, count(*)
  from J_PURCHASEORDER j
 group by j.PO_DOCUMENT.CostCenter 
 order by j.PO_DOCUMENT.CostCenter 
</code>


<p>The second query shows how to fetch the JSON document where the PONumber key has the value 450:</p>

<code>
select j.PO_DOCUMENT
  from J_PURCHASEORDER j
 where j.PO_DOCUMENT.PONumber = 450
</code>

<p>The third query shows how to fetch data from any document where the key PONumber contains the value 450. The statement returns the values of the keys Reference, Requestor and CostCenter as well as the value of the key city which is a property of the object identified by the key Address which in turn is a property of the object identified by the key ShippingInstructions</p>

<code>
select j.PO_DOCUMENT.Reference,
       j.PO_DOCUMENT.Requestor,
       j.PO_DOCUMENT.CostCenter,
       j.PO_DOCUMENT.ShippingInstructions.Address.city
  from J_PURCHASEORDER j
 where j.PO_DOCUMENT.PONumber = 450</code>

<p>The fourth query shows how to fetch the content of the Address object:</p>

<code>
select j.PO_DOCUMENT.ShippingInstructions.Address
  from J_PURCHASEORDER j
 where j.PO_DOCUMENT.""PONumber"" = 450</code>
 
<p>The Address object is returned as JSON text in a VARCHAR2(4000).</p>",04-JAN-17 05.57.47.103297000 AM,"SYS",02-FEB-23 01.52.39.071057000 PM,"ROGER.FORD@ORACLE.COM"
92046253613404584283052935962128185174,92046253613399748579774477445429360470,"Accessing JSON using JSON_VALUE and JSON_QUERY",40,"<h3>Accessing scalar values using JSON_VALUE</h3>
<p>The JSON_VALUE operator uses a JSON path expression to access a single scalar value. It is typically used in the select list to return the value associated with the JSON path expression or in the WHERE clause to filter a result set based on the content of a JSON document.</p>

<p>JSON_VALUE takes two arguments, a JSON column and a JSON path expression. It provides a set of modifiers that allow control over the format in which the data is returned and how to handle any errors encountered while evaluating the JSON path expression.</p>

<p>The following examples demonstrate how to use the JSON_VALUE operator to extract scalar values from a JSON document using JSON path expressions and to filter a result set based on the content of a JSON document. The first query shows the count of PurchaseOrder documents by cost center:</p>

<code>
select JSON_VALUE(PO_DOCUMENT ,'$.CostCenter'), count(*)
  from J_PURCHASEORDER
 group by JSON_VALUE(PO_DOCUMENT ,'$.CostCenter')
</code>

<p>The query uses the JSON_VALUE operator to return the value of the CostCenter key from each document in the J_PURCHASEORDER table. The SQL Count and Group operators are then applied to the values returned, to generate the desired result. This shows how by storing JSON documents in the Oracle Database makes it possible to bring the full power of SQL to bear on JSON content without giving up any of the advantages of the JSON paradigm.</p>
 
<p>The second query shows how to access a value from within an array. This example uses index 0 to access the value of the UPCCode from the Part object inside the first member of the LineItems array.  This query also shows how to use JSON_VALUE as a filter in the WHERE clause of the SQL statement, so that the JSON_VALUE operator in the select list is executed only for those rows where the JSON_VALUE based condition in the WHERE clause evaluates to TRUE.</p>

<code>
select JSON_VALUE(PO_DOCUMENT ,'$.LineItems[0].Part.UPCCode')
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>JSON_VALUE also provides control over the value returned. You can specify size when returning a VARCHAR2 value, and size and precision when returning a NUMBER value. The third query shows how to specify the SQL data type that is returned by JSON_VALUE. In this case, the value is returned as a NUMBER(5,3) value.</p>

<code>
select JSON_VALUE(PO_DOCUMENT, '$.LineItems[0].Part.UnitPrice' returning NUMBER(5,3))
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>The JSON_VALUE function also provides options for handling errors that might be encountered when applying the JSON PATH expression to a JSON document. Options available include:</p>

<ul>
  <li><strong>NULL ON ERROR:</strong> The default. If an error is encountered while applying a JSON path expression to a JSON document the result is assumed to be SQL NULL and no error is raised.</li>
  <li><strong>ERROR ON ERROR:</strong> An error is raised in the event that an error is encountered while applying a JSON path expression to a JSON document.</li>
  <li><strong>DEFAULT on ERROR:</strong> The developer specifies a literal value that is returned in the event that an error is encountered while applying a JSON path expression to a JSON document.</li>
</ul>

<p>The most common source of an error with JSON_VALUE is that the JSON path expression resolves to something other than a scalar value. JSON, by its nature is highly flexible, which raises the possibility of having a small number of documents that do not conform to a particular pattern. The NULL ON ERROR behavior ensures that a single error caused by one outlier does not abort an entire operation.</p>

 
<p>The fourth query shows the default behavior for JSON_VALUE in the event of an error during JSON path evaluation.</p>
<code>
select JSON_VALUE(PO_DOCUMENT ,'$.ShippingInstruction.Address')
  from J_PURCHASEORDER
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>In this example, there is an error in the first key in the JSON path expression. The first key should be ShippingInstructions, not ShippingInstruction. Since the JSON path expression does match a scalar value, the NULL ON ERROR behavior kicks in and the JSON_VALUE operator returns NULL.</p>

<p>
The fifth query demonstrates the DEFAULT on ERROR behavior</p>

<code>
select JSON_VALUE(PO_DOCUMENT, '$.ShippingInstruction.Address' DEFAULT 'N/A' ON ERROR)
  from J_PURCHASEORDER
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the JSON path expression does not evaluate to a scalar value, when the JSON path expression is evaluated the DEFAULT on ERROR behavior kicks in, and the JSON_VALUE operator returns â€œN/Aâ€.</p>

<p>The sixth query demonstrates the ERROR ON ERROR behavior.</p>

<code>
select JSON_VALUE(PO_DOCUMENT, '$.ShippingInstruction.Address' ERROR ON ERROR)
  from J_PURCHASEORDER
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the JSON path expression does not evaluate to a scalar value, when the JSON path expression is evaluated the ERROR ON ERROR behavior kicks in and results in error â€œ<span style=""color:red;"">ORA-40462: JSON_VALUE evaluated to no value</span>â€ being raised.</p>

 
<p>The seventh query is another example of the ERROR ON ERROR behavior.</p> 

<code>
select JSON_VALUE(PO_DOCUMENT, '$.ShippingInstructions.Address' ERROR ON ERROR)
  from J_PURCHASEORDER
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>


<p>In this case, the JSON path expression does evaluate to a key in the document; however the value associated with the key is an object, not a scalar. Since JSON_VALUE can only return a scalar value, when the JSON path expression is evaluated the ERROR ON ERROR behavior kicks in and results in error â€œ<span style=""color:red;"">ORA-40456: JSON_VALUE evaluated to non-scalar value</span>â€ being raised.</p>

<p>The eighth query shows that the error handling clauses only apply to runtime errors that arise when the JSON path expression is applied to a set of JSON documents:</p>

<code>
select JSON_VALUE(PO_DOCUMENT, '$.ShippingInstructions,Address' NULL ON ERROR)
  from J_PURCHASEORDER
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>


<p>In this example, the error is that a comma, rather than a period, has been used as the separator between the first and second keys in the JSON path expression. Despite the fact that NULL ON ERROR semantics were requested, when the statement is executed error â€œ<span style=""color:red;"">ORA-40597: JSON path expression syntax error ('$.ShippingInstructions,Address') JZN-00209: Unexpected characters after end of path at position 23</span>â€ is raised due to the fact that an invalid JSON path expression is a compile time error rather than a runtime error.</p>

<h3>Accessing objects and arrays using JSON_QUERY</h3>

<p>JSON_QUERY is the complement of JSON_VALUE. Whereas JSON_VALUE can return only a scalar value, JSON_QUERY can return only an object or an array. Like JSON_VALUE, JSON_QUERY takes two arguments, a JSON column and a JSON path expression. It provides a set of modifiers that allow control over the format in which the data is returned and how to handle any errors encountered while evaluating the JSON path expression. The following examples show the use of JSON_QUERY.</p>

<p>This module demonstrates how to use the JSON_QUERY operator to extract objects and arrays from a JSON document using JSON path expressions.</p>

<p>The first query shows how to return the contents of the key ShippingInstructions:</p>

<code>
select JSON_QUERY(PO_DOCUMENT ,'$.ShippingInstructions')
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>


<p>Since the contents of ShippingInstructions is an object, the value is returned as a JSON object delimited by curly braces â€˜{â€˜ and â€˜}â€™.</p>

<p>The second query shows how to return the contents of the key LineItems:</p>

<code>
select JSON_QUERY(PO_DOCUMENT ,'$.LineItems')
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the contents of LineItems is an array, the value is returned as a JSON array delimited by square brackets â€˜[â€˜ and â€˜]â€™.</p>

<p>By default, for maximum efficiency, objects and arrays are printed with the minimal amount of whitespace. This minimizes the number of bytes needed to represent the value, and is fine when the object will be consumed by a computer. However, sometimes it is necessary for the output of a JSON_QUERY operation to be human readable. JSON_QUERY provides the keyword PRETTY for this purpose.</p> 

 
<p>The third query shows the use of the PRETTY keyword to format the output of the JSON_QUERY operator.</p>

<code>
select JSON_QUERY(PO_DOCUMENT ,'$.LineItems' PRETTY)
  from J_PURCHASEORDER p 
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>The array is output as neatly indented JSON, making it much easier for a human to understand. However this comes at the cost of increasing the number of bytes needed to represent the content of the array.</p>

<p>The fourth query shows how to use an array offset to access one item from an array of objects:</p>

<code>
select JSON_QUERY(PO_DOCUMENT ,'$.LineItems[0]' PRETTY)
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the selected item is a JSON object, it is delimited by curly braces â€˜{â€˜ and â€˜}â€™.</p>

<p>The fifth query shows how to access the value of a key from one specific member of an array of objects.</p>
	
<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[0].Part')
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the contents of Part is an object, the value is returned as a JSON object delimited by curly braces â€˜{â€˜ and â€˜}â€™.
 
<p>The JSON_QUERY function also provides options for handling errors that might be encountered when applying the JSON path expression to a JSON document. Options available include:
	
<ul>
  <li><strong>NULL ON ERROR:</strong> The default. If an error is encountered while applying a JSON path expression to a JSON document, the result is assumed to be NULL and no error condition is raised.</li>
  <li><strong>ERROR ON ERROR</strong>: An error is raised in the event that an error is encountered while applying a JSON path expression to a JSON document.</li>
  <li><strong>EMPTY ON ERROR</strong>: An empty array, â€œ[]â€, is returned in the event that an error is encountered while applying a JSON path expression to a JSON document.</li>
</ul>

<p>The most common source of an error with JSON_QUERY is that the JSON path expression resolves to something other than an object or array. The NULL ON ERROR behavior ensures that a single error caused by one outlier does not abort an entire operation.</p>

<p>The sixth query shows the default behavior for JSON_QUERY in the event of an error during JSON path evaluation.</p> 

<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[0].Part.UPCCode')
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>In this example the error is that the JSON path expression maps to a scalar value rather than an object or an array. Since JSON_QUERY can only return an object or an array, the NULL ON ERROR handling kicks in and results in the query returning NULL.</p>

<p>The seventh query shows the ERROR ON ERROR behavior for JSON_QUERY in the event of an error during JSON path evaluation.</p> 

<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[0].Part.UPCCode' ERROR ON ERROR)
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the result of the JSON path expression is a scalar value, the ERROR ON ERROR handling kicks in and results in an â€œ<span style=""color:red;"">ORA-40480: result cannot be returned without array wrapper</span>â€ error being raised.
</p>

<p>The eighth query shows the use of the EMPTY ON ERROR behavior</p>

<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[0].Part.UPCCode' EMPTY ON ERROR)
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the result of the JSON path expression is a scalar value, the EMPTY ON ERROR handling kicks in and the result of the JSON_QUERY operation is an empty array, â€œ[]â€.</p>


<p>The ninth query shows how to use the â€œWITH CONDITIONAL ARRAY WRAPPERâ€ option to avoid a JSON_QUERY error when a JSON path expression evaluates to a scalar value. When this option is specified, a JSON path expression that evaluates to a scalar value is returned as an array. The array consists of one element containing a scalar value.</p>

<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[0].Part.UPCCode' WITH CONDITIONAL ARRAY WRAPPER)
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>Since the result of the JSON path expression is a scalar value, JSON_QUERY automatically converts the result into an array with that one item and returns the array.</p>

 
<p>The tenth query shows how to use WITH ARRAY WRAPPER to force JSON_QUERY to always return the result as an array. In this example, the index is an asterisk, indicating that all members of the LineItems array should be processed, and the last key in the JSON path expression is also an asterisk, indicating that all children of the Part key should be processed.</p>

<code>
select JSON_QUERY(PO_DOCUMENT, '$.LineItems[*].Part.*' WITH ARRAY WRAPPER)
  from J_PURCHASEORDER p
 where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) = 450
</code>

<p>The result of executing this query is an array with 6 items. The first 3 items come from the Description, UnitPrice and UPCCode associated with the Part key of the first member of the LineItems array. The second 3 come from the Description, UnitPrice and UPCCode associated with the Part key of the second member of the LineItems array. Also note that since Description and UPCCode are strings, and UnitPrice is a numeric, the resulting array is heterogeneous in nature, containing a mixture of string and numeric values.</p>",04-JAN-17 05.57.47.105152000 AM,"SYS",04-JAN-17 05.57.47.105210000 AM,"SYS"
92046253613405793208872550591302891350,92046253613399748579774477445429360470,"Relational access to JSON content using JSON_TABLE",50,"<p>The JSON_TABLE operator is used in the FROM clause of a SQL statement. It enables the creation of an inline relational view of JSON content. The JSON_TABLE operator uses a set of JSON path expressions to map content from a JSON document into columns in the view. Once the contents of the JSON document have been exposed as columns, all of the power of SQL can be brought to bear on the content of JSON document.</p>

<p>The input to the JSON_TABLE operator is a JSON object or a JSON array. The JSON_TABLE operator is driven by a row pattern which consists of a JSON path expression. The row pattern determines how many rows the JSON_TABLE operator will generate. It will generate 1 row from each key that matches the supplied row pattern. If the row pattern matches a single key then the JSON_TABLE operator will generate exactly one row. If the row pattern references one or more arrays then it will generate a row from each item in the deepest array.</p>

<p>The rows output by a JSON_TABLE operator are laterally joined to the row that generated them. There is no need to supply a WHERE clause to join the output of the JSON_TABLE operator with the table containing the JSON document.</p>

<p>Columns are defined by one or more column patterns that appear after the COLUMNS keyword. The inline view contains 1 column for each entry in the columns clause. Each column pattern consists of a column name, a SQL data type and a JSON path expression. The column name determines the SQL name of the column, the SQL data type determines the data type of the column and the JSON path expression maps a value from the document to the column. The JSON path expressions in the columns clause are relative to the row pattern.</p>

<p>This module shows how to use JSON_TABLE to generate inline relational views of JSON documents.</p>

<p>The first query shows how to project a set of columns from values that occur at most once in the document. The values can come from any level of nesting, as long as they do not come from keys that are part of, or descended from an array, unless an index is used to identify one item in the array.<p>

<code>
select M.*
  from J_PURCHASEORDER p,
       JSON_TABLE(
       p.PO_DOCUMENT ,
       '$' 
       columns 
         PO_NUMBER  NUMBER(10)        path '$.PONumber',
         REFERENCE  VARCHAR2(30 CHAR) path '$.Reference',
         REQUESTOR  VARCHAR2(32 CHAR) path '$.Requestor',
         USERID     VARCHAR2(10 CHAR) path '$.User',
         COSTCENTER VARCHAR2(16 CHAR) path '$.CostCenter',
         TELEPHONE  VARCHAR2(16 CHAR) path '$.ShippingInstructions.Phone[0].number'
       ) M
 where PO_NUMBER between 450 and 455
</code>

<p>This example generates a view with 5 columns, REFERENCE, REQUESTOR, USERID, COSTCENTER and TELEPHONE. The use of â€˜$â€™ as the JSON path expression for the ROW pattern means the entire JSON document. This means that all column patterns are descended directly from the top level object.<p> 

<p>The second query shows how to work with arrays. In order to expose the contents of an array as a set of rows, the array must be processed using the NESTED PATH syntax. When a JSON_TABLE operator contains a NESTED PATH clause it will output one row for each member of the array referenced by the deepest NESTED PATH clause. The row will contain all of the columns defined by each level of the JSON_TABLE expression.<p>

<code>
select D.*
  from J_PURCHASEORDER p,
       JSON_TABLE(
         p.PO_DOCUMENT ,
         '$' 
         columns(
           PO_NUMBER    NUMBER(10)            path  '$.PONumber',
           REFERENCE    VARCHAR2(30 CHAR)     path  '$.Reference',
           REQUESTOR    VARCHAR2(32 CHAR)     path  '$.Requestor',
           USERID       VARCHAR2(10 CHAR)     path  '$.User',
           COSTCENTER   VARCHAR2(16)          path  '$.CostCenter',
           NESTED PATH '$.LineItems[*]'
           columns(
             ITEMNO       NUMBER(16)           path '$.ItemNumber', 
             DESCRIPTION  VARCHAR2(32 CHAR)    path '$.Part.Description', 
             UPCCODE      VARCHAR2(14 CHAR)    path '$.Part.UPCCode', 
             QUANTITY     NUMBER(5,4)          path '$.Quantity', 
             UNITPRICE    NUMBER(5,2)          path '$.Part.UnitPrice'
           )
         )
       ) D
 where PO_NUMBER between 450 and 455
</code>

<p>The NESTED PATH option simplifies the processing of nested collections. The NESTED PATH clause removes the need to define columns whose sole purpose is to pass a collection from one operator to the next, making it possible to use a â€œ*â€ for the select list since the collections are no longer emitted by the JSON_TABLE operator.<p> 
 
<h3>Creating Relational views of JSON content</h3>
<p>One common use of JSON_TABLE is to create relational views of JSON content which can then be operated on using standard SQL syntax. This has the advantage of allowing programmers and tools that have no concept of JSON data and JSON path expressions to work with JSON documents that have been stored in the Oracle Database.<p>

<p>In Oracle 12.1.0.2.0 it highly recommended to avoid joins between these views. A fully expanded detail view will provide much better performance than defining a m
aster view and a detail view and then attempting to write a query that returns a result set consisting of columns selected from both the master view and the detail view.<p> 

<p>This module shows how to use JSON_TABLE to create relational views of JSON content that can be queried using standard SQL.<p>

<p>The first statement creates a relational view called PURCHASEORDER_MASTER_VIEW which exposes values that occur at most once in each document.<p>

<code>
create or replace view PURCHASEORDER_MASTER_VIEW
as
select m.* 
 from J_PURCHASEORDER p,
      JSON_TABLE(
        p.PO_DOCUMENT ,
        '$'
        columns (
          PO_NUMBER        NUMBER(10)          path '$.PONumber',
          REFERENCE        VARCHAR2(30 CHAR)   path '$.Reference',
          REQUESTOR        VARCHAR2(128 CHAR)  path '$.Requestor',
          USERID           VARCHAR2(10 CHAR)   path '$.User',
          COSTCENTER       VARCHAR2(16)        path '$.CostCenter',
          SHIP_TO_NAME     VARCHAR2(20 CHAR)   path '$.ShippingInstructions.name',
          SHIP_TO_STREET   VARCHAR2(32 CHAR)   path '$.ShippingInstructions.Address.street',
          SHIP_TO_CITY     VARCHAR2(32 CHAR)   path '$.ShippingInstructions.Address.city',
          SHIP_TO_COUNTY   VARCHAR2(32 CHAR)   path '$.ShippingInstructions.Address.county',
          SHIP_TO_POSTCODE VARCHAR2(32 CHAR)   path '$.ShippingInstructions.Address.postcode',
          SHIP_TO_STATE    VARCHAR2(2 CHAR)    path '$.ShippingInstructions.Address.state',
          SHIP_TO_PROVINCE VARCHAR2(2 CHAR)    path '$.ShippingInstructions.Address.province',
          SHIP_TO_ZIP      VARCHAR2(8 CHAR)    path '$.ShippingInstructions.Address.zipCode',
          SHIP_TO_COUNTRY  VARCHAR2(32 CHAR)   path '$.ShippingInstructions.Address.country',
          SHIP_TO_PHONE    VARCHAR2(24 CHAR)   path '$.ShippingInstructions.Phones[0].number',
          INSTRUCTIONS     VARCHAR2(2048 CHAR) path '$.SpecialInstructions'
        )
      ) m
</code>

<p>As can be seen from the output of a describe operation, this view looks like any other relational view. The JSON operators and JSON path expressions are hidden away in the DDL statement that created the view.
 
<p>The second statement creates a relational view called PURCHASEORDER_DETAIL_VIEW that exposes the contents of the LineItems array as a set of rows.<p>

<code>
create or replace view PURCHASEORDER_DETAIL_VIEW
as
select D.*
  from J_PURCHASEORDER p,
       JSON_TABLE(
         p.PO_DOCUMENT ,
         '$'
         columns (
           PO_NUMBER        NUMBER(10)           path  '$.PONumber',
           REFERENCE        VARCHAR2(30 CHAR)    path '$.Reference',
           REQUESTOR        VARCHAR2(128 CHAR)   path '$.Requestor',
           USERID           VARCHAR2(10 CHAR)    path '$.User',
           COSTCENTER       VARCHAR2(16)         path '$.CostCenter',
           SHIP_TO_NAME     VARCHAR2(20 CHAR)    path '$.ShippingInstructions.name',
           SHIP_TO_STREET   VARCHAR2(32 CHAR)    path '$.ShippingInstructions.Address.street',
           SHIP_TO_CITY     VARCHAR2(32 CHAR)    path '$.ShippingInstructions.Address.city',
           SHIP_TO_COUNTY   VARCHAR2(32 CHAR)    path '$.ShippingInstructions.Address.county',
           SHIP_TO_POSTCODE VARCHAR2(10 CHAR)    path '$.ShippingInstructions.Address.postcode',
           SHIP_TO_STATE    VARCHAR2(2 CHAR)     path '$.ShippingInstructions.Address.state',
           SHIP_TO_PROVINCE VARCHAR2(2 CHAR)     path '$.ShippingInstructions.Address.province',
           SHIP_TO_ZIP      VARCHAR2(8 CHAR)     path '$.ShippingInstructions.Address.zipCode',
           SHIP_TO_COUNTRY  VARCHAR2(32 CHAR)    path '$.ShippingInstructions.Address.country',
           SHIP_TO_PHONE    VARCHAR2(24 CHAR)    path '$.ShippingInstructions.Phones[0].number',
           INSTRUCTIONS     VARCHAR2(2048 CHAR)  path '$.SpecialInstructions',
           NESTED PATH '$.LineItems[*]'
           columns (
             ITEMNO         NUMBER(38)           path '$.ItemNumber', 
             DESCRIPTION    VARCHAR2(256 CHAR)   path '$.Part.Description', 
             UPCCODE        VARCHAR2(14 CHAR)    path '$.Part.UPCCode', 
             QUANTITY       NUMBER(12,4)         path '$.Quantity', 
             UNITPRICE      NUMBER(14,2)         path '$.Part.UnitPrice'
           )
         )
       ) D        
/
</code>

<p>The following statements show how, once the relational views have been created, the full power of SQL can now be applied to JSON content, without requiring any knowledge of the structure of the JSON or how to manipulate JSON using SQL.<p> 

<h4>Relation queries with simple predicates</h4>

<code>
select SHIP_TO_STREET, SHIP_TO_CITY, SHIP_TO_STATE, SHIP_TO_ZIP
  from PURCHASEORDER_MASTER_VIEW
 where PO_NUMBER = 450
</code>
<p></p>
<code>
select PO_NUMBER, REFERENCE, SHIP_TO_PHONE, DESCRIPTION, QUANTITY, UNITPRICE
  from PURCHASEORDER_DETAIL_VIEW
 where UPCCODE = '27616854773'
</code>
<p></p>
<code>
select PO_NUMBER, REFERENCE, SHIP_TO_PHONE, QUANTITY, DESCRIPTION, UNITPRICE
  from PURCHASEORDER_DETAIL_VIEW
 where UPCCODE in ('27616854773', '56775053895', '75993852820')
 order by PO_NUMBER
</code>

<h4>Relational group by queries</h4>

<code>
select COSTCENTER, count(*)
  From PURCHASEORDER_MASTER_VIEW
 group by COSTCENTER
 order by COSTCENTER
</code>
<p></p>
<code>
select COSTCENTER, sum (QUANTITY * UNITPRICE) TOTAL_VALUE
  from PURCHASEORDER_DETAIL_VIEW
 group by COSTCENTER
</code>

<h4>Relational queries with multiple predicates</h4>

<code>
select PO_NUMBER, REFERENCE, INSTRUCTIONS, ITEMNO, UPCCODE, DESCRIPTION, QUANTITY, UNITPRICE
  from PURCHASEORDER_DETAIL_VIEW d
 where REQUESTOR = 'Steven King'
   and QUANTITY  > 7
   and UNITPRICE > 25.00
</code>
<p></p>
<h4>SQL Analytics</h4>
<code>
select UPCCODE, count(*) ""Orders"", Quantity ""Copies""
  from PURCHASEORDER_DETAIL_VIEW
 where UPCCODE in ('27616854773', '56775053895', '75993852820')
 group by rollup(UPCCODE, QUANTITY)
</code>
<p></p>
<code>
select UPCCODE, PO_NUMBER, REFERENCE, QUANTITY, QUANTITY - LAG(QUANTITY,1,QUANTITY) over (ORDER BY PO_NUMBER) as DIFFERENCE
  from PURCHASEORDER_DETAIL_VIEW
 where UPCCODE = '27616854773'
 order by PO_NUMBER DESC
</code>
<p></p>

<p>Relational views effectively provide schema-on-query semantics; making it possible to define multiple views of the JSON content. Application developers are still free to evolve the content of the JSON as required. New variants of the JSON can be stored without affecting applications that rely on the existing views.</p> 

<p>As can be seen from the queries used, all the power of the SQL language can be used to access the JSON data. These views allow developers and, more importantly, tools that understand only the relational paradigm to work with JSON content.</p>
 ",04-JAN-17 05.57.47.107452000 AM,"SYS",04-JAN-17 05.57.47.107510000 AM,"SYS"
92046253613407002134692165220477597526,92046253613399748579774477445429360470,"Filtering result sets using JSON_EXISTS",60,"<p>The JSON_EXISTS operator is used in the WHERE clause of a SQL statement. It is used to test whether or not a JSON document contains content that matches the provided JSON path expression.</p>

<p>The JSON_EXISTS operator takes two arguments, a JSON column and a JSON path expression. It returns TRUE if the document contains a key that matches the JSON path expression, FALSE otherwise. JSON_EXISTS provides a set of modifiers that provide control over how to handle any errors encountered while evaluating the JSON path expression.</p> 

<p>The following examples show the use of JSON_EXISTS.The first query counts the number of JSON documents that contain a ShippingInstructions key with an Address key that contains a state key:</p>

<code>
select count(*)
  from J_PURCHASEORDER
 where JSON_EXISTS(PO_DOCUMENT ,'$.ShippingInstructions.Address.state')
</code>

<p>JSON_EXISTS makes it possible to differentiate between a document where a key is not present and a document where the key is present but the value is null. The next three queries show the difference between using JSON_EXISTS to test for the presence of a key and using JSON_VALUE to check if a key is null or empty.</p> 

<p>The first query simply shows the set of possible values for the county key in the Address object.</p>

<code>
select JSON_VALUE(PO_DOCUMENT ,'$.ShippingInstructions.Address.county'), 
       count(*)
  from J_PURCHASEORDER
 group by JSON_VALUE(PO_DOCUMENT ,'$.ShippingInstructions.Address.county') 
</code>

<p>The results show that there are two possible values for the county key; it is either NULL or  contains the value â€œOxon.â€. The problem with this is that, for the NULL values, we cannot differentiate between cases where JSON_VALUE returned NULL because the key did not exist in the document and the cases where it returned NULL because the key was present but the value contains a null or empty value.</p>

<p>With JSON_EXISTS it is possible to differentiate between the case where the key is not present and the case where the key has a null or empty value. The following query shows how this is done:</p>


<code>
select JSON_VALUE(PO_DOCUMENT ,'$.ShippingInstructions.Address.county'),
       count(*)
  from J_PURCHASEORDER
where JSON_EXISTS(PO_DOCUMENT ,'$.ShippingInstructions.Address.county')
group by JSON_VALUE(PO_DOCUMENT ,'$.ShippingInstructions.Address.county') 
</code>

<p>The results show that there are 13 documents where the key is present but empty.</p>

<h3>Predicate support in JSON Path Expressions</h3>

<p>Starting with Oracle Database 12c Release 2, The JSON path expression used with JSON_EXISTS supported predicates. This significantly extends the power of JSON_EXISTS by enabling JSON_PATH expressions to include conditions on the value of the key, as well as the presence of a key.</p>

<p>Predicates are specified by adding a â€˜?â€™ following the parent key and then specifying conditions in parenthesis. Within the predicate the @ sign is used to reference the parent key. A variable is indicated by a name prefixed with a $ sign. The value of the variable is set using the PASSING clause of the JSON_EXISTS operator. This allows the value to supplied using a bind variable when developing applications.</p>

<p>The following statement shows a very simple example of using a predicate in a JSON path Expression/ In this example a predicate is placed on the value of the PONumber key. PONumber is a member of the top level JSON object.</p>

<code>
select j.PO_DOCUMENT
  from J_PURCHASEORDER j
 where JSON_EXISTS(
         PO_DOCUMENT,
         '$?(@.PONumber == $PO_NUMBER)' 
         passing 450 as ""PO_NUMBER""
       )
</code>

<p>The predicate is this case is â€œ@.PONumber == $PO_.NUMBERâ€.  Since the PONumber key is part of the top-level object the JSON path expression in this example starts with a â€˜$â€™. The presence of the predicate is indicated by adding â€œ ?()â€ to the JSON path expression. The predicate is then placed inside the parenthesis.</p> 

<p>The value of the variable PO_NUMBER is supplied using the PASSING clause of the JSON_EXISTS operator.</p>

<p>In the next example a predicate is placed on the value of the UPCCode key. UPCCode occurs inside Part which in turn is one of the keys found in the objects that make up the LineItems array.</p>

<code>
select j.PO_DOCUMENT.PONumber PO_NUMBER
  from J_PURCHASEORDER j
 where JSON_EXISTS(
         PO_DOCUMENT,
         '$?(@.LineItems.Part.UPCCode == $UPC)' 
         passing 27616854773 as ""UPC""
  
     )
</code>

<p>The predicate in this case is â€œ@.LineItems.Part.UPCode == $UPCâ€. Since LineItems contains an array, the condition will be met if any member of the array contains a Part object with a UPCCode key whose value matches the specified condition.</p>

<p>The next statement shows how to provide multiple predicates and how to perform a numeric range query. The first condition is on the value of the User key. User is member of the top-level object. The second condition is on the value of the Quantity key. Quantity occurs in each of the objects that make up the array LineItems.  The two conditions are linked using the â€˜&&â€™ operator. The â€˜>â€™ conditional in the second condition indicates that a range search is required.</p>

<code>
select count(*)
  from J_PURCHASEORDER j
 where JSON_EXISTS(
         PO_DOCUMENT,
         '$?(@.User == $USER && @.LineItems.Quantity > $QUANTITY)' 
         passing 'AKHOO' as ""USER"", 8 as ""QUANTITY""
       )
</code>

<p>In this example the first predicate contains two conditions.</p>

<p>The first condition is on the value of the User key. The second condition is an existence check on the contents of the LineItems array. The two conditions are linked using the â€˜&&â€™ operator.</p> 

<p>The exists operator is used when working with arrays. The exists operation returns true if a single member of the array contains an object that satisfies all of the conditions specified by the supplied path expression. It returns false otherwise.</p>

Without the exists operator the document could be selected even though no single member of the array satisfies all the conditions specified in the path expression. As long as each of the individual conditions specified in the path expression is satisfied by at least one member of the array the document would be selected.</p>

<p>The above example uses a nested predicate to check if any member of the array contains a Part object where the value of UPCCode  key matches the value specified using the variable UPC and a Quantity key whose value is greater than the value specified using the variable QUANTITY. If any member of the array satisfies the supplied predicates the exists operator returns true and the document is included in the results set.</p>

<code>
select j.PO_DOCUMENT.PONumber PO_NUMBER
  from J_PURCHASEORDER j
 where JSON_EXISTS(
         PO_DOCUMENT,
         '$?(
              @.User == $USER 
              && 
              exists(
               @.LineItems?(@.Part.UPCCode == $UPC && @.Quantity > $QUANTITY)
              )
         )' 
         passing 'AKHOO' as ""USER"", 
                 17153100211 as ""UPC"", 
                 8 as ""QUANTITY""
       )
/
</code>",04-JAN-17 05.57.47.112959000 AM,"SYS",02-FEB-23 01.53.21.002091000 PM,"ROGER.FORD@ORACLE.COM"
92046253613408211060511779849652303702,92046253613399748579774477445429360470,"Indexing JSON documents stored in Oracle Database",70,"<p>Oracle Database supports multiple ways of indexing JSON documents. There are two approaches to indexing JSON content. The first is to create functional indexes based on the JSON_VALUE operator. The second is to leverage Oracleâ€™s full-text indexing capabilities to create an inverted list index on the JSON documents.</p>

<p>Indexing scalar values using JSON_VALUE</p>

<P>JSON_VALUE can be used to create an index on any key in a JSON document as long as the following two conditions are met.</p>

<ol>
  <li>The target of the JSON path expression must be a scalar value.</li>
  <li>The target of the JSON path expression must occur at most once in each document.</li>
</ol>

<p>Indexes created using JSON_VALUE can be conventional B-TREE indexes or BITMAP indexes.</p>

<p>The first statement shows how to create a unique B-Tree index on the contents of the PoNumber key.</p>

<code>
create unique index PO_NUMBER_IDX
    on J_PURCHASEORDER (
          JSON_VALUE(
             PO_DOCUMENT,'$.PONumber' returning NUMBER(10) ERROR ON ERROR NULL ON EMPTY
          )
       )
</code>

As can be seen when a query includes a predicate on the expression that was used to create the index the index is used to satisfy the predicate 

<code>
select PO_DOCUMENT
  from J_PURCHASEORDER j
 where JSON_VALUE(
             PO_DOCUMENT,'$.PONumber' returning NUMBER(10) ERROR ON ERROR
          ) = 123

/
explain plan for
select PO_DOCUMENT
  from J_PURCHASEORDER j
 where JSON_VALUE(
             PO_DOCUMENT,'$.PONumber' returning NUMBER(10) ERROR ON ERROR
          ) = 123
/
select * from TABLE(DBMS_XPLAN.DISPLAY)
/
</code>

<p>The second statement shows how to create a BITMAP index on the contents of the CostCenter key</p>
	
<code>
create bitmap index COSTCENTER_IDX 
    on J_PURCHASEORDER (JSON_VALUE(PO_DOCUMENT ,'$.CostCenter'))
</code>
 
<p>The third statement shows how to create a B-Tree index on the contents of the zipCode key</p>

<code>
create index ZIPCODE_IDX
     on J_PURCHASEORDER(
          JSON_VALUE(
             PO_DOCUMENT,
             '$.ShippingInstructions.Address.zipCode'
             returning NUMBER(5)
          )
        )
</code>

<p>Note the use of the returning clause in the JSON_VALUE operator to create a numeric index.</p>",04-JAN-17 05.57.47.114670000 AM,"SYS",02-FEB-23 01.50.13.837594000 PM,"ROGER.FORD@ORACLE.COM"
92046253613409419986331394478827009878,92046253613399748579774477445429360470,"JSON Search Index",80,"<p>In addition to creating index on specific nodes, Oracle Database also supports creating a JSON Search Index. A JSON Search Index allows a single index to optimize JSON Path operations on any key. A JSON search index required no advanced knowledge of structure of the JSON documents that are being indexed, or the JSON path expressions that will be used when searching.</p>

<p>In Oracle Database 12c release 2 the DDL statements required to create a JSON search index were significantly simplified. The following statement will create the JSON Search Index.</p>

<code>
create search Index JSON_SEARCH_INDEX
    on J_PURCHASEORDER (PO_DOCUMENT) for json
</code>

<p>The new â€œCREATE SEARCH INDEXâ€ syntax makes creating a JSON Search Index much more like creating any other kind of index. It is no longer necessary to create preferences, and provide information about section groups and lexars when creating a search index. The parameters clause may still be used to override the default settings of certain configurable options, such as when to sync the index.</p>

<p>As well as being much easier to create,  in Oracle Database 12c Release 2 the JSON search index had a number of significant enhancements

<ul>
<li>It add support for range-based searching on numeric values.</li>
<li>It is transactional (SYNC on COMMIT) by default.</li>
<li>It support RANGE, LIST, INTERVAL and HASH partitioned tables</li>
</ul>

<p>These enhancements make it a much more general solution for indexing JSON Documents</p>",04-JAN-17 05.57.47.118480000 AM,"SYS",02-FEB-23 01.56.45.004594000 PM,"ROGER.FORD@ORACLE.COM"
92046253613410628912151009108001716054,92046253613399748579774477445429360470,"JSON DataGuide",90,"<p>The JSON Dataguide allows you to discover information about the structure and content of JSON documents stored in Oracle Database. This information can be used in a number of ways including</p>

<ol>
<li>Generating a JSON Schema document that describes the set of JSON documents
<li>Creating views that enable relational SQL operations on the JSON documents
<li>Automatically adding virtual columns based on simple key:value pairs
</ol>

<p>In order to generate a JSON dataguide we need to perform an analysis of the JSON content that the database is managing. The following statements show how to create a dataguide on a set of JSON documents that have been stored in an Oracle Database, and how to use the dataguide to discover metadata about the documents stored in the database. It also shows how to use the dataguide to create relational views that expose the content of the JSON documents in a manner that enables SQL-based reporting and analytics.</p>

<p>There are two ways to create a dataguide. The first one is to use json_dataguide aggregate operator.</p>

<code>select json_dataguide(po_document) from j_purchaseorder
</code>

<p>Just like any other aggregate operator, json_dataguide scans the column with JSON documents and builds dataguide on the fly. Users can add a where clause or a sampling clause to the statement.</p>

<p>Where clause:</p>

<code>select json_dataguide(po_document) from j_purchaseorder where JSON_VALUE(PO_DOCUMENT ,'$.PONumber' returning NUMBER(10)) <= 450
</code>

<p>Sampling clause to reduce the cost of the operation on a large table:</p>

<code>select json_dataguide(po_document) from j_purchaseorder sample (20)
</code>

<p><i>json_dataguide</i> returns a flat formatted dataguide by default. It is a relational view of the data showing all of the possible JSON path expressions that can be used to access the content stored in the JSON documents. Users can change the returning dataguide format to be <a href=""http://json-schema.org/documentation.html"" target=""_blank"">JSON Schema</a> by passing argument dbms_json.format_hierarchical to the operator.</p>

<code>select json_dataguide(po_document, dbms_json.format_hierarchical) from j_purchaseorder
</code>

<p>Users can also pass dbms_json.pretty argument to pretty print the dataguide with indentation.</p>

<code>select json_dataguide(po_document, dbms_json.format_hierarchical, dbms_json.pretty) from j_purchaseorder
</code>

<p>Once the JSON dataguide has been created, we can use it to understand the structure of the JSON documents that have been stored in the column that has been indexed. A new PL/SQL package, DBMS_JSON, has been introduced to assist with this.</p>

<p>The next statement shows how to use the create_view procedure to create a relational view based on the information in the dataguide. The procedure takes 4 arguments: The name of the view, the name of the underlying table, the name of the column and the hierarchical formatted dataguide from json_dataguide operator.</p>

<code>declare
  dg clob;
begin
  select json_dataguide(po_document, dbms_json.format_hierarchical) into dg from j_purchaseorder;
  dbms_json.create_view('J_PURCHASEORDER_VIEW1', 'J_PURCHASEORDER', 'PO_DOCUMENT', dg, resolveNameConflicts=>TRUE);
end;
/
</code>
 
<p>Starting from 19c, create_view procedure can automatically resolve view name conflicts if users specify ""resolveNameConflicts=>TRUE"" in the procedur; it can also take ""path"" argument to only expand the given nested collection path. Please refer to create_view_on_path below for more detailed explanation.</p>

<p>The other way to create a dataguide is to use JSON search index infrastructure. The syntax used to create a database guide is similar to the syntax used to create a search index.</p>

<code>drop index JSON_SEARCH_INDEX
/
create search index JSON_SEARCH_INDEX on J_PURCHASEORDER (PO_DOCUMENT) for json parameters('search_on none dataguide on');
</code>

<p>This example creates a basic JSON dataguide that will track of all the keys used by all the documents stored in the PO_DOCUMENT column of table J_PURCHASEORDER.</p>
Since the dataguide is based on the JSON search index infrastructure we have the option of using a single index to drive both the dataguide and search operations. However if we only require the dataguide functionality we can avoid the overhead associated with maintaining the search index by using the parameters clause of the create search index statement to specify the options â€œsearch_on noneâ€ and â€œdataguide onâ€.

<p>Comparing to json_dataguide, this approach automatically maintains dataguide when JSON documents with new fields are inserted. On the other hand, it increases DML overhead.</p>

<p>The method get_index_dataguide() can return the dataguide stored in JSON search index in two formats, just like json_datataguide operator. The second provides a relational view of the data showing all of the possible JSON path expressions that can be used to access the content stored in the JSON documents. The second uses JSON Schema (http://json-schema.org/documentation.html) to represent the information. Both representations also provide information about the data type and size of the value associated with the JSON path expression.</p>

<p>The next statement shows how to use the get_index_dataguide method to generate a JSON Schema document that describes the documents analyzed by the dataguide.</p>

<code>select DBMS_JSON.GET_INDEX_DATAGUIDE('J_PURCHASEORDER', 'PO_DOCUMENT', DBMS_JSON.FORMAT_HIERARCHICAL, DBMS_JSON.PRETTY ) ""HIERARCHICAL DATA GUIDE"" from dual;
</code>

<p>The next statement shows how to request the flattened representation of the dataguide.</p>

<code>select DBMS_JSON.GET_INDEX_DATAGUIDE('J_PURCHASEORDER', 'PO_DOCUMENT', DBMS_JSON.FORMAT_FLAT, DBMS_JSON.PRETTY ) ""FLAT DATA GUIDE"" from dual;
</code>

<p>The next statement shows how to use the create_view_on_path procedure to create a relational view based on the information in the dataguide. The procedure takes 4 arguments: The name of the view, the name of the underlying table, the name of the column and a JSON path expression that identifies which JSON path expressions are to be included in the generated view.</p>

<code>begin 
  DBMS_JSON.CREATE_VIEW_ON_PATH( 'J_PURCHASEORDER_VIEW', 'J_PURCHASEORDER', 'PO_DOCUMENT', '$' ); 
end;
</code>

<p>In this example the JSON path expression supplied to the procedure is â€˜$â€™. This means that the view will contain one column for each JSON path expression that can occur in the set of documents that have been analyzed. The view will expand all possible nested collections and contain one row for each member of the deepest nested arrays. If the document contains more than one hierarchy, right outer join logic is used to ensure that each of the hierarchies present is represented by rows in the view.</p>

<p>The names of the columns in the view are generated from the JSON path expressions that provide the content of the column. Name mangling will occur when the path expression cannot be converted directly into a valid SQL column name. This can lead to some less than intuitive column names. In order to avoid this, package DBMS_JSON provides a procedure called rename that can be used to associate a user-friendly name with a JSON path expression.</p>

<p>The next statement shows how to use the rename procedure to provide user-friendly names for some of the columns derived from the PurchaseOrder documents. The procedure takes five arguments, the name of the table and column that contains the JSON documents, the JSON path expression, the data type that should be used for the value identified by the JSON path expression, and the name that should be used for any column based on the JSON path.</p>

<code>begin 
  DBMS_JSON.RENAME_COLUMN( 'J_PURCHASEORDER','PO_DOCUMENT', '$.PONumber',DBMS_JSON.TYPE_NUMBER,'PO_NUMBER'); 
  DBMS_JSON.RENAME_COLUMN( 'J_PURCHASEORDER','PO_DOCUMENT', '$.Reference',DBMS_JSON.TYPE_STRING,'REFERENCE'); 
  DBMS_JSON.RENAME_COLUMN( 'J_PURCHASEORDER','PO_DOCUMENT', '$.LineItems.Part.UPCCode',DBMS_JSON.TYPE_NUMBER, 'UPC_CODE'); 
end;
</code>

<p>Once user-friendly names have been associated with the JSON path expressions, creating or recreating the view using procedure create_view_on_path will result in the creation of views which use the user-friendly column names.</p>

<code>begin 
  DBMS_JSON.CREATE_VIEW_ON_PATH( 'J_PURCHASEORDER_VIEW', 'J_PURCHASEORDER', 'PO_DOCUMENT', '$' );
end;
</code>

<p>The next statement shows that views created using this technique function just like any other relational view. This allows the full power of the Oracle SQL language to be used to query JSON content and ensure that tools and programmers who only understand relational techniques can work with content stored as JSON in an Oracle database.</p>

<code>select REFERENCE from J_PURCHASEORDER_VIEW where PO_NUMBER between 500 and 504
</code>

<p>JSON Dataguide and virtual columns</p>
Virtual columns can also be used to expose JSON content in a relational manner. The Virtual Columns are appended to the table that includes the column that contains the JSON content. Each Virtual Column can expose the value identified by a JSON path expression, as long as the JSON path expression identifies a key that can occur at most once in each document.

<p>This section of the Hands-on-Lab shows how to create a JSON dataguide that automatically appends Virtual Columns to a table containing JSON documents as new JSON path expressions are detected by the dataguide.</p>

<p>The following statements show how to use a dataguide that will expose JSON content using Virtual Columns.</p>

<p>The second statement shows how to modify the parameters clause of the create search index statement to create a JSON search index that will automatically add Virtual columns to the base table. Note the use of the â€œon change add_vcâ€ clause rather than the â€œdataguide onâ€ clause.</p>

<code>DROP INDEX JSON_SEARCH_INDEX
/
create search index JSON_SEARCH_INDEX on J_PURCHASEORDER (PO_DOCUMENT) for json parameters('sync (on commit) SEARCH_ON NONE DATAGUIDE ON CHANGE ADD_VC');
</code>

<p>The third statement shows that after the dataguide has been created the base table has been modified. A Virtual Column has been appended to the base table for each JSON path expression that references a leaf level key that occurs no more than once per document.</p>

<code>describe J_PURCHASEORDER
</code>

<p>This technique cannot be used with keys that are occur within an array, so in this example no columns are generated for JSON path expressions that include the keys Phone and LineItems, since the content of these items are arrays.</p>

<p>The fourth statement shows that you can use SQL to query the Virtual Columns added by the dataguide just like any other column in the table.</p>

<code>select ""PO_DOCUMENT$PONumber"", ""PO_DOCUMENT$User"" from J_PURCHASEORDER where ""PO_DOCUMENT$PONumber"" between 500 and 504
</code>

<p>The fifth statement shows what happens when you insert a JSON document that contains keys that were not present in any of the documents stored in the table when the dataguide was created.</p>

<code>insert into J_PURCHASEORDER (ID, DATE_LOADED, PO_DOCUMENT) values (SYS_GUID(),SYSTIMESTAMP,'{""NewKey1"":1,""NewKey2"":""AAAA""}');
commit;
</code>

<p>As can be seen the insert succeeded, as would be expected since the document contained valid JSON. However, if we describe the table after the insert has been committed, and after the index is synchronized (which happens automatically since the â€œsync (on commit)â€ option was specified as one of the parameters passed to the create search index statement), we see that the dataguide has automatically appended Virtual Columns corresponding to the new keys to the base table.</p>

<code>describe J_PURCHASEORDER
</code>

<p>The last statement shows that the new columns can be queried just like any other columns.</p>

<code>select ""PO_DOCUMENT$NewKey1"" KEY, ""PO_DOCUMENT$NewKey2"" VALUE from J_PURCHASEORDER where ""PO_DOCUMENT$NewKey1"" = 1
</code>",04-JAN-17 05.57.47.120285000 AM,"SYS",31-MAR-22 04.33.17.397619000 PM,"ROGER.FORD@ORACLE.COM"
92046253613411837837970623737176422230,92046253613399748579774477445429360470,"JSON Generation",99,"<p>Oracle Database includes support for new SQL/JSON operators that allow JSON documents to be generated directly from a SQL statement without having to resort to error-prone string concatenation techniques. These operators are</p>

<ul>
  <li>JSON_ARRAY</li>

  <li>JSON_OBJECT</li>
  <li>JSON_ARRAY</li>
  <li>JSON_ARRAYAGG</li>
  <li>JSON_OJBECTAGG</li>
</ul>

<p>The first statement shows how to use JSON_ARRAY. JSON_ARRAY returns each row of data generated by the SQL query as a JSON Array.  The input to JSON_ARRAY is a list of columns. Each column will be mapped to one member of the array.</p>

<code>
select JSON_ARRAY(
         DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID
       ) DEPARTMENT
  from HR.DEPARTMENTS
 where DEPARTMENT_ID < 110;
</code>
 
<p>The second statement shows how to use JSON_OBJECT. JSON_OBJECT returns each row of data generated by the SQL Query as a JSON object. The generated object contains a key:value pair for each column referenced in the JSON_OBJECT operator. The name of the key can be supplied using a SQL string or a column.</p>

<code>
SELECT JSON_OBJECT( 
          'departmentId' IS d.DEPARTMENT_ID,
          'name'         IS d.DEPARTMENT_NAME,
          'manager'      IS d.MANAGER_ID,
          'location'     IS d.LOCATION_ID
       ) DEPARTMENT
  from HR.DEPARTMENTS d
 where DEPARTMENT_ID < 110;
</code>

<p>The third statement shows how a column name can be used generate the name of the key from the value of the column</p>

<code>
select JSON_OBJECT(OBJECT_TYPE is OBJECT_NAME, 'Status' is STATUS) 
  from USER_OBJECTS
 where ROWNUM < 10;
</code>

<p>The fourth statement shows how to nest JSON_OBJECT operators to generate nested JSON structures. In this example a nested JSON_OBJECT operator is used to generate the object associated with the â€œmanagerâ€ key. The object contains information about the departmentâ€™s manager.</p>

<code>
SELECT JSON_OBJECT( 
          'departmentId' IS d.DEPARTMENT_ID,
          'name'         IS d.DEPARTMENT_NAME,
          'manager'      is JSON_OBJECT(
                              'employeeId'   is EMPLOYEE_ID,
                              'firstName'    is FIRST_NAME,
                              'lastName'     is LAST_NAME,
                              'emailAddress' is EMAIL
                            ),
          'location' is d.LOCATION_ID
       ) DEPT_WITH_MGR
  from HR.DEPARTMENTS d, HR.EMPLOYEES e
 where d.MANAGER_ID is not null
   and d.MANAGER_ID = e.EMPLOYEE_ID
   and d.DEPARTMENT_ID = 10;
</code>

<p>The fifth statement shows how to use the operator JSON_ARRAYAGG to generate JSON arrays from the results of a sub query that returns multiple rows. In this example we are showing the employees for each department as a JSON array inside the department document.</p>

<code>
select JSON_OBJECT(
         'departmentId' is d.DEPARTMENT_ID,
         'name' is d. DEPARTMENT_NAME,
         'employees' is(
            select JSON_ARRAYAGG(
 		         JSON_OBJECT(
                       'employeeId'   is EMPLOYEE_ID,
                       'firstName'    is FIRST_NAME,
                       'lastName'     is LAST_NAME,
                       'emailAddress' is EMAIL
                     )
                   )
              from HR.EMPLOYEES e
             where e.DEPARTMENT_ID = d.DEPARTMENT_ID 
           )
       ) DEPT_WITH_EMPLOYEES
  from HR.DEPARTMENTS d
 where DEPARTMENT_NAME = 'Executive';
</code>

<p>The sixth statement shows the contents of view EMPLOYEE_KEY_VALUE. This view stores information about employees using a Key:Value pair storage model.</p>

<code>
select ID, KEY, VALUE
  from EMPLOYEE_KEY_VALUE
 where ID < 105
 order by ID, KEY;
</code>

<p>The seventh statement shows the use of JSON_OBJECTAGG. JSON_OBJECTAGG makes it easy to generate a JSON object from data that has been stored using Key, Value pair storage.</p>

<code>
select JSON_OBJECTAGG(KEY,VALUE)
  from EMPLOYEE_KEY_VALUE
 where ID < 105
 group by ID;
</code>",04-JAN-17 05.57.47.122855000 AM,"SYS",02-FEB-23 01.46.30.139514000 PM,"ROGER.FORD@ORACLE.COM"
92046253613413046763790238366351128406,92046253613399748579774477445429360470,"PL/SQL Integration: Introducing the PL/SQL API for JSON",100,"<p>Oracle Database provides new PL/SQL APIs for manipulating JSON documents stored in an Oracle Database. These APIs make it possible to add, update and remove key:value pairs. The APIs allow you to navigate the structure of a JSON document, in a manner that is very similar to the way in which the XML Document Object Model (DOM) enables navigation of an XML document.</p> 

<p>The PL/SQL JSON API consists of three PL/SQL objects, JSON_ELEMENT_T, JSON_OBJECT_T and JSON_ARRAY_T. JSON_OJBECT_T and JSON_ARRAY_T extend JSON_ELEMENT_T, so they inherit all of JSON_ELEMENT_Tâ€™s methods.</p> 

<p>Any JSON document can be represented by combining instances of these objects. All PL/SQL operators that return JSON content return an instance of JSON_ELEMENT_T. The JSON_ELEMENT_T instance can be cast to either JSON_OBJECT_T or JSON_ARRAY_T before further operations are performed on the data.</p>

<p>Let's start of by taking a look at the new PL/SQL objects that are used when processing JSON documents.</p>

<code>
--
-- desc JSON_ELEMENT_T
--
select xdburitype('/public/tutorials/json/describe/JSON_ELEMENT_T.log').getClob() ""DESCRIBE JSON_ELEMENT_T"" from dual
/
</code>

<p>JSON_ELEMENT_T provides a number of important methods. The most important are PARSE and STRINGIFY. PARSE is used to convert textual JSON into an instance of JSON_ELEMENT_T.  STRINGIFY is used to generate textual JSON from a PL/SQL JSON_ELEMENT_T object.</p>

<p>JSON_ELEMENT_T also provides a set of methods for extracting the value part of a key:value pair as a PL/SQL data type, and methods for determining what the actual concrete type of any given instance of JSON_ELEMENT_T is.</p> 

<p>Now let's take a look at the PL/SQL object JSON_OBJECT_T</p>

<code>
--
-- desc JSON_OBJECT_T
--
select xdburitype('/public/tutorials/json/describe/JSON_OBJECT_T.log').getClob() ""DESCRIBE JSON_OBJECT_T"" from dual
/
</code>

<p>In addition to the methods inherited from JSON_ELEMENT_T, JSON_OBJECT_T provides methods for manipulating the keys associated with a JSON object. This includes getters and setters for all of the data types supported by the API and methods for listing, adding, removing and renaming keys.</p>

<p>Next we'll look at the PL/SQL object JSON_ARRAY_T.</p>

<code>
--
-- desc JSON_ARRAY_T
--
select xdburitype('/public/tutorials/json/describe/JSON_ARRAY_T.log').getClob() ""DESCRIBE JSON_ARRAY_T"" from dual
/
</code>

<p>In addition to the methods inherited from JSON_ELEMENT_T, JSON_ ARRAY _T provides methods for working with the contents of a JSON array. This includes methods for counting the number of items in an array, accessing an item of the array, adding an item to the array and removing an item from the array.</p>
 
<h3>Using the PL/SQL API for JSON</h3>

<p>The next section of the Hands-on-Lab shows how we can use the PL/SQL API for JSON to access and update JSON content stored in Oracle Database without having to resort to complex and error-prone string manipulation techniques. In this example we are going to update the value of the UPCCode key for any object where the current value of the UPCCode key  is '17153100211' and add a DateModified key to the affec
ted objects.</p>

<p>This code sample shows how to use the new PL/SQL API for JSON to access and update JSON content stored in an Oracle Database.</p>

<p>The first statement uses JSON_TABLE to show the current values of the keys PONumber, ItemNumber, and DateModified from documents where the LineItems array contains a object with a UPCCode key whose value is 17153100211. Since the UPCCode predicate is specified as a SQL condition on the output of the JSON table operator only rows generated from objects which satisfy the UPCCode condition are included in the results set.</p>

<code>
select LI.*
  from J_PURCHASEORDER j,
       JSON_TABLE(
         J.PO_DOCUMENT,
         '$'
         columns(
           PO_NUMBER NUMBER(5) PATH '$.PONumber',
           NESTED PATH '$.LineItems[*]'
           columns( 
             ITEM_NUMBER NUMBER(4)              PATH '$.ItemNumber',
             UPC_CODE    VARCHAR2(14)           PATH '$.Part.UPCCode',    
             UPDATED     VARCHAR2(38)           PATH '$.Part.DateModified'
           )
         )
       ) LI 
 where UPC_CODE = '17153100211'
</code>

<p>The next statement shows a PL/SQL block that performs the following operations</p>

<ol>
  <li>Open a cursor that locates the documents to be updated and loop over the selected documents. The cursor uses JSON_EXISTS to locate documents that contain a LineItems element with the required UPC.</li>
  <li>Use the PARSE method on JSON_ELEMENT_T to parse the JSON document. This returns an instance of JSON_ELEMENT_T.</li>
  <li>Use the TREAT operator to cast the result as an instance of JSON_OBJECT_T.</li>
  <li> Use the GET method on JSON_OBJECT_T to get the content of the LineItems key. This returns an instance of JSON_ELEMENT_T.</li>
  <li>Check that the object returned by the GET method is a JSON array.</li>
  <li>Cast the result as an instance of JSON_ARRAY_T.</li>
  <li>Loop over the items in the array. Use the COUNT property to determine the number of items in the array.</li>
  <li>Use the GET method on JSON_ARRAY_T to access each item in the array. Note that the first item in the array has an index of 0.</li>
  <li>Check that the object returned by the GET method is a JSON object</li>
  <li>Use the TREAT operator to cast the result as an instance of JSON_OBJECT_T</li>
  <li>Use the GET method on JSON_OBJECT_T to get the content of the Part key. This returns an instance of JSON_ELEMENT_T. Use the TREAT operator to cast this value as an instance of JSON_OBJECT_T.</li>
  <li>Use the GETSTRING method on JSON_OBJECT to get the value of the UPCCode key.</li>
  <li>Check if the value of the UPCCode key matches the value we are searching for</li>
  <li>Use the PUT method on JSON_OBJECT_T to update the value of the UPCCode key</li>
  <li>Use the PUT method on JSON_OBJECT_T to create or update the DateModified key</li>
  <li>After all the elements in the array have been processed use the STRINGIFY method on the top level object to serialize the in-memory representation of the document as textual JSON.</li>
  <li>Update the on-disk representation of the document</li>
</ol>
<code>
declare
  cursor getDocuments 
  is
  select PO_DOCUMENT
    from J_PURCHASEORDER  j
   where JSON_EXISTS(
           PO_DOCUMENT,
           '$?(@.LineItems.Part.UPCCode == $UPC)' 
           passing 17153100211 as ""UPC""
         )
     for UPDATE;
   
   V_RESULT            JSON_ELEMENT_T;    
   V_DOCUMENT_OBJECT   JSON_OBJECT_T;
   V_LINEITEMS_ARRAY   JSON_ARRAY_T;
   V_LINEITEM_OBJECT   JSON_OBJECT_T;
   V_PART_OBJECT       JSON_OBJECT_T;
  
   V_NEW_DOCUMENT      VARCHAR2(4000);

begin
  for doc in getDocuments loop
    V_RESULT        := JSON_ELEMENT_T.parse(doc.PO_DOCUMENT);
    V_DOCUMENT_OBJECT := treat (V_RESULT as JSON_OBJECT_T);
    V_RESULT := V_DOCUMENT_OBJECT.get('LineItems');

    if (V_RESULT.is_array()) then
      V_LINEITEMS_ARRAY := treat ( V_RESULT as JSON_ARRAY_T);
      for i in 1..V_LINEITEMS_ARRAY.GET_SIZE() loop
        V_RESULT := V_LINEITEMS_ARRAY.get(i-1);
        if (V_RESULT.is_Object()) then
          V_LINEITEM_OBJECT := treat (V_RESULT as JSON_OBJECT_T);
          V_PART_OBJECT := treat(V_LINEITEM_OBJECT.get('Part')
                              as JSON_OBJECT_T);
          if (V_PART_OBJECT.get_String('UPCCode') = '17153100211') then
            V_PART_OBJECT.put('UPCCode','9999999999');
            V_PART_OBJECT.put('DateModified',to_char(SYSTIMESTAMP,
                                            'YYYY-MM-DD""T""HH24:MI:SSTZH:TZM'));
          end if;
        end if;
      end loop;
    end if;
    V_NEW_DOCUMENT := V_DOCUMENT_OBJECT.stringify();
    update J_PURCHASEORDER
       set PO_DOCUMENT = V_NEW_DOCUMENT
     where current of getDocuments;
  end loop;
  commit;
end;
</code>
 
<p>The fourth statement shows that after the PL/SQL block has executed no documents exist where the UPCCode key contains the value 17153100211</p>

<code>
select count(*)
  from J_PURCHASEORDER j
 where JSON_EXISTS(
           PO_DOCUMENT,'$?(@.LineItems.Part.UPCCode == $UPC)' 
           passing 17153100211 as ""UPC""
)
</code>

<p>The final statement shows that the objects that satisfied the predicate UPCCODE = 17153100211 now satisfy the predicate UPCCODE = 9999999999 and that each of these objects now has a value associated with the key DateModified.</p>

<code>
select LI.*
  from J_PURCHASEORDER j,
       JSON_TABLE(
         J.PO_DOCUMENT,
         '$'
         columns(
           PO_NUMBER NUMBER(5) PATH '$.PONumber',
           NESTED PATH '$.LineItems[*]'
           columns( 
             ITEM_NUMBER NUMBER(4)              PATH '$.ItemNumber',
             UPC_CODE    VARCHAR2(14)           PATH '$.Part.UPCCode',    
             UPDATED     VARCHAR2(38)           PATH '$.Part.DateModified'
           )
         )
       ) LI 
 where UPC_CODE = '9999999999'
</code>",04-JAN-17 05.57.47.124706000 AM,"SYS",02-FEB-23 01.57.50.278548000 PM,"ROGER.FORD@ORACLE.COM"
83400951990330450459360799320263245957,83392487823947840787102120821717075963,"Summary",70,"<p>Hopefully this tutorial has helped you to understand the importance of using the keywords <strong>PARTITION BY</strong> and <strong>ORDER BY</strong> within your MATCH_RECOGNIZE statements to ensure that the correct results are returned.
</p>
<p>The last example illustrated how predicates are applied when used in conjunction with MATCH_RECOGNIZE statement. Essentially, they are applied after the pattern matching process has been completed. That is to day that the pattern matching process is run against the complete data set and then any predicates that are part of the WHERE clause are finally applied.
</p>",13-OCT-16 11.39.46.748551000 AM,"KEITH.LAKER@ORACLE.COM",13-OCT-16 11.39.46.748606000 AM,"KEITH.LAKER@ORACLE.COM"
92046253613417882467068696883049953110,92046253613416673541249082253875246934,"Introduction to Analytic Views",10,"<p>Analytic Views organize data using a hierarchical model and allow you to extend the data set with aggregate data and calculated measures.  Data is presented in a view that is easily queried with relatively simple SQL.  Like standard relational views, analytic views:</P>

<ul>
<li>Are metadata objects (that is, they do not store data).</li>

<li>Can be queried using SQL.</li>

<li>Can access data from other database objects such as tables, views and external tables.</li>

<li>Can join multiple tables into a single view.</li>
</ul>

<p>In addition, analytic views:</P>

<ul>
<li>Organize data into a rich business model using hierarchical concepts.</li>

<li>Include system-generated columns with hierarchical data.</li>

<li>Automatically aggregates data.</li>

<li>Include embedded measure calculations that are easily defined using syntax based on the logical model.</li>

<li>Includes presentation metadata.</li>
</ul>

<p>A typical query selecting from tables includes each table in the FROM clause, joins and aggregations (GROUP BY).  If measure calculations are required those must be expressed in the SELECT list.  The FROM clause, joins, aggregations and measures calculations are included in the definition of the analytic view.  As a result, queries selecting from analytic views can be much simpler.  This approach has several benefits, including:</p>

<ul>
<li>Simplified and faster application development.  It is much easier to define calculations within analytic views than it is to write or generate complex SELECT statements.</li>

<li>Calculation rules can be defined once in the Database and re-used by any number of applications, providing end-users with greater freedom in the choice of reporting tools without concern for inconsistent results.</li>

<li>The ability to increase the value of applications by enhancing data sets with aggregate data and measure calculations.</li>
</ul>",04-JAN-17 05.57.49.974838000 AM,"SYS",04-JAN-17 05.57.49.974902000 AM,"SYS"
92046253613419091392888311512224659286,92046253613416673541249082253875246934,"Setup",20,"<p>This tutorial requires an analytic view and supporting objects.  Create these objects before continuing on to the next module by clicking on ""Execute the SQL required by this tutorial"" before the list of modules in this tutorial.</p>

<p>Note that the sample analytic view objects select from tables in the AV sample schema.</p>

<p>After running the setup script you can see that the objects have been created with the following queries.</p>

<p>Attribute dimensions.</p>

<code>SELECT * FROM user_attribute_dimensions;</code>

<p>Hierarchies.</p>

<code>SELECT * FROM user_hierarchies;</code>

<p>Analytic view.</p>

<code>SELECT * FROM user_analytic_views;</code>",04-JAN-17 05.57.49.976769000 AM,"SYS",30-JAN-17 05.49.20.694927000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613420300318707926141399365462,92046253613416673541249082253875246934,"Examining the Sample Data",30,"<p>Hierarchies and analytic views access data from tables or views, typically a star schema.  This tutorial uses dimension tables from a star schema with sales data that varies by time, product and 
geography. In each dimension table there _ID columns that are used as keys and _NAME columns that are used as textual descriptors.  Other columns might be used for purposes such as sorting.</p>

<p>Notes:</p>

<ul>
<li>All data is in the AV schema.</li>

<li>There are 1:1 relationships between values in the _ID and _NAME columns.</li>

<li>Time periods may be sorted using _END_DATE columns.</li>
</ul>

<p>View data in the AV.TIME_DIM table.</p>

<code>SELECT * FROM av.time_dim ORDER BY month_end_date;</code>

<p>View years.</p>

<code>SELECT DISTINCT year_id,
  year_name,
  year_end_date
FROM av.time_dim
ORDER BY year_end_date;</code>

<p>View months.</p>

<code>SELECT DISTINCT quarter_id,
  quarter_name,
  quarter_of_year,
  quarter_end_date,
  year_id,
  year_name
FROM av.time_dim
ORDER BY quarter_end_date;</code>

<p>View data in the AV.PRODUCT_DIM table.</p>

<code>SELECT * FROM av.product_dim;</code>

<p>View the AV.GEOGRAPHY_DIM table.</p>

<code>SELECT * FROM av.geography_dim;</code>

<p>View data in the AV,SALES_FACT Table.<p>

<code>SELECT * FROM av.sales_fact WHERE rownum < 20;</code>

<p>The following query returns data aggregated by year, department and country.</p>

<code>SELECT t.year_name,
  p.department_name,
  g.country_name,
  SUM(f.sales)
FROM av.time_dim t,
  av.product_dim p,
  av.geography_dim g,
  av.sales_fact f
WHERE t.month_id        = f.month_id
AND p.category_id       = f.category_id
AND g.state_province_id = f.state_province_id
GROUP BY t.year_name,
  p.department_name,
  g.country_name
ORDER BY t.year_name,
  p.department_name,
  g.country_name;</code>",04-JAN-17 05.57.49.977455000 AM,"SYS",13-DEC-19 08.45.07.558756000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613421509244527540770574071638,92046253613416673541249082253875246934,"About Analytic View Objects",40,"<p>There are three types of objects that are used with analytic views:  attribute dimensions, hierarchies, and analytic views.</p>

<p>An attribute dimension is a metadata object that references tables or views and organizes columns into higher-level objects such as attributes and levels.  Most metadata related to dimensions and hierarchies is defined in the attribute dimension object.  </p>

<p>Hierarchies are a type of view.  Hierarchies return detail and aggregate level keys (""hierarchy values"") and attributes of those values.  As the name implies, hierarchies organize data using hierarchical relationships between data.  Hierarchies reference attribute dimension objects.</p>

<p>Analytic views are a type of view that return fact data.  Analytic views reference both fact tables and hierarchies.  Both hierarchy and measure data is selected from analytic views.</p>

<p>Hierarchies are typically used when an application needs to lookup hierarchy values and attributes.  For example, to get a list of values for a pick list.  Analytic views are used to query measure (fact) data.  Because all columns of each hierarchy are included in the analytic view there is no need to join hierarchies and analytic views.</p>",04-JAN-17 05.57.49.979080000 AM,"SYS",13-JAN-17 05.04.21.948128000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
83209048563539624388785623857250318751,83205148911047716319241478701553687837,"Empty Matches and Unmatched Rows",80,"<h3>Can a query contain all three types of match?</h3>
<p>
Now the big question: Can I have a query where it is possible to have both UNMATCHED ROWS and EMPTY MATCHES? Short answer: Yes.</strong>
</p>
<p>When the PATTERN clause allows empty matches, nothing in the DEFINE clause can stop the empty matches from happening. However, there are special PATTERN symbols that are called anchors. Anchors work in terms of positions rather than rows. They match a position either at the start or end of a partition, or it used together then across the whole partition.</p>
<ul>
<li>^ matches the position before the first row in the partition</li>
<li>$ matches the position after the last row in the partition</li>
</ul>
<h3>Finding Empty Matches</h3>
<p>Therefore, using these symbols it is possible to create a PATTERN where the keywords SHOW EMPTY MATCHES, OMIT EMPTY MATCHES, and WITH UNMATCHED ROWS all produce different results from the same result set. For example, letâ€™s start with the following:</p>
<code>SELECT symbol, tstamp, price, mnm, nmr, cls
FROM ticker MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES match_number() AS mnm, 
          count(*) AS nmr, 
          classifier() AS cls
 ALL ROWS PER MATCH SHOW EMPTY MATCHES
 PATTERN ((^A*)|A+)
 DEFINE A AS price > 11)
WHERE symbol = 'GLOBEX'
ORDER BY 1, 2;
</code>
<p>this shows row 1 as an empty match for the pattern A* because we are matching from the start of the partition. This sets the MATCH_NUMBER() counter to 1. After the empty match the state moves to the pattern A+ for the remainder of the rows. The first match for this pattern starts at row 2 and completes at row 4. The final match in our data set is found at the row containing 15-APR-11.</P>
<h3>Ignoring Empty Matches</h3>
<p>Therefore, if we omit the empty match at row 1 we only get 4 rows returned as shown here:</P>
<code>SELECT symbol, tstamp, price, mnm, nmr, cls
FROM ticker MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES match_number() AS mnm, 
          count(*) AS nmr, 
          classifier() AS cls
 ALL ROWS PER MATCH OMIT EMPTY MATCHES
 PATTERN ((^A*)|A+)
 DEFINE A AS price > 11)
WHERE symbol = 'GLOBEX'
ORDER BY 1, 2;
</code>
<p>returns only 4 rows.</p>
<h3>Unmatched rows</h3>
<p>Now if we use the last iteration of this example the MATCH_RECOGNIZE statement returns all the rows from the input data. The actual â€œunmatched rowsâ€ are identified as having a NULL match number and NULL classifier. The â€œempty matchesâ€ are identified as having a NULL classifier and in this example the COUNT(*) function returns zero.</p>
<code>SELECT symbol, tstamp, price, mnm, nmr, cls
FROM ticker MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES match_number() AS mnm, 
          count(*) AS nmr, 
          classifier() AS cls
 ALL ROWS PER MATCH WITH UNMATCHED ROWS
 PATTERN ((^A*)|A+)
 DEFINE A AS price > 11)
WHERE symbol = 'GLOBEX'
ORDER BY 1, 2;
</code>
<p>returns all 20 rows from our data set</p>",11-OCT-16 03.28.21.899105000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.01.06.958754000 PM,"SHARON.KENNEDY@ORACLE.COM"
83209048563571056460095604215792679327,83205148911047716319241478701553687837,"Summary",90,"<p>I hope this helps to explain how the various output keywords that are part of the <strong>ALL ROWS PER MATCH</strong> syntax can affect the results you get back. You should now understand why your results contains match_number values that are not contiguous and why classifier can return a NULL value along with specific aggregate functions. I expect the hardest concept to understand is the idea of empty matches. 
</p>
<p>As I stated earlier it is always a good idea to determine from the start if your pattern is capable of returning an empty match: are you using an asterisk * within the PATTERN clause? Then you can determine how you want to manage those rows: include the empty matches (SHOW EMPTY MATCHES) or exclude them (OMIT EMPTY MATCHES). Be careful if you are using MATCH_NUMBER() within the DEFINE section as part of a formula because empty matches increment the MATCH_NUMBER() counter.
</p>
<p>What should be immediately obvious is that in all the examples I have used the default skip behaviour: AFTER MATCH SKIP PAST LAST ROW. There is a separate tutorial that explores the various skip keywords and how they can impact the results returned by your MATCH_RECOGNIZE statement.</p>",11-OCT-16 03.31.13.163561000 PM,"KEITH.LAKER@ORACLE.COM",11-OCT-16 03.31.13.163617000 PM,"KEITH.LAKER@ORACLE.COM"
92046253613422718170347155399748777814,92046253613416673541249082253875246934,"Selecting from Hierarchies",50,"<p>Hierarchies return data from one of more levels that have been defined in an attribute dimension.  Every hierarchy will have data from levels and a single row for an ALL level (a single top level aggregate).</p>

<p>Hierarchies include two types of columns:</p>

<ul>
<li>Attribute columns contain values for each attribute used by a level in the hierarchy.  These values typically come directly from dimension ta
ble(s).  For example, the _ID and _NAME columns in the sample tables will be presented as attribute columns.</li>

<li>Hierarchical attribute columns contain values that have been created by the database using data from attribute columns.  Hierarchical attributes help aggregate, navigate, filter and present hierarchies.</li>
</ul>

<p>Begin familiarizing yourself with hierarchies by selecting from them using SELECT *.</p>

<p>At this point, just notice that there are many different types of columns and that rows are returned for values at detail and aggregate levels.</p>

<p>View data in the TIME_HIER hierarchy.</p>

<code> SELECT *
FROM time_hier;</code>

<p>View data in the PRODUCT_HIER hierarchy.</p>

<code>SELECT *
FROM product_hier;</code>

<p>View data in the GEOGRAPHY_HIER hierarchy.</p>

<code>SELECT *
FROM geography_hier;</code>",04-JAN-17 05.57.49.980160000 AM,"SYS",30-JAN-17 05.50.46.701459000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613423927096166770028923483990,92046253613416673541249082253875246934,"Attribute Columns",60,"<p>Each column of the referenced dimension table may be presented in a hierarchy.  The column name may be used as is or be aliased.</p>

<p>The following queries present the attribute columns of the TIME_HIER hierarchy. Note that each attribute is presented as a separate column and there are rows for each level (years, quarters and months).</p>

<code>SELECT month_id,
  month_name,
  month_end_date,
  quarter_id,
  quarter_name,
  quarter_end_date,
  year_id,
  year_name,
  year_end_date
FROM time_hier;</code>

<p>Select attribute columns from the PRODUCT_HIER hierarchy.</p>

<code>SELECT category_id,
  category_name,
  department_id,
  department_name
FROM product_hier;</code>

<p>Select attribute columns from the GEOGRAPHY_HIER hierarchy.</p>

<code>SELECT region_id,
  region_name,
  country_id,
  country_name,
  state_province_id,
  state_province_name
FROM geography_hier;</code>",04-JAN-17 05.57.49.982469000 AM,"SYS",30-JAN-17 05.51.03.660513000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613425136021986384658098190166,92046253613416673541249082253875246934,"Hierarchical Attribute Columns",70,"<p>Hierarchical attributes are created by the database.  In some cases attribute columns are unpivoted into a single hierarchical attribute column, creating new rows for aggregate levels.  In other cases, the database calculates values for hierarchical attribute columns.</p>

<p><b>MEMBER_NAME and LEVEL_NAME Columns</b></p>

<p>The MEMBER_NAME column is typically used to return a text description of the key value.  For example, â€œBrazil â€œ for COUNTRY_ID â€œBRâ€ or â€œCamera and Accessoriesâ€ for CATEGORY_ID â€œ-534â€.  The LEVEL_NAME column includes the level name of the key attribute value.</p>

<p>Select MEMBER_NAME and LEVEL_NAME columns from the TIME_HIER hierarchy.</p>

<code>
SELECT member_name,
  level_name
FROM time_hier;
</code>

<p>Select MEMBER_NAME and LEVEL_NAME columns from the PRODUCT_HIER hierarchy.</p>

<code>
SELECT member_name,
  level_name
FROM product_hier;
</code>

<p>Select MEMBER_NAME and LEVEL_NAME columns from the GEOGRAPHY_HIER hierarchy.</p>

<code>
SELECT member_name,
  level_name
FROM geography_hier;
</code>

<p>Hierarchical columns can be used for a variety of purposes, including filtering and sorting.  For example, the LEVEL_NAME column can be used to filter based on level of aggregation.</p>

<p>Select MEMBER_NAME and LEVEL_NAME columns from the TIME_HIER hierarchy filtered to 'YEAR'.</p>

<code>
SELECT member_name,
  level_name
FROM time_hier
WHERE level_name = 'YEAR';
</code>

<p>Select MEMBER_NAME and 
LEVEL_NAME columns from the PRODUCT_HIER hierarchy filtered to 'DEPARTMENT'.</p>

<code>
SELECT member_name,
  level_name
FROM product_hier
WHERE level_name = 'DEPARTMENT';
</code>

<p>Select MEMBER_NAME and LEVEL_NAME columns from the GEOGRAPHY_HIER hierarchy filtered to 'REGION' and 'COUNTRY'.</p>

<code>
SELECT member_name,
  level_name
FROM geography_hier
WHERE level_name IN ('REGION','COUNTRY');
</code>

<p><b>HIER_ORDER Column</b></p>

<p>The HIER_ORDER column sorts values within parents in the hierarchy.  For example, Quarters within Years and Months within Quarters.  You can choose how HIER_ORDER is calculated when you define levels in the attribute dimension object.</p>

<p>Select from the TIME_HIER hierarchy ordering by HIER_ORDER.</p>

<code>
SELECT member_name,
  level_name,
  hier_order
FROM time_hier
ORDER BY hier_order;
</code>

<p>Select from the PRODUCT_HIER hierarchy ordering by HIER_ORDER.</p>

<code>
SELECT member_name, 
  level_name,
  hier_order
FROM product_hier
ORDER BY hier_order;
</code>

<p>Select from the GEOGRAPHY_HIER hierarchy ordering by HIER_ORDER.</p>

<code>
SELECT member_name,
  level_name,
  hier_order
FROM geography_hier
ORDER BY hier_order;
</code>

<p><b>IS_LEAF and DEPTH Columns</b></p>

<p>The IS_LEAF column indicates if a value is at the detail (lowest) level of the hierarchy or at an aggregate level.  In the following examples, IS_LEAF is used to filter for the detail rows of the hierarchies.  (Only the attribute columns are selected, so these queries return the same rows and columns as the dimension tables that the hierarchies refer to.)</p>

<p>Use IS_LEAF to select only the detail rows of the TIME_HIER hierarchy.</p>

<code>
SELECT month_id,
  month_name,
  month_end_date,
  quarter_id,
  quarter_name,
  quarter_end_date,
  year_id,
  year_name,
  year_end_date
FROM time_hier
WHERE is_leaf = 1
ORDER BY hier_order;
</code>

<p>Use IS_LEAF to select only the detail rows of the PRODUCT_HIER hierarchy.</p>

<code>
SELECT category_id,
  category_name,
  department_id,
  department_name
FROM product_hier
WHERE is_leaf = 1
ORDER BY hier_order;
</code>

<p>Use IS_LEAF to select only the detail rows of the GEOGRAPHY_HIER hierarchy.</p>

<code>
SELECT state_province_id,
  state_province_name,
  country_id,
  country_name,
  region_id,
  region_name
FROM geography_hier
WHERE is_leaf = 1
ORDER BY hier_order;
</code>

<p>IS_LEAF might also be used to indicate whether value is drillable to a lower level of detail.  The DEPTH column indicates how distant a value is from the top most value in the hierarchy.</p>

<p>Use IS_LEAF and DEPTH to query the TIME_HIER hierarchy in outline form.</p>

<code>
SELECT
 CASE is_leaf
   WHEN 0 then lpad(' ',depth * 2,'.') || '+ ' || member_name 
   ELSE lpad(' ',depth * 3,'.') || member_name END AS DRILL,
   depth
FROM time_hier
ORDER BY hier_order;
</code>

<p>Use IS_LEAF and DEPTH to query the PRODUCT_HIER hierarchy in outline form.</p>
 
<code>
SELECT
 CASE is_leaf
   WHEN 0 then lpad(' ',depth * 2,'.') || '+ ' || member_name 
   ELSE lpad(' ',depth * 3,'.') || member_name END AS DRILL,
   depth
FROM product_hier
ORDER BY hier_order;
</code>

<p>Use IS_LEAF and DEPTH to query the GEOGRAPHY_HIER hierarchy in outline form.</p>

<code>
SELECT
 CASE is_leaf
   WHEN 0 then lpad(' ',depth * 2,'.') || '+ ' || member_name 
   ELSE lpad(' ',depth * 3,'.') || member_name END AS DRILL,
   depth
FROM geography_hier
ORDER BY hier_order;
</code>

<p><b>MEMBER_UNIQUE_NAME and PARENT_UNIQUE_NAME Columns</b></p>

<p>The MEMBER_UNIQUE_NAME is the primary key of the hierarchy.  As the primary key, it creates values that are unique across all level of the hierarchy.  Depending on your data, the values in MEMBER_UNIQUE_NAME might be a simple concatenation of level name and key attribute value or a m
ore complex value that also includes ancestors.</p>

<p>For now, simply note that MEMBER_UNIQUE_NAME is a unique value.  The PARENT_UNIQUE_NAME column contains the unique name of the parent value in the hierarchy.</p>

<p>The following examples are the simplest form of MEMBER_UNIQUE_NAME.</p>

<p>Select MEMBER_UNIQUE_NAME from the TIME_HIER hierarchy.</p>

<code>
SELECT year_id,
  quarter_id,
  month_id,
  level_name,
  member_unique_name,
  parent_unique_name
FROM time_hier
ORDER BY hier_order;
</code>

<p>Select MEMBER_UNIQUE_NAME from the PRODUCT_HIER hierarchy.</p>

<code>
SELECT department_id,
  category_id,
  level_name,
  member_unique_name,
  parent_unique_name
FROM product_hier
ORDER BY hier_order;
</code>

<p>Select MEMBER_UNIQUE_NAME from the GEOGRAPHY_HIER hierarchy.</p>

<code>
SELECT state_province_id,
  country_id,
  region_id,
  level_name,
  member_unique_name,
  parent_unique_name
FROM geography_hier
ORDER BY hier_order;
</code>

<p>The MEMBER_UNIQUE_NAME and PARENT_UNIQUE_NAME are particularly useful for drill down type queries.</p>

<p>Start with Year level data.</p>

<code>
SELECT member_name,
  member_unique_name
FROM time_hier
WHERE level_name = 'YEAR'
ORDER BY hier_order;
</code>

<p>Drill to children of CY2014 using PARENT_UNIQUE_NAME.</p>

<code>
SELECT member_name,
  member_unique_name
FROM time_hier
WHERE parent_unique_name = '[YEAR].&[14]'
ORDER BY hier_order;
</code>

<p>Drill to children of Q3CY2014.</p>

<code>
SELECT member_name,
  member_unique_name
FROM time_hier
WHERE parent_unique_name = '[QUARTER].&[314]'
ORDER BY hier_order;
</code>

<p><b>MEMBER_CAPTION and MEMBER_DESCRIPTION Columns</b></p>

<p>The MEMBER_CAPTION and MEMBER_DESCRPTION columns are also typically used for text descriptors.</p>

<p>Select MEMBER_CAPTION and MEMBER_DESCRIPTION from the TIME_HIER hierarchy.</p>

<code>
SELECT member_name,
  member_caption,
  member_description
FROM time_hier
ORDER BY hier_order;
</code>",04-JAN-17 05.57.49.983816000 AM,"SYS",13-JAN-17 05.04.21.955934000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613426344947805999287272896342,92046253613416673541249082253875246934,"Selecting from Analytic Views",80,"<p>Analytic views join hierarchies to a fact table and include all columns from each hierarchy, measures from the fact table and calculated measures. Like hierarchies, analytic views return detail and aggregate level data.</p>

<p>A SELECT statement querying an analytic view contains a SELECT list, a FROM clause and in most cases should include a WHERE clause.  The SELECT list includes attributes and measures.  The FROM clause includes only the analytic view.  The HIERARCHIES parameter lists the hierarchies to be used in the query.</p>

<p>Columns from hierarchies are always qualified by hierarchy name and may also be qualified by owner and attribute dimension name.  In most cases each hierarchy referenced in the SELECT list is included in the HIERARCHIES parameter.</p>

<p>Select the MEMBER_NAME column from the TIME_HIER hierarchy and SALES for data at the YEAR level.</p>

<code>SELECT time_hier.member_name AS TIME,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR'
ORDER BY time_hier.hier_order;</code>

<p>Note that this query:</p>

<ul>
<li>Does not include an aggregation function (e.g., SUM) or GROUP BY clause.  This is because the aggregation rules are included in the definition analytic view.</li>

<li>Does not include joins.  Joins are also included in the definition of the analytic view.</li>

<li>Does not select the YEAR_ID or YEAR_NAME columns.  Instead, it selects the MEMBER_NAME column.  Either will return the same data, but the MEMBER_NAME column returns data at level of aggregation.<
/li>
</ul>

<p>Select SALES at the YEAR, QUARTER and MONTH levels.</p>

<code>SELECT time_hier.member_name AS TIME,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name IN ('YEAR','QUARTER','MONTH')
ORDER BY time_hier.hier_order;</code>

<p><b>HIERARCHIES Parameter</b></p>

<p>The HIERACHIES parameter allows a query to return detail and aggregate level values of the listed hierarchies and to filter using attributes of a hierarchy.  If a column is selected from a hierarchy which is not listed in the hierarchies parameter only data from the database defined top-most value is returned (that is, where DEPTH = 0).</p>

<p>The following query selects time values at the year level and includes the TIME_HIER hierarchy in the HIERARCHIES parameter.</p>

<code>SELECT time_hier.member_name AS TIME,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR'
ORDER BY time_hier.hier_order;</code>

<p>PRODUCT_HIER.MEMBER_NAME as been added to the SELECT list, but PRODUCT_HIER has not been added to the HIERARCHIES parameter.  Only data for the ALL PRODUCTS member (that top most aggregate value) are returned by this query.</p>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name AS PRODUCT,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR'
ORDER BY time_hier.hier_order;</code>

<p>In the next query a filter using PRODUCT_HIER.LEVEL_NAME has been added, but PRODUCT_HIER is still not in the HIERACHIES parameter.  This query will return no data.</p>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name AS PRODUCT,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
ORDER BY time_hier.hier_order;</code>

<p>PRODUCT_HIER has been added to the HIERARACHIES parameter.  This query returns data at the DEPARTMENT level of the PRODUCT_HIER hierarchy.</p>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name AS PRODUCT,
  sales
FROM sales_av HIERARCHIES (time_hier, product_hier)
WHERE time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
ORDER BY time_hier.hier_order;</code>

<p><strong>The Importance of Using Filters</strong></p>

<p>Even analytic views defined over small tables have the potential to return large numbers of rows because they return both detail level and aggregate level data.  Therefore, it is usually very important to include filters (that is, a WHERE clause) in queries selecting from analytic views.</p>

<p>Select SALES for all rows of the TIME_HIERARCHY hierarchy.</p>

<code>SELECT time_hier.member_name AS TIME,
  time_hier.depth            AS TIME_DEPTH,
  sales
FROM sales_av HIERARCHIES (time_hier)
ORDER BY time_hier.hier_order;</code>

<p>With only a single, small hierarchy such as the TIME_HIER hierarchy (86 rows) the lack of a WHERE clause is not a problem.  As more hierarchies are used (that is, listed in the HIERARCHIES parameter), the analytic view has the capacity to return many more rows.  Even though the sample dimension tables and fact table used in this tutorial are very small, a query using all three hierarchies has the potential to return up to 260,064 rows  (86 time periods * 12 products * 252 geographies).</p>

<p>Select Sales data from the SALES_AV analytic view using filters on each hierarchy.</p>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name   AS product,
  geography_hier.member_name AS geography,
  sales
FROM sales_av HIERARCHIES (
  time_hier,
  product_hier,
  geography_hier
  )
WHERE time_hier.member_name  = 'CY2014'
AND product_hier.level_name = 'DEPARTMENT' 
AND geography_hier.parent_unique_name = '[REGION].&[EUROPE]'
ORDER BY time_hier.hier_order,
   geography_hier.hier_order,
   product_hier.hier_order;</code>",04-JAN-17 05.57.49.987043000 AM,"SYS",16-AUG-17 07.42.04.986162000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613427553873625613916447602518,92046253613416673541249082253875246934,"Selecting Calculated Measures",90,"<p>Calculated measures are defined in the analytic view and presented as columns.  To select a calculated measure, just include it in the SELECT list.</p>

<p>Select the SALES, SALES_SHARE_GEOG_PARENT and SALES_PRIOR_PERIOD measures from the SALES_AV analytic view.</p>

</p>Note the following:</p>

<ul>
<li>The query filters to the time period â€˜CY2014â€™ and reports both the current year sales and the prior period (CY2103) in this case.  The database has automatically expands filters to access CY2103 data (so you donâ€™t have to).</li>

<li>The SALES_SHARE_GEOG_PARENT measure calculates the ratio of Sales for the current time, product and geography to the Sales of the parent geography (North America, in this case).  This is another example of the database automatically expanding filters to access data needed by calculations.</li>
</ul>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name   AS product,
  geography_hier.member_name AS geography,
  sales,
  ROUND(sales_share_geog_parent,2) as sales_share_geog_parent,
  sales_prior_period
FROM sales_av HIERARCHIES (
  time_hier,
  product_hier,
  geography_hier
  )
WHERE time_hier.member_name  = 'CY2014'
AND product_hier.level_name = 'DEPARTMENT' 
AND geography_hier.parent_unique_name = '[REGION].&[NORTH_AMERICA]'
ORDER BY time_hier.hier_order,
  geography_hier.hier_order,
  product_hier.hier_order;</code>

<p>The query is easily changed to drill down on Mexico.  Note that the only part of the query that needs to change is the value in the PARENT_UNIQUE_NAME filter.  Otherwise, the query was unchanged.</p>

<p>Drill down to Mexico using the PARENT_UNIQUE_NAME value.</p>

<code>SELECT time_hier.member_name AS TIME,
  product_hier.member_name   AS product,
  geography_hier.member_name AS geography,
  sales,
  round(sales_share_geog_parent,2) AS sales_share_geog_parent,
  sales_prior_period
FROM sales_av HIERARCHIES (
  time_hier,
  product_hier,
  geography_hier
  )
WHERE time_hier.member_name  = 'CY2014'
AND product_hier.level_name = 'DEPARTMENT' 
AND geography_hier.parent_unique_name = '[COUNTRY].&[MX]'
ORDER BY time_hier.hier_order,
  geography_hier.hier_order,
  product_hier.hier_order;</code>",04-JAN-17 05.57.49.988757000 AM,"SYS",30-JAN-17 05.54.06.410663000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613428762799445228545622308694,92046253613416673541249082253875246934,"Using SQL Functions",100,"<p>SQL functions may be used with analytic views, just like any other table or view.  For example, SQL functions might be used to format output or perform calculations over the top of an analytic view.  Because the analytic view provides easy access to aggregate data, using SQL to perform calculations over the analytic view is typically easier than the equivalent query against tables.</p>

<p>Use SQL functions and the DEPTH column to create a formatted report.</p>

<code>SELECT lpad(' ',time_hier.depth * 2) || time_hier.member_name AS TIME,
  lpad(' ',product_hier.depth * 2) || product_hier.member_name   AS product,
  TO_CHAR(sales,'$999,999,999,999') AS sales,
  lpad(round(sales_share_prod_parent,2) * 100 || '%',24,' ') as sales_share_prod_parent,
  TO_CHAR(sales_prior_period,'$999,999,999,999') AS sales_prior_period
FROM sales_av HIERARCHIES (
  time_hier,
  product_hier
  )
WHERE time_hier.level_name IN ('YEAR')
AND product_hier.level_name IN ('DEPARTMENT','CATEGORY')
ORDER BY time_hier.hier_order,
  product_hier.hier_order;</code> 

<p>The next example queries the analytic view in an inner query and uses the RANK window function in the outer query.  The analytic view conveniently provides input to the RANK (TIME and PRODUCT_LEVEL to PARTITION BY and ORDER BY at an aggregate level of SALES).

<p>Use the RANK function over the SALES_AV analytic view to rank values at the DEPARTMENT level within YEAR.</p>

<code>WITH sales as (
  SELECT time_hier.member_name      AS time,
    time_hier.hier_order            AS time_order,
    product_hier.member_name        AS product,
    product_hier.hier_order         AS product_order,
    product_hier.level_name         AS product_level,
    product_hier.parent_unique_name AS product_parent,
    sales AS sales
  FROM sales_av HIERARCHIES (
    time_hier,
    product_hier
    )
  WHERE time_hier.level_name IN ('YEAR')
  AND product_hier.level_name IN ('DEPARTMENT')
  )
SELECT time,
  product,
  sales,
  rank() OVER (PARTITION BY time, product_parent ORDER BY sales DESC) AS prod_rank_within_parent
FROM sales
ORDER BY
  time_order,
  sales DESC;</code> 

<p>As another example of the ease of using the hierarchical attribute columns, the previous query is updated to rank Categories within Departments.  Only the value in the product LEVEL_NAME filter needs to change, the remainder of the query remains unchanged.</p>

<p>Change the PRODUCT_HIER.LEVEL_NAME filter to CATEGORY to rank within DEPARTMENT and YEAR.</p>

<code>WITH sales as (
  SELECT time_hier.member_name AS TIME,
    time_hier.hier_order       AS time_order,
    product_hier.member_name   AS product,
    product_hier.hier_order    AS product_order,
    product_hier.level_name   AS product_level,
    product_hier.parent_unique_name AS product_parent,
    sales AS sales
  FROM sales_av HIERARCHIES (
    time_hier,
    product_hier
    )
  WHERE time_hier.level_name IN ('YEAR')
  AND product_hier.level_name IN ('CATEGORY')
  )
SELECT time,
  product,
  sales,
  rank() OVER (PARTITION BY time, product_parent ORDER BY sales DESC) AS prod_rank_within_parent
FROM sales
ORDER BY
  time_order,
  sales DESC; </code>",04-JAN-17 05.57.49.991214000 AM,"SYS",16-AUG-17 08.03.05.279496000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613431180651084457803971721046,92046253613429971725264843174797014870,"Introduction",10,"<p>You will create several different versions of an analytic view, each with different calculations.  These analytic views require 3 attribute dimensions and 3 hierarchies.  Create these objects clicking on ""Execute the SQL required by this tutorial"" which precedes the list of 
modules before continuing.</p>

<p>Analytic views provide a variety of expressions that can be used to define time series calculations.  Time series calculations allow you to compare the measure values between different time periods or aggregate values across time periods.  Common examples include:</p>

<ul>
<li>Year ago</li>
<li>Change from quarter ago</li>
<li>Percent change from month ago</li>
<li>Year-to-date</li>
<li>Percent change this year-to-date compared to the same period last year</li>
</ul>

<p>The primary building blocks for time series calculations include the following expressions:</p>

<ul>
<li>LAG</i>
<li>LAG_DIFF</>
<li>LAG_DIFF_PERCENT</li>
<li>LEAD</i>
<li>LEAD_DIFF</>
<li>LEAD_DIFF_PERCENT</li>
<li>QUALIFY</li>
<li>An aggregation operator and a windowing clause.</li>
</ul>",04-JAN-17 05.57.51.319742000 AM,"SYS",19-MAR-21 03.35.48.219755000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613432389576904072433146427222,92046253613429971725264843174797014870,"How Time Series Calculations Work",20,"<p>Most time series calculations are created using the LEAD and LAG functions or an aggregation operator and a windowing clause.  Time series calculations use the order of values in a hierarchy (as determined by the ORDER BY property of a level) to locate prior or feature values.  That is, LEAD and LAG expressions locate other hierarchy values based on the relative position in the hierarchy.  For example, a prior period calculation would use an expression such as LAG 1 ... WITHIN LEVEL which locates the hierarchy value 1 position back within the same level.</p>

<p>A year ago calculation would use an expression such as LAG 1 ... ACROSS ANCESTOR AT LEVEL YEAR which locates the ancestor value at YEAR level, then the YEAR value 1 position back and then the value at the current level which has the same position relative to the year.  For example to locate the year ago value for August 2015 the expression would first locate the year (2015), the year value one position back (2014) and then the 8th month of 2014 (August 2015).</p>

<p>A 12 period moving average calculation would use an expression such as AVG(sales) ... BETWEEN 11 PRECEDING AND CURRENT MEMBER which locates each of the values back to the 11th value and averages these values plus the current value.</p>

<p>Because the LAG and LEAD expressions and the windowing clause use the relative order by value rather than actual dates these expressions can be used with any hierarchy.</p>

<p>The order of values in the TIMES_DIM table can be viewed with the following query.  The DENSE_RANK function to return the order number of hierarchy values within the level, a parent or an ancestor.</p>

<code>
SELECT year_name,
  dense_rank() over (ORDER BY year_name) AS year_num,
  quarter_name,
  dense_rank() over (PARTITION BY year_name ORDER BY quarter_name) AS quarter_num,
  month_name,
  month_end_date,
  dense_rank() over (partition BY quarter_name ORDER BY month_end_date) AS month_of_quarter,
  dense_rank() over (partition BY year_name ORDER BY month_end_date)    AS month_of_year
FROM av.time_dim
ORDER BY year_name,
  quarter_name,
  month_end_date;
</code>

<p>To return the year ago period for Aug-15 find the year of Aug-15 (CY2015, year_num = 5), the previous year (CY2014, year_num = 4) and the 8th month of that year (month_of_year = 8). Using this approach, the year ago value for Aug-15 is Aug-14.</p>

<p>What if there was a gap in the calendar and Jul-14 did not exist?  The year ago value for Aug-15 would be Sep-14 because that would be the 8th month of 2014.  This can be seen using the following query which filters out 'Jul-14' in the WITH clause.</p>

<code>
WITH time_view AS
  ( SELECT * FROM av.time_dim WHERE MONTH_NAME != 'Jul-14'
  )
SELECT year_name,
  dense_rank() over (ORDER BY year_name
) AS year_num,
  quarter_name,
  dense_rank() over (PARTITION BY year_name ORDER BY quarter_name) AS quarter_num,
  month_name,
  month_end_date,
  dense_rank() over (partition BY quarter_name ORDER BY month_end_date) AS month_of_quarter,
  dense_rank() over (partition BY year_name ORDER BY month_end_date)    AS month_of_year
FROM time_view
ORDER BY year_name,
  quarter_name,
  month_end_date;
</code>

<p>Time series calculations automatically 'reach out' to get data that is outside the WHERE clause of the query.  For example if a query selects data WHERE year_name = '2015', a calculation such as Sales Change from Year Ago will automatically access Sales data for 2014 to calculate the difference between 2014 and 2015.</p>",04-JAN-17 05.57.51.320537000 AM,"SYS",18-AUG-17 07.17.12.555509000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613433598502723687062321133398,92046253613429971725264843174797014870,"Prior and Future Periods Within Level",30,"<p>You can return prior or future values using the LAG and LEAD expressions.  LAG_DIFF, LAG_DIFF_PERCENT, LEAD_DIFF and LEAD_DIFF_PERCENT expressions returns the difference and percent difference between the current member and the prior or future members.</p>

<p>The following example returns sales, change in sales and percent change in sales.  OFFSET indicates the number of prior or future periods.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN LEVEL)),
  sales_change_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN LEVEL)),
  sales_percent_change_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN LEVEL))
  )
DEFAULT MEASURE SALES;</code>

<p>Prior and future periods 'within level' means that the LAG or LAG function returns the prior or future value within the same level as the current hierarchy value regardless of whether a value crosses the boundary of a parent or ancestor.  For example, the prior period of Jan 2016 is Dec 2015.  The LAG and LEAD expressions return values at any level of the hierarchy.  For example, the same expression would return CY2015 as the prior period to year CY2016.</p>

<p>The following query selects sales and prior period measures at the Year level.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_prior_period,
  sales_change_prior_period,
  ROUND(sales_percent_change_prior_period,2) AS sales_percent_change_prior_period
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The next query select sales and prior period measures at the Quarter level.  Note that the prior periods for 4th quarters are the 1st quarter of the prior year.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_prior_period,
  sales_change_prior_period,
  ROUND(sales_percent_change_prior_period,2) AS sales_percent_change_prior_period
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The next query selects data for both the Quarter and Year levels.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_prior_period,
  sales_change_prior_period,
  ROUND(sales_percent_change_prior_period,2) AS sales_percent_change_prior_period
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER','YEAR')
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>If WITHIN LEVEL is omitted, LEAD and LAG default to WITHIN LEVEL.  This can be seen in the following example.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_prior_period_1 AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN LEVEL)),
  sales_prior_period_2 AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
  )
DEFAULT MEASURE SALES;</code>

<p>Note that SALES_PRIOR_PERIOD_1 and SALES_PRIOR_PERIOD_2 return the same values.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_prior_period_1,
  sales_prior_period_2
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.322044000 AM,"SYS",18-AUG-17 07.11.55.403614000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613434807428543301691495839574,92046253613429971725264843174797014870,"Prior and Future Values Within Parent",40,"<p>The WITHIN PARENT keyword restricts the hierarchy values considered by LEAD and LAG to values that share the same parent value.  For example the LAG ... WITHIN PARENT of Feb-16 will return Jan-16 because both Feb-16 and Jan-16 have the parent Q1CY2016.  LAG ... WITHIN PARENT of Jan-16 will return NULL because Jan-16 is the first month of Q1CY2016.</p>

<p>Create an analytic view with sales prior period, change from prior period and percent change from period parent WITHIN PARENT.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT)),
  sales_change_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT)),
  sales_percent_change_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT))
  )
DEFAULT MEASURE SALES;</code>

<p>Run the following query and note that each of the Q1 periods return null values.</p>

<code>SELECT
  time_hier.year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_change_prior_period,
  ROUND(sales_percent_change_prior_period,2)
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND geography_hier.member_name = 'Europe'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>Create the analytic view with LAG ...WITHIN LEVEL and LAG ... WITHIN PARENT.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_lag_level AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN LEVEL)),
  sales_lag_parent AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT))
  )
DEFAULT MEASURE SALES;</code>

<p>Query both SALES_LAG_LEVEL and SALES_LAG_PARENT and observe the difference.</p>

<code>SELECT
  time_hier.year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_lag_level,
  sales_lag_parent
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND geography_hier.member_name = 'Europe'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>Create the analytic view with both LEAD and LAG.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT)),
  sales_next_period AS (LEAD(sales) OVER (HIERARCHY time_hier OFFSET 1 WITHIN PARENT))
  )
DEFAULT MEASURE SALES;</code>

<p>Note that the last quarter of a year returns NULL for the SALES_NEXT_PERIOD measure.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_prior_period,
  sales_next_period
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.324828000 AM,"SYS",18-AUG-17 07.12.43.777918000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613436016354362916320670545750,92046253613429971725264843174797014870,"Lead and Lag within Ancestor (e.g., Year Ago)",50,"<p>LEAD and LAG within ancestor returns values for the same period within a future or prior ancestor value.  This type of calculation is use to create measures such as Sales Year Ago.  This calculation works by first finding the lead or lag of the ancestor value and then finding the value within that ancestor that has the same position as the current member.  For example the same period year ago for February 2001 is February 2000.</p>

<p>The following analytic view includes Sales Year Ago, Sales Change Year Ago and Sales Percent Change Year Ago.</p>


<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_change_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_percent_change_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
  )
DEFAULT MEASURE SALES;</code>

<p>The following query returns Sales Year Ago measures at the Quarter level.</p>

<code>SELECT
  time_hier.year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_year_ago,
  sales_change_year_ago,
  ROUND(sales_percent_change_year_ago,2)
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The same period ancestor ago calculations can return data for any hierarchy value at or below the ancestor level.  In this example, Month level data.</p>

<code>SELECT
  time_hier.year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_year_ago,
  sales_change_year_ago,
  ROUND(sales_percent_change_year_ago,2)
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'MONTH'
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The following example adds Quarter Ago measures</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_change_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_percent_change_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_qtr_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter)),
  sales_change_qtr_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter)),
  sales_percent_change_qtr_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter))
  )
DEFAULT MEASURE SALES;</code>

<p>The following query selects all measures each level in CY2014 and CY2015.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_qtr_ago,
  sales_change_qtr_ago,
  ROUND(sales_percent_change_qtr_ago,2)
  sales_year_ago,
  sales_change_year_ago,
  ROUND(sales_percent_change_year_ago,2)
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH','QUARTER','YEAR')
  AND year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'South America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.327521000 AM,"SYS",18-AUG-17 07.13.20.151501000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613437225280182530949845251926,92046253613429971725264843174797014870,"Period-to-Date (e.g,, Sales Year-to-Date)",60,"<p>Period-to-date calculations aggregate data from the beginning or ending value of a parent or ancestor to the current value.  For example Sales Year-to-Date and Sales Quarter-to-Date.  Period-to-date calculations are created using a aggregation operator and a windowing clause</p>

<p>The following example creates a Sales Year-to-Date measure.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_to_date AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL YEAR))
  )
DEFAULT MEASURE SALES;</code>

<p>Select Sales and Sales Year-to-Date.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_year_to_date
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>To create Sales Quarter-to-Date, just change the the level from Year to Quarter.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_quarter_to_date AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL quarter)), 
  sales_year_to_date AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)) 
  )
DEFAULT MEASURE SALES;</code>

<p>Select Sales, Sales Quarter-to-Date and Sales Year-to-Date.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_quarter_to_date,
  sales_year_to_date
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  and time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.329840000 AM,"SYS",18-AUG-17 07.20.13.607471000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613438434206002145579019958102,92046253613429971725264843174797014870,"Period-to-Date, Ancestor Ago (E.g., Year-to-Date, Year Ago)",70,"<p>By nesting a period-to-date expression within a ancestor ago expression you can create a calculation such as Sales Year-to-Date, Year Ago.</p>

<p>Calculations can be nested by defining the inner calculation as a measure and referencing that calculation in another calculation or be nesting the inner expression in the outer expression.</p>

<p>In the following example, SALES_YEAR_TO_DATE is defined as a measure and the SALES_YEAR_TO_DATE measure is used within the SALES_YTD_YEAR_AGO measure.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_to_date AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL YEAR)),
  sales_ytd_year_ago AS (LAG(sales_year_to_date) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
  )
DEFAULT MEASURE SALES;</code>

<p>Select each of the measures at the Quarter level.</>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_year_to_date,
  sales_ytd_year_ago
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The next example creates the Sales YTD Year Ago measure by nesting the Sales Year-to-Date expression in the Year Ago expression.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_ytd_year_ago AS (LAG(
     SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL YEAR))
   OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
  )
DEFAULT MEASURE SALES;</code>

<p>Select each of the measures at the Quarter level.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_ytd_year_ago
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.332136000 AM,"SYS",18-AUG-17 07.14.05.944752000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
94172526454743848255326330517809977997,94172526449499528049838069157934586509,"Overview of SQL Pattern Matching",10,"<p>This simple tutorial provides a nice gentle introduction to the various keywords within the 12c MATCH_RECOGNIZE clause. It's broken into three parts</p>
<blockquote>
<p>Part 1 covers setting up the ticker dataset</p>
<p>Part 2 covers running your first simple pattern matching query. This focuses on the PARTITION BY and ORDER BY clauses showing what happens if you omit these optional keywords.</p>
<p>Part 3 covers running a more complex pattern matching query where you are going to search for V-shaped patters within the ticker data. This section looks at the other keywords that make up the MATCH_RECOGNIZE clause such as PATTER, DEFINE, SKIP TO..., ALL ROWS, ONE ROW etc.</p>
</blockquote>
If you have any questions about this tutorial or the MATCH_RECOGNIZE clause then please contact me via email: keith.laker@oracle.com.",24-JAN-17 03.02.21.352241000 PM,"KEITH.LAKER@ORACLE.COM",06-FEB-17 03.05.41.687586000 PM,"KEITH.LAKER@ORACLE.COM"
94172526454748683958604789034508802701,94172526449499528049838069157934586509,"Part 1: Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);
INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);

commit;
END;
</code>",24-JAN-17 03.02.48.363147000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.38.51.939644000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613439643131821760208194664278,92046253613429971725264843174797014870,"Period-to-Go (E.g., Remaining Sales Forecast)",80,"<p>Period-to-go calculations can be created by using a windowing clause that aggregates following periods.  For example if SALES in the SALES_FACT table is a sales forecast, the remaining forecasted amount could be calculated using BETWEEN CURRENT MEMBER and UNBOUNDED FOLLOWING.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales_forecast FACT sales,
  sales_forcast_to_go_in_quarter AS (SUM(sales_forecast) OVER (HIERARCHY time_hier BETWEEN CURRENT MEMBER AND UNBOUNDED FOLLOWING WITHIN ANCESTOR AT LEVEL quarter)), 
  sales_forcast_to_go_in_year AS (SUM(sales_forecast) OVER (HIERARCHY time_hier BETWEEN CURRENT MEMBER AND UNBOUNDED FOLLOWING WITHIN ANCESTOR AT LEVEL year))
    )
DEFAULT MEASURE sales_forecast;</code>

<p>View the remaining sales forecast for the quarter and year.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales_forecast,
  sales_forcast_to_go_in_quarter,
  sales_forcast_to_go_in_year
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  and time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.334590000 AM,"SYS",18-AUG-17 07.14.30.514546000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613440852057641374837369370454,92046253613429971725264843174797014870,"Moving Averages and Totals (E.g., Trailing 12 Periods)",90,"<p>Moving averages and totals are calculated with an aggregation operator and windowing clause.  The windowing clause can be within level, within parent or within ancestor.</p>

<p><b>Within Level</b><p>

<p>The following example creates a moving average and total over the trailing 11 periods plus the current period (for a total of 12 periods).  This example is within level.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_12_period_moving_total AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN 11 PRECEDING AND CURRENT MEMBER WITHIN LEVEL)), 
  sales_12_period_moving_avg AS (AVG(sales) OVER (HIERARCHY time_hier BETWEEN 11 PRECEDING AND CURRENT MEMBER WITHIN LEVEL))
    )
DEFAULT MEASURE sales;</code>

<p>If the number of preceding or following values is less than the number of available values, the expression will return the aggregate of the available values (rather than return NULL).  For example for the 11 months of CY2011, where 11 preceding periods are not available, the expression is calculated using the available periods.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_12_period_moving_total,
  ROUND(sales_12_period_moving_avg,2)  AS sales_12_period_moving_avg
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p><b>Within Parent</b><p>

<p>A moving aggregate within parent restarts aggregation with each change in parent.  For example months restarts with each change of quarter and quarter restarts with each change of year. The following example creates a 3 period moving total and moving average within parent.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_3_period_mvg_total_within_parent AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN PARENT)), 
  sales_3_period_mvg_avg_within_parent AS (AVG(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN PARENT))
  )
DEFAULT MEASURE sales;</code>

<p>The following query selects data at the month level.  Note that the moving aggregates restarts with each new quarter.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.quarter_name     AS quarter_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_3_period_mvg_total_within_parent,
  ROUND(sales_3_period_mvg_avg_within_parent,2) AS sales_3_period_mvg_avg_within_parent
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  AND time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The next query selects data at the quarter level.  Note that the moving aggregates restarts at each year.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.quarter_name     AS quarter_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_3_period_mvg_total_within_parent,
  ROUND(sales_3_period_mvg_avg_within_parent,2) AS sales_3_period_mvg_avg_within_parent
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER')
  AND time_hier.year_name in ('CY2013','CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p><b>Within Ancestor</b></p>

<p>A moving aggregate within ancestor restarts aggregates at each change of an ancestor value.  For example, aggregation of months or quarters restarts with each new year.  The following example creates 3 period moving totals and averages of sales within quarters and years.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_3_period_mvg_total_within_quarter AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL quarter)), 
  sales_3_period_mvg_avg_within_quarter AS (AVG(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL quarter)),
  sales_3_period_mvg_total_within_year AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)), 
  sales_3_period_mvg_avg_within_year AS (AVG(sales) OVER (HIERARCHY time_hier BETWEEN 2 PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year))
    )
DEFAULT MEASURE sales;</code>

<p>The following query selects data at the month level.  Notice where aggregation restarts.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.quarter_name     AS quarter_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sales_3_period_mvg_total_within_quarter,
  ROUND(sales_3_period_mvg_avg_within_quarter,0) AS sales_3_period_mvg_avg_within_quarter,
  sales_3_period_mvg_total_within_year,
  ROUND(sales_3_period_mvg_avg_within_year,2) AS sales_3_period_mvg_avg_within_year
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  AND time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.336456000 AM,"SYS",18-AUG-17 07.15.10.216372000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613442060983460989466544076630,92046253613429971725264843174797014870,"Aggregation Operators and Windowing Expressions",100,"<p>Expressions that aggregate using windowing functions (e.g., period-to-date and moving average) support most SQL aggregation operators that accept a single expression as an argument.  These aggregation operators include:</p>

<ul>
<li>APPROX_COUNT_DISTINCT</li>
<li>AVG</li>
<li>COUNT</li>
<li>MAX</li>
<li>MIN</li>
<li>STATS_MODE</li>
<li>STDDEV</li>
<li>SUM</li>
<li>VARIANCE</li>
</ul>

<p>The following example creates sales year-to_date measures using the SUM, AVG, MIN, MAX and STDDEV aggregation operators.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sum_sales_year_to_date AS (SUM(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)), 
  avg_sales_year_to_date AS (AVG(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)), 
  min_sales_year_to_date AS (MIN(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)), 
  max_sales_year_to_date AS (MAX(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year)), 
  stddev_sales_year_to_date AS (STDDEV(sales) OVER (HIERARCHY time_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL year))
    )
DEFAULT MEASURE sales;</code>

<p>Query the data at the month level.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.quarter_name     AS quarter_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sum_sales_year_to_date,
  ROUND(avg_sales_year_to_date,0)  AS avg_sales_year_to_date,
  min_sales_year_to_date,
  max_sales_year_to_date,
  ROUND(stddev_sales_year_to_date,0) AS  stddev_sales_year_to_date
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('MONTH')
  AND time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>Another query at the quarter level.</p>

<code>SELECT
  time_hier.year_name        AS year_name,
  time_hier.quarter_name     AS quarter_name,
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  sum_sales_year_to_date,
  ROUND(avg_sales_year_to_date,0)  AS avg_sales_year_to_date,
  min_sales_year_to_date,
  max_sales_year_to_date,
  ROUND(stddev_sales_year_to_date,0) AS  stddev_sales_year_to_date
FROM
  sales_av HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name in ('QUARTER')
  AND time_hier.year_name in ('CY2014','CY2015')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.340028000 AM,"SYS",19-MAR-21 03.36.54.200556000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613443269909280604095718782806,92046253613429971725264843174797014870,"Index to a Benchmark Time Period (e.g., Index to Year 2011)",110,"<p>Measures that calculate an index (ratio) of a time period to a specific time period can be created using SHARE_OF and the QUALIFY expressions. For example, a measure can be created that calculates the index (ratio) of any time period to the value of Sales for CY2011.</p>

<p>Both expressions reference a specific time period using the key attribute value. Run the following query to find the key attribute value of CY2011.</p>

<code>SELECT DISTINCT year_name, year_id
FROM time_hier
WHERE year_name is not null;</code>

<p>The key attribute value for CY2011 is '11'.  The following examples uses both the SHARE_OF and QUALIFY expressions to create an index of sales to CY2011 measure.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  share_of_sales_to_cy2011 AS (SHARE_OF(sales HIERARCHY time_hier MEMBER year['11'])),
  sales_qualify_to_cy2011 AS (sales / (QUALIFY(sales, time_hier = year['11'])))
  )
DEFAULT MEASURE sales;</code>

<p>Select the measures by Year in North America.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  ROUND(share_of_sales_to_cy2011,2)  AS share_of_sales_to_cy2011,
  ROUND(sales_qualify_to_cy2011,2)   AS sales_qualify_to_cy2011
FROM
  sales_av HIERARCHIES (
  time_hier,
  geography_hier)
WHERE
  time_hier.level_name in ('YEAR')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>

<p>The percent change is easily calculated by subtracting 1 from either of these expressions.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  share_of_sales_to_cy2011_pct_change AS (SHARE_OF(sales HIERARCHY time_hier MEMBER year['11']) -1),
  sales_qualify_to_cy2011_pct_change AS (sales / (QUALIFY(sales, time_hier = year['11'])) - 1)
  )
DEFAULT MEASURE sales;</code>

<p>A query of the percent change measures.</p>

<code>SELECT
  time_hier.member_name      AS time_hier,
  geography_hier.member_name AS geography_hier,
  sales,
  ROUND(share_of_sales_to_cy2011_pct_change,2)  AS share_of_sales_pct_change_from_cy2011,
  ROUND(sales_qualify_to_cy2011_pct_change,2)   AS sales_qualify_percent_change_from_cy2011
FROM
  sales_av HIERARCHIES (
  time_hier,
  geography_hier)
WHERE
  time_hier.level_name in ('YEAR')
  AND geography_hier.member_name = 'North America'
ORDER BY
  geography_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.57.51.342814000 AM,"SYS",18-AUG-17 07.16.09.527071000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
83304989169659232411787783775684553611,83301768980930787340849483629290212021,"Non-contiguous set of matches?",60,"<h3>Is it possible to generate a non-contiguous set of MATCH_NUMBER()s?</h3>
<p>Simple answer is yes - if the results contains <em>empty matches</em> (there is a separate tutorial on this topic if you are unsure about the meaning of this specific term)
</p>
<p>Let's change the details of our pattern to search for zero or more matches by using the asterisk (*) quantifier</p>
<code>SELECT symbol, tstamp, price, mn, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS mn
  ALL ROWS PER MATCH OMIT EMPTY MATCHES
  PATTERN (X* Y* W* Z*)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>We can now see that our results shows a non-contiguous sequence of matches returned by the MATCH_NUMBER() function. The reason for this is that we have excluded the empty matches (generated because we are using the asterisk (*) quantifier) by using the keywords <strong>OMIT EMPTY MATCHES</strong>
</p>
<p>If we remove these keywords then we can see the full resultset:
</p>
<code>SELECT symbol, tstamp, price, mn, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS mn
  ALL ROWS PER MATCH
  PATTERN (X* Y* W* Z*)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>These unmatched rows still increments the MATCH_NUMBER() counter which is why in our previous example we saw the column mn start at the value 3.<p>",12-OCT-16 01.44.32.724375000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.49.46.256788000 PM,"SHARON.KENNEDY@ORACLE.COM"
83304989169660441337607398404859259787,83301768980930787340849483629290212021,"Using the CLASSIFIER() function",70,"<h3>Matching each row to its respective pattern variable</h3>
<p>The CLASSIFIER() function allows us to see which pattern variable is matched to each row
</p>
<code>SELECT symbol, tstamp, price, mn, p_var, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS mn,
           CLASSIFIER() AS p_var
  ALL ROWS PER MATCH
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>Here we can see that pattern variable X (where the price is declining) is matched to the first three rows in our result set, followed by one occurrence of Y, one occurrence of W and finally one occurrence of Z.
</p>",12-OCT-16 01.47.11.527911000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.50.08.024326000 PM,"SHARON.KENNEDY@ORACLE.COM"
83392487825682649338249113687420438523,83392487823947840787102120821717075963,"Overview of SQL Pattern Matching",10,"The following tutorial provides an introduction to the various keywords within the 12c MATCH_RECOGNIZE clause.",13-OCT-16 09.58.29.750270000 AM,"KEITH.LAKER@ORACLE.COM",13-OCT-16 09.58.29.750325000 AM,"KEITH.LAKER@ORACLE.COM"
83392487825741886703410230516981041147,83392487823947840787102120821717075963,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE ticker PURGE;
</code>",13-OCT-16 09.58.54.910015000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.02.14.826196000 PM,"SHARON.KENNEDY@ORACLE.COM"
83392657078491405256338974133057101282,83392487823947840787102120821717075963,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);
INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);

commit;

END;
</code>",13-OCT-16 09.59.45.489031000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.02.28.514276000 PM,"SHARON.KENNEDY@ORACLE.COM"
83392657078492614182158588762231807458,83392487823947840787102120821717075963,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>
<p>You can view the full data set using the following code:</p>
<code>SELECT * FROM ticker ORDER BY symbol, tstamp;</code>
<p> Compared to previous MATCH_RECOGNIZE tutorials, here there is a small change to the data for ticker symbol ACME. The price on Apr-16 has been changed to <strong>14</strong> so that there are now three consecutive rows (15-Apr, 16-Apr and 17-Apr) with the same value (14) for price.</p>

<code>SELECT * FROM ticker 
WHERE symbol = 'ACME' 
ORDER BY tstamp;
</code>",13-OCT-16 10.00.07.494849000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.02.46.246462000 PM,"SHARON.KENNEDY@ORACLE.COM"
83392487826663088177956577948107147259,83392487823947840787102120821717075963,"Correctly bucketing your data",40,"<p>The following pattern matching query just uses the mandatory order by clause and is using an always true event (price=price). This means that every row matches the pattern
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES 
  e.symbol as symbol,
  e.tstamp as tstamp,
  e.price as price 
 ONE ROW PER MATCH 
 PATTERN (e) 
 DEFINE 
   e AS price=price);
</code>

<h3>Why is the data in a random order?</h3>
<p>You will see that the data is in random order when it comes to the stock ticker because there is more than one record per tstamp. Now we do it the right way and add the PARTITION BY clause
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 MEASURES
  e.tstamp as tstamp,
  e.price as price 
 ONE ROW PER MATCH 
 PATTERN (e) 
 DEFINE 
   e AS price=price);
</code>",13-OCT-16 10.04.44.051286000 AM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.03.11.855934000 PM,"SHARON.KENNEDY@ORACLE.COM"
85388767221910038247790868358756974986,66677775709556657797196623725084693197,"SKIP TO where?",65,"<p>After we have found a match (and this actually includes empty matches as well!) we need to decide where to start searching for the next match. This is controlled by the AFTER MATCH SKIP clause. There are five possible options for controlling the start point and the default for the clause is AFTER MATCH SKIP PAST LAST ROW.
</p>
<p>In this case we are going to use the default.</p>
<code>SELECT 
 tstamp,
 userid,
 session_id
FROM clickdata MATCH_RECOGNIZE(         
   PARTITION BY userid ORDER BY tstamp
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   AFTER MATCH SKIP PAST LAST ROW
   PATTERN (b s+)
   DEFINE
       s as (tstamp - prev(tstamp) <=10)
 );
</code>
<p>Note the position of the SKIP TO syntax within the MATCH_RECOGNIZE code - it comes just before the PATTERN clause</p>
<p>The results from the above code are exactly the same as the results from the previous example which indicates that although the previous example did not include a SKIP TO clause the default processing was implemented.
</p>",01-NOV-16 12.31.00.611298000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.12.33.292947000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613492835867884803891881736022,92046253613491626942065189262707029846,"Introduction",10,"<p>This is an in-depth tutorial that walks you through designing a very functional analytic view.  Please take your time.  You will be rewarded with a solid understanding of the most important features and characteristics of analytic views.</p>

<p>An analytic view is a type of view that can be used to extend a data set with aggregate data, calculated measures and description metadata.  Analytic views dramatically simplify the SQL needed to execute analytic queries.</p>

<p>Typical use cases for analytic views include:</p>

<ul>
<li>Extending a data set with time series, hierarchical (e.g., share of parent) and other calculations.</li>
<li>Simplifying the development of business intelligence applications (for example, a BI dashboard built in Oracle Application Express) due to embedded calculations and simplified SQL generation.</li>
<li>Improving query performance of applications that cannot otherwise access aggregate tables and are not optimized specifically for the Oracle Database.</li>
</ul>

<p>Analytic views do not store data.  Analytic views can access data in:</p>

<ul>
<li>Tables, views and external tables.</li>
<li>Star and snowflake schema</li>
<li>Flat/denormalized fact tables.</li>
<li>Aggregate fact tables.</li>
</ul>

<p>As a general rule:</p>

<ul>
<li>Smaller data sets will perform well when the analytic view is layered over flat/denormalized fact tables.</li>
<li>Larger data sets will perform well using a star schema (better yet, a star schema loaded into the in-memory column store using the Oracle Database In-Memory option).</li>
<li>Aggregate tables can dramatically improve query performance for any style schema.</li>
</ul>

<p>The analytic view created in this tutorial will use a star schema consisting of 3 dimension tables and a fact table.  All tables are in the AV sample schema.</p>

<ul>
<li>The time dimension table includes data for months, quarters and years.</li>
<li>The geography dimension table includes data for states/provinces, countries and regions.</li>
<li>The product dimension table contains data for categories and departments.</li>
<li>The fact table contains sales data.
</ul>

<p>Before creating the analytic view, review the sample data.</p>

<p>Time dimension data.</p>

<code>
SELECT * FROM av.time_dim ORDER BY month_end_date;
</code>

<p>Year level data.</p>

<code>
SELECT DISTINCT year_id,
  year_name,
  year_end_date
FROM av.time_dim
ORDER BY year_end_date;
</code>

<p>Quarter level data.</p>

<code>
SELECT DISTINCT quarter_id,
  quarter_name,
  quarter_of_year,
  quarter_end_date,
  year_id,
  year_name
FROM av.time_dim
ORDER BY quarter_end_date;
</code>

<p>Product dimension data.</p>

<code>
SELECT * FROM av.product_dim;
</code>

<p>Geography dimension data.</p>

<code>
SELECT * FROM av.geography_dim;
</code>

<p>Fact data.</p>

<code>
SELECT * FROM av.sales_fact WHERE rownum <= 20;
</code>

<p>A star query joins the dimension tables to the fact table and aggregates sales.</p>

<code>
SELECT
  t.year_name,
  p.department_name,
  g.region_name,
  SUM(f.sales)
FROM
 av.time_dim t,
 av.product_dim p,
 av.geography_dim g,
 av.sales_fact f
WHERE
 t.month_id = f.month_id
 AND p.category_id = f.category_id
 and g.state_province_id = f.state_province_id
GROUP BY 
  t.year_name,
  p.department_name,
  g.region_name
ORDER BY
  t.year_name,
  p.department_name,
  g.region_name;
</code>

<p>In this tutorial you will create 1 attribute dimension and 1 hierarchy each using the AV.TIME_DIM, AV.PRODUCT_DIM and AV.GEOGRAPHY_DIM tables and 1 analytic view using the AV.SALES_FACT table.</p>",04-JAN-17 05.57.58.824626000 AM,"SYS",18-AUG-20 08.26.54.974301000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613494044793704418521056442198,92046253613491626942065189262707029846,"Creating Attribute Dimensions and Hierarchies",20,"<p>Most metadata used by dimensions and hierarchies is defined in an <i>attribute dimension</i> object.  This allows the metadata for attributes and levels to be re-used by multiple hierarchies, promoting consistency and simplifying the definition of the hierarchy.</p>

<p>An attribute dimension may be as simple as a list of attributes and levels defined only with a <i>key attribute</i> or a rich collection of metadata that provides applications with a variety of hierarchical attributes and presentation metadata.</p>

<p>You will start by creating a simple attribute dimension and a hierarchy.  Later, you will add a variety of properties to the attribute dimensions and hierarchies.</p>

<p>The first attribute dimension and hierarchy will use only the _ID columns from the TIME_DIM table.  These will support a hierarchy with three levels.</p>

<p>Review data in the _ID columns in the TIME_DIM table.</p>

<code>SELECT month_id,
  quarter_id,
  year_id
FROM av.time_dim
ORDER BY year_id,
  quarter_id;</code>

<p>Some notes about the attribute dimension object:</p>

<ul>

<li>The USING clause identifies the dimension table (or view).  A single table or view name may be provided.</li>

<li>The ATTRIBUTES list includes a list of columns in the dimension table that will be used by hierarchies.  Column names may be used 'as is' or they may be aliased.</li>

<li>The LEVEL clause defines a level that can be used in hierarchy for aggregation and navigation.  Each attribute dimension must have at least one level.</li>

<li>The KEY property of a level specifies the attribute that provides the key values of that level.  The <i>key attribute</i> is the value at that level that is used when aggregating data and executing calculated measures.  You might think of the key attribute at the primary key of the level.</li>

<li>The DETERMINES property of a level includes a list of attributes that have values which can be uniquely identified by the KEY attribute of the level.  For example for each MONTH_ID value there is only one QUARTER_ID value.  When levels are used in a hierarchy a parent level inherits the determined attributes of child level.  For example, if the MONTH level determines the QUARTER_ID attribute and the QUARTER level is a child of the YEAR level, it is assumed that the MONTH level also determines YEAR_ID.</li>

<li>Optionally, the attribute dimension may include an ALL MEMBER clause.  This allows you to specify the value for the single, top-most hierarchy value. If an ALL MEMBER clause is not included the top-most value defaults to 'ALL'.</li>
</ul>

<p>IMPORTANT:  The DETERMINES property is one of the more difficult concepts to understand about analytic views.  Pay careful attention to any descriptions and examples involving the DETERMINES property.  The DETERMINES property plays three very important roles:</p>

<ul>
<li>Whether an attribute is included as a column in a hierarchy.</li>
<li>How SQL generated by the analytic views when it is queries in optimized.</>
<li>How the MEMBER_UNIQUE_NAME value is formed.</li>
</ul>

<p>Whether an attribute is determined by the KEY attribute of a level can be answered by a simple question -  ""for each value of the key attribute is there only 1 value of the determined attribute?"" If the answer is yes, it may (and probably should) be a determined attribute.<p>

<p>This question can be answered with a simple query:</p>

<code>
SELECT
  year_id,
  COUNT(DISTINCT year_name)
FROM
  av.time_dim
GROUP BY
  year_id;
</code>

<p>If the values of COUNT DISTINCT is always 1, the attribute may be determined by the level KEY attribute.  In the case of year_name, it may be determined by YEAR_ID at the YEAR level.</p>

<p>Can QUARTER_NAME be determined by YEAR_ID (KEY attribute of the YEAR level)?  Let's check:</p>

<code>
SELECT
  year_id,
  COUNT(DISTINCT quarter_name)
FROM
  av.time_dim
GROUP BY
  year_id;
</code>

<p>No, COUNT DISTINCT is greater than 1 for at least one year.</p>

<p>There are hierarchy and analytic view validation procedures that can check if data meets the rules of DETERMINES.  (That will be covered in a different tutorial).</p> 

<p>Well, that discussion was probably a bit exhausting for a 'getting started' tutorial but it's important.  Thanks for hanging in there!</p>

<p>Create the attribute dimension for time.  (Note that in this very simple attribute dimension the ALL MEMBER clause been omitted.)</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim 
ATTRIBUTES 
 (year_id,
  quarter_id,
  month_id)
LEVEL MONTH
  KEY month_id
  DETERMINES (quarter_id)
LEVEL QUARTER
  KEY quarter_id
  DETERMINES (year_id)
LEVEL YEAR
  KEY year_id;</code>

<p>You cannot select from an attribute dimension, but you can see it in the data dictionary. Note that the ALL_MEMBER_NAME as defaulted to ALL because the CREATE statement did not include the ALL MEMBER NAME clause.</p>

<code>SELECT * FROM user_attribute_dimensions WHERE dimension_name = 'TIME_ATTR_DIM';</code>

<p>The list of attributes in the attribute dimension.</p>

<code>SELECT * FROM user_attribute_dim_attrs WHERE dimension_name = 'TIME_ATTR_DIM';</code>

<p>A list of levels of the attribute dimension.</p>

<code>SELECT * FROM USER_ATTRIBUTE_DIM_LEVELS;</code>

<p>The hierarchy object is simply a list of levels.  Lower (more detailed) levels are children of higher (more aggregated) levels.  The hierarchy object inherits all the metadata about levels and attributes from the attribute dimension.</p>

<p>Create a hierarchy using the TIME_ATTR_DIM attribute dimension.</p>

<code>CREATE OR REPLACE HIERARCHY time_hier
USING time_attr_dim
 (month  CHILD OF
 quarter CHILD OF
 year);</code>

<p>Levels of the hierarchy</p>

<code>SELECT * FROM USER_HIER_LEVELS WHERE hier_name = 'TIME_HIER';</code>
 
<p>Select from the TIME_HIER hierarchy.</p>

<code>SELECT *
FROM time_hier
ORDER BY hier_order;</code>

<p>This is a simple but functional hierarchy.  While functional, there are important features not included in this hierarchy.  Note that values in the MEMBER_NAME column might not be easily readable and the Quarter and Year levels, the MEMBER_CAPTION and MEMBER_DESCRIPTION columns do not return data and the ordering of time periods is not correct for reporting or time series calculations (for example, note that February comes before January).</p>",04-JAN-17 05.57.58.826096000 AM,"SYS",18-AUG-20 08.54.15.913816000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613495253719524033150231148374,92046253613491626942065189262707029846,"MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION Properties",30,"<p>The MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION properties of a level are used to populate hierarchical attribute columns of the same name.  These attributes are typically used for textual descriptions of the KEY attribute.  For example â€˜CY2014â€™ for YEAR_ID 14, â€˜Q1CY2014â€™ for QUARTER_ID 114 and â€˜Cameras and Accessoriesâ€™ for CATEGORY_ID -532.  These properties provide the opportunity to use different text descriptions for the same KEY attribute, for example â€˜Jan-14â€™ and â€˜January 2014.â€™</p>

<p>IMPORTANT:  There should be a a 1:1 relationship between KEY attribute values and values of the MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION columns.  If this is not true, there might be unexpected results when querying the hierarchy and analytic views that reference the hierarchy.  It is assumed that the KEY attribute determines the MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION, so those attributes do not need to be listed in the DETERMINES property.</p>

<p>To add these properties:</p>

<ul>
<li>Add the _NAME columns to the ATTRIBUTES list.</li>
<li>Add MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION properties to levels.</li>
</ul>

<p>Create the attribute dimension object.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim
ATTRIBUTES
 (year_id,
  year_name,
  quarter_id,
  quarter_name,
  month_id,
  month_name,
  month_long_name)
LEVEL MONTH
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  DETERMINES (quarter_id)
LEVEL QUARTER
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  DETERMINES (year_id)
LEVEL YEAR
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name; </code>
  
<p>If attributes used for MEMBER NAME, MEMBER CAPTION or MEMBER DESCRIPTION are not a text data type, convert them to text using TO_CHAR.  For example, TO_CHAR(category_id) could be used if you wanted to use the CATEGORY_ID column in the PRODUCT_DIM table.</p>

<p>TIME_HIER hierarchy will automatically use these new properties.  The hierarchy does not need to be replaced or compiled.</p>

<p>SELECT the _NAME, MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION columns from the TIME_HIER hierarchy.</p>

<code>SELECT year_name,
  quarter_name,
  month_name,
  month_long_name,
  member_name,
  member_caption,
  member_description
FROM time_hier
ORDER BY hier_order;</code>",04-JAN-17 05.57.58.827669000 AM,"SYS",16-AUG-17 08.42.10.375744000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
83309226191461727054943601582704051186,83301768980930787340849483629290212021,"CLASSIFIER with ONE ROW PER MATCH",75,"<h3>What happens is you combine ONE ROW PER MATCH and CLASSIFIER()?</h3>
<p>It is possible to use the CLASSIFIER() function in conjunction with the keywords ONE ROW PER MATCH, however, the results are not very useful as you an see...</p> 
<code>SELECT symbol, 
  tstamp,
  first_price,
  last_price,
  match_number,
  classifier,
  first_x, 
  last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(tstamp) as tstamp,
           FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS match_number,
           CLASSIFIER() AS classifier,
           FIRST(x.price) as first_price,
           LAST(z.price) as last_price 
  ONE ROW PER MATCH
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price <= PREV(price)),
         Y AS (price >= PREV(price)), 
         W AS (price <= PREV(price)),
         Z AS (price >= PREV(price)))
WHERE symbol='OSCORP';
</code>

<p>You should notice that the value returned by the CLASSIFIER() function is now truncated to the last variable listed in the DEFINE clause, which in this case is the pattern variable â€œZâ€. 
</p>
<p>Therefore, when using CLASSIFIER() function to check how the pattern is being applied you should use one of the ALL ROWS PER MATCH options which allows you to see which rows are assigned to which pattern variable. The real benefit of these functions will become obvious if you look at the tutorial on greedy vs. reluctant quantifiers.
</p>
<p>Note that even with the ONE ROW PER MATCH syntax the MATCH_NUMBER() function continues to operate as before.</p>",12-OCT-16 02.35.08.368073000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.53.18.006917000 PM,"SHARON.KENNEDY@ORACLE.COM"
83309226191657573037721171509006451698,83301768980930787340849483629290212021,"A few restrictions",80,"<p>There are a few restrictions on the use of these built-in measures which are related to wrapping other aggregate functions around them within the MEASURE clause.
</p>
<p>For example, extending the example above where we are using ONE ROW PER MATCH...if we wanted to pull-out the first and last instance of the pattern variables we could try the following:
</p>
<code>SELECT symbol, 
  tstamp,
  first_price,
  last_price,
  match_number,
  f_c,
  l_c,
  first_x, 
  last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(tstamp) as tstamp,
           FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS match_number,
           FIRST(CLASSIFIER()) AS f_c,
           LAST(CLASSIFIER()) AS l_c,
           FIRST(x.price) as first_price,
           LAST(z.price) as last_price 
  ONE ROW PER MATCH
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price <= PREV(price)),
         Y AS (price >= PREV(price)), 
         W AS (price <= PREV(price)),
         Z AS (price >= PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>But as you can see, by wrapping FIRST() and LAST() functions around the CLASSIFIER() function generates a SQL error:
</p>
<blockquote>
ORA-62507: illegal use of MATCH_NUMBER or CLASSIFIER in MATCH_RECOGNIZE clause
</blockquote>
<p>The same is true if we use the MAX and MIN functions:
</p>
<code>SELECT symbol, 
  tstamp,
  first_price,
  last_price,
  match_number,
  f_c,
  l_c,
  first_x, 
  last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(tstamp) as tstamp,
           FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS match_number,
           MIN(CLASSIFIER()) AS f_c,
           MAX(CLASSIFIER()) AS l_c,
           FIRST(x.price) as first_price,
           LAST(z.price) as last_price 
  ONE ROW PER MATCH
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price <= PREV(price)),
         Y AS (price >= PREV(price)), 
         W AS (price <= PREV(price)),
         Z AS (price >= PREV(price)))
WHERE symbol='OSCORP';
</code>",12-OCT-16 02.40.19.069035000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.50.51.137252000 PM,"SHARON.KENNEDY@ORACLE.COM"
85402068267710558801707367118104350705,85402068267675499952938542872037871601,"Overview of SQL Pattern Matching",10,"<h3>Finding patterns with SQL</h3>
<p>
Recognizing patterns in a sequence of rows has been a capability that was widely desired, but not really possible with SQL until now. There were many workarounds, but these were difficult to write, hard to understand, and inefficient to execute.</p> 
<p>
The aim of this tutorial is to explain how to use the AFTER MATCH SKIP syntax which contains a lot of powerful features.
</p>",01-NOV-16 03.32.48.595748000 PM,"KEITH.LAKER@ORACLE.COM",01-NOV-16 03.32.48.595831000 PM,"KEITH.LAKER@ORACLE.COM"
85402068267723856985723128039026118641,85402068267675499952938542872037871601,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE ticker PURGE;
</code>",01-NOV-16 03.34.50.663405000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.55.07.156426000 PM,"SHARON.KENNEDY@ORACLE.COM"
85402068267728692689001586555724943345,85402068267675499952938542872037871601,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>",01-NOV-16 03.35.17.629200000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.55.37.055694000 PM,"SHARON.KENNEDY@ORACLE.COM"
85402068267748035502115420622520242161,85402068267675499952938542872037871601,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);

INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);

INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);

commit;

END;

</code>",01-NOV-16 03.37.14.882635000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.55.24.825396000 PM,"SHARON.KENNEDY@ORACLE.COM"
85402068267801228238178464306207313905,85402068267675499952938542872037871601,"Basic Syntax for Skipping ",11,"<p>We use the AFTER MATCH SKIP clause to determine the precise point to resume row pattern matching after a non-empty match is found. If you donâ€™t supply an AFTER MATCH SKIP clause then the default is AFTER MATCH SKIP PAST LAST ROW.
</p>
<p>Of course there are quite a few options available:</p>
<blockquote>
<ul>
<li>AFTER MATCH SKIP TO NEXT ROW - Resume pattern matching at the row after the first row of the current match.</li>
<li>AFTER MATCH SKIP PAST LAST ROW - Resume pattern matching at the next row after the last row of the current match.</li>
<li>AFTER MATCH SKIP TO FIRST pattern_variable - Resume pattern matching at the first row that is mapped to the pattern variable.</li>
<li>AFTER MATCH SKIP TO LAST pattern_variable - Resume pattern matching at the last row that is mapped to the pattern variable.</li>
<li>AFTER MATCH SKIP TO pattern_variable - The same as AFTER MATCH SKIP TO LAST pattern_variable.</li>
</ul>
</blockquote>
<p>During this tutorial we will explore how to use some these clauses</p>",01-NOV-16 03.40.31.403931000 PM,"KEITH.LAKER@ORACLE.COM",01-NOV-16 03.41.48.087409000 PM,"KEITH.LAKER@ORACLE.COM"
85402068267842331716045361698147323889,85402068267675499952938542872037871601,"Skipping to next row [DEFAULT]",40,"<p>Looking at the source data for the sessionization example itâ€™s clear that as we walk through the entries in the log file to check if an entry is part of the current session or not, there is no point in stepping backwards to begin searching again once a match has been found. The sessionization tutorial is a good example of when to use this type of SKIP operation.
</p>
<p>See here <a href=""https://livesql.oracle.com/apex/livesql/file/tutorial_C863BMBWSE9GK32X19LDPO2SF.html"" target=""_blank"">https://livesql.oracle.com/apex/livesql/file/tutorial_C863BMBWSE9GK32X19LDPO2SF.html</a>
</p>",01-NOV-16 03.45.03.104877000 PM,"KEITH.LAKER@ORACLE.COM",01-NOV-16 03.55.22.853638000 PM,"KEITH.LAKER@ORACLE.COM"
83304741729122164361603043621750863693,83301768980930787340849483629290212021,"MATCH_NUMBER and Partition Boundaries",55,"<h3>What happens to the MATCH_NUMBER counter across partition boundaries</h3>
<p>Using the code below we can see how MATCH_NUMBER() functions across the partition boundary, defined as the column symbol.</p>

<code>SELECT symbol, tstamp, price, mn, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS mn
  ALL ROWS PER MATCH OMIT EMPTY MATCHES
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)));
</code>
<p>Note that we have removed the WHERE clause so that you can see the how the MATCH_NUMBER() sequencing is handled across partition boundaries - i.e. you should note that the match numbering starts at 1 within each symbol-based partition. For symbol â€˜ACMEâ€™ we have one matched one W-shaped pattern and within the OSCORP partition we matched two W-shapes.</p>
<p>This shows that the MTACH_NUMBER() functions works independently within each partition.<p>",12-OCT-16 01.50.35.723180000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.49.01.346438000 PM,"SHARON.KENNEDY@ORACLE.COM"
83304741730031276577953244761129908045,83301768980930787340849483629290212021,"Summary",90,"<p>The two built-in functions MATCH_NUMBER() and CLASSIFIER() functions are part of MATCH_RECOGNIZE. They allow you to check how your pattern is being matched to your data set. They are really useful debugging tools when working with MATCH_RECOGNIZE but don't forget that you can also use them within your result set.</p>",12-OCT-16 02.27.09.130829000 PM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 02.35.20.118249000 PM,"KEITH.LAKER@ORACLE.COM"
85402068267946299336532219807172055025,85402068267675499952938542872037871601,"Using Pattern Variables and ORA-62154",50,"<p>Note that you can set the restart point to be linked to a specific pattern variable which allows you to work with overlapping patterns - i.e. where you are searching for â€œshapesâ€ within your data set such as â€œWâ€ shaped patterns within our ticker data stream. But what happens if the pattern variable within the SKIP TO clause is not matched? Letâ€™s look at the following example:
</p>
<code>SELECT *
FROM Ticker MATCH_RECOGNIZE (
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES STRT.tstamp AS start_tstamp,
                     LAST(UP.tstamp) as end_tstamp,
          MATCH_NUMBER() AS match_num,
          CLASSIFIER() AS var_match
 ALL ROWS PER MATCH
 AFTER MATCH SKIP TO DOWN
 PATTERN (STRT DOWN* UP)
 DEFINE
       DOWN AS DOWN.price < PREV(DOWN.price),
       UP AS UP.price > PREV(UP.price) 
) MR
WHERE symbol='ACME'
ORDER BY MR.symbol, MR.tstamp;
</code>
<p>here we are stating that we need at least zero or more matches of the variable DOWN to occur and once a match has been found then we will resume the search for the next pattern at the DOWN event. With this pattern it is possible that DOWN will never get matched so the AFTER MATCH SKIP TO DOWN cannot happen even though a complete match for the pattern is found. Therefore, the compiler throws an error to let you know that this code will not work:
</p>
<blockquote>
ORA-62514: AFTER MATCH SKIP TO variable is not bounded in the match found.<br>
62514. 00000 - ""AFTER MATCH SKIP TO variable is not bounded in the match found.""<br>
*Cause: AFTER MATCH SKIP TO variable was not bound in the match found due<br>
to pattern operators such as |, *, ?, and so on.<br>
*Action: Modify the query and retry the operation<br>
</blockquote>",01-NOV-16 03.49.15.765444000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.56.09.250805000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613461403796574823533339375446,92046253613454150241657135758291138390,"Using LISTAGG",50,"<p>In this first example we will use the pre-12.2 syntax to show what happens if <strong>LISTAGG</strong> tries to return a value that exceeds 4K characters, or in this particular case the 32J limit.</p>
<p>We will group the data by department number and use <strong>LISTAGG</strong> to create a concatenated list of values based on the column ""longname"" that we created in the previous step:</p>
<code>SELECT
 deptno,
 LISTAGG(longname, '; ') WITHIN GROUP (ORDER BY EMPNO)
FROM empln
GROUP BY deptno;</code>
<p>Notice that we get the standard overflow error:</p>
<blockquote><strong>ORA-01489</strong>: result of string concatenation is too long</blockquote>",04-JAN-17 05.57.54.429007000 AM,"SYS",17-MAR-17 09.04.45.272202000 AM,"KEITH.LAKER@ORACLE.COM"
92046253613462612722394438162514081622,92046253613454150241657135758291138390,"Stopping ORA-01489 errors",60,"<p>With 12.2, <strong>LISTAGG</strong> has an enhanced syntax which allows developers to control what happens if we exceed the 4K character limit or in this case the 32K character limit. Aw with previous versions an error can returned and this is the default behaviour.<p>
<p>However, we can now also trap the error using the new <strong>ON OVERFLOW TRUNCATE</strong> syntax</p>
<p><strong><em>Note: </em></strong>In the code below I have chopped-up the string returned by LISTAGG so that you don't have to scroll across 32,000 characters to see the effect on the new keywords</p>
<code>SELECT
 deptno,
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE) WITHIN GROUP (ORDER BY EMPNO), 1, 50) AS ""Starts..."",
 '........',
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE) WITHIN GROUP (ORDER BY EMPNO), -50) AS ""...Ends""
FROM empln
WHERE deptno = 20
GROUP BY deptno;</code>
<p>Note that where the contents of a row is truncated we add three dots to indicate that truncation has occurred and there is a count of the number of values missing (truncated) from the list.
</p>",04-JAN-17 05.57.54.443689000 AM,"SYS",17-MAR-17 09.01.10.879480000 AM,"KEITH.LAKER@ORACLE.COM"
92046253613463821648214052791688787798,92046253613454150241657135758291138390,"Changing the truncation indicator",70,"<p>By default when a truncation occurs we indicate this using three dots at the end of the string (...). It is possible to change this by adding your own truncation characters after the <strong>ON OVERFLOW TRUNCATE</strong> keywords. Here are replacing the three dots with an exclamation mark (!).
</p>
<code>SELECT
 deptno,
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE '!') WITHIN GROUP (ORDER BY EMPNO), 1, 50) AS ""Starts..."",
 '........',
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE '!') WITHIN GROUP (ORDER BY EMPNO), -50) AS ""...Ends""
FROM empln
WHERE deptno = 20
GROUP BY deptno;</code>",04-JAN-17 05.57.54.450511000 AM,"SYS",17-MAR-17 09.00.57.028440000 AM,"KEITH.LAKER@ORACLE.COM"
92046253613465030574033667420863493974,92046253613454150241657135758291138390,"Removing the count of missing values",80,"<p>The final set of new keywords control whether the count of truncated values is shown or not shown. By default, the count is shown and this can be explicitly stated using the <strong>WITH COUNT</strong> syntax.</p>
<p>Similarly, it is possible to remove the count by using the <strong>WITHOUT COUNT</strong> keywords after the on overflow... keywords.
</p>
<code>SELECT
 deptno,
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE WITHOUT COUNT) WITHIN GROUP (ORDER BY EMPNO), 1, 50) AS ""Starts..."",
 '........',
 SUBSTR(LISTAGG(longname, ';' ON OVERFLOW TRUNCATE WITHOUT COUNT) WITHIN GROUP (ORDER BY EMPNO), -50) AS ""...Ends""
FROM empln
WHERE deptno = 20
GROUP BY deptno;</code>",04-JAN-17 05.57.54.451182000 AM,"SYS",17-MAR-17 09.00.46.004111000 AM,"KEITH.LAKER@ORACLE.COM"
83301768981016621074042122300694350517,83301768980930787340849483629290212021,"What does CLASSIFIER() do?",12,"<p>To get you started here is a quick definition of CLASSIFIER(</p>
<p>
<blockquote>
Along with knowing which MATCH_NUMBER you are seeing, you may want to know which component of a pattern applies to a specific row. This is done using the CLASSIFIER() function. The classifier of a row is the pattern variable that the row is mapped to by a row pattern match. The CLASSIFIER() function returns a character string whose value is the name of the pattern variable defined within the PATTERN clause.
</blockquote>
</p>",12-OCT-16 01.05.27.448349000 PM,"KEITH.LAKER@ORACLE.COM",12-OCT-16 01.05.27.448405000 PM,"KEITH.LAKER@ORACLE.COM"
83301768981135095804364355959815555765,83301768980930787340849483629290212021,"Reset my tutorial environment",15,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE ticker PURGE;
</code>",12-OCT-16 01.07.28.018066000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.47.33.827312000 PM,"SHARON.KENNEDY@ORACLE.COM"
83301768981136304730183970588990261941,83301768980930787340849483629290212021,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);

INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);

INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);

commit;

END;
</code>",12-OCT-16 01.07.54.988117000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.47.53.114206000 PM,"SHARON.KENNEDY@ORACLE.COM"
83301768981314016825667321077672069813,83301768980930787340849483629290212021,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>
<p>You can view the full data set using the following code:</p>
<code>SELECT * FROM ticker ORDER BY symbol, tstamp;</code>
<br>",12-OCT-16 01.08.36.999522000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.48.10.927762000 PM,"SHARON.KENNEDY@ORACLE.COM"
83301768981362373858451906244660316853,83301768980930787340849483629290212021,"Finding W-Patterns",40,"<p>First letâ€™s start by using our normal stock ticker data set and searching for W-shaped patterns. Here is the code:</p>
<code>SELECT symbol, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z
  ONE ROW PER MATCH
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>For the symbol OSCORP we have matched two W-shapes: the first starts on 03-APR-11 and ends on 08-APR-11 and the second starts on 12-APR-11 and ends on 16-APR-11.</p>",12-OCT-16 01.09.52.183247000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.48.24.484053000 PM,"SHARON.KENNEDY@ORACLE.COM"
83301768981515907437542964149848001205,83301768980930787340849483629290212021,"Getting detailed feedback",50,"<h3>Line-by-line analysis of how MATCH_RECOGNIZE is working</h3>
<p>Let's switch to displaying the more detailed report so that we can understand how the pattern is being applied to our ticker data set. First let's use the MATCH_NUMBER() function to see how many occurrences of our W-pattern are contained within the data set for OSCORP:
</p>  
<code>SELECT symbol, tstamp, price, mn, first_x, last_z 
FROM ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol ORDER BY tstamp
  MEASURES FIRST(x.tstamp) AS first_x,
           LAST(z.tstamp) AS last_z,
           MATCH_NUMBER() AS mn
  ALL ROWS PER MATCH OMIT EMPTY MATCHES
  PATTERN (X+ Y+ W+ Z+)
  DEFINE X AS (price < PREV(price)),
         Y AS (price > PREV(price)), 
         W AS (price < PREV(price)),
         Z AS (price > PREV(price)))
WHERE symbol='OSCORP';
</code>
<p>What you should spot is that the MATCH_NUMBER() column, mn, contains a contiguous set of numbers. Our first W-shape which starts on 03-Apr-11 and the second W-shape starts on 12-Apr-11</p>",12-OCT-16 01.12.51.146344000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.52.37.914251000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613477119832229813712610555734,92046253613467448425672896679212906326,"Control Using Database Parameters",70,"<p>
Transparent replacement of exact percentile and median calculations is possible using database parameters.
</p>
<p>
The umbrella parameter <i>approx_for_aggregation</i> controls both approximate count distinct and the approximate percentile functions. Setting to TRUE will cause the exact count distinct and percentile calculations to be replaced with the equivalent approximate functions.
</p>
<p>
Finer control is achieved using <i>approx_for_count_distinct</i> (demonstrated above) and <i>approx_for_percentile</i>.
<p>
The parameter <i>approx_for_percentile</i> can be used to finely control what exact percentile functions are replaced and whether to use the deterministic or non-deterministic algorithm. The choices for this parameter are: all deterministic, percentile_disc deterministic, percentile_cont deterministic, all, percentile_disc, percentile_cont and none.
</p>
<code>
REM Set ""all deterministic"" so that all percentile calculations are replaced with approximate, deterministic calculation...

alter session set approx_for_percentile = 'all deterministic';

REM This query uses MEDIAN, which will be replaced with the approx deterministic calculation...

select median(volume) from approxt where state = 'CA';

REM Check the execution plan...

explain plan for select median(volume) from approxt where state = 'CA';

select * from table(dbms_xplan.display);

REM Back to the default

alter session set approx_for_percentile = 'none';
</code>",04-JAN-17 05.57.55.813684000 AM,"SYS",04-JAN-17 05.57.55.813740000 AM,"SYS"
92046253613478328758049428341785261910,92046253613467448425672896679212906326,"New Aggregation Capabilities",80,"<p>
Approximate aggregations can be materialized and converted back to approximate count
distinct and approximate percentiles. In this way, a single table can be used for queries that require approximate answers for aggregations sliced-and-diced over different dimensions.
</p>
<code>
declare
  ORA_00942 exception; pragma Exception_Init(ORA_00942, -00942);
begin
  execute immediate 'drop table agg';
exception when ORA_00942 then null;
end;
/

REM Materialize the aggregations in a table called <i>agg</i>.
REM The <i>detail</i> column stores a binary object that encodes the distinct value information
REM for interpretation by the <i>approx_percentile_agg</i> function for different aggregations.

create table agg as
select state,
       county,
       approx_percentile_detail(volume) detail 
from   approxt group by state,county;

REM Look at the non-readable content of the <i>detail</i> column...
select county, utl_raw.cast_to_raw( dbms_lob.substr( detail, 128, 1 ) ) as detail
from agg
fetch first 5 rows only;

REM Aggregation by state.
REM The table was created using an aggregation by state and 
REM county but below, the ""agg"" function is able to calculate
REM the approximate percentile when aggregating by state alone.

REM First, here is the exact result from the base table...
select state,median(volume) 
from approxt 
group by state;

REM This is the approximate result generated using the intermediate <i>agg</i> table. 

select state, to_approx_percentile(approx_percentile_agg(detail),0.5) 
from agg 
group by state;

REM This is the foundation for materialized view rewrite functionality (coming up next).
</code>",04-JAN-17 05.57.55.814455000 AM,"SYS",26-JAN-17 01.21.42.066217000 PM,"NIGEL.BAYLISS@ORACLE.COM"
92046253613479537683869042970959968086,92046253613467448425672896679212906326,"New Materialized View Support",90,"<p>
The approximate query functions can be used with materialized views for transparent query re-write.
</p>
<code>
REM Clean up...

declare
  ORA_12003 exception; pragma Exception_Init(ORA_12003, -12003);
begin
  execute immediate 'drop materialized view count_d_mview';
exception when ORA_12003 then null;
end;
/

REM Create a materialized view (MV) for APPROX PERCENTILE

create materialized view pctl_mview enable query rewrite as
select state, county, approx_percentile_detail(volume) detail
from   approxt
group by state, county;

REM Gather stats...

exec dbms_stats.gather_table_stats(ownname=>null,tabname=>'pctl_mview');

REM The following query re-writes to the materialized view, aggregating by state.
REM Note how Approx Median can use the Approx Percentile that was defined in the MV.

select state, approx_median(volume) 
from approxt
where state = 'CA'
group by state;

REM Take a look at the plan...

explain plan for 
select state, approx_median(volume) 
from approxt
where state = 'CA'
group by state;

SELECT * FROM TABLE(dbms_xplan.display);
</code>",04-JAN-17 05.57.55.815091000 AM,"SYS",04-JAN-17 05.57.55.815148000 AM,"SYS"
92046253613480746609688657600134674262,92046253613467448425672896679212906326,"MV and Approx Count Distinct",100,"<p>
Query re-write can be used with approximate count distinct too.
</p>
<code>
declare
  ORA_12003 exception; pragma Exception_Init(ORA_12003, -12003);
begin
  execute immediate 'drop materialized view count_d_mview';
exception when ORA_12003 then null;
end;
/

REM Create an MV for approximate count distinct

create materialized view count_d_mview enable query rewrite as
select state, county, approx_count_distinct_detail(volume) detail
from  approxt
group by state, county;

REM Gather stats...

exec dbms_stats.gather_table_stats(ownname=>null,tabname=>'count_d_mview');

REM Query re-writes to MV aggregating by state and county...

select state, county, approx_count_distinct(volume) 
from  approxt
where state = 'CA'
group by state,county;

REM check the execution plan...

explain plan for select state, county, approx_count_distinct(volume) 
from  approxt
where state = 'CA'
group 
by state,county;

SELECT * FROM TABLE(dbms_xplan.display);

REM Query can also re-write to the MV, aggregating by state alone...

select state, approx_count_distinct(volume) 
from approxt
where state = 'CA'
group by state;

REM Check the execution plan...

explain plan for
select state, approx_count_distinct(volume) 
from approxt
where state = 'CA'
group by state;

SELECT * FROM TABLE(dbms_xplan.display);
</code>",04-JAN-17 05.57.55.815778000 AM,"SYS",04-JAN-17 05.57.55.815835000 AM,"SYS"
92046253613481955535508272229309380438,92046253613467448425672896679212906326,"Re-write Exact Queries",110,"<p>
This example is very similar to the previous step, but this time it demonstrates how it is possible to re-write an exact query transparently to use an approximate calculation defined in a  materialized view. This is a very powerful feature: it is possible to achieve huge performance benefits without changing existing queries.
</p>
<code>
declare
  ORA_12003 exception; pragma Exception_Init(ORA_12003, -12003);
begin
  execute immediate 'drop materialized view count_d_mview';
exception when ORA_12003 then null;
end;
/

REM Create an MV for approximate count distinct

create materialized view count_d_mview enable query rewrite as
select state, county, approx_count_distinct_detail(volume) detail
from  approxt
group by state, county;

REM Gather stats...

exec dbms_stats.gather_table_stats(ownname=>null,tabname=>'count_d_mview');

REM Alter the session to enable approx for count distinct...

alter session set approx_for_count_distinct = TRUE ;

REM Query re-writes to MV aggregating by state and county...

select state, county, count(distinct volume) 
from  approxt
where state = 'CA'
group by state,county;

REM check the execution plan...

explain plan for 
select state, county, count(distinct volume) 
from  approxt
where state = 'CA'
group by state,county;

SELECT * FROM TABLE(dbms_xplan.display);

REM Query can also re-write to the MV, aggregating by state alone...

select state, count(distinct volume) 
from approxt
where state = 'CA'
group by state;

REM Check the execution plan...

explain plan for
select state, count(distinct volume) 
from approxt
where state = 'CA'
group by state;

SELECT * FROM TABLE(dbms_xplan.display);

REM Back to ""normal""...

alter session set approx_for_count_distinct = FALSE ;

</code>",04-JAN-17 05.57.55.816587000 AM,"SYS",04-JAN-17 05.57.55.816644000 AM,"SYS"
92046253613483164461327886858484086614,92046253613467448425672896679212906326,"Incremental Refresh",120,"<p>
Materialized views with approximate aggregations can be incrementally refreshed
</p>
<code>
REM Clean up...

declare
  ORA_12003 exception; pragma Exception_Init(ORA_12003, -12003);
begin
  execute immediate 'drop materialized view count_d_mview';
exception when ORA_12003 then null;
end;
/

declare
  ORA_12002 exception; pragma Exception_Init(ORA_12002, -12002);
begin
  execute immediate 'drop materialized view log on approxt';
exception when ORA_12002 then null;
end;
/

REM Create MV log...

create materialized view log on approxt
with sequence, rowid (id,volume,state,county)
including new values;

REM Create MV...

create materialized view count_d_mview 
refresh fast
enable query rewrite 
as
select state, county, approx_count_distinct_detail(volume) detail
from approxt
group by state, county;


REM Gather stats...

exec dbms_stats.gather_table_stats(ownname=>null,tabname=>'count_d_mview');

REM Get status of materialized view...

select mview_name, staleness 
from user_mviews where mview_name='COUNT_D_MVIEW';

REM Do some dml...


insert into approxt
        select level+50000 id,  floor(dbms_random.value()*1000) volume, 'FL' , 'COUNTY5' 
                from dual
                connect by level <= 100;
                
commit;

REM Check status of materialized view...

select mview_name, staleness 
from user_mviews where mview_name='COUNT_D_MVIEW';

REM Do an incremental refresh...

begin
   DBMS_MVIEW.REFRESH('count_d_mview', method => 'F');
end;
/

REM Check status of materialized view...

select mview_name, staleness 
from user_mviews where mview_name='COUNT_D_MVIEW';
</code>",04-JAN-17 05.57.55.817231000 AM,"SYS",28-JUN-17 12.42.19.239794000 PM,"NIGEL.BAYLISS@ORACLE.COM"
92046253613496462645343647779405854550,92046253613491626942065189262707029846,"ORDER BY Property",40,"<p>By default, values are sorted alphabetically within level by the KEY attribute value.  This can be seen in the TIME_HIER hierarchy with the following query.</p>

<p>Select from TIME_HIER ordering by HIER_ORDER.  Note the sort order of months within quarters.</p>

<code>SELECT year_id,
  quarter_id,
  month_id,
  member_name,
  hier_order
FROM time_hier
ORDER BY hier_order;</code>

<p>Sorting time periods alphabetically is probably not how a user would like to display data in a report and is not appropriate for time series calculations.  For example, the LAG of Jan-12 should Dec-11 rather than Feb-11.</p>

<p>The AV.TIME_DIM table includes _END_DATE columns that can be used to sort time values.</p>

<p>Select from the TIME_DIM table ordering by YEAR_END_DATE.</p>

<code>SELECT distinct year_id,
 year_name,
 year_end_date
FROM av.time_dim
ORDER BY year_end_date;</code>

<p>Select from the TIME_DIM table ordering by QUARTER_END_DATE.</p>

<code>SELECT distinct quarter_id,
  quarter_name,
  quarter_end_date
FROM av.time_dim
ORDER BY quarter_end_date;</code>

<p>Select from the TIME_DIM table ordering by MONTH_END_DATE.</p>

<code>SELECT distinct month_id,
  month_name,
  month_end_date
FROM av.time_dim
ORDER BY month_end_date;</code>

<p>To use the _END_DATE columns to sort time values:</p>

<ul>
<li>Add the _END_DATE attributes the attribute list.</li>
<li>Add the _END_DATE columns to the ORDER BY property of each level.</li>
</ul>

<p>Create the attribute dimension.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim
ATTRIBUTES
 (year_id,
  year_name,
  year_end_date,
  quarter_id,
  quarter_name,
  quarter_end_date,
  month_id,
  month_name,
  month_long_name,
  month_end_date)
LEVEL MONTH
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  ORDER BY month_end_date
  DETERMINES (quarter_id)
LEVEL QUARTER
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY quarter_end_date
  DETERMINES (year_id)
LEVEL YEAR
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY year_end_date;</code>

<p>Select from TIME_HIER ordering by HIER_ORDER.  Note that all time periods are now correctly sorted.</p>

<code>SELECT year_id,
  year_name,
  quarter_id,
  quarter_name,
  month_id,
  member_name,
  hier_order
FROM time_hier
ORDER BY hier_order;</code>

<p>The ORDER BY property for each level could also be set using only the MONTH_END_DATE column by using the the MIN or MAX operator (either will work because the sort order is the same using each method).</p>

<Create the attribute dimension using ORDER BY MAX(month_end_date) in the QUARTER and YEAR levels.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim
ATTRIBUTES
 (year_id,
  year_name,
  --year_end_date
  quarter_id,
  quarter_name,
  -- quarter_end_date,
  month_id,
  month_name,
  month_long_name,
  month_end_date)
LEVEL MONTH
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  ORDER BY month_end_date
  DETERMINES (quarter_id)
LEVEL QUARTER
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY MAX month_end_date
  DETERMINES (year_id)
LEVEL YEAR
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY MAX month_end_date;</code>

<p>Query the TIME_HIER hierarchy and note that the values are correctly sorted.</p>

<code>SELECT year_id,
  year_name,
  quarter_id,
  quarter_name,
  month_id,
  member_name,
  hier_order
FROM time_hier
ORDER BY hier_order;</code>",04-JAN-17 05.57.58.830781000 AM,"SYS",16-AUG-17 08.37.28.791920000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613497671571163262408580560726,92046253613491626942065189262707029846,"Columns of the TIME_HIER Heararchy (and more about DETERMINES)",50,"<p>All columns of the TIME_HIER hierarchy now return data.  Select the key attribute columns.</p>

<code>SELECT year_id,
  quarter_id,
  month_id
FROM time_hier
ORDER BY hier_order;</code>

<p>Note that some columns return NULL values.  This is because the hierarchy returns rows for values at each level of the hierarchy.  Rows with lower level data will return data for attributes in the current level or higher levels providing that the attributes at the higher level are determined by the lower level.  For example, month level rows can return data for attributes at the quarter and year levels.  This is more easily seen by adding the LEVEL_NAME and MEMBER_UNIQUE_NAME columns to the query.</p>

<code>SELECT year_id,
  quarter_id,
  month_id,
  level_name,
  member_unique_name
FROM time_hier
ORDER BY hier_order;</code>

<p>Select the MEMBER_NAME, MEMBER_CAPTION and MEMBER_DECRIPTION columns.  The values returned for these columns are always the values at the level of the key attribute value (as seen in MEMBER_UNIQUE_NAME).</p>

<code>SELECT member_name,
  member_caption,
  member_description,
  month_name,
  level_name,
  member_unique_name
FROM time_hier
ORDER BY hier_order; </code>

<p>The hierarchy includes columns for each of the attributes used by the MEMBER NAME, MEMBER DESCRIPTION and MEMBER CAPTION properties.  Like the key attributes, values are returned at and above the level of the row.</p>

<code>SELECT year_name,
  quarter_name,
  month_name,
  level_name,
  member_unique_name
FROM time_hier
ORDER BY hier_order;</code>

<p>The last CREATE ATTRIBUTE DIMENSION TIME_ATTR_DIM statement used the MONTH_END_DATE attribute for the ORDER BY property of all levels.  Can that attribute by queried from the TIME_HIER hierarchy?  Give it a try.</p>

<code>SELECT year_name,
  quarter_name,
  month_name,
  month_end_date,
  level_name,
  member_unique_name
FROM time_hier
ORDER BY hier_order;</code>

<p>The MONTH_END_DATE column does not exist in the hierarchy because it is not a determined attribute.  Because the attributes used by the MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION properties are assumed to have a 1:1 relationship with the key attribute values, they are automatically considered to be determined attributes.  It is not necessary to list these attributes in the DETERMINES property of a level.</p>

<p>The ORDER BY property can use an attribute that does not have a 1:1 relationship with the key attribute value if either the MIN or MAX operator is used.  Therefore, they are not automatically considered to be determined attributes. For example the YEAR level can use the MAX of the MONTH_END_DATE attribute even though there are 12 MONTH_END_DATE values for each year.</p>

<code>SELECT distinct year_id,
  year_name,
  month_end_date
FROM av.time_dim
ORDER BY month_end_date;</code>

<p>Setting ORDER BY to MAX(month_end_date) reduces the number of values for each year to 1 as can be see in the following query.</p>

<code>SELECT year_id,
  year_name,
  MAX(month_end_date)
FROM av.time_dim
GROUP BY year_id,
  year_name
ORDER BY MAX(month_end_date);</code>

<p>MONTH_END_DATE could be used as a determined attribute at the MONTH level because there is a 1:1 relationship between MONTH_ID and MONTH_END_DATE, but it cannot be used as a determined attribute at the QUARTER or YEAR levels because there is not a 1:1 relationship with the key attributes of those levels.</p>

<p>The following CREATE statement adds the MONTH_END_DATE column to hierarchy views by including MONTH_END_DATE in the DETERMINES property of the MONTH level.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim
ATTRIBUTES
 (year_id,
  year_name,
  --year_end_date
  quarter_id,
  quarter_name,
  -- quarter_end_date,
  month_id,
  month_name,
  month_long_name,
  month_end_date)
LEVEL MONTH
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  ORDER BY month_end_date
  DETERMINES (quarter_id, month_end_date)
LEVEL QUARTER
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY MAX month_end_date
  DETERMINES (year_id)
LEVEL YEAR
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY MAX month_end_date;</code>

<p>Run the query selecting MONTH_END_DATE again.</p>

<code>SELECT year_name,
  quarter_name,
  month_name,
  month_end_date,
  level_name,
  member_unique_name
FROM time_hier
ORDER BY hier_order;</code>

<p>Attributes that are created by the hierarchy are referred to as <i>hierarchical attributes</i>.  The following query selects each of the hierarchical attribute columns of the hierarchy.</p>

<code>SELECT member_unique_name,
  member_name,
  member_caption,
  member_description,
  level_name,
  hier_order,
  depth,
  is_leaf,
  parent_level_name,
  parent_unique_name
 FROM time_hier
 ORDER BY hier_order;</code>",04-JAN-17 05.57.58.832174000 AM,"SYS",18-MAY-20 03.56.16.061302000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613498880496982877037755266902,92046253613491626942065189262707029846,"Creating an Analytic View",60,"<p>An analytic view requires at least 1 hierarchy and a fact table with at least 1 measure 
column.  Keys of the fact table should join to the key attribute of the lowest level in the attribute dimension (which should be the primary key of the dimension table).</p>

<p>The first analytic view will use the TIME_HIER hierarchy and the AV.SALES_FACT table.  It will contain a single measure, SALES.</p>

<p>Some notes about the analytic view object:</p>

<ul>

<li>The USING clause identifies the fact table (or view).  A single table name or view name may be provided.</li>

<li>Joins and hierarchies are specified in the DIMENSION BY clause.</li>

<ul>

<li>Include the attribute dimensions of any hierarchies that will be used with the analytic view.</li>

<li>The KEY property identifies the join key of the fact table. The KEY property references an attribute, not a column.  If you have not aliased attributes in the attribute dimension, the attribute and column names are the same.</li>

<li>The REFERENCES property identifies the key attribute in the attribute dimension that the fact table joins to.  This should be the key attribute that is derived from the primary key of the dimension table.  In the case of a flat/denormalized fact table use REFERENCES DISTINCT.</li>

<li>Each hierarchy that will be used with the analytic view is listed in the HIERARCHIES parameter.  Identify 1 hierarchy as the DEFAULT hierarchy.</li>

</ul>

<li>Measures are listed in the MEASURES clause.</li>

<ul>

<li>Use the FACT keyword to specify that the measure maps to a column in the fact table in the form <i>measure name</i> FACT <i>fact table column.</i></li>

<li>Calculated measures are defined in the MEASURES clause.  You will see that a later module.</li>

</ul>

<p>Create the analytic view.<p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact                   -- Refers to the SALES_FACT table.
DIMENSION BY                          -- List of attribute dimensions
  (time_attr_dim                      -- TIME_ATTR_DIM attribute dimension
    KEY month_id REFERENCES month_id  -- Dimension KEY joins to fact column.
    HIERARCHIES (                     -- List of hierarchies from the attar dim.
      time_hier DEFAULT))
MEASURES                              -- List of measures
 (sales FACT sales)                   -- SALES measure references SALES column.
DEFAULT MEASURE SALES;</code>

<p>You can see that the analytic view has been created with the following query.</p>

<code>SELECT * FROM user_analytic_views;</code>

<p>Because analytic views return both detail and aggregate level data across all hierarchies in the query it is very likely that the analytic view has to potential to return very large numbers of rows.  Therefore, nearly every query of an analytic view should include filters to limit the number of rows returned.  With only 1 hierarchy that returns only 86 rows, the SALES_AV analytic view is the exception to this rule.  (This is probably the only time you will SELECT * from an analytic view!)</p>

<code>SELECT *
FROM sales_av HIERARCHIES(time_hier)
ORDER BY time_hier.hier_order;</code>

<p>Note that the analytic view includes all the columns of the TIME_HIER hierarchy and SALES at detail and aggregate levels.</p>

<p>Select sales at the year level.</p>

<code>SELECT member_name,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR';</code>

<p>Select sales at the quarter level.</p>

<code>SELECT year_name,
  member_name,
  sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'QUARTER';</code>

<p>Select sales for months in year CY2015.</p>

<code>SELECT member_name,
  sales
FROM sales_av HIERARCHIES(time_hier)
WHERE time_hier.level_name = 'MONTH'
AND time_hier.year_name = 'CY2015'
ORDER BY time_hier.hier_order;</code>

<p>Note that aggregation operators, GROUP BY and joins are not included in these queries.  They are not needed because these are part of the definition of the analytic view and the analytic view returns both detail and aggregate level data.</p>",04-JAN-17 05.57.58.835489000 AM,"SYS",18-AUG-20 09.01.58.311117000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613500089422802491666929973078,92046253613491626942065189262707029846,"Adding a Fact Table Measure to the Analytic View",70,"<p>To add additional measures from the fact table, simply include the measure in the MEASURES list.</p>

<p>Add the UNITS measure to the SALES_AV analytic view.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT))
MEASURES
 (sales FACT sales,
  units FACT units)
DEFAULT MEASURE SALES;</code>

<p>Measures that are derived from the fact table can be seen in the following query.</p>

<code>SELECT * FROM user_analytic_view_base_meas;</code>

<p>View the SALES and UNITS data at the YEAR level.</p>

<code>SELECT time_hier.member_name as TIME,
 sales,
 units
FROM
 sales_av HIERARCHIES(time_hier)
WHERE time_hier.level_name = 'YEAR';</code>",04-JAN-17 05.57.58.836799000 AM,"SYS",16-AUG-17 08.37.55.508907000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613501298348622106296104679254,92046253613491626942065189262707029846,"Setting Aggregation Operators",80,"<p>The analytic views you have created so far have not included aggregation functions.  In this case the analytic view defaults to using SUM.  This can be seen by running the following query.</p>

<code>SELECT analytic_view_name,
  default_aggr
FROM user_analytic_views;
</code>
<p>The measure definitions do not include aggregation functions.  As a result, they default to the aggregation operator of the analytic view.</p>

<code>SELECT a.analytic_view_name,
  a.default_aggr,
  b.measure_name,
  b.aggr_function
FROM user_analytic_views a,
  user_analytic_view_base_meas b
WHERE a.analytic_view_name = b.analytic_view_name;</code>

<p>You can set the default aggregation operator of the analytic view using the DEFAULT AGGREGATE BY clause.  Change the default aggregation operator to AVG.  (Note that the DEFAULT AGGREGATE BY clause follows the DEFAULT MEASURE clause.)</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT))
MEASURES
 (sales FACT sales,
  units FACT units)
DEFAULT MEASURE SALES
DEFAULT AGGREGATE BY AVG;</code>

<p>Check the default aggregation operator.</p>

<code>SELECT analytic_view_name,
  default_aggr
FROM user_analytic_views;</code>

<p>Select sales and units at the year_level.</p>

<code>SELECT member_name,
  sales AS avg_sales,
  units AS avg_units
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR';</code>

<p>Aggregation functions can also be set for each measure.  The next analytic view will set the default aggregation operator to SUM and add the AVG_SALES and AVG_UNITS measures.  Also, note that more than one measure can be created using a single fact column.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT))
MEASURES
 (sales FACT sales,
  avg_sales FACT sales AGGREGATE BY AVG,
  units FACT units,
  avg_units FACT units AGGREGATE BY AVG 
  )
DEFAULT MEASURE SALES
DEFAULT AGGREGATE BY SUM;</code>

<p>Select both the sums and the averages. Note that the average is the average of the lowest levels of the hierarachy. In the example, the average of days.</p>

<code>SELECT member_name,
  sales              AS sum_sales,
  units              AS sum_units,
  ROUND(avg_sales,0) AS avg_sales,
  ROUND(avg_units,0) AS avg_units
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR';</code>

<p>Create the analytic view with sales measures creating using a variety of aggregation functions.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT))
MEASURES
 (sales FACT sales,
  avg_sales FACT sales AGGREGATE BY AVG,
  count_sales FACT sales AGGREGATE BY COUNT,
  max_sales FACT sales AGGREGATE BY MAX,
  min_sales FACT sales AGGREGATE BY MIN,
  stddev_sales FACT sales AGGREGATE BY STDDEV,
  variance_sales FACT sales AGGREGATE BY VARIANCE,
  units FACT units,
  avg_units FACT units AGGREGATE BY AVG 
  )
DEFAULT MEASURE SALES
DEFAULT AGGREGATE BY SUM;</code>

<p>The aggregation functions of the analytic view and the measures.</p>

<code>SELECT a.analytic_view_name,
  a.default_aggr,
  b.measure_name,
  b.aggr_function
FROM user_analytic_views a,
  user_analytic_view_base_meas b
WHERE a.analytic_view_name = b.analytic_view_name;</code>

<p>Select all of the sales measures.</p>

<code>SELECT time_hier.member_name,
  sales,
  ROUND(avg_sales,0) AS avg_sales,
  count_sales,
  max_sales,
  min_sales,
  ROUND(stddev_sales,0) AS stddev_sales,
  ROUND(variance_sales,0) AS variance_sales
FROM sales_av HIERARCHIES (time_hier)
WHERE time_hier.level_name = 'YEAR';</code>",04-JAN-17 05.57.58.839319000 AM,"SYS",16-AUG-17 08.38.38.206590000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613502507274441720925279385430,92046253613491626942065189262707029846,"Adding Hierarchies to the Analytic View",90,"<p>Most analytic views will use more than one hierarchy from one or more attribute dimensions.  In the case of the SALES_AV analytic view, start by creating attribute dimensions and hierarchies from the PRODUCT_DIM and GEOGRAPHY_DIM tables.</p>

<p>Create the PRODUCT_ATTR_DIM attribute dimension.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING av.product_dim 
ATTRIBUTES
 (department_id,
  department_name,
  category_id,
  category_name)
LEVEL DEPARTMENT
  KEY department_id
  MEMBER NAME department_name
  MEMBER CAPTION department_name
  ORDER BY department_name
LEVEL CATEGORY
  KEY category_id
  MEMBER NAME category_name
  MEMBER CAPTION category_name
  ORDER BY category_name
  DETERMINES(department_id)
ALL MEMBER NAME 'ALL PRODUCTS';</code>

<p>Create the PRODUCT_HIER hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY product_hier
USING product_attr_dim
 (CATEGORY
  CHILD OF department);</code>

<p>Select data from the PRODUCT_HIER hierarchy.</p>

<code>SELECT
  member_name,
  level_name,
  member_unique_name
FROM product_hier
ORDER BY hier_order;</code>

<p>Create the GEOGRAPHY_ATTR_DIM attribute dimension.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION geography_attr_dim
USING av.geography_dim
ATTRIBUTES
 (region_id,
  region_name,
  country_id,
  country_name,
  state_province_id,
  state_province_name)
LEVEL REGION
  KEY region_id
  MEMBER NAME region_name
  MEMBER CAPTION region_name
  ORDER BY region_name
LEVEL COUNTRY
  KEY country_id
  MEMBER NAME country_name
  MEMBER CAPTION country_name
  ORDER BY country_name
  DETERMINES(region_id)
LEVEL STATE_PROVINCE
  KEY state_province_id
  MEMBER NAME state_province_name
  MEMBER CAPTION state_province_name
  ORDER BY state_province_name
  DETERMINES(country_id)
ALL MEMBER NAME 'ALL CUSTOMERS';</code>

<p>Create the GEOGRAPHY_HIER hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY geography_hier
USING geography_attr_dim
 (state_province
  CHILD OF country
  CHILD OF region);</code>

<p>Select data from the GEOGRAPHY_HIER hierarchy.</p>

<code>SELECT
  member_name,
  level_name,
  member_unique_name
FROM geography_hier
ORDER BY hier_order;</code>

<p>Add the attribute dimensions and hierarchies to the DIMENSION BY list.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  units FACT units
  )
DEFAULT MEASURE SALES;</code>

<p>With only the TIME_HIER hierarchy in the HIERARCHIES parameter, the query returns rows for only time periods.</p>

<p>Select from the SALES_AV analytic view using only the TIME_HIER hierarchy.</p>

<code>SELECT time_hier.member_name as TIME,
 sales,
 units
FROM
 sales_av HIERARCHIES(time_hier)
WHERE time_hier.level_name in ('YEAR')
ORDER BY time_hier.hier_order;</code>

<p>To access rows for the products and geographies, add the PRODUCT_HIER and GEOGRAPHY_HIER hierarchies to the HIERARCHIES parameter.  Note that as more hierarchies are added it is important to use filters in the query to reduce the potential of returning very large numbers of rows.</p>

<p>Select from the SALES_AV analytic view with all three hierarchies.</p>

<code>SELECT time_hier.member_name AS Time,
 product_hier.member_name AS Product,
 geography_hier.member_name AS Geography,
 sales,
 units
FROM
 sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE time_hier.level_name in ('YEAR')
  AND product_hier.level_name in ('DEPARTMENT')
  AND geography_hier.level_name in ('REGION')
ORDER BY time_hier.hier_order,
  product_hier.hier_order,
  geography_hier.hier_order;</code>",04-JAN-17 05.57.58.860319000 AM,"SYS",16-AUG-17 08.39.25.669000000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613503716200261335554454091606,92046253613491626942065189262707029846,"Adding a Calculated Measure",100,"<p>Calculated measures are added to the MEASURES list using expressions. Expressions reference elements of the analytic view rather than columns.  SQL single row expressions can be used in addition to analytic view expressions.</p>

<p>Add the SALES_YEAR_AGO and SALES_PCT_CHG_YEAR_AGO calculated measures to the analytic view.  Note that ROUND (a SQL single row function) is used in the SALES_PCT_CHG_YEAR_AGO expression.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_pct_chg_year_ago AS (ROUND(LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year),2)),
  units FACT units
  )
DEFAULT MEASURE SALES;</code>

<p>The SALES and SALES_PCT_CHG_YEAR_AGO measures can be queried by including them in the SELECT list.</p>

<code>SELECT time_hier.member_name AS Time,
 product_hier.member_name AS Product,
 geography_hier.member_name AS Geography,
 sales,
 sales_year_ago,
 sales_pct_chg_year_ago
FROM
 sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE time_hier.level_name in ('YEAR')
  AND product_hier.level_name in ('DEPARTMENT')
  AND geography_hier.level_name in ('REGION')
  AND time_hier.member_name in ('CY2014','CY2015')
ORDER BY time_hier.hier_order,
  product_hier.hier_order,
  geography_hier.hier_order;</code>",04-JAN-17 05.57.58.861737000 AM,"SYS",16-AUG-17 08.39.48.567891000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613506134051900564812803503958,92046253613504925126080950183628797782,"Introduction",10,"<p>This tutorial creates an analytic view and supporting objects using data in the Sales History (SH) schema.  The analytic view uses 6 hierarchies, 2 measures from the fact table and includes a number of calculated measures.</p>",04-JAN-17 05.58.00.124392000 AM,"SYS",04-JAN-17 05.58.00.124449000 AM,"SYS"
92046253613507342977720179441978210134,92046253613504925126080950183628797782,"Time Attribute Dimension and Hierarchies",20,"<p> The TIMES table contains data that can suport both a calendar year and fiscal year hierarchies.</p>

<p>View the data in the TIMES table.</p>

<code>SELECT * FROM sh.times WHERE rownum <= 30;</code>

<p>Columns selected in the following query can be used to build the calendar year hierarchy.  Note that:</p>

<ul>

<li>TIME_ID contains unique values within CALENDAR_MONTH_DESC.</li>

<li>CALENDAR_MONTH_DESC contains unique values within CALENDAR_QUARTER_DESC.</li>

<li>CALENDAR_QUARTER_DESC contains unique values within CALENDAR_YEAR.</li>

<li>The columns in the ORDER BY clause can be used to sort values in chronological order.</li>

</ul> 

<code>SELECT
  time_id,
  calendar_month_desc,
  end_of_cal_month,
  calendar_quarter_desc,
  end_of_cal_quarter,
  calendar_year
 FROM
  sh.times
 ORDER BY
  calendar_year,
  end_of_cal_quarter,
  end_of_cal_month,
  time_id;</code>

<p>Columns in the following query can be used to create a fiscal year hierarchy.</p>

<code>SELECT
  time_id,
  fiscal_month_desc,
  end_of_fis_month,
  fiscal_quarter_desc,
  end_of_fis_quarter,
  fiscal_year
 FROM
  sh.times
 ORDER BY
  fiscal_year,
  end_of_fis_quarter,
  end_of_fis_month,
  time_id;</code>

<p><b>Attribute Dimension for Time</b></p>

<p>An attribute dimension is a metadata object that contains attributes and levels created from columns in a dimension table. Hierarchy objects reference attribute dimensions and inherit attributes and levels from them.  The attribute dimension allows you to create attributes and levels once and reuse them in any number of hierarchies.</p>

<p>An attribute dimension must have at least one attribute and one level.</p>

<p>An attribute references a column in a table (or view) and includes metadata which may be useful to applications.  An attribute may assume the name of a column or be aliased to create a different attribute name.</p>

<p>A level has the following properties:</p>

<ul>

<li>A name.  E.g., LEVEL day or LEVEL calendar_month.</li>

<li>A KEY which continues unique values in the level.
</li>

<li>MEMBER NAME, MEMBER CAPTION and MEMBER DESCRIPTION, all of which are typically used for text descriptors of the KEY value.</li>

<li>ORDER BY, which is used for sorting within the level.</li>

<li>DETERMINES, which may include one or more attributes that are determined by the KEY attribute of the level.  For example, for each value in TIME_ID, there are only one value in the CALENDAR_MONTH_DESC and FISCAL_MONTH_DESC attributes.  In most cases, the parent attribute of the level is listed in DETERMINES.</li>

</ul>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_times_attr_dim
USING sh.times
ATTRIBUTES
 (time_id,
  calendar_month_desc,
  end_of_cal_month, 
  calendar_quarter_desc,
  end_of_cal_quarter, 
  calendar_year,
  end_of_cal_year,
  fiscal_month_desc,
  end_of_fis_month, 
  fiscal_quarter_desc,
  end_of_fis_quarter,
  fiscal_year,
  end_of_fis_year
    )
  LEVEL day
     KEY time_id
     MEMBER NAME to_char(time_id)
     MEMBER CAPTION to_char(time_id)
     MEMBER DESCRIPTION to_char(time_id)
     ORDER BY time_id
     DETERMINES(calendar_month_desc,fiscal_month_desc)
  LEVEL calendar_month
    KEY calendar_month_desc
    MEMBER NAME calendar_month_desc
    MEMBER CAPTION calendar_month_desc
    MEMBER DESCRIPTION calendar_month_desc
    ORDER BY end_of_cal_month
    DETERMINES(end_of_cal_month,calendar_quarter_desc)
  LEVEL calendar_quarter
    KEY calendar_quarter_desc
    MEMBER NAME calendar_quarter_desc
    MEMBER CAPTION calendar_quarter_desc
    MEMBER DESCRIPTION calendar_quarter_desc
    ORDER BY end_of_cal_quarter
    DETERMINES(end_of_cal_quarter,calendar_year)
  LEVEL calendar_year
    KEY calendar_year
    MEMBER NAME TO_CHAR(calendar_year)
    MEMBER CAPTION TO_CHAR(calendar_year)
    MEMBER DESCRIPTION TO_CHAR(calendar_year)
    ORDER BY end_of_cal_year
    DETERMINES (end_of_cal_year)
  LEVEL fiscal_month
    KEY fiscal_month_desc
    MEMBER NAME fiscal_month_desc
    MEMBER CAPTION fiscal_month_desc
    MEMBER DESCRIPTION fiscal_month_desc
    ORDER BY end_of_fis_month
    DETERMINES(end_of_fis_month,fiscal_quarter_desc)
  LEVEL fiscal_quarter
    KEY fiscal_quarter_desc
    MEMBER NAME fiscal_quarter_desc
    MEMBER CAPTION fiscal_quarter_desc
    MEMBER DESCRIPTION fiscal_quarter_desc
    ORDER BY end_of_fis_quarter
    DETERMINES(end_of_fis_quarter,fiscal_year)
  LEVEL fiscal_year
    KEY fiscal_year
    MEMBER NAME TO_CHAR(fiscal_year)
    MEMBER CAPTION TO_CHAR(fiscal_year)
    MEMBER DESCRIPTION TO_CHAR(fiscal_year)
    ORDER BY end_of_fis_year
    DETERMINES (end_of_fis_year)
  ALL MEMBER NAME 'ALL YEARS';</code>

<p>The previous attribute dimension is fully functional, but it does not contain metadata to describe the elements of the attributes or levels.  The following CREATE ATTRIBUTE DIMENSION statement adds classifications.</p>

<p>CLASSIFICATION caption< and CLASSIFICATION description are typically used by applications as descriptive labels (rather than displaying the object name).  For example, the TIME_ID column is created as an attribute with the caption and description 'Day'.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_times_attr_dim
  CLASSIFICATION caption VALUE 'Time'
  CLASSIFICATION description VALUE 'Time'
USING sh.times
ATTRIBUTES
 (time_id
    CLASSIFICATION caption VALUE 'Day'
    CLASSIFICATION description VALUE 'Day',
  calendar_month_desc
    CLASSIFICATION caption VALUE 'Calendar Month'
    CLASSIFICATION description VALUE 'Calendar Month',
  end_of_cal_month
    CLASSIFICATION caption VALUE 'End of Calendar Month'
    CLASSIFICATION description VALUE 'End of Calendar Month', 
  calendar_quarter_desc
    CLASSIFICATION caption VALUE 'Calendar Quarter'
    CLASSIFICATION description VALUE 'Calendar Quarter',
  end_of_cal_quarter
    CLASSIFICATION caption VALUE 'End of Calendar Quarter'
    CLASSIFICATION description VALUE 'End of Calendar Quarter', 
  calendar_year
    CLASSIFICATION caption VALUE 'Calendar Year'
    CLASSIFICATION description VALUE 'Calendar Year',
  end_of_cal_year
    CLASSIFICATION caption VALUE 'End of Calendar Year'
    CLASSIFICATION description VALUE 'End of Calendar Year', 
  fiscal_month_desc
    CLASSIFICATION caption VALUE 'Fiscal Month'
    CLASSIFICATION description VALUE 'Fiscal Month',
  end_of_fis_month
    CLASSIFICATION caption VALUE 'End of Fiscal Month'
    CLASSIFICATION description VALUE 'End of Fiscal Month', 
  fiscal_quarter_desc
    CLASSIFICATION caption VALUE 'Fiscal Quarter'
    CLASSIFICATION description VALUE 'Calendar Quarter',
  end_of_fis_quarter
    CLASSIFICATION caption VALUE 'End of Fiscal Quarter'
    CLASSIFICATION description VALUE 'End of Fiscal Quarter', 
  fiscal_year
    CLASSIFICATION caption VALUE 'Fiscal Year'
    CLASSIFICATION description VALUE 'Fiscal Year',
  end_of_fis_year
    CLASSIFICATION caption VALUE 'End of Fiscal Year'
    CLASSIFICATION description VALUE 'End of Fiscal Year' 
    )
  LEVEL day
    CLASSIFICATION caption VALUE 'Day'
    CLASSIFICATION description VALUE 'Day'
     KEY time_id
     MEMBER NAME to_char(time_id)
     MEMBER CAPTION to_char(time_id)
     MEMBER DESCRIPTION to_char(time_id)
     ORDER BY time_id
     DETERMINES(calendar_month_desc,fiscal_month_desc)
  LEVEL calendar_month
    CLASSIFICATION caption VALUE 'Calendar Month'
    CLASSIFICATION description VALUE 'Calendar Month'
    KEY calendar_month_desc
    MEMBER NAME calendar_month_desc
    MEMBER CAPTION calendar_month_desc
    MEMBER DESCRIPTION calendar_month_desc
    ORDER BY end_of_cal_month
    DETERMINES(end_of_cal_month,calendar_quarter_desc)
  LEVEL calendar_quarter
    CLASSIFICATION caption VALUE 'Calendar Quarter'
    CLASSIFICATION description VALUE 'Calendar Quarter'
    KEY calendar_quarter_desc
    MEMBER NAME calendar_quarter_desc
    MEMBER CAPTION calendar_quarter_desc
    MEMBER DESCRIPTION calendar_quarter_desc
    ORDER BY end_of_cal_quarter
    DETERMINES(end_of_cal_quarter,calendar_year)
  LEVEL calendar_year
    CLASSIFICATION caption VALUE 'Calendar Year'
    CLASSIFICATION description VALUE 'Calendar Year'
    KEY calendar_year
    MEMBER NAME TO_CHAR(calendar_year)
    MEMBER CAPTION TO_CHAR(calendar_year)
    MEMBER DESCRIPTION TO_CHAR(calendar_year)
    ORDER BY end_of_cal_year
    DETERMINES (end_of_cal_year)
  LEVEL fiscal_month
    CLASSIFICATION caption VALUE 'Fiscal Month'
    CLASSIFICATION description VALUE 'Fiscal Month'
    KEY fiscal_month_desc
    MEMBER NAME fiscal_month_desc
    MEMBER CAPTION fiscal_month_desc
    MEMBER DESCRIPTION fiscal_month_desc
    ORDER BY end_of_fis_month
    DETERMINES(end_of_fis_month,fiscal_quarter_desc)
  LEVEL fiscal_quarter
    CLASSIFICATION caption VALUE 'Fiscal Quarter'
    CLASSIFICATION description VALUE 'Fiscal Quarter'
    KEY fiscal_quarter_desc
    MEMBER NAME fiscal_quarter_desc
    MEMBER CAPTION fiscal_quarter_desc
    MEMBER DESCRIPTION fiscal_quarter_desc
    ORDER BY end_of_fis_quarter
    DETERMINES(end_of_fis_quarter,fiscal_year)
  LEVEL fiscal_year
    CLASSIFICATION caption VALUE 'Fiscal Year'
    CLASSIFICATION description VALUE 'Fiscal Year'
    KEY fiscal_year
    MEMBER NAME TO_CHAR(fiscal_year)
    MEMBER CAPTION TO_CHAR(fiscal_year)
    MEMBER DESCRIPTION TO_CHAR(fiscal_year)
    ORDER BY end_of_fis_year
    DETERMINES (end_of_fis_year)
  ALL MEMBER NAME 'ALL YEARS';</code>

<p>Attribute values are queried from hierarchies.  A hierarchy references an attribute dimension, inheriting metadata from it.  As a result, the CREATE ATTRIBUTE statement is relatively simple.</p>

<p><b>Hierarchies for Time</b></p>

<p>The calendar year hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY sh_times_calendar_hier
  CLASSIFICATION caption VALUE 'Calendar Year'
  CLASSIFICATION description VALUE 'Calendar Year'
  USING sh_times_attr_dim
    (day CHILD OF
     calendar_month CHILD OF
     calendar_quarter CHILD OF
     calendar_year);</code>

<p>The fiscal year hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY sh_times_fiscal_hier
  CLASSIFICATION caption VALUE 'Fiscal Year'
  CLASSIFICATION description VALUE 'Fiscal Year'
  USING sh_times_attr_dim
    (day CHILD OF
     fiscal_month CHILD OF
     fiscal_quarter CHILD OF
     fiscal_year);</code>

<p>Query the TIMES_CALENDAR_HIER hierarchy.</p>

<code>SELECT * FROM sh_times_calendar_hier WHERE calendar_year = '2001';</code>

<p>Query the TIMES_FISCAL_HIER hierarchy.</p>

<code>SELECT * FROM sh_times_fiscal_hier WHERE fiscal_year = '2001';</code>",04-JAN-17 05.58.00.125314000 AM,"SYS",19-APR-22 03.56.28.963041000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613508551903539794071152916310,92046253613504925126080950183628797782,"Product Attribute Dimension and Hierarchy",30,"<p>The PRODUCTS table supports a hierarchy of levels Product > Subcategory > Category.</p>

<p>SELECT all columns from PRODUCTS.</p>

<code>SELECT * FROM sh.products;</code>

<p>A hierarchy can be created from the following columns.</p>

<code>SELECT
  prod_id,
  prod_name,
  prod_subcategory,
  prod_category
FROM
  sh.products
ORDER BY
  prod_category,
  prod_subcategory,
  prod_name;</code>

<p>Create the attribute dimension for product.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_products_attr_dim
USING sh.products
ATTRIBUTES (
  prod_id
    CLASSIFICATION caption VALUE 'Product'
    CLASSIFICATION description VALUE 'Product',
  prod_name
    CLASSIFICATION caption VALUE 'Product'
    CLASSIFICATION description VALUE 'Product',
  prod_subcategory
    CLASSIFICATION caption VALUE 'Subcategory'
    CLASSIFICATION description VALUE 'Subcategory',
  prod_category
    CLASSIFICATION caption VALUE 'Category'
    CLASSIFICATION description VALUE 'Category'
    )
  LEVEL PRODUCT
    CLASSIFICATION caption VALUE 'Product'
    CLASSIFICATION description VALUE 'Product'
    KEY prod_id
    MEMBER NAME prod_name
    MEMBER CAPTION prod_name
    MEMBER DESCRIPTION prod_name
    ORDER BY prod_name
    DETERMINES (prod_subcategory)
  LEVEL SUBCATEGORY
    CLASSIFICATION caption VALUE 'Subcategory'
    CLASSIFICATION description VALUE 'Subcategory'
    KEY prod_subcategory
    MEMBER NAME prod_subcategory
    MEMBER CAPTION prod_subcategory
    MEMBER DESCRIPTION prod_subcategory
    ORDER BY prod_subcategory
    DETERMINES (prod_category)
  LEVEL CATEGORY
    CLASSIFICATION caption VALUE 'Category'
    CLASSIFICATION description VALUE 'Category'
    KEY prod_category
    MEMBER NAME prod_category
    MEMBER CAPTION prod_category
    MEMBER DESCRIPTION prod_category
    ORDER BY prod_category
  ALL MEMBER NAME 'ALL PRODUCTS';</code>

<p>Create a hierarchy for products.</p>

<code>CREATE OR REPLACE HIERARCHY sh_products_hier
  CLASSIFICATION caption VALUE 'Products'
  CLASSIFICATION description VALUE 'Products'
USING sh_products_attr_dim
 (product CHILD OF
  subcategory CHILD OF
  category);</code>

<Query the products hierarchy.</p>

<code>SELECT * FROM sh_products_hier;</code>",04-JAN-17 05.58.00.127262000 AM,"SYS",30-JAN-17 06.15.10.781612000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613509760829359408700327622486,92046253613504925126080950183628797782,"Customer Attribute Dimension and Hierarchy",40,"<p>Customer is a little more complicated than time and product because:</p>

<ul>
<li>Customer data is in two tables (CUSTOMERS and COUNTRIES) rather than one.</li>
<li>Cities might not always be unique within states and states might not always be unique within countries.  There could, for example be a Miami in Florida and a Miami in Ohio.  (With this small sample data set that condition does not actually exist, but you will create the hierarchy to cover this case.)</li>
</ul>

<p>Examine data in the CUSTOMERS table.</p>

<code>SELECT * FROM sh.customers WHERE rownum <= 30;</code>

<p>Examine data in the COUNTRIES table.</p>

<code>SELECT * FROM sh.countries;</code>

<p>Create a view that joins the tables and creates unique values for customer_name, city_id, and state_province_id.</p>

<code>CREATE OR REPLACE VIEW sh_customers_dim_view AS
SELECT
  a.cust_id,
  a.cust_last_name || ', ' || a.cust_first_name as customer_name,
  a.cust_city || ', ' || a.cust_state_province || ', ' || a.country_id as city_id,
  a.cust_city as city_name,
  a.cust_state_province || ', ' || a.country_id as state_province_id,
  a.cust_state_province as state_province_name,
  b.country_id,
  b.country_name,
  b.country_subregion as subregion,
  b.country_region as region
FROM sh.customers a, sh.countries b
where a.country_id = b.country_id;</code>

<p>Query the view.</p>

<code>SELECT *
FROM sh_customers_dim_view
WHERE rownum <= 50
ORDER BY region,
  subregion,
  country_name,
  state_province_id,
  city_id;</code>

<p>Create the attribute dimension for customers.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_customers_attr_dim
USING sh_customers_dim_view
ATTRIBUTES (
 cust_id
    CLASSIFICATION caption VALUE 'Customer'
    CLASSIFICATION description VALUE 'Customer',
  customer_name
    CLASSIFICATION caption VALUE 'Customer'
    CLASSIFICATION description VALUE 'Customer',
  city_id
    CLASSIFICATION caption VALUE 'City'
    CLASSIFICATION description VALUE 'City',
  city_name
    CLASSIFICATION caption VALUE 'City'
    CLASSIFICATION description VALUE 'City',
  state_province_id
    CLASSIFICATION caption VALUE 'State Province'
    CLASSIFICATION description VALUE 'State Province',
  state_province_name
    CLASSIFICATION caption VALUE 'State Province'
    CLASSIFICATION description VALUE 'State Province',
  country_id
    CLASSIFICATION caption VALUE 'Country'
    CLASSIFICATION description VALUE 'Country',
   country_name
    CLASSIFICATION caption VALUE 'Country'
    CLASSIFICATION description VALUE 'Country',
   subregion
    CLASSIFICATION caption VALUE 'Subregion'
    CLASSIFICATION description VALUE 'Subregion',
    region
    CLASSIFICATION caption VALUE 'Region'
    CLASSIFICATION description VALUE 'Region'
    )
  LEVEL CUSTOMER
    CLASSIFICATION caption VALUE 'Customer'
    CLASSIFICATION description VALUE 'Customer'
    KEY cust_id
    MEMBER NAME customer_name
    MEMBER CAPTION customer_name
    MEMBER DESCRIPTION customer_name
    ORDER BY customer_name
    DETERMINES (city_id)
  LEVEL CITY
    CLASSIFICATION caption VALUE 'City'
    CLASSIFICATION description VALUE 'City'
    KEY city_id
    MEMBER NAME city_name
    MEMBER CAPTION city_name
    MEMBER DESCRIPTION city_name
    ORDER BY city_name
    DETERMINES (state_province_id)
  LEVEL STATE_PROVINCE
    CLASSIFICATION caption VALUE 'State_Province'
    CLASSIFICATION description VALUE 'State Province'
    KEY state_province_id
    MEMBER NAME state_province_name
    MEMBER CAPTION state_province_name
    MEMBER DESCRIPTION state_province_name
    ORDER BY state_province_name
    DETERMINES (country_id)
  LEVEL COUNTRY
    CLASSIFICATION caption VALUE 'Country'
    CLASSIFICATION description VALUE 'Country'
    KEY country_id
    MEMBER NAME country_name
    MEMBER CAPTION country_name
    MEMBER DESCRIPTION country_name
    ORDER BY country_name
    DETERMINES (subregion)
 LEVEL SUBREGION
    CLASSIFICATION caption VALUE 'Subregion'
    CLASSIFICATION description VALUE 'Subregion'
    KEY subregion
    MEMBER NAME subregion
    MEMBER CAPTION subregion
    MEMBER DESCRIPTION subregion
    ORDER BY subregion
    DETERMINES (region)
  LEVEL REGION
    CLASSIFICATION caption VALUE 'Region'
    CLASSIFICATION description VALUE 'Region'
    KEY region
    MEMBER NAME region
    MEMBER CAPTION region
    MEMBER DESCRIPTION region
    ORDER BY region
  ALL MEMBER NAME 'ALL CUSTOMERS';</code>

<p>Create the hierarchy for customers.</p>

<code>CREATE OR REPLACE HIERARCHY sh_customers_hier
  CLASSIFICATION caption VALUE 'Customers'
  CLASSIFICATION description VALUE 'Customers'
USING sh_customers_attr_dim
 (customer CHILD OF
  city CHILD OF
  state_province CHILD OF
  country CHILD OF
  subregion CHILD OF
  region);</code>

<p>Query the customer hierarchy.</p>

<code>SELECT * FROM sh_customers_hier WHERE rownum <= 50;</code>

<p>With 5 levels, notice that this hierarchy returns quite a few columns.  Some columns are <i>hierarchical attribute columns</i> and other columns are simply <i>attribute columns</i>.</p>

<p>Hierarchical columns are created by the database.   Some hierarchical columns combine unpivot attributes into a single column, creating rows for aggregate level attributes.  For example, each of the MEMBER NAME attributes are combined into a single MEMBER_NAME column. Other hierarchical columns are calculated, for example the HIER_ORDER column.</p>

<p>Query the hierarchical attributes in the SH_CUSTOMERS_HIER hierarchy.</p>

<code>SELECT member_name,
  member_unique_name,
  member_caption,
  member_description,
  level_name,
  hier_order,
  depth,
  parent_level_name,
  parent_unique_name
FROM sh_customers_hier
WHERE rownum <= 50;</code>

<p>These hierarchical attribute columns will exist in every hierarchy.</p>

<p>Note that the hierarchy view returns data at each level.  That is, at both detail and aggregate levels.  Also, the HIER_ORDER column sorts children within parents.</p>

<code>SELECT member_name,
  member_unique_name,
  level_name,
  hier_order
FROM sh_customers_hier
WHERE rownum <= 50
ORDER BY hier_order;</code>

<p>Attribute columns contain data from each attribute in the ATTRIBUTES list of the attribute dimension.</p>

<code>SELECT cust_id,
  customer_name,
  city_id,
  city_name,
  state_province_id,
  state_province_name,
  country_id,
  country_name,
  subregion,
  region
FROM sh_customers_hier
WHERE rownum <= 50;</code>",04-JAN-17 05.58.00.129402000 AM,"SYS",30-JAN-17 06.16.04.300406000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613510969755179023329502328662,92046253613504925126080950183628797782,"Channels and Promotions Attribute Dimensions and Hierarchies",50,"<p>There no new concepts with these attribute dimensions and hierarchies for Channels and Promotions, so just go ahead and create them.</p>

<p>View data in the CHANNELS table.</p>

<code>SELECT * from sh.channels;</code>

<p>Create the attribute dimension for channels.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_channels_attr_dim
  CLASSIFICATION caption VALUE 'Channels'
  CLASSIFICATION description VALUE 'Channels'
USING sh.channels
ATTRIBUTES (
  channel_id
    CLASSIFICATION caption VALUE 'Channel'
    CLASSIFICATION description VALUE 'Channel',
  channel_desc
    CLASSIFICATION caption VALUE 'Channel'
    CLASSIFICATION description VALUE 'Channel',
  channel_class
    CLASSIFICATION caption VALUE 'Channel Class'
    CLASSIFICATION description VALUE 'Channel Class'
  )   
  LEVEL CHANNEL
    CLASSIFICATION caption VALUE 'Channel'
    CLASSIFICATION description VALUE 'Channel'
    KEY channel_id
    MEMBER NAME channel_desc
    MEMBER CAPTION channel_desc
    ORDER BY channel_desc
    DETERMINES (channel_class)
  LEVEL CHANNEL_CLASS
    CLASSIFICATION caption VALUE 'Channel_Class'
    CLASSIFICATION description VALUE 'Channel Class'
    KEY channel_class
    MEMBER NAME channel_class
    MEMBER CAPTION channel_class
    ORDER BY channel_class 
  ALL MEMBER NAME 'ALL CHANNELS';</code>

<p>Create the hierarchy for channels.</p>

<code>CREATE OR REPLACE HIERARCHY sh_channels_hier
  CLASSIFICATION caption VALUE 'Channels'
  CLASSIFICATION description VALUE 'Channels'
USING sh_channels_attr_dim
 (channel CHILD OF
  channel_class);</code>

<p>View the SH_CHANNELS_HIER hierarchy.</p>

<code>SELECT * FROM sh_channels_hier;</code>

<p>View the PROMOTIONS table.</p>

<code>SELECT * from sh.promotions;</code>

<p>Create the attribute dimension for promotions.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION sh_promotions_attr_dim
  CLASSIFICATION caption VALUE 'Promotions'
  CLASSIFICATION description VALUE 'Promotions'
USING sh.promotions
ATTRIBUTES (
  promo_id
    CLASSIFICATION caption VALUE 'promotion'
    CLASSIFICATION description VALUE 'promotion',
  promo_name
    CLASSIFICATION caption VALUE 'promotion'
    CLASSIFICATION description VALUE 'promotion',
  promo_subcategory
    CLASSIFICATION caption VALUE 'Subcategory'
    CLASSIFICATION description VALUE 'Subcategory',
  promo_category
    CLASSIFICATION caption VALUE 'Category'
    CLASSIFICATION description VALUE 'Category'  
  )
  LEVEL PROMOTION
    CLASSIFICATION caption VALUE 'promotion'
    CLASSIFICATION description VALUE 'promotion'
    KEY promo_id
    MEMBER NAME promo_name
    MEMBER CAPTION promo_name
    ORDER BY promo_name
    DETERMINES (promo_subcategory)
  LEVEL SUBCATEGORY
    CLASSIFICATION caption VALUE 'Subcategory'
    CLASSIFICATION description VALUE 'Subcategory'
    KEY promo_subcategory
    MEMBER NAME promo_subcategory
    MEMBER CAPTION promo_subcategory
    ORDER BY promo_subcategory
    DETERMINES (promo_category)
  LEVEL CATEGORY
    CLASSIFICATION caption VALUE 'Category'
    CLASSIFICATION description VALUE 'Category'
    KEY promo_category
    MEMBER NAME promo_category
    MEMBER CAPTION promo_category
    ORDER BY promo_category
  ALL MEMBER NAME 'ALL PROMOTIONS';</code>

<p>Create the promotions hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY sh_promotions_hier
  CLASSIFICATION caption VALUE 'Promotions'
  CLASSIFICATION description VALUE 'Promotions'
USING sh_promotions_attr_dim
 (promotion CHILD OF
  subcategory CHILD OF
  category);</code>

<p>View data in the SH_PROMOTIONS_HIER hierarchy.</p>

<code>SELECT * FROM SH_PROMOTIONS_HIER;</code>",04-JAN-17 05.58.00.133080000 AM,"SYS",30-JAN-17 06.16.59.910340000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613512178680998637958677034838,92046253613504925126080950183628797782,"Creating the Analytic View for Sales Data",60,"<p>The analytic view joins the hierarchies to the fact table to present fact data as <i>measures</i>.  A measure may come directly from the fact table or it may be calculated using an expression.</p>

<p>The fact table contains keys for product, customer, time, channel and promotions and the measures QUANTITY_SOLD and AMOUNT_SOLD.</p>

<p>View data in the SALES table.</p>

<code>SELECT * FROM sh.sales WHERE rownum <= 50;</code>

<p>Create the analytic view. Note the comments in the CREATE statement.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sh_sales_history_av
 CLASSIFICATION caption VALUE 'Sales History (SH Sample Schema)'
 CLASSIFICATION description VALUE 'Sales History by Time, Product, Customer, Channel and Promotion'
 -- This AV references the SALES fact table.
USING sh.sales 
-- This is where hierarchies are joined into the analytic view.
DIMENSION BY
 (
  sh_times_attr_dim KEY time_id REFERENCES time_id HIERARCHIES (sh_times_calendar_hier DEFAULT, sh_times_fiscal_hier),
  sh_products_attr_dim KEY prod_id REFERENCES prod_id
 HIERARCHIES (sh_products_hier DEFAULT),
  sh_customers_attr_dim KEY cust_id REFERENCES cust_id HIERARCHIES (sh_customers_hier DEFAULT),
  sh_channels_attr_dim KEY channel_id REFERENCES channel_id HIERARCHIES (sh_channels_hier DEFAULT),
  sh_promotions_attr_dim KEY promo_id REFERENCES promo_id HIERARCHIES (sh_promotions_hier DEFAULT)  
  )
MEASURES (
  -- Amount sold maps to the fact table.
  amount_sold FACT amount_sold
    CLASSIFICATION caption VALUE 'Amount Sold'
    CLASSIFICATION description VALUE 'Amount Sold'
    CLASSIFICATION format_string VALUE '999,999,999,999.99',
  -- Quantity sold maps to the fact table.
  quantity_sold FACT quantity_sold
    CLASSIFICATION caption VALUE 'Quantity Sold'
    CLASSIFICATION description VALUE 'Quantity Sold'
    CLASSIFICATION format_string VALUE '999,999,999,999'
  )
DEFAULT MEASURE amount_sold;</code>

<p>The following query returns data at the calendar year, category and region levels. Note that a query of an analytic view does not need to select from all hierarchies (The SH_CHANNELS_HIER hierarchy is not used in this query).</p>

<code>SELECT
  sh_times_calendar_hier.member_name AS TIMES_CALENDAR_HIER,
  sh_products_hier.member_name AS PRODUCTS_HIER,
  sh_customers_hier.member_name AS CUSTOMERS_HIER,
  amount_sold,
  quantity_sold
FROM
   sh_sales_history_av HIERARCHIES (
   sh_times_calendar_hier,
   sh_products_hier,
   sh_customers_hier)
WHERE
  sh_times_calendar_hier.level_name = 'CALENDAR_YEAR'
  AND sh_products_hier.level_name = 'CATEGORY'
  AND sh_customers_hier.level_name = 'REGION'
ORDER BY
  sh_times_calendar_hier.hier_order,
  sh_products_hier.hier_order,
  sh_customers_hier.hier_order;</code>

<p>The next query includes all hierarchies.</p>

<code>SELECT
  sh_times_calendar_hier.member_name AS TIMES_CALENDAR_HIER,
  sh_products_hier.member_name AS PRODUCTS_HIER,
  sh_customers_hier.member_name AS CUSTOMERS_HIER,
  sh_channels_hier.member_name AS CHANNELS_HIER,
  sh_promotions_hier.member_name AS PROMOTIONS_HIER,
  amount_sold,
  quantity_sold
FROM
  sh_sales_history_av HIERARCHIES (
  sh_times_calendar_hier,
  sh_products_hier,
  sh_customers_hier,
  sh_channels_hier,
  sh_promotions_hier)
WHERE
  sh_times_calendar_hier.level_name = 'CALENDAR_YEAR'
  AND sh_products_hier.level_name = 'CATEGORY'
  AND sh_customers_hier.level_name = 'REGION'
  AND sh_channels_hier.level_name = 'CHANNEL_CLASS'
  AND sh_promotions_hier.level_name = 'CATEGORY'
ORDER BY
  sh_times_calendar_hier.hier_order,
  sh_products_hier.hier_order,
  sh_customers_hier.hier_order,
  sh_channels_hier.hier_order,
  sh_promotions_hier.hier_order;</code>",04-JAN-17 05.58.00.134432000 AM,"SYS",30-JAN-17 06.17.25.402289000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613513387606818252587851741014,92046253613504925126080950183628797782,"Adding Calculated Measures",70,"<p>Calculated measures are added to the analytic views using expressions that reference attributes in hierarchies, hierarchy values and other measures.  The following analytic view includes a variety of examples.</p>

<p>Create the analytic view with calculated measures.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sh_sales_history_av
 CLASSIFICATION caption VALUE 'Sales History (SH Sample Schema)'
 CLASSIFICATION description VALUE 'Sales History by Time, Product, Customer, Channel and Promotion'
USING sh.sales 
DIMENSION BY
 (
  sh_times_attr_dim KEY time_id REFERENCES time_id HIERARCHIES (sh_times_calendar_hier DEFAULT, sh_times_fiscal_hier),
  sh_products_attr_dim KEY prod_id REFERENCES prod_id HIERARCHIES (sh_products_hier DEFAULT),
  sh_customers_attr_dim KEY cust_id REFERENCES cust_id HIERARCHIES (sh_customers_hier DEFAULT),
  sh_channels_attr_dim KEY channel_id REFERENCES channel_id HIERARCHIES (sh_channels_hier DEFAULT),
  sh_promotions_attr_dim KEY promo_id REFERENCES promo_id HIERARCHIES (sh_promotions_hier DEFAULT)  
  )
MEASURES (
  -- Amount sold maps to the fact table.
  amount_sold FACT amount_sold
    CLASSIFICATION caption VALUE 'Amount Sold'
    CLASSIFICATION description VALUE 'Amount Sold'
    CLASSIFICATION format_string VALUE '999,999,999,999.99',
  -- Quantity sold maps to the fact table.
  quantity_sold FACT quantity_sold
    CLASSIFICATION caption VALUE 'Quantity Sold'
    CLASSIFICATION description VALUE 'Quantity Sold'
    CLASSIFICATION format_string VALUE '999,999,999,999',
  -- Ratio of amount sold for the current value to the parent product value. 
  amt_sold_shr_parent_prod AS (SHARE_OF(amount_sold HIERARCHY sh_products_hier PARENT))
    CLASSIFICATION caption VALUE 'Sales Product Share of Parent'
    CLASSIFICATION description VALUE 'Sales Product Share of Parent'
    CLASSIFICATION format_string VALUE '999.99',
  -- Ratio of amount sold for the current value to the parent customer value. 
  sales_shr_parent_cust AS (SHARE_OF(amount_sold HIERARCHY sh_customers_hier PARENT))
    CLASSIFICATION caption VALUE 'Sales Customer Share of Parent'
    CLASSIFICATION description VALUE 'Sales Customer Share of Parent'
    CLASSIFICATION format_string VALUE '999,999,999,999.99',
  --
  -- Calendar Year measures
  --
  -- Sales Calendar Year to Date  
  sales_cal_ytd AS (SUM(amount_sold) OVER (HIERARCHY sh_times_calendar_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL calendar_year))
    CLASSIFICATION caption VALUE 'Sales Calendar YTD'
    CLASSIFICATION description VALUE 'Sales Calendar YTD'
    CLASSIFICATION format_string VALUE '999,999,999,999.99',
   -- Sales same period 1 year ago.
  sales_cal_year_ago as (LAG(amount_sold) OVER (HIERARCHY sh_times_calendar_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL calendar_year))
    CLASSIFICATION caption VALUE 'Sales Calendar Year Ago'
    CLASSIFICATION description VALUE 'Sales Year Ago'
    CLASSIFICATION format_string VALUE '$999,999,999,999.99',
   -- Change in sales for the current period as compared to the same period 1 year ago.
  sales_chg_cal_year_ago as (LAG_DIFF(amount_sold) OVER (HIERARCHY sh_times_calendar_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL calendar_year))
    CLASSIFICATION caption VALUE 'Sales Change Calendar Year Ago'
    CLASSIFICATION description VALUE 'Sales Change Calendar Year Ago'
    CLASSIFICATION format_string VALUE '$999,999,999,999.99',
   -- Percent change in sales for the current period as compared to the same period 1 year ago. 
  sales_pctchg_cal_year_ago as (LAG_DIFF_PERCENT(amount_sold) OVER (HIERARCHY sh_times_calendar_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL calendar_year))
    CLASSIFICATION caption VALUE 'Sales Percent Change Calendar Year Ago'
    CLASSIFICATION description VALUE 'Sales Percent Change Calendar Year Ago'
    CLASSIFICATION format_string VALUE '999.99',
  --
  -- Fiscal Year measures
  --
  sales_fis_ytd AS (SUM(amount_sold) OVER (HIERARCHY sh_times_fiscal_hier BETWEEN UNBOUNDED PRECEDING AND CURRENT MEMBER WITHIN ANCESTOR AT LEVEL fiscal_year))
    CLASSIFICATION caption VALUE 'Sales Fiscal YTD'
    CLASSIFICATION description VALUE 'Sales Fiscal YTD'
    CLASSIFICATION format_string VALUE '999,999,999,999.99',
  sales_fis_year_ago as (LAG(amount_sold) OVER (HIERARCHY sh_times_fiscal_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL fiscal_year))
    CLASSIFICATION caption VALUE 'Sales Fiscal Year Ago'
    CLASSIFICATION description VALUE 'Sales Fiscal Year Ago'
    CLASSIFICATION format_string VALUE '$999,999,999,999.99',
   -- Change in sales for the current period as compared to the same period 1 year ago.
  sales_chg_fis_year_ago as (LAG_DIFF(amount_sold) OVER (HIERARCHY sh_times_fiscal_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL fiscal_year))
    CLASSIFICATION caption VALUE 'Sales Change Fiscal Year Ago'
    CLASSIFICATION description VALUE 'Sales Change Fiscal Year Ago'
    CLASSIFICATION format_string VALUE '$999,999,999,999.99',
   -- Percent change in sales for the current period as compared to the same period 1 year ago. 
  sales_pctchg_fis_year_ago as (LAG_DIFF_PERCENT(amount_sold) OVER (HIERARCHY sh_times_fiscal_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL fiscal_year))
    CLASSIFICATION caption VALUE 'Sales Percent Change Fiscal Year Ago'
    CLASSIFICATION description VALUE 'Sales Percent Change Fiscal Year Ago'
    CLASSIFICATION format_string VALUE '999.99'
  )
DEFAULT MEASURE amount_sold;</code>",04-JAN-17 05.58.00.137622000 AM,"SYS",30-JAN-17 06.19.08.373601000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613514596532637867217026447190,92046253613504925126080950183628797782,"Sample Queries",80,"<p>Let's look at some data using the analytic view.</p>

<p>Amount sold by Calendar Year, Category and Region.</p>

<code>SELECT sh_times_calendar_hier.member_name AS time,
  sh_products_hier.member_name            AS product,
  sh_customers_hier.member_name           AS customer,
  amount_sold
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.level_name  = 'CALENDAR_YEAR'
AND sh_products_hier.level_name  = 'CATEGORY'
AND sh_customers_hier.level_name = 'REGION'
ORDER BY sh_times_calendar_hier.hier_order,
  sh_products_hier.hier_order,
  sh_customers_hier.hier_order;</code>

<p>Add Sales Percent Change Calendar Year Ago. Note that the sort order is changed to make it easier to view year over year changes.</p>

<code>SELECT sh_times_calendar_hier.member_name AS time,
  sh_products_hier.member_name            AS product,
  sh_customers_hier.member_name           AS customer,
  amount_sold,
  sales_cal_year_ago,
  ROUND(sales_pctchg_cal_year_ago,2) AS sales_pctchg_cal_year_ago
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.level_name  = 'CALENDAR_YEAR'
AND sh_products_hier.level_name  = 'CATEGORY'
AND sh_customers_hier.level_name = 'REGION'
ORDER BY sh_products_hier.hier_order,
  sh_customers_hier.hier_order,
  sh_times_calendar_hier.hier_order;</code>

<p>Sales Calendar Year to Date at the Calendar Month level in calendar year 2001 for Electronics in Europe.</p>

<code>SELECT sh_times_calendar_hier.member_name AS time,
  sh_products_hier.member_name            AS product,
  sh_customers_hier.member_name           AS customer,
  amount_sold,
  sales_cal_ytd
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.level_name  = 'CALENDAR_MONTH'
AND sh_products_hier.member_name  = 'Electronics'
AND sh_customers_hier.member_name = 'Europe'
ORDER BY sh_times_calendar_hier.hier_order,
  sh_products_hier.hier_order,
  sh_customers_hier.hier_order;</code>

<p>The Share of Sales for each region to sales of all customers in calendar year 2001 for Electronics.</p>

<code>SELECT sh_times_calendar_hier.member_name AS time,
  sh_products_hier.member_name            AS product,
  sh_customers_hier.member_name           AS customer,
  amount_sold,
  ROUND(sales_shr_parent_cust,2) * 100 || '%' AS sales_shr_parent_cust
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.member_name  = '2001'
AND sh_products_hier.member_name  = 'Electronics'
AND sh_customers_hier.level_name = 'REGION'
ORDER BY amount_sold desc;</code>

<p>Add all Calendar Years to the query and note that the Share of Sales automatically breaks out by year.</p>

<code>SELECT sh_times_calendar_hier.member_name AS time,
  sh_products_hier.member_name            AS product,
  sh_customers_hier.member_name           AS customer,
  amount_sold,
  ROUND(sales_shr_parent_cust,2) * 100 || '%' AS sales_shr_parent_cust
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.level_name = 'CALENDAR_YEAR'
AND sh_products_hier.member_name  = 'Electronics'
AND sh_customers_hier.level_name = 'REGION'
ORDER BY sh_times_calendar_hier.hier_order,
  amount_sold desc;</code>

<p>Selecting hierarchical columns such as MEMBER_NAME simplifies SQL generation because the hierarchical columns do not need to change depending on the hierarchy or level of aggregation. It is, however, perfectly ok to select from 'regular' attribute columns.</p>

<code>SELECT sh_times_calendar_hier.calendar_year,
  sh_products_hier.prod_category,
  sh_customers_hier.region,
  amount_sold,
  ROUND(sales_shr_parent_cust,2) * 100 || '%' AS sales_shr_parent_cust
FROM sh_sales_history_av HIERARCHIES (sh_times_calendar_hier, sh_products_hier, sh_customers_hier)
WHERE sh_times_calendar_hier.level_name = 'CALENDAR_YEAR'
AND sh_products_hier.member_name  = 'Electronics'
AND sh_customers_hier.level_name = 'REGION'
ORDER BY sh_times_calendar_hier.hier_order,
  amount_sold desc;</code>",04-JAN-17 05.58.00.139740000 AM,"SYS",18-AUG-17 07.28.30.686780000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613517014384277096475375859542,92046253613515805458457481846201153366,"Introduction",10,"<p>You will create several different versions of an analytic view, each with different calculations.  These analytic views require 3 attribute dimensions and 3 hierarchies.  Before continuing, create these objects clicking on ""Execute the SQL required by this tutorial"" which precedes the list of modules.</p>

<p>SHARE_OF expressions calculate the ratio of a hierarchy value to the parent value, an ancestor value or to a specific hierarchy value. For example, the ratio of sales for a month to the parent of that month or to the year of that month.  In same cases you might also define certain ratios using the QUALIFY expression.</p>

<p>The SHARE_OF expression is similar to the SQL RATIO_TO_REPORT expression in that it calculates the ratio of a row to the aggregate (e.g., SUM) of a grouping of rows.  RATIO_TO_REPORT requires that you explicitly express the aggregate groupings across all columns using the <i>query partition clause</i>.  The SHARE_OF expression only requires that you express the aggregate within a single hierarchy.  Aggregates across all other hierarchies are calculated automatically.</p>",04-JAN-17 05.58.01.172218000 AM,"SYS",04-JAN-17 05.58.01.172313000 AM,"SYS"
92046253613518223310096711104550565718,92046253613515805458457481846201153366,"How SHARE_OF Calculations Work",30,"<p>The SHARE_OF expression works by computing the ratio of a measure of a hierarchy value to the measure of all hierarchy values in the same level, to the parent of the hierarchy value or to an ancestor of a hierarchy value.</p>

</p>The analytic view will automatically expand the SQL to access hierarchy values that are outside the scope of a WHERE clause.  For example if the query selects SALES and SALES_SHARE_OF_PARENT at the month level, the analytic view will automatically expand the query to access the quarter level data needed for the calculation.</p>",04-JAN-17 05.58.01.173037000 AM,"SYS",04-JAN-17 05.58.01.173095000 AM,"SYS"
92046253613519432235916325733725271894,92046253613515805458457481846201153366,"SHARE_OF Parent",40,"<p>SHARE_OF parent calculates the ratio of the current member to the parent of the current member in the same hierarchy.  The following example creates measure for Sales Share of Parent in the time, product and geography hierarchies.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_share_of_time_parent AS (SHARE_OF(sales HIERARCHY time_hier PARENT)),
  sales_share_of_product_parent AS (SHARE_OF(sales HIERARCHY product_hier PARENT)),
  sales_share_of_geog_parent AS (SHARE_OF(sales HIERARCHY geography_hier PARENT))
  )
DEFAULT MEASURE SALES;
</code>

<p>The following query selects time members, sales and the share of sales in the time hierarchy.</p> 

<code>
SELECT
  product_hier.member_name    AS product_member,
  time_hier.member_name       AS time_member,
  time_hier.level_name        AS time_level,
  sales,
  ROUND(sales_share_of_time_parent,2)    AS sales_share_of_time_parent
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  time_hier.level_name in ('YEAR','QUARTER','MONTH')
  AND product_hier.member_name = 'Cameras and Accessories'
ORDER BY
  time_hier.hier_order;</code>

<p>The following query selects product members, sales and the share of sales in the product hierarchy.</p> 

<code>SELECT
  time_hier.member_name          AS time_member,
  product_hier.member_name       AS product_member,
  product_hier.level_name        AS product_level,
  sales,
  ROUND(sales_share_of_product_parent,2)    AS sales_share_of_product_parent
FROM
  sales_av HIERARCHIES (time_hier, product_hier)
WHERE
  product_hier.level_name in ('ALL','DEPARTMENT','CATEGORY')
  AND time_hier.member_name = 'CY2011'
ORDER BY
  product_hier.hier_order;</code>",04-JAN-17 05.58.01.173704000 AM,"SYS",13-NOV-19 03.45.28.861261000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613520641161735940362899978070,92046253613515805458457481846201153366,"SHARE_OF Ancestor",50,"<p>SHARE_OF ANCESTOR calculates the ratio of the current hierarchy value to an ancestor value.  For example, the ratio of Sales at a Day, a Month or a Quarter to a Year.  The following example creates measures for:</p>

<ul>
<li>Sales Share of Year.</li>
<li>Sales Share of Department.</li>
<li>Sales Share of Region.</li>
</ul>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_share_of_year AS (SHARE_OF(sales HIERARCHY time_hier LEVEL year)),
   sales_share_of_department AS (SHARE_OF(sales HIERARCHY product_hier LEVEL department)),
   sales_share_of_region AS (SHARE_OF(sales HIERARCHY geography_hier LEVEL region))
  )
DEFAULT MEASURE SALES;</code>

<p>Query the Time hierarchy.  Note that data at each level is a ratio to the ancestor at the Year level.</p>

<code>SELECT
  time_hier.member_name       AS time_member,
  time_hier.level_name        as time_level,
  sales,
  ROUND(sales_share_of_year,2)    AS sales_share_of_time_year
FROM
  sales_av HIERARCHIES (time_hier)
WHERE
  time_hier.level_name in ('YEAR','QUARTER','MONTH')
ORDER BY
  time_hier.hier_order;</code>

<p>This might be a little easier to see by querying only the Year and one descendant level at a time.  First with Year and Quarter.</p>

<code>SELECT
  time_hier.member_name       AS time_member,
  time_hier.level_name        as time_level,
  sales,
  ROUND(sales_share_of_year,2)    AS sales_share_of_time_year
FROM
  sales_av HIERARCHIES (time_hier)
WHERE
  time_hier.level_name in ('YEAR','QUARTER')
ORDER BY
  time_hier.hier_order;</code>

<p>Next by Year and Month.</p>

<code>SELECT
  time_hier.member_name       AS time_member,
  time_hier.level_name        as time_level,
  sales,
  ROUND(sales_share_of_year,2)    AS sales_share_of_time_year
FROM
  sales_av HIERARCHIES (time_hier)
WHERE
  time_hier.level_name in ('YEAR','MONTH')
ORDER BY
  time_hier.hier_order;</code>

<p>The next query illustrates that Year level data does not need to selected for the calculations to return data.  The query will be automatically expanded to fetch Year level data for the SHARE_OF expression.</p>

<code>SELECT
  time_hier.member_name       AS time_member,
  time_hier.level_name        as time_level,
  sales,
  ROUND(sales_share_of_year,2)    AS sales_share_of_time_year
FROM
  sales_av HIERARCHIES (time_hier)
WHERE
  time_hier.level_name = 'MONTH'
ORDER BY
  time_hier.hier_order;</code>

<p>Query the Product hierarchy.  Note that data at each level is a ratio of Sales of the current hierarchy value to Sales of the ancestor at the Department level.</p>

<code>SELECT
  product_hier.member_name       AS product_member,
  product_hier.level_name        as product_level,
  sales,
  ROUND(sales_share_of_department,2)    AS sales_share_of_department
FROM
  sales_av HIERARCHIES (product_hier)
WHERE
  time_hier.level_name in ('ALL','DEPARTMENT','CATEGORY')
ORDER BY
  product_hier.hier_order;</code>",04-JAN-17 05.58.01.175388000 AM,"SYS",30-JAN-17 06.21.08.571348000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613521850087555554992074684246,92046253613515805458457481846201153366,"SHARE_OF Within Level",60,"<p>To calculate the ratio of a hierarchy value to all values in the same level create a SHARE_OF measure that calculates the ratio of the current value to the top most aggregate value.  The can be done using the MEMBER and ALL keywords.</p>

<p>For example:</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_share_of_all_time AS (SHARE_OF(sales HIERARCHY time_hier MEMBER ALL)),
  sales_share_of_all_product AS (SHARE_OF(sales HIERARCHY product_hier MEMBER ALL)),
  sales_share_of_all_geography AS (SHARE_OF(sales HIERARCHY geography_hier MEMBER ALL))
  )
DEFAULT MEASURE SALES;</code>

<p>Both MEMBER and ALL are keywords and thus are not quoted and case insensitive.  The MEMBER keyword indicates that a specific hierarchy value will be specified.  The ALL keyword is a shortcut to referencing the ALL MEMBER NAME value which is the top most hierarchy value.</p>

<p>Select the SALES_SHARE_OF_ALL_PRODUCT measure at the Department level.</p>

<code>SELECT
  time_hier.member_name                            AS time,
  product_hier.member_name                         AS product,
  product_hier.level_name                          AS product_level,
  TO_CHAR(sales,'999,999,999,999')                 AS sales,
  ROUND(sales_share_of_all_product,3) * 100 || '%' AS sales_share_of_all_product
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  product_hier.level_name = 'DEPARTMENT'
  AND time_hier.member_name = 'CY2015'
ORDER BY
  product_hier.hier_order;</code>

<p>Select the SALES_SHARE_OF_ALL_PRODUCT measure at the Category level.</p>

<code>SELECT
  time_hier.member_name                            AS time,
  product_hier.member_name                         AS product,
  product_hier.level_name                          AS product_level,
  TO_CHAR(sales,'999,999,999,999')                 AS sales,
  ROUND(sales_share_of_all_product,3) * 100 || '%' AS sales_share_of_all_product
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  product_hier.level_name = 'CATEGORY'
  AND time_hier.member_name = 'CY2015'
ORDER BY
  product_hier.hier_order;</code>",04-JAN-17 05.58.01.176659000 AM,"SYS",24-OCT-17 05.13.14.365817000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613523059013375169621249390422,92046253613515805458457481846201153366,"Ratio to a Specific Hierarchy Value Using SHARE_OF ... MEMBER",70,"<p>In each of the previous examples the SHARE_OF expression was used to create a measure that was the ratio of a hierarchy value to a parent or ancestor value.  You can use the MEMBER keyword to create a measure that is the ratio of a hierarchy value to a specific hierarchy value.</p>

<p>In the previous module you saw how MEMBER ALL can be used to reference the top most value of a hierarchy.  The MEMBER keyword can also be use to reference a specific value.  For example, if you would like to create a measure that is the ratio of any hierarchy value to the value for year 2001 you could use SHARE_OF ... MEMBER .. VALUE.</p>

<p>The value used by the MEMBER keyword is the key attribute value of the level preceded by the level name.  In the TIME_HIER hierarchy the key attribute value for year is in the YEAR_ID attribute.  The key attribute value can also be seen in the last element of the MEMBER_UNIQUE_NAME attribute.  This can be seen in the following query.</p>

<code>SELECT member_name, year_id, member_unique_name FROM time_hier WHERE level_name = 'YEAR' ORDER BY hier_order;</code>

<p>The following example creates a ratio of sales to the value of sales for year 2001.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_ratio_to_year_2011 AS (SHARE_OF(sales HIERARCHY time_hier MEMBER year ['11']))
  )
DEFAULT MEASURE SALES;</code>

<p>The following query selects at the year and department levels.</p>

<code>SELECT
  time_hier.member_name                            AS time,
  product_hier.member_name                         AS product,
  TO_CHAR(sales,'999,999,999,999')                 AS sales,
  ROUND(sales_ratio_to_year_2011,3) * 100 || '%'   AS sales_ration_to_year_2011
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  product_hier.level_name = 'DEPARTMENT'
  AND time_hier.level_name = 'YEAR'
ORDER BY
  product_hier.hier_order,
  time_hier.hier_order;</code>",04-JAN-17 05.58.01.178068000 AM,"SYS",30-JAN-17 06.21.43.526954000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613525476865014398879598802774,92046253613524267939194784250424096598,"Overview",10,"<p>
Doesnâ€™t matter who much testing you do (well, it actually does but thatâ€™s a whole different issue!) you can guarantee that at some point your beautiful code that parses data input from a web form or loads data from some external file will pop up with the error:
</p>
<blockquote>
SQL Error: ORA-01722: invalid number<br>
<br>
01722. 00000 - ""invalid number""<br>
*Cause: The specified number was invalid.<br>
*Action: Specify a valid number.<br>
</blockquote>
<p>
Of course, whatâ€™s is really annoying at this point is that you donâ€™t know which column value of the record failed (assuming that you have more than one numeric column)
</p>",04-JAN-17 05.58.02.114127000 AM,"SYS",04-JAN-17 05.58.02.114183000 AM,"SYS"
92046253613527894716653628137948215126,92046253613524267939194784250424096598,"Creating the target table",30,"<p>Now let's create our target table</p>
<code>CREATE TABLE EMP<
 ( EMPNO NUMBER(4,0), 
   ENAME VARCHAR2(10 BYTE),
   JOB VARCHAR2(9 BYTE),
   MGR NUMBER(4,0),
   HIREDATE DATE,
   SAL NUMBER(7,2),
   COMM NUMBER(7,2),
   DEPTNO NUMBER(2,0),
CONSTRAINT ""PK_EMP"" PRIMARY KEY (EMPNO));
</code>",04-JAN-17 05.58.02.116217000 AM,"SYS",18-DEC-18 03.21.45.146092000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613468657351492511308387612502,92046253613467448425672896679212906326,"Create Test Schema",10,"<p>
We'll start by creating a simple table called <i>approxt</i>. It includes a <i>value</i> column we'll use for calculations and we'll be aggregating the results by US <i>state</i> and <i>county</i>.
<p>
<code>
declare
  ORA_00942 exception; pragma Exception_Init(ORA_00942, -00942);
begin
  execute immediate 'drop table approxt';
exception when ORA_00942 then null;
end;
/

create table approxt (
id number, 
volume number, 
state varchar2(5), 
county varchar2(30));

insert into approxt
        select level id, floor(dbms_random.value()*1000) volume, 'CA' state, 'COUNTY1' county
                from dual
                connect by level <= 10000;

insert into approxt
        select level+10000 id, floor(dbms_random.value()*1000) volume, 'CA' , 'COUNTY2' town
                from dual
                connect by level <= 10000;

insert into approxt
        select level+20000 id,  floor(dbms_random.value()*1000) volume, 'NM' , 'COUNTY3' 
                from dual
                connect by level <= 10000;

insert into approxt
        select level+30000 id,  floor(dbms_random.value()*1000) volume, 'DL' , 'COUNTY4' 
                from dual
                connect by level <= 10000;

insert into approxt
        select level+40000 id,  floor(dbms_random.value()*1000) volume, 'MA' , 'COUNTY5' 
                from dual
                connect by level <= 10000;

commit;

exec dbms_stats.gather_table_stats(ownname=>null,tabname=>'approxt');
</code>",04-JAN-17 05.57.55.766696000 AM,"SYS",04-JAN-17 05.57.55.766759000 AM,"SYS"
92046253613469866277312125937562318678,92046253613467448425672896679212906326,"Approximate Count Distinct",20,"<p>
Let us recap approximate count distinct, which has been available since Oracle Database 12.1.
<p>
<code>
REM Use ALTER SESSION to ensure that COUNT DISTINCT will not be converted to APPROX COUNT DISTINCT. More on this later.

alter session set approx_for_count_distinct = FALSE ;

REM Exact count(distinct volume) calculation.

select count(distinct volume) from approxt;

REM Looking at the execution execution plan, you'll see that it includes a hash group by and sort aggregate.

explain plan for select COUNT(distinct volume) from approxt;

select * from table(dbms_xplan.display);

REM This time, use the approximate calculation using the explicit function: APPROX_COUNT_DISTINCT(volume).

select APPROX_COUNT_DISTINCT(volume) from approxt;

REM Notice that the execution plan for approximate count distinct includes a ""sort aggregate approx"".
REM This operation is much cheaper than the equivalent exact sort operation. 
REM The performance benefits of approximate processing are, to a large extent, associated 
REM with the removal of the expensive sort operation. 
REM This saving becomes more significant for larger data volumes on columns with 
REM large numbers of distinct values.

explain plan for select APPROX_COUNT_DISTINCT(volume) from approxt;

select * from table(dbms_xplan.display);
</code>",04-JAN-17 05.57.55.809548000 AM,"SYS",04-JAN-17 05.57.55.809607000 AM,"SYS"
92046253613471075203131740566737024854,92046253613467448425672896679212906326,"Zero Code Changes",30,"<p>
Oracle Database 12.2 added the ability to replace exact calculations with approximate calculations <i>without</i> making code changes. This can be done using a database parameter, ""approx_for_count_distinct"", which can be set at the session or database-level.
</p>
<code>
REM Transparently replace exact COUNT DISTINCT with APPROX COUNT DISTINCT

alter session set approx_for_count_distinct = TRUE ;

REM Approximate count distinct, even though query is ""count(distinct)"". 
REM Check the result with the previous one.

select count(distinct volume) from approxt;

REM An now, ""back to normal""...

alter session set approx_for_count_distinct = FALSE ;

REM Exact...

select count(distinct volume) from approxt;
</code>",04-JAN-17 05.57.55.810234000 AM,"SYS",04-JAN-17 05.57.55.810307000 AM,"SYS"
92046253613472284128951355195911731030,92046253613467448425672896679212906326,"New Median Calculation",40,"<p>The new approximate median calculation.</p>
<code>
REM Exact median calculation...

select median(volume) from approxt where state = 'CA';

REM The new approximate calculation for median...

select approx_median(volume) from approxt where state = 'CA';

REM Look at the execution plan for approximate median...

explain plan for select approx_median(volume) from approxt where state = 'CA';

select * from table(dbms_xplan.display);
</code>",04-JAN-17 05.57.55.810880000 AM,"SYS",04-JAN-17 05.57.55.810937000 AM,"SYS"
92046253613473493054770969825086437206,92046253613467448425672896679212906326,"New Percentile Calculation",45,"<p>The new approximate percentile calculation.</p>

<p>""Approximate percentile"" is an umbrella term for ""approx_percentile"" and ""approx_median"" because these functions are closely related to one another. A percentile of ""0.5"" is equivalent to a median. For the rest of this tutorial, the term ""approximate percentile"" can be taken to be referring to the percentile <i>and</i> median functions.</p>
<code>
REM A percentile of ""0.5"" is equivalent to MEDIAN...

select approx_percentile(0.5) within group (order by volume) 
from approxt where state = 'CA';

select approx_percentile(0.1) within group (order by volume) 
from approxt where state = 'CA';
</code>",04-JAN-17 05.57.55.811561000 AM,"SYS",04-JAN-17 05.57.55.811618000 AM,"SYS"
92046253613474701980590584454261143382,92046253613467448425672896679212906326,"Error Rate and Confidence Level",50,"<p>
The percentile calculations offer error rate and confidence reporting.
</p>
<code>
REM MEDIAN error rate...
select approx_median(volume,'ERROR_RATE') from approxt where state = 'CA';
REM MEDIAN confidence...
select approx_median(volume,'CONFIDENCE') from approxt where state = 'CA';
REM PERCENTILE error rate...
select approx_percentile(0.1,'ERROR_RATE') within group (order by volume) 
from approxt where state = 'CA';
REM PERCENTILE confidence...
select approx_percentile(0.1,'CONFIDENCE') within group (order by volume) 
from approxt where state = 'CA';
</code>",04-JAN-17 05.57.55.812421000 AM,"SYS",26-JAN-17 01.21.21.109312000 PM,"NIGEL.BAYLISS@ORACLE.COM"
92046253613475910906410199083435849558,92046253613467448425672896679212906326,"Deterministic and Non-deterministic",60,"<p>
Two approximate algorithms are offered for the approximate percentile functions:
<ul>
<li>Non-deterministic: fastest method, whenever reproducibility is not required. This is the default method.</li>
<li>Deterministic: slightly slower, but delivers deterministic results.</li>
</ul>
<br>Note that the differences will not be apparent with the small dataset in use here.
</p>
<code>
REM Approximate median, NON DETERMINISTIC

select approx_median(volume), approx_median(volume,'ERROR_RATE') 
from approxt where state = 'CA';

REM Approximate median, DETERMINISTIC

select approx_median(volume deterministic), approx_median(volume deterministic,'ERROR_RATE') 
from approxt 
where state = 'CA';

REM Approximate percentile, DETERMINISTIC

select approx_percentile(0.1 deterministic) within group (order by volume) 
from approxt where state = 'CA';
</code>",04-JAN-17 05.57.55.813033000 AM,"SYS",04-JAN-17 05.57.55.813090000 AM,"SYS"
92046253613538775049030159800520570710,92046253613535148271571315912996452182,"Reviewing the Sample Data",30,"<p>The sample data includes 3 dimension tables and 1 fact table.</p>

<p>You can review the data using the following queries.</p>

<p>Select the columns from the TIME_DIM table that are referenced by the TIME_HIER hierarchy.</p>

<code>SELECT month_id,
  month_name,
  month_long_name,
  month_end_date,
  quarter_id,
  quarter_name,
  quarter_end_date,
  year_id,
  year_name,
  year_end_date
FROM time_dim
ORDER BY month_end_date;</code>

<p>Select the columns from the PRODUCT_DIM table that are referenced by the PRODUCT_HIER hierarchy.</p>

<code>SELECT category_id,
  category_name,
  department_id,
  department_name
FROM product_dim
ORDER BY category_name;</code>

<p>Select the columns from the GEOGRAPHY_DIM table that are referenced by the GEOGRAPHY_HIER hierarchy.</p>

<code>SELECT state_province_id,
  state_province_name,
  country_id,
  country_name,
  region_id,
  region_name
FROM geography_dim
ORDER BY region_name,
  country_name,
  state_province_name;</code>

<p>Select a sample of rows from the SALES_FACT table;</p>

<code>SELECT * FROM sales_fact WHERE rownum <= 20;</code>

<p>The following query selects aggregate level data from the dimension and fact tables.  This query illustrates the joins between the tables.</p>

<code>SELECT
  t.year_name,
  p.department_name,
  g.region_name,
  SUM(f.sales) AS sales,
  SUM(f.units) AS units
FROM
  time_dim t,
  product_dim p,
  geography_dim g,
  sales_fact f
WHERE
  t.month_id = f.month_id
  AND p.category_id = f.category_id
  AND g.state_province_id = f.state_province_id
GROUP BY
  t.year_name,
  p.department_name,
  g.region_name
ORDER BY 
  t.year_name,
  p.department_name,
  g.region_name;</code>

<p>You can use the following queries to select from the hierarchies.</p>

<code>SELECT * FROM time_hier WHERE rownum <= 20;</code>

<code>SELECT * FROM product_hier WHERE rownum <= 20;</code>

<code>SELECT * FROM geography_hier WHERE rownum <= 20;</code>",04-JAN-17 05.58.03.367878000 AM,"SYS",18-AUG-17 07.05.57.586237000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613539983974849774429695276886,92046253613535148271571315912996452182,"Create Analytic View for Sales Data",40,"<p>The following CREATE ANALYTIC VIEW statement creates an analytic view without the CACHE clause. </p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  units FACT units,
  sales_prior_period AS
    (LAG(SALES) OVER (HIERARCHY time_hier OFFSET 1)),
  sales_share_prod_parent AS
   (SHARE_OF(sales HIERARCHY product_hier PARENT)),
  sales_share_geog_parent AS
   (SHARE_OF(sales HIERARCHY geography_hier PARENT))
  )
DEFAULT MEASURE SALES;</code>

<p>The following query selects the data at the Year, Department and Region levels.</p>

<code>SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'REGION'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>The SQL execution plan will show that the SALES_FACT table is being accessed.</p>

<p>Write the plan to the plan table.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '1' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'REGION'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_FACT table is accessed.  Note that the query of the plan table filters for ACCESS operations.</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '1'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '1';</code>",04-JAN-17 05.58.03.368583000 AM,"SYS",09-JAN-24 06.49.12.948110000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613541192900669389058869983062,92046253613535148271571315912996452182,"Creating an Aggregate Cache for the Analytic View",60,"<p>A materialized view that supports an analytic view is very simple and efficient - it only needs to SELECT and GROUP BY the columns mapped to the key attributes of appropriate the aggregate levels.  In this tutorial you will make a materialized view that aggregates data to the Year, Department and Country levels.  This materialized view can be used by any query selecting from the analytic view at these levels or above.</p>

<p>The DBMS_HIERARCHY.GET_MV_SQL_FOR_AV_CACHE procedure returns a query that can be used to create an aggregate table or a materialized view.  The following PL/SQL code creates an aggregate table.</p>

<code>DECLARE
  cache_sql CLOB;
BEGIN
  cache_sql := dbms_hierarchy.get_mv_sql_for_av_cache (
              analytic_view_name => 'SALES_AV'
               , cache_idx => 0    -- VALUE FROM THE PREVIOUS QUERY
            );

  EXECUTE IMMEDIATE
    'CREATE MATERIALIZED VIEW sales_av_cache_0 AS ' || cache_sql;

END;
/</code>

<p>Check that the materialized view was created.</p>

<code>SELECT * FROM user_mviews;</code>

<p>If you want to see the SQL, you run the following SELECT statement.</p>

<code>SELECT TO_CHAR(dbms_hierarchy.get_mv_sql_for_av_cache (
    analytic_view_name => 'SALES_AV'
    , cache_idx => 0 ))
    FROM dual;</code>

<p>The table contains aggregate data at the specified levels. The analytic view will access this table for any query at or above the levels of the materialized cache.</p>

<p>Note that this query, which selects relatively short key values and does not include upper level aggregate columns (for example, a column for region), is likely to be much smaller than the typical query (with text descriptor columns and upper level aggregate columns) used in a materialized view.  This materialized view will be quicker to build, use less disk space and more easily fit into the in-memory column store than a typical materialized view supporting a star schema.</p>

<p>Enable the materialized view for query rewrite.</p>

<code>ALTER MATERIALIZED VIEW sales_av_cache_0 ENABLE QUERY REWRITE;</code>

<p>You can check the state of the SALES_AV_CACHE_0 materialized view with the following query.</p>

<code>SELECT
  mview_name,
  rewrite_capability,
  staleness,
  rewrite_enabled
FROM user_mviews
WHERE mview_name = 'SALES_AV_CACHE_0';</code>",04-JAN-17 05.58.03.369786000 AM,"SYS",09-JAN-24 08.34.54.492467000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95118980521741963542156827869835825193,95114558141387237177774563848501259509,"Introduction",10,"<p>
Analytic views provide a logical dimension model, mappings to physical tables and views that may be queried by applications using SQL.  They provide an easy method to extend a star schema with structural and descriptive metadata, aggregate level data and measures calculations.  Analytic views present aggregate data and measure calculations as pre-joined and pre-solved rows and columns, simplifying the SQL that needs to be generated by business intelligence applications.
</p>

<p>
The logical dimensional model includes structural elements such as attributes, hierarchies and measures and descriptive elements such as 'friendly' display names that can be used to represent the model to end users.  For example, a hierarchy named GEOGRAPHY_HIER might be represented to a business user as 'Geography'.
</p>

<p>
The mappings to physical tables include mappings to columns uses as keys (<i>key attributes</i>)for data are various levels of aggregation and typically including mappings to columns that provide 'friendly' names for hierarchy values.  For example a key attribute might be mapped to integer key values and descriptive attributes might be mapped to a column with text values.
</p>

<p>
Analytic views provide support using multiple languages with both the descriptive elements of the model and mappings to descriptive elements, providing business users with a reporting experience in their native or preferred language.
</p>

<p>
This tutorial will add German language support to an analytic view that uses English as the default language.</p>",02-FEB-17 04.18.36.719924000 PM,"WILLIAM.ENDRESS@ORACLE.COM",03-FEB-17 09.21.35.889850000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95119644444362669805253863300762588656,95114558141387237177774563848501259509,"Setup",20,"<p>If you have not already done so, run the setup script for this tutorial by clicking on <u>Execute the SQL required for this tutorial</u> link which precedes the list of modules in this tutorial.  This script will:</p>

<ul>
<li>Copy dimension tables and the fact table in the AV schema to the current schema.</li>
<li>Add new columns to the dimension tables that will be used for German language data.</li>
<li>Update the dimension tables with German language data.</li>
</ul>",02-FEB-17 04.37.55.001165000 PM,"WILLIAM.ENDRESS@ORACLE.COM",02-FEB-17 04.37.55.001227000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95126191452083982135111307853097003896,95114558141387237177774563848501259509,"Creating a Hierarchy With Only a Default Lanuage",40,"<p>We will start by defining an attribute dimension and hierarchy with support for only a default language and then add support for German.</p>

<p>If support is not added to analytic view objects for specific languages, the classifications and descriptive labels of hierarchy values will be used for all users regardless of the NLS_LANGUAGE setting of the session.</p>

<p>The following attribute dimension and hierarchy objects do not include support for multiple languages. Note that CLASSIFICATIONs do not include a language keyword and only one column is mapped to each MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION property of the levels.</p>

<p>Create the attribute dimension for Time.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
  CLASSIFICATION caption VALUE 'Time Attribute Dimension'
  CLASSIFICATION description VALUE 'Company standard time attributes and levels'
USING time_dim
ATTRIBUTES
 (year_id
    CLASSIFICATION caption VALUE 'Year Key Value'
    CLASSIFICATION description VALUE 'Year Key Value',
  year_name
    CLASSIFICATION caption VALUE 'Year Descriptive Value'
    CLASSIFICATION description VALUE 'Year Descriptive Value',
  quarter_id
    CLASSIFICATION caption VALUE 'Quarter Key Value'
    CLASSIFICATION description VALUE 'Quarter Key Value',
  quarter_name
    CLASSIFICATION caption VALUE 'Quarter Descriptive Value'
    CLASSIFICATION description VALUE 'Quarter Descriptive Value',
  month_id
    CLASSIFICATION caption VALUE 'Month Key Value'
    CLASSIFICATION description VALUE 'Month Key Value',
  month_name
    CLASSIFICATION caption VALUE 'Month Short Descriptive Value'
    CLASSIFICATION description VALUE 'Month Short Descriptive Value',
  month_long_name
    CLASSIFICATION caption VALUE 'Month Long Descriptive Value'
    CLASSIFICATION description VALUE 'Month Long Descriptive Value',
  month_end_date
    CLASSIFICATION caption VALUE 'Ending Date of Month Key Value'
    CLASSIFICATION description VALUE 'Ending Date of Month Key Value')
LEVEL MONTH
  CLASSIFICATION caption VALUE 'Month Level'
  CLASSIFICATION description VALUE 'Month Aggregate Groupings'
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  ORDER BY month_end_date
  DETERMINES (quarter_id, month_end_date)
LEVEL QUARTER
  CLASSIFICATION caption VALUE 'Quarter Level'
  CLASSIFICATION description VALUE 'Quarter Aggregate Groupings'
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY MAX month_end_date
  DETERMINES (year_id)
LEVEL YEAR
  CLASSIFICATION caption VALUE 'Year Level'
  CLASSIFICATION description VALUE 'Year Aggregate Groupings'
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY MAX month_end_date;</code>

<p>Create the hierarchy for Time.</p>

<code>CREATE OR REPLACE HIERARCHY time_hier
  CLASSIFICATION caption VALUE 'Time'
  CLASSIFICATION description VALUE 'Time Hierarchy'
USING time_attr_dim
 (month  CHILD OF
 quarter CHILD OF
 year);</code>

<p>The following dictionary queries return only one value for each classification and the LANGUAGE column is NULL.</p>

<code>SELECT * FROM user_attribute_dim_class;</code>

<code>SELECT * FROM user_attribute_dim_attr_class;</code>

<code>SELECT * FROM user_attribute_dim_lvl_class;</code>

<code>SELECT * FROM user_hier_class;</code>

<p>The MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION columns return the same values regardless of the current NLS_LANGUAGE value.</p>

<p>Check the current NLS_LANGUAGE value.</p>

<code>SELECT sys_context('USERENV','LANGUAGE') FROM dual;</code>

<p>View the MEMBER_NAME columns in the TIME_HIER hierarchy at the Month level.</p>

<code>SELECT member_name, member_caption, member_description FROM time_hier WHERE level_name = 'MONTH';</code>

<p>Change the language in the current session to German.</p>

<code>ALTER SESSION SET nls_language = GERMAN;</code>

<p>Note the values of MEMBER_NAME are unchanged.</p>

<code>SELECT member_name, member_caption, member_description FROM time_hier WHERE level_name = 'MONTH';</code>",02-FEB-17 05.48.42.538763000 PM,"WILLIAM.ENDRESS@ORACLE.COM",17-SEP-20 06.25.58.157799000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95132984388802310733980152618409407079,95114558141387237177774563848501259509,"Adding Language Support to Hierarchical Attributes",60,"The MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION hierarchical attributes may be mapped to columns representing different languages.  When these attributes are selected from either hierarchies or analytic views the database will return values for either a specific language or the default language based on the NLS_LANGUAGE setting of the session.</p>

<p>To map hierarchical attributes to different columns based on language:</p>

<ul>
<li>Add attributes to the attribute dimension for each column in the dimension table.</li>
<li>Use a CASE statement to map to the appropriate attribute.</li>
</ul>

<p>The following attribute dimension includes hierarchical attributes at the Month level with support for German.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
  CLASSIFICATION caption VALUE 'Time Attribute Dimension'
  CLASSIFICATION description VALUE 'Company standard time attributes and levels'
  CLASSIFICATION caption VALUE 'Time Attribute Dimension' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Company standard time attributes and levels' LANGUAGE 'GERMAN'
USING time_dim
ATTRIBUTES
 (year_id
    CLASSIFICATION caption VALUE 'Year Key Value'
    CLASSIFICATION description VALUE 'Year Key Value'
    CLASSIFICATION caption VALUE 'Jahr-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Jahr-ID-Wert' LANGUAGE 'GERMAN',
  year_name
    CLASSIFICATION caption VALUE 'Year Descriptive Value'
    CLASSIFICATION description VALUE 'Year Descriptive Value'
    CLASSIFICATION caption VALUE 'Jahr Text Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Jahr Text Wert' LANGUAGE 'GERMAN',
  quarter_id
    CLASSIFICATION caption VALUE 'Quarter Key Value'
    CLASSIFICATION description VALUE 'Quarter Key Value'
    CLASSIFICATION caption VALUE 'Viertel-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Viertel-ID-Wert' LANGUAGE 'GERMAN',
  quarter_name
    CLASSIFICATION caption VALUE 'Quarter Descriptive Value'
    CLASSIFICATION description VALUE 'Quarter Descriptive Value'
    CLASSIFICATION caption VALUE 'Quarter Textwert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Quarter Textwert' LANGUAGE 'GERMAN',
  month_id
    CLASSIFICATION caption VALUE 'Month Key Value'
    CLASSIFICATION description VALUE 'Month Key Value'
    CLASSIFICATION caption VALUE 'Monatlicher ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Monatlicher ID-Wert' LANGUAGE 'GERMAN',
  month_name
    CLASSIFICATION caption VALUE 'Month Short Descriptive Value'
    CLASSIFICATION description VALUE 'Month Short Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN',
  month_long_name
    CLASSIFICATION caption VALUE 'Month Long Descriptive Value'
    CLASSIFICATION description VALUE 'Month Long Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des langen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des langen Monats' LANGUAGE 'GERMAN',
  month_name_de
    CLASSIFICATION caption VALUE 'Month Short Descriptive Value'
    CLASSIFICATION description VALUE 'Month Short Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN',
  month_long_name_de
    CLASSIFICATION caption VALUE 'Month Long Descriptive Value'
    CLASSIFICATION description VALUE 'Month Long Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des langen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des langen Monats' LANGUAGE 'GERMAN',
  month_end_date
    CLASSIFICATION caption VALUE 'Ending Date of Month Key Value'
    CLASSIFICATION description VALUE 'Ending Date of Month Key Value'
    CLASSIFICATION caption VALUE 'Enddatum des Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Enddatum des Monats' LANGUAGE 'GERMAN')
LEVEL MONTH
  CLASSIFICATION caption VALUE 'Month Level'
  CLASSIFICATION description VALUE 'Month Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Monatsebene' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Monat Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY month_id
  MEMBER NAME 
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN month_name_de
      ELSE month_name
    END
  MEMBER CAPTION 
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN month_name_de
      ELSE month_name
    END
  MEMBER DESCRIPTION 
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN month_long_name_de
      ELSE month_long_name
    END
  ORDER BY month_end_date
  DETERMINES (quarter_id, month_end_date)
LEVEL QUARTER
  CLASSIFICATION caption VALUE 'Quarter Level'
  CLASSIFICATION description VALUE 'Quarter Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Viertelniveau' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Quarter Aggregate Gruppierung' LANGUAGE 'GERMAN'
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY MAX month_end_date
  DETERMINES (year_id)
LEVEL YEAR
  CLASSIFICATION caption VALUE 'Year Level'
  CLASSIFICATION description VALUE 'Year Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Jahresniveau' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Jahres-Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY MAX month_end_date;</code>

<p>No changes are required to the TIME_HIER hierarchy.  The hierarchy will inherit the changes from the attribute dimension.</p>

<p>Set NLS_LANGUAGE = AMERICAN</p>

<code>ALTER SESSION SET nls_language = AMERICAN;</code>

<p>Select the MEMBER_NAME, MEMBER_CAPTION and MEMBER_DESCRIPTION attributes at the Month level</p>

<code>SELECT member_name,
  member_caption,
  member_description
FROM time_hier
WHERE level_name = 'MONTH';</code>

<p>Change the language in the current session to German.</p>

<code>ALTER SESSION SET nls_language = GERMAN;</code>

<p>Note the values of MEMBER_NAME are now returned from the attributes mapped to the _DE columns of the dimension table.</p>

<code>SELECT member_name,
  member_caption,
  member_description
FROM time_hier
WHERE level_name = 'MONTH';</code>",02-FEB-17 07.23.26.719910000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-NOV-19 06.26.36.247826000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92085961121496950370618538310591236805,92046253613399748579774477445429360470,"Spatial Integration and Support for GeoJSON",110,"<p>GeoJSON is a popular standard for representing spatial information in JSON documents. Oracle Database provides an extension to the JSON_VALUE operator that make it possible to use Oracle Databaseâ€™s powerful spatial features on JSON content containing information encoded using the GeoJSON standard. This allows location based queries to be performed on JSON documents.</p>

<p>In the following example, table CITY_LOT_FEATURES contains information about San Francisco city lots, including their coordinates. The coordinates are specified using GeoJSON. We'll start by creating a table containing FEATURE documents from the contents of the cityLots.json file.</p>

<p>First create a table with JSON Documents that include GeoJSON encoded information.</p>

<code>
drop table J_PURCHASEORDER
/
drop table CITY_LOT_FEATURES
/
create table CITY_LOT_FEATURES
as
select JSON_DOCUMENT as FEATURE
  from JSON_TABLE(
         xdburitype('/public/tutorials/json/testdata/cityLots.json').getClob(),
         '$[*]'
         columns (
           JSON_DOCUMENT VARCHAR2(32000) FORMAT JSON PATH '$'
         )
       )
 where rownum < 1000
</code>

<p>Let start by looking at the details of a feature object.</p>

<code>
select JSON_QUERY(FEATURE, '$' PRETTY) 
  from CITY_LOT_FEATURES
 where JSON_EXISTS(
        FEATURE,
        '$?(@.properties.MAPBLKLOT == $LOT)' 
         passing '0001001' as ""LOT""
       )
</code>

<p>As can be seen a â€œFeatureâ€ consists of a properties object that contains information that describes the feature and a GeoJSON â€œgeometryâ€ object that, in this example, consists of a set of coordinates that define a polygon that identifies where the feature is located.</p> 

<p>The remaining code snippets show how to query and index JSON documents that contain GeoJSON content. </p>

<p>The firststatement shows how to use JSON_VALUE to extract the GeoJSON information as an Oracle SDO_GEOMETRY object.</p>

<code>
select XMLTYPE(JSON_VALUE(
          FEATURE, 
          '$.geometry' 
          returning SDO_GEOMETRY  
          ERROR ON ERROR 
       ))
  from CITY_LOT_FEATURES
 where JSON_EXISTS(
         FEATURE,
         '$?(@.properties.MAPBLKLOT == $LOT)'
         passing '0001001' as ""LOT""
       )
</code>

<p>JSON_VALUE has been extended to support using Oracle Spatial to convert GeoJSON encoded location information into SDO_GEOMETRY objects. Generating SDO_GEOMETRY objects from GeoJSON allows all of the Oracle Spatial functionality to be used on this kind of content.</p>

<p>Since LiveSQL does not currently support rendering Oracle Object data types (in the case an SDO_GEOMETRY object) the SDO_GEOMETRY object returned by JSON_VALUE has been wrapped in an XMLTYPE constructor to generate an XML representation of the SDO_GEOMETRY object which can be rendered using LiveSQL</p>
â€ƒ
<p>The second statement shows how this allows the creation of spatial indexes on GeoJSON content.</p>

<code>
create index FEATURE_GEO_INDEX
          on CITY_LOT_FEATURES(
                   JSON_VALUE(
                       FEATURE, 
                       '$.geometry' 
                       returning SDO_GEOMETRY
                       ERROR ON ERROR
                   )
             ) 
             indextype is mdsys.spatial_index
</code>

<p>The fourth statement shows how we can use Oracle Spatial operators to query GeoJSON content. In this example we return the GeoJSON that describes any objects that are within 50 meters of the chosen object.</p>

<code>
select JSON_QUERY(s.FEATURE, '$.geometry')   SOURCE_LOCATION,
       JSON_QUERY(t.FEATURE, '$.geometry')   TARGET_LOCATION,       
       JSON_QUERY(t.FEATURE, '$.properties') TARGET_PROPERTIES,
       SDO_GEOM.SDO_DISTANCE (
          JSON_VALUE(t.FEATURE, '$.geometry' returning SDO_GEOMETRY ERROR ON ERROR), 
          JSON_VALUE(s.FEATURE, '$.geometry' returning SDO_GEOMETRY ERROR ON ERROR),
          .05
       ) DISTANCE
  from CITY_LOT_FEATURES s, CITY_LOT_FEATURES t
 where JSON_EXISTS(
         s.FEATURE,
         '$?(@.properties.MAPBLKLOT == $LOT)'
         passing '0001001' as ""LOT""
       )
   and SDO_WITHIN_DISTANCE(
          JSON_VALUE(t.FEATURE, '$.geometry' returning SDO_GEOMETRY ERROR ON ERROR),
          JSON_VALUE(s.FEATURE, '$.geometry' returning SDO_GEOMETRY ERROR ON ERROR),
          'distance=50'
       ) = 'TRUE'
  and  NOT JSON_EXISTS(
         t.FEATURE,
         '$?(@.properties.MAPBLKLOT == $LOT)'
         passing '0001001' as ""LOT""
       )
/
</code>",04-JAN-17 03.05.10.314606000 PM,"SHARON.KENNEDY@ORACLE.COM",02-FEB-23 01.58.42.128809000 PM,"ROGER.FORD@ORACLE.COM"
94172526454753519661883247551207627405,94172526449499528049838069157934586509,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>",24-JAN-17 03.03.13.778850000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.36.13.553370000 PM,"SHARON.KENNEDY@ORACLE.COM"
94172526455130704517603011853715954317,94172526449499528049838069157934586509,"Part 2: Your first simple pattern",40,"<p>For this data set we would expect to be searching for patterns within each stock symbol and to correctly find a ""pattern"" we will need to have the data ordered by the timestamp for each trade. Hopefully, that is an obvious statement.</p>
 
<p>Here is your first MATCH_RECOGNIZE statement. It's relatively simple and we are going to built on it during this tutorial. Firstly, notice that we have used the ORDER BY clause to sort the data by the timestamp for each trade.</p>
<p>Let's just run the code and view the results</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 ORDER BY tstamp 
 MEASURES e.tstamp as st, e.symbol as s, e.price as p 
 ONE ROW PER MATCH 
 PATTERN (e) 
 DEFINE 
 e AS price=price);
</code>
<p>
First observation: you should have noticed that the data is in random order when it comes to
the stock ticker because there is more than one record per tstamp. This is why the results appear in a random order in terms of the stock ticker column.</p>",24-JAN-17 03.05.27.020760000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.36.23.113739000 PM,"SHARON.KENNEDY@ORACLE.COM"
94175389750020502485684496388470791701,94172526449499528049838069157934586509,"What do PARTITION BY + ORDER BY do?",50,"<p>
If you have used the <a href=""https://docs.oracle.com/database/121/DWHSG/analysis.htm#DWHSG021"" target=""_blank"">analytic SQL functions (windows)</a> then you will already be familiar with how these keywords are used and MATCH_RECOGNIZE simply refuses these same concepts.</p>
<p>
The keywords <strong>PARTITION BY</strong> divides the data from the Ticker table into logical groups where, in this case, each group contains one stock symbol.</p>
<p>
This clause is optional but it is highly likely that every MATCH_RECOGNIZE clause that you write will contain a PARTITION BY clause</p>
<P>
The <strong>ORDER BY</strong> clause orders the data within each logical group. This clause is also optional but as before, it is highly likely that every MATCH_RECOGNIZE clause that you write will contain a ORDER BY clause</p>",24-JAN-17 04.07.34.106787000 PM,"KEITH.LAKER@ORACLE.COM",07-FEB-17 12.13.06.432447000 PM,"KEITH.LAKER@ORACLE.COM"
94175389750051934556994476747013152277,94172526449499528049838069157934586509,"What does the keyword MEASURES do?",60,"<p> 
The pattern matching clause enables you to create expressions useful in a wide range of analyses. These are presented as columns in the output table by using the <strong>MEASURES</strong> clause. This clause defines row pattern measure columns, whose value is computed by evaluating an expression related to a particular match.</p>
<p>
In our example we are defining three measures: 
<ol>
<li>the timestamp at the beginning of a V-shape (start_ts)</li> 
<li>the timestamp at the bottom of a V-shape (bottom_ts)</li> 
<li>the timestamp at the end of the a V-shape (end_ts)</li> 
</ol>
</p>
<p> 
The start_ts uses the FIRST() function to extract the first timestamp value from the rows matched to the DOWN pattern variable. The bottom_ts and end_ts measures use the LAST() function to extract the last timestamp values from the rows matched to the DOWN and UP pattern variables.</p>",24-JAN-17 04.09.15.954395000 PM,"KEITH.LAKER@ORACLE.COM",07-FEB-17 12.13.25.896515000 PM,"KEITH.LAKER@ORACLE.COM"
94179046562590156018239029797496425394,94172526449499528049838069157934586509,"How to control the output",70,"<p>
You will sometimes want summary data about the matches and other times need more detailed information. You can control this by using one of following clauses...</p>
<ul>
<li>ONE ROW PER MATCH</li>
<li>ALL ROWS PER MATCH</li> 
</ul>
<p>
<strong>ONE ROW PER MATCH</strong> means that for every match found, there will be one row of output.</p>
<p>
The alternative is to use the <strong>ALL ROWS PER MATCH</strong> keywords which provides a more verbose report and opens up the opportunity to use some of the built-in MEASURES that can be used to help you debug your pattern matching process</p>
<P>
Compare the output of the two statements:</p>
<code>SELECT *
FROM Ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES 
  FIRST(down.tstamp) AS start_ts,
  LAST(DOWN.tstamp) AS bottom_ts,
  LAST(UP.tstamp) AS end_ts
 ALL ROWS PER MATCH
 AFTER MATCH SKIP PAST LAST ROW
 PATTERN (STRT DOWN+ UP+)
 DEFINE
  DOWN AS price < PREV(price), 
  UP AS price > PREV(price)
) MR
ORDER BY MR.symbol, MR.start_ts;
</code>
<br><br>
<p>Notice that when we run the code below the number of columns returned is reduced as well as the number of rows</p>
<code>SELECT *
FROM Ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES 
  FIRST(down.tstamp) AS start_ts,
  LAST(DOWN.tstamp) AS bottom_ts,
  LAST(UP.tstamp) AS end_ts
 ONE ROW PER MATCH
 AFTER MATCH SKIP PAST LAST ROW
 PATTERN (STRT DOWN+ UP+)
 DEFINE
  DOWN AS price < PREV(price), 
  UP AS price > PREV(price)
) MR
ORDER BY MR.symbol, MR.start_ts;
</code>
<p>",24-JAN-17 04.17.28.481484000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.38.05.117756000 PM,"SHARON.KENNEDY@ORACLE.COM"
94179046564235504058734540104271530930,94172526449499528049838069157934586509,"Summary",110,"<p>This simple tutorial has explained the main keywords in the MATCH_RECOGNIZE clause. There is a lot more information about these keywords in the related tutorials that I have posted on this site.</p>",24-JAN-17 04.26.22.887482000 PM,"KEITH.LAKER@ORACLE.COM",24-JAN-17 04.26.22.887542000 PM,"KEITH.LAKER@ORACLE.COM"
95118622443906170559053485853920375102,95114558141387237177774563848501259509,"Examining the Sample Data",30,"<p>The dimension tables include columns for both English and German language data values that can be used as descriptive labels in hierarchies.  The German language column names can be identified by the _DE suffix.</p>

<p>View the values in the TIME_DIM table.</p>

<code>SELECT * FROM time_dim ORDER BY month_end_date;</code>

<p>Note that descriptive columns for months are available in English and German.</p>

<p>View the values in the PRODUCT_DIM table.</p>

<code>SELECT * FROM product_dim;</code>

<p>View the values in the GEOGRAPHY_DIM table.</p>

<code>SELECT * FROM geography_dim;</code>

<p>The _ID columns are used as keys and are joined to the fact table.  No changes to the to fact table are needed for support additional languages.</p>

<code>SELECT * FROM sales_fact WHERE rownum <= 10;</code>",02-FEB-17 04.51.13.541540000 PM,"WILLIAM.ENDRESS@ORACLE.COM",03-FEB-17 09.22.49.320716000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95127730628561727950764713897924127972,95114558141387237177774563848501259509,"Adding Language Support to Classifications",50,"<p>Use the LANGUAGE keyword to specify a language for a CLASSIFICATION.  When multiple language support is added to analytic view objects, the CLASSIFICATION is typically defined once without a language keyword to be used as the default and repeated with a language keyword to support additional languages.</p>

<p>The following attribute dimension includes classifications with support for German.</p>

<code> CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
  CLASSIFICATION caption VALUE 'Time Attribute Dimension'
  CLASSIFICATION description VALUE 'Company standard time attributes and levels'
  CLASSIFICATION caption VALUE 'Time Attribute Dimension' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Company standard time attributes and levels' LANGUAGE 'GERMAN'
USING time_dim
ATTRIBUTES
 (year_id
    CLASSIFICATION caption VALUE 'Year Key Value'
    CLASSIFICATION description VALUE 'Year Key Value'
    CLASSIFICATION caption VALUE 'Jahr-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Jahr-ID-Wert' LANGUAGE 'GERMAN',
  year_name
    CLASSIFICATION caption VALUE 'Year Descriptive Value'
    CLASSIFICATION description VALUE 'Year Descriptive Value'
    CLASSIFICATION caption VALUE 'Jahr Text Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Jahr Text Wert' LANGUAGE 'GERMAN',
  quarter_id
    CLASSIFICATION caption VALUE 'Quarter Key Value'
    CLASSIFICATION description VALUE 'Quarter Key Value'
    CLASSIFICATION caption VALUE 'Viertel-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Viertel-ID-Wert' LANGUAGE 'GERMAN',
  quarter_name
    CLASSIFICATION caption VALUE 'Quarter Descriptive Value'
    CLASSIFICATION description VALUE 'Quarter Descriptive Value'
    CLASSIFICATION caption VALUE 'Quarter Textwert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Quarter Textwert' LANGUAGE 'GERMAN',
  month_id
    CLASSIFICATION caption VALUE 'Month Key Value'
    CLASSIFICATION description VALUE 'Month Key Value'
    CLASSIFICATION caption VALUE 'Monatlicher ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Monatlicher ID-Wert' LANGUAGE 'GERMAN',
  month_name
    CLASSIFICATION caption VALUE 'Month Short Descriptive Value'
    CLASSIFICATION description VALUE 'Month Short Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des kurzen Monats' LANGUAGE 'GERMAN',
  month_long_name
    CLASSIFICATION caption VALUE 'Month Long Descriptive Value'
    CLASSIFICATION description VALUE 'Month Long Descriptive Value'
    CLASSIFICATION caption VALUE 'Text des langen Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Text des langen Monats' LANGUAGE 'GERMAN',
  month_end_date
    CLASSIFICATION caption VALUE 'Ending Date of Month Key Value'
    CLASSIFICATION description VALUE 'Ending Date of Month Key Value'
    CLASSIFICATION caption VALUE 'Enddatum des Monats' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Enddatum des Monats' LANGUAGE 'GERMAN')
LEVEL MONTH
  CLASSIFICATION caption VALUE 'Month Level'
  CLASSIFICATION description VALUE 'Month Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Monatsebene' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Monat Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY month_id
  MEMBER NAME month_name
  MEMBER CAPTION month_name
  MEMBER DESCRIPTION month_long_name
  ORDER BY month_end_date
  DETERMINES (quarter_id, month_end_date)
LEVEL QUARTER
  CLASSIFICATION caption VALUE 'Quarter Level'
  CLASSIFICATION description VALUE 'Quarter Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Viertelniveau' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Quarter Aggregate Gruppierung' LANGUAGE 'GERMAN'
  KEY quarter_id
  MEMBER NAME quarter_name
  MEMBER CAPTION quarter_name
  MEMBER DESCRIPTION quarter_name
  ORDER BY MAX month_end_date
  DETERMINES (year_id)
LEVEL YEAR
  CLASSIFICATION caption VALUE 'Year Level'
  CLASSIFICATION description VALUE 'Year Aggregate Groupings'
  CLASSIFICATION caption VALUE 'Jahresniveau' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Jahres-Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY year_id
  MEMBER NAME year_name
  MEMBER CAPTION year_name
  MEMBER DESCRIPTION year_name
  ORDER BY MAX month_end_date;</code>

<p>The following hierarchy includes classifications with support for German.</p>

<code>CREATE OR REPLACE HIERARCHY time_hier
  CLASSIFICATION caption VALUE 'Time'
  CLASSIFICATION description VALUE 'Time Hierarchy'
  CLASSIFICATION caption VALUE 'Zeit' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Zeithierarchie' LANGUAGE 'GERMAN'
USING time_attr_dim
 (month  CHILD OF
 quarter CHILD OF
 year);</code>

<p>Classifications are available from the data dictionary in both the default language and German.</p>

<code>SELECT * FROM user_attribute_dim_class;</code>

<code>SELECT * FROM user_attribute_dim_attr_class;</code>

<code>SELECT * FROM user_attribute_dim_lvl_class;</code>

<code>SELECT * FROM user_hier_class;</code>",02-FEB-17 06.22.02.815219000 PM,"WILLIAM.ENDRESS@ORACLE.COM",17-SEP-20 06.29.09.229272000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613542401826489003688044689238,92046253613535148271571315912996452182,"Adding the CACHE clause to the Analytic View",50,"<p>The CACHE clause includes a MEASURE GROUP list and a LEVELS list.  In Oracle 19c the cache type is always MATERIALIZED.  The measures and levels should match with key attribute columns in the materialized view. A cache can support multiple LEVEL lists.  Simply repeat the LEVEL list (with different values) and the MATERIALIZED keyword.</p>

<p>The CACHE clause in the following analytic view matches the materialized view created in the previous module.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  units FACT units,
  sales_prior_period AS
    (LAG(SALES) OVER (HIERARCHY time_hier OFFSET 1)),
  sales_share_prod_parent AS
   (SHARE_OF(sales HIERARCHY product_hier 
PARENT)),
  sales_share_geog_parent AS
   (SHARE_OF(sales HIERARCHY geography_hier PARENT))
  )
DEFAULT MEASURE SALES
-- Start cache clause
    CACHE
      -- List of measures in the materailized view.
      MEASURE GROUP (
          sales,
          units)
        LEVELS (
          -- List of attributes that matches the group by of the materialized view.
          time_hier.year,
          product_hier.department,
          geography_hier.country)
        -- Indicates a materialized view (no other cache types are currently supported).
        MATERIALIZED;</code>

<p>You can see the analytic view CACHE clause properties with the following query.</p>

<code>SELECT
  analytic_view_name,
  cache_type,
  hier_alias,
  level_name,
  measure_name
FROM user_analytic_view_lvlgrps;</code>

<p>Although the cache has yet to be created, the analytic view may still be queried.  The database will recognize the cache doesn't exist and access the detail tables.</p>",04-JAN-17 05.58.03.373537000 AM,"SYS",18-JAN-19 05.57.38.910157000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613543610752308618317219395414,92046253613535148271571315912996452182,"Querying the Analytic View",70,"<p>When the CACHE clause is added to the analytic view and there is a matching, fresh, rewrite enabled materialized view the database will automatically rewrite SQL generated by the analytic view to the materialized view.  No hints are required.</p>

<p>The following query aggregates data to the same levels of the materialized view (Year, Department and Country).  Write the SQL execution plan to the plan table.</p>

<code><p>Enable the materialized view for query rewrite.</p>

<code>ALTER MATERIALIZED VIEW sales_av_cache_0 ENABLE QUERY REWRITE;</code>

<p>You can check the state of the SALES_AV_CACHE_0 materialized view with the following query.</p>

<code>SELECT
  mview_name,
  rewrite_capability,
  staleness,
  rewrite_enabled
FROM user_mviews
WHERE mview_name = 'SALES_AV_CACHE_0';</code>
EXPLAIN PLAN
SET STATEMENT_ID = '3' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'COUNTRY'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_AV_0 table is accessed using query rewrite.</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '3'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '3';</code>

<p>The next query access data above the materialized view (Year, Department and Region levels). Write the plan to the plan table.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '4' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'REGION'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_AV_CACHE_0 table is accessed using query rewrite</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '4'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '4';</code>

<p>The database can rewrite the query to the materialized view when there are a subset of hierarchies included in the query. In this example the query uses only the TIME_HIER and PRODUCT_HIER hierarchies. Write the plan to the plan table.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '5' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name
  sales,
  units
FROM
  sales_av HIERARCHIES
 (time_hier, product_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_AV_CACHE_0 table is accessed via query rewrite and that there is further aggregation with GROUP BY.</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '5'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '5';</code>",04-JAN-17 05.58.03.374298000 AM,"SYS",09-JAN-24 08.53.37.374065000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613544819678128232946394101590,92046253613535148271571315912996452182,"About the State of the MV and Query Rewrite",80,"<p>The analytic view respects the state of materialized view and query rewrite parameters, just like any other query.</p>

<ul>
<li>The materialized view must be enabled for query rewrite. </li>
<li>Query rewrite must be enabled (e.g., ALTER SESSION ENABLE QUERY REWRITE;).</li>
<li>The materialized by must be fresh or QUERY_REWRITE_INTEGRITY must be TRUE.</li>
</ul>

<p>The following update statement will cause the SALES_AV_CACHE_0 materialized view to become stale.</p>

<code>UPDATE sales_fact
SET units = 100
WHERE month_id = 'Apr-11'
  AND category_id = '-535'
  AND state_province_id = 'ALBERTA_CA';
COMMIT;</code>

<p>Note that the materialized view is now stale.</p>

<code>SELECT
  mview_name,
  rewrite_capability,
  staleness,
  rewrite_enabled
FROM user_mviews
WHERE mview_name = 'SALES_AV_CACHE_0';</code>

<p>Examine the SQL execution plan and note that the query does not rewrite to the materialized view.</p>

<code>EXPLAIN PLAN
SET STATEMENT_ID = '6' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'COUNTRY'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '6'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '6';</code>

<p>Alter the session QUERY_REWRITE_INTEGRITY = STALE_TOLERATED;</p>

<code>ALTER SESSION SET QUERY_REWRITE_INTEGRITY = STALE_TOLERATED;</code>

<p>Run the SQL execution plan again. Note that the query will now rewrite to the materialized view.</p>

<code>EXPLAIN PLAN
SET STATEMENT_ID = '7' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'COUNTRY'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>
  
<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '7'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '7';</code>",04-JAN-17 05.58.03.375776000 AM,"SYS",09-JAN-24 07.22.51.865250000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613546028603947847575568807766,92046253613535148271571315912996452182,"Using Multiple Aggregate Caches",90,"<p>You can use multiple aggregate caches with an analytic view by including multiple LEVELS groups in the CACHE clause.</p>

<p>The first aggregate cache included data the the Year, Department and Country levels.  The following statement add a second LEVELS group at the Year, Department, and Region levels..</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  units FACT units,
  sales_prior_period AS
    (LAG(SALES) OVER (HIERARCHY time_hier OFFSET 1)),
  sales_share_prod_parent AS
   (SHARE_OF(sales HIERARCHY product_hier PARENT)),
  sales_share_geog_parent AS
   (SHARE_OF(sales HIERARCHY geography_hier PARENT))
  )
DEFAULT MEASURE SALES
    -- Start cache clause
    CACHE
      -- List of measures in the aggregate cache.
      MEASURE GROUP (
          sales,
          units)
        LEVELS (
          -- List of levels in the aggregate cache.
          time_hier.year,
          product_hier.department,
          geography_hier.country)
        -- Indicates a materialized view.  On Autonomous Database or 23c,
        -- MATERIALIZED USING table_name directly accesses a table.
        MATERIALIZED
        LEVELS (
          -- List of levels in the aggregate cache.
          time_hier.year,
          product_hier.department,
          geography_hier.region)
        -- Indicates a materialized view.  On Autonomous Database or 23c,
        -- MATERIALIZED USING table_name directly accesses a table.
        MATERIALIZED;</code>

<p>You can see the analytic view CACHE clause properties with the following query.  Note that the new cache is CACHE_IDX 1.</p>

<code>SELECT
  analytic_view_name,
  av_lvlgrp_order,
  cache_type,
  hier_alias,
  level_name,
  measure_name
FROM user_analytic_view_lvlgrps
ORDER BY 
  av_lvlgrp_order,
  level_name NULLS LAST,
  hier_alias NULLS FIRST,
  measure_name NULLS FIRST;</code>

<p>Create the materialized view for CACHE_IDX 1.</p>

<code>DECLARE
  cache_sql CLOB;
BEGIN
  cache_sql := dbms_hierarchy.get_mv_sql_for_av_cache (
              analytic_view_name => 'SALES_AV'
               , cache_idx => 1    -- VALUE FROM THE PREVIOUS QUERY
            );

  EXECUTE IMMEDIATE
    'CREATE MATERIALIZED VIEW sales_av_cache_1 AS ' || cache_sql;

END;
/</code>

<p>Check that the materialized view was created.</p>

<code>SELECT * FROM user_mviews;</code>

<p>If you want to see the SQL, you run the following SELECT statement.</p>

<code>SELECT TO_CHAR(dbms_hierarchy.get_mv_sql_for_av_cache (
    analytic_view_name => 'SALES_AV'
    , cache_idx => 1 ))
    FROM dual;</code>

<p>Enable the materialized view for query rewrite.</p>

<code>ALTER MATERIALIZED VIEW sales_av_cache_1 ENABLE QUERY REWRITE;</code>

<p>You can check the state of the SALES_AV_CACHE_1 materialized view with the following query.</p>

<code>SELECT
  mview_name,
  rewrite_capability,
  staleness,
  rewrite_enabled
FROM user_mviews
WHERE mview_name = 'SALES_AV_CACHE_1';</code>

<p>The following query selects at the Year, Department and Country.  This query will rewrite to SALES_AV_CACHE_0.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '8' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'COUNTRY'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_AV_CACHE_0 table is accessed</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '8'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '8';</code>

<p>The next query accesses data at the Year, Department and Region levels. Write the plan to the plan table.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '9' INTO plan_table FOR
SELECT
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name,
  sales,
  units
FROM
  sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'REGION'
ORDER BY 
  time_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;</code>

<p>Query the plan table. Note that the SALES_AV_CACHE_1 table is accessed</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
WHERE operation like '%ACCESS%'
START WITH id       = 0
 AND statement_id      = '9'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '9';</code>",04-JAN-17 05.58.03.379789000 AM,"SYS",09-JAN-24 08.49.28.129764000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
92046253613547237529767462204743513942,92046253613535148271571315912996452182,"Implementation Tips",100,"<p>Consider the following tips when using materialized views with analytic views.</p>

<ul>
<li>Using integer keys for the key attributes in the attribute dimension and thus for the SELECT list in the defining query of the materialized view can result in a materialized view that is quicker to create, smaller and faster to query as compared to using longer text values.</li>
<li>If possible, use Database In-Memory for both the fact table and the materialized view.  A materialized view will create much more quickly from a fact table that is in the in-memory column store.  Include the VECTOR_TRANSFORM hint in the defining query of the materialized view to force the vector transform SQL execution plan (a.k.a. in-memory aggregation).  Load the materialized view into the in-memory column store for faster query (e.g., ALTER TABLE sales_av_mv INMEMORY).
<li>Generally speaking, queries at higher levels of aggregation benefit more from materialized views because those queries access and aggregate larger numbers of fact rows.  Try a few different level groupings to see what works best.</li> 
<li>Consider using more than one materialized view.  For example, one MV for the top-most aggregates and another for mid-level aggregates. Include a CACHE clause for each MV.  The analytic view will try to use the closest MV to a query.</li>
</ul>",04-JAN-17 05.58.03.381618000 AM,"SYS",18-AUG-17 07.04.45.154529000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
94175641853958692698441400836814463251,94172526449499528049838069157934586509,"What does the keyword PATTERN do?",90,"<p>
The <strong>PATTERN</strong> clause defines a regular expression in similar way to the existing Oracle regular expression functions. However, within this clause we have create a completely new a highly expressive way to search for patterns.
</p>
<p>In the above example, the <strong>PATTERN</strong> clause is defined as:
<br><br>
<strong>PATTERN (STRT DOWN+ UP+)</strong>
<br><br>
this syntax indicates that the pattern we are searching for has three pattern variables: STRT, DOWN, and UP. The plus sign (+) after DOWN and UP means that at least one row must be mapped to each of them.</p> 
<p>
The plus sign (+) is known as a ""quantifier"" and there is a large library of quantifiers available to help you define your own patterns. Quantifiers define the number of iterations accepted for a match. Use the link below to access the documentation to learn more about quantifiers.</p>",24-JAN-17 03.17.21.921646000 PM,"KEITH.LAKER@ORACLE.COM",07-FEB-17 12.14.17.885051000 PM,"KEITH.LAKER@ORACLE.COM"
94175641854360056070553457722816913683,94172526449499528049838069157934586509,"What does the keyword DEFINE do?",100,"<p>
The <strong>DEFINE</strong> keyword gives us the conditions that must be met for a row to map to your row pattern variables <strong>STRT</strong>, <strong>DOWN</strong>, and <strong>UP</strong>. Because there is no condition for STRT, any row can be mapped to STRT. It is in effect and ""always-true"" event.</p>
<p>
Why have a pattern variable with no condition?</p>
<p> 
We are using the STRT variable as a starting point for testing and anchoring our matching process. It ensures that we capture and test our pattern beginning at the very first row within each of our partition.</p>
<p>
Both DOWN and UP take advantage of the PREV() function, which lets them compare the price in the current row to the price in the prior row. DOWN is matched when a row has a lower price than the row that preceded it, so it defines the downward (left) leg of our V-shape.</p>
<p> 
A row can be mapped to UP if the row has a higher price than the row that preceded it.â€¨</p>",24-JAN-17 03.28.27.087289000 PM,"KEITH.LAKER@ORACLE.COM",07-FEB-17 12.14.28.725020000 PM,"KEITH.LAKER@ORACLE.COM"
94175641855160364963138342236472402195,94172526449499528049838069157934586509,"What does AFTER MATCH SKIP do?",80,"<p>
The <strong>AFTER MATCH SKIP PAST LAST ROW</strong> clause means that whenever we find a match, we restart the search process for the next match at the next row after the last row mapped to the UP pattern variable. This is the default behavior.</p>
<p>
We have a lot of control over where to restart the matching process. For this example what we want to explore is the impact on the results if we change the default behavior. Lets try <strong>AFTER MATCH SKIP TO LAST UP</strong> and see what happens</p>
<code>SELECT *
FROM Ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES 
  FIRST(down.tstamp) AS start_ts,
  LAST(DOWN.tstamp) AS bottom_ts,
  LAST(UP.tstamp) AS end_ts
 ONE ROW PER MATCH
 AFTER MATCH SKIP TO LAST UP
 PATTERN (STRT DOWN+ UP+)
 DEFINE
  DOWN AS price < PREV(price), 
  UP AS price > PREV(price)
) MR
ORDER BY MR.symbol, MR.start_ts;
</code>
<p>You will now find that more rows are returned because we are starting our search for the next match at a point where we a guaranteed to find that the price of next row is lower than the price of the previous row. The STRT variable is mapped to the last row mapped to up - in effect we have a row being double mapped.</p>
<p>Hopefully, it is obvious that this is an important clause and you will need to think very carefully about where you to restart searching for your patterns.</p>",24-JAN-17 03.49.26.800335000 PM,"KEITH.LAKER@ORACLE.COM",11-JAN-21 07.24.20.561182000 AM,"KEITH.LAKER@ORACLE.COM"
92046253613529103642473242767122921302,92046253613524267939194784250424096598,"Inserting staging data",40,"<p>Now letâ€™s try inserting the data from our staging table into the EMP table and see what happens:</p>
<code>INSERT INTO emp SELECT * FROM staging_emp;
</code>
<p>â€¦ and not surprisingly I get the following error:</p>
<p>
<blockquote>
SQL Error: ORA-01722: invalid number<br>
</blockquote>
</p>
<p>because the data in the staging table contains values that fail the conversion. But which values (rows/columns) caused the error?</p> ",04-JAN-17 05.58.02.116991000 AM,"SYS",18-DEC-18 03.21.58.511939000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613530312568292857396297627478,92046253613524267939194784250424096598,"Finding rogue values with VALIDATE_CONVERSION function",50,"<p>We can deal with this situation in a couple of different ways. Firstly letâ€™s try and discover which rows and columns in my staging table contain values that are likely to cause data conversion errors. </p>
<p>To do this I am going to use the new VALIDATE_CONVERSION() function which identifies problem data that cannot be converted to the required data type. It returns 1 if a given expression can be converted to the specified data type, else it returns 0.</p>
<code>SELECT
  VALIDATE_CONVERSION(empno AS NUMBER) AS is_empno,
  VALIDATE_CONVERSION(mgr AS NUMBER) AS is_mgr,
  VALIDATE_CONVERSION(hiredate AS DATE) AS is_hiredate,
  VALIDATE_CONVERSION(sal AS NUMBER) AS is_sal,
  VALIDATE_CONVERSION(comm AS NUMBER) AS is_comm,
  VALIDATE_CONVERSION(deptno AS NUMBER) AS is_deptno
FROM staging_emp;
</code>
<p>the output from the above query enables us to easily pick out the rows where the data conversion is going to succeed (column value is 1) and fail (column value is 0).</p>
<p>We can use this information to filter the data in my staging table as we insert it into the EMP table or we can use the enhanced CAST and TO_xxx functions within the INSERT INTO â€¦.. SELECT statements.</p>",04-JAN-17 05.58.02.128077000 AM,"SYS",18-DEC-18 03.22.24.659697000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613531521494112472025472333654,92046253613524267939194784250424096598,"Using using the enhanced CAST function",60,"<p>We can use this information to filter the data in our staging table as I insert it into the EMP table or we could use the enhanced CAST and TO_xxx functions within the INSERT INTO â€¦.. SELECT statements.</p>
<p>The CAST function (along with TO_NUMBER, TO_BINARY_FLOAT, TO_BINARY_DOUBLE, TO_DATE, TO_TIMESTAMP, TO_TIMESTAMP_TZ, TO_DSINTERVAL, and TO_YMINTERVAL functions) can now return a user-specified value, instead of an error, when data type conversion errors occur. This reduces failures during an data transformation and data loading processes.</p>
<p>Therefore, the new 12.2 self-validating SELECT statement looks like this:</p>
<code>INSERT INTO emp
SELECT
  empno,
  ename,
  job,
  CAST(mgr AS NUMBER DEFAULT 9999 ON CONVERSION ERROR),
  CAST(hiredate AS DATE DEFAULT sysdate ON CONVERSION ERROR),
  CAST(sal AS NUMBER DEFAULT 0 ON CONVERSION ERROR),
  CAST(comm AS NUMBER DEFAULT null ON CONVERSION ERROR),
  CAST(deptno AS NUMBER DEFAULT 99 ON CONVERSION ERROR)
FROM staging_emp
WHERE VALIDATE_CONVERSION(empno AS NUMBER) = 1;
</code>",04-JAN-17 05.58.02.129590000 AM,"SYS",18-DEC-18 03.22.53.386919000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613532730419932086654647039830,92046253613524267939194784250424096598,"New staging data correctly inserted",70,"<p>We now have five rows added to our EMP table. Here is the data that was loaded:</p>
<code>SELECT * FROM emp;
</code>
<p>We can see that on row 1 the HIERDATE was invalid so it was replaced by the value from sys date (07-JUL-16). Row 2 the value of DEPTNO is the conversion default of 99 and on row 4 the value for MGR is the conversion default of 9999.</p>
<p>The fact that we only loaded 5 rows obviously this means that 4 rows were rejected during the insert process. We look at the rows in the staging table that failed to be inserted by using the VALIDATE_CONVERSION function:</p>
<code>SELECT * FROM staging_emp
WHERE VALIDATE_CONVERSION(empno AS NUMBER) = 0;
</code>
<p>We can see the rows that were rejected because they contain errors converting the value in the empno column to a number to create the empno key.</p>",04-JAN-17 05.58.02.130206000 AM,"SYS",18-DEC-18 03.23.09.332387000 PM,"SHARON.KENNEDY@ORACLE.COM"
92046253613533939345751701283821746006,92046253613524267939194784250424096598,"Conclusion",80,"<p>The enhanced  CAST function (along with TO_NUMBER, TO_BINARY_FLOAT, TO_BINARY_DOUBLE, TO_DATE, TO_TIMESTAMP, TO_TIMESTAMP_TZ, TO_DSINTERVAL, and TO_YMINTERVAL functions) can help you deal with data conversion errors without having to resort to complicated PL/SQL code or writing data validation routines within your application code.</p>
<p>The new VALIDATE_CONVERSION() function can be used to help you identify column values that cannot be converted to the required data type.</p>
<p>Hope these two features are useful. Enjoy!</p>",04-JAN-17 05.58.02.130896000 AM,"SYS",04-JAN-17 05.58.02.130953000 AM,"SYS"
92046253613536357197390930542171158358,92046253613535148271571315912996452182,"Introduction",10,"<p>Materialized views with aggregate level data can be used to accelerate queries that select from an analytic view.  The database uses automatic query rewrite to redirect the query from the detailed fact table to the materialized view.  In order to encourage the database to rewrite queries to the materialized view, the analytic view can generate SQL that is materialized view aware.</p>

<p>The <strong>CACHE</strong> clause is used to cause the analytic view to generate materialized view aware SQL.  The effect on query performance can be significant.  For example, a query that selects aggregate level data from a large fact table might be reduced from many seconds to less than a second when a materialized view is use.</p>

<p>This tutorial provides an example of using a materialized view using the AV sample schema.  You will create a materialized view, add the CACHE clause to the definition of an analytic view and query the analytic view with and without the materialized view.</p>",04-JAN-17 05.58.03.366340000 AM,"SYS",04-JAN-17 05.58.03.366398000 AM,"SYS"
92046253613537566123210545171345864534,92046253613535148271571315912996452182,"Setup",20,"<p>If you have not already done so, click <i>Execute the SQL required by this tutorial</i> which is before the list of modules in this tutorial.  The setup script will copy tables from the AV schema into the current schema and create attribute dimensions and hierarchies for times, products and geographies.</p>

<p>You can check that these objects have been created using the following queries.</p>

<p>Attribute dimensions.</P>

<code>SELECT * FROM user_attribute_dimensions;</code>

<p>Hierarchies.</p>

<code>SELECT * FROM user_hierarchies;</code>",04-JAN-17 05.58.03.367183000 AM,"SYS",30-JAN-17 06.40.17.930081000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
95519916158275603442217197932263393857,94172526449499528049838069157934586509,"How to group your data",41,"<p>What we need to do is group the data by each stock ticker symbol so we need to use the PARTITION BY clause</p>
<p>Let's amend our example and now we do it the right way by adding a PARTITION BY clause:</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 MEASURES e.tstamp as st, e.symbol as s, e.price as p 
 ONE ROW PER MATCH 
 PATTERN (e) 
 DEFINE 
 e AS price=price);
</code>
<p>There is more information about PARTITION BY and ORDER BY in section 14 of this tutorial</p>",06-FEB-17 12.10.18.717655000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.36.32.620796000 PM,"SHARON.KENNEDY@ORACLE.COM"
95519916158504090422124362846282861121,94172526449499528049838069157934586509,"Correct number of rows?",42,"<p>Now let's change our simple pattern and test to see if the timestamp in the current row is greater than the timestamp in the previous row. Which it obviously will be because we are ordering by the timestamp column. The new pattern is described within the DEFINE clause.
</p> 
<p>How many rows will be returned when you run this query?
</p>

<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 ALL ROWS PER MATCH 
 PATTERN (e+) 
 DEFINE 
  e AS tstamp > prev(tstamp));
</code>

<p>You should be expecting to see 60 rows returned because that is how many rows are in our source table. So why do we only get 57 rows returned and not 60?
</p>
<p> This answer is because the very first record in each partition has no previous timestamp to compare it with, therefore, the first record in each partition is not matched. As we have three partitions (3 symbols: ACME, GLOBEX, OSCORP) that means we lose three rows.</p>",06-FEB-17 12.20.15.352258000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.36.42.852881000 PM,"SHARON.KENNEDY@ORACLE.COM"
95519916158822037912683010319230585409,94172526449499528049838069157934586509,"Using ALWAYS TRUE events",43,"<p>To correct the ""missing first row"" issue we need to add an always true event to our PATTERN clause, as shown below.
</p>
<p>This query will now return all 60 rows from our dataset as expected because the timestamp for each row is greater than the timestamp in the previous row.
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol ORDER BY tstamp 
 ALL ROWS PER MATCH 
 PATTERN (strt e+) 
 DEFINE 
  e AS tstamp > prev(tstamp));
</code>",06-FEB-17 12.35.19.953433000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.36.51.790941000 PM,"SHARON.KENNEDY@ORACLE.COM"
95519916159066240928245165412521232961,94172526449499528049838069157934586509,"Checking results",44,"<p>How can we check the results from our MATCH_RECOGNIZE query? We can use two of the built-in measures to help us make sure that our pattern is being correctly applied to our data set. These two measures are CLASSIFIER and MATCH_NUMBER().
</p>
<p>Let's run the code below and focus on the results just for symbol ACME by adding a WHERE clause.
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
PARTITION BY symbol ORDER BY tstamp 
MEASURES classifier() event, 
         match_number() match 
ALL ROWS PER MATCH 
PATTERN (strt e+) 
DEFINE 
 e AS tstamp > prev(tstamp))
WHERE symbol='ACME';
</code>
<p>What you should see is that the first row (01-Apr-11) is mapped to our always-true event ""STRT"" and all the other rows in our partition are mapped to our timestamp-based event ""E"".</p>",06-FEB-17 12.46.59.876128000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.37.08.960141000 PM,"SHARON.KENNEDY@ORACLE.COM"
95519916159074703408982467816744176193,94172526449499528049838069157934586509,"MATCH_NUMBER and partitions",45,"<p>Now let's look at what happens to our MATCH_NUMBER() function within each PARTITION by removing the WHERE clause.
</p>
<p>Let's run the code below:
</p>
<code>SELECT * FROM ticker 
MATCH_RECOGNIZE (
PARTITION BY symbol ORDER BY tstamp 
MEASURES e.tstamp as t, 
         e.symbol as s, 
         e.price as p, 
         classifier() event, 
         match_number() match 
ALL ROWS PER MATCH 
PATTERN (strt e+) 
DEFINE 
 e AS tstamp > prev(tstamp));
</code>
<p>What you should see is that the first row (01-Apr-11) in each partition is mapped to the ""STRT"" event and the MATCH_NUMBER function starts at 1 within each partition.</p>",06-FEB-17 12.49.21.007299000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.37.20.791338000 PM,"SHARON.KENNEDY@ORACLE.COM"
95523525717113967070932714709261674718,94172526449499528049838069157934586509,"Part 3: Searching for V-Shaped Patterns",46,"<p>Now let's build a slightly more complex example where we search our dataset for V-shaped trading patterns, i.e. where the price goes down and then the price goes up.
</p>
<p>Below we have changed the PATTERN and DEFINE clauses to use an always-true event (so we can capture the first row in each partition) as before along with two pattern variables that will test the value of the price in the current row against the price in the previous row</p>
<p>Let's run the code:</p>

<code>SELECT *
FROM Ticker 
MATCH_RECOGNIZE (
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES 
  FIRST(DOWN.tstamp) AS start_ts,
  LAST(DOWN.tstamp) AS bottom_ts,
  LAST(UP.tstamp) AS end_ts
 ONE ROW PER MATCH
 AFTER MATCH SKIP PAST LAST ROW
 PATTERN (STRT DOWN+ UP+)
 DEFINE
  DOWN AS price < PREV(price), 
  UP AS price > PREV(price)
) MR
ORDER BY MR.symbol, MR.start_ts;
</code>
<p>You should see 6 rows returned</p> 
<p>The following sections will explain each of the main keywords in more detail.</p>",06-FEB-17 12.56.37.102717000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.37.32.905057000 PM,"SHARON.KENNEDY@ORACLE.COM"
95135412885674036825671476906071564924,95114558141387237177774563848501259509,"Product and Geography Hierarchies",70,"<p>Use the following statements to create additional attribute dimension and hierarchies with support for German.</p>

<p>Product attribute dimension.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
  CLASSIFICATION caption VALUE 'Product Attribute Dimension'
  CLASSIFICATION description VALUE 'Product attributes and levels'
  CLASSIFICATION caption VALUE 'Produkt-Attribute und Ebenen' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Produkt-Attribute und Ebenen' LANGUAGE 'GERMAN'
USING product_dim 
ATTRIBUTES
 (department_id
    CLASSIFICATION caption VALUE 'Department Key Value'
    CLASSIFICATION description VALUE 'Department Key Value'
    CLASSIFICATION caption VALUE 'Abteilungs-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Abteilungs-ID-Wert' LANGUAGE 'GERMAN',
  department_name
    CLASSIFICATION caption VALUE 'Department Text Value'
    CLASSIFICATION description VALUE 'Department Text Value'   
    CLASSIFICATION caption VALUE 'Abteilung Text Wert (Englisch)' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Abteilung Text Wert (Englisch)' LANGUAGE 'GERMAN',
  department_name_de
    CLASSIFICATION caption VALUE 'Department Text Value (German)'
    CLASSIFICATION description VALUE 'Department Text Value (German)'   
    CLASSIFICATION caption VALUE 'Abteilung Text Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Abteilung Text Wert' LANGUAGE 'GERMAN',
  category_id
    CLASSIFICATION caption VALUE 'Category Key Value'
    CLASSIFICATION description VALUE 'Category Key Value'
    CLASSIFICATION caption VALUE 'Kategorie-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Kategorie-ID-Wert' LANGUAGE 'GERMAN',
  category_name
    CLASSIFICATION caption VALUE 'Category Text Value'
    CLASSIFICATION description VALUE 'Category Text Value' 
    CLASSIFICATION caption VALUE 'Kategorien Text Wert (Englisch)' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Kategorien Text Wert (Englisch)' LANGUAGE 'GERMAN',
  category_name_de
    CLASSIFICATION caption VALUE 'Category Text Value (German)'
    CLASSIFICATION description VALUE 'Category Text Value (German)' 
    CLASSIFICATION caption VALUE 'Kategorien Text Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Kategorien Text Wert' LANGUAGE 'GERMAN')
LEVEL DEPARTMENT
    CLASSIFICATION caption VALUE 'Department'
    CLASSIFICATION description VALUE 'Department Aggregate Grouping'
    CLASSIFICATION caption VALUE 'Abteilungsebene' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Abteilung Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY department_id
  MEMBER NAME 
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN department_name_de
      ELSE department_name
    END
  MEMBER CAPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN department_name_de
      ELSE department_name
    END
  MEMBER DESCRIPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN department_name_de
      ELSE department_name
    END
  ORDER BY department_name
LEVEL CATEGORY
  CLASSIFICATION caption VALUE 'Category'
  CLASSIFICATION description VALUE 'Category Aggregate Grouping'
  CLASSIFICATION caption VALUE 'Kategorien Ebene' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Kategorie Aggregatgruppierung' LANGUAGE 'GERMAN'
  KEY category_id
  MEMBER NAME
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN category_name_de
      ELSE category_name
    END
  MEMBER CAPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN category_name_de
      ELSE category_name
    END
  MEMBER DESCRIPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN category_name_de
      ELSE category_name
    END
  ORDER BY category_name
  DETERMINES(department_id)
ALL MEMBER NAME 'ALL PRODUCTS';</code>

<p>Product hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY product_hier
  CLASSIFICATION caption VALUE 'Product Hierarchy'
  CLASSIFICATION description VALUE 'Product Hierarchy'
  CLASSIFICATION caption VALUE 'Produkthierarchie' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Produkthierarchie' LANGUAGE 'GERMAN'
USING product_attr_dim
 (CATEGORY
  CHILD OF department);</code>

<p>Geography attribute dimension.  Note that German is available in the dimension table for only the Region and Country levels.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION geography_attr_dim
  CLASSIFICATION caption VALUE 'Geography Attribute Dimension'
  CLASSIFICATION description VALUE 'Geography attributes and levels'
  CLASSIFICATION caption VALUE 'Erdkunde-Attribute und Ebenen' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Erdkunde-Attribute und Ebenen' LANGUAGE 'GERMAN'
USING geography_dim
ATTRIBUTES
 (region_id
   CLASSIFICATION caption VALUE 'Region Key Value'
    CLASSIFICATION description VALUE 'Region Key Value'
    CLASSIFICATION caption VALUE 'Region-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Region-ID-Wert' LANGUAGE 'GERMAN',
  region_name
    CLASSIFICATION caption VALUE 'Region Text Value'
    CLASSIFICATION description VALUE 'Region Text Value'
    CLASSIFICATION caption VALUE 'Region-Text-Wert (Englisch)' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Region-Text-Wert (Englisch)' LANGUAGE 'GERMAN', 
  region_name_de
    CLASSIFICATION caption VALUE 'Region Text Value (German)'
    CLASSIFICATION description VALUE 'Region Text Value (German)'
    CLASSIFICATION caption VALUE 'Region-Text-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Region-Text-Wert' LANGUAGE 'GERMAN', 
  country_id
    CLASSIFICATION caption VALUE 'Country Key Value'
    CLASSIFICATION description VALUE 'Country Key Value'
    CLASSIFICATION caption VALUE 'Land-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Land-ID-Wert' LANGUAGE 'GERMAN',
  country_name
    CLASSIFICATION caption VALUE 'Country Text Value'
    CLASSIFICATION description VALUE 'Country Text Value'
    CLASSIFICATION caption VALUE 'Land-Text-Wert (Englisch)' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Land-Text-Wert (Englisch)' LANGUAGE 'GERMAN',
  country_name_de
    CLASSIFICATION caption VALUE 'Country Text Value (German)'
    CLASSIFICATION description VALUE 'Country Text Value (German)'
    CLASSIFICATION caption VALUE 'Land-Text-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Land-Text-Wert' LANGUAGE 'GERMAN',
  state_province_id
    CLASSIFICATION caption VALUE 'State/Province Key Value'
    CLASSIFICATION description VALUE 'State/Province Key Value'
    CLASSIFICATION caption VALUE 'aBundesland-ID-Wert' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Bundesland-ID-Wert' LANGUAGE 'GERMAN',  
  state_province_name
    CLASSIFICATION caption VALUE 'State/Province Text Value'
    CLASSIFICATION description VALUE 'State/Province Text Value'
    CLASSIFICATION caption VALUE 'Bundesland-Text-Wert (Englisch)' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Bundesland-Text-Wert (Englisch)' LANGUAGE 'GERMAN'
  )
LEVEL REGION
  KEY region_id
  ALTERNATE KEY region_name
  MEMBER NAME
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN region_name_de
      ELSE region_name
    END
  MEMBER CAPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN region_name_de
      ELSE region_name
    END
  MEMBER DESCRIPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN region_name_de
      ELSE region_name
    END
  ORDER BY region_name
LEVEL COUNTRY
  KEY country_id
  MEMBER NAME
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN country_name_de
      ELSE country_name
    END
  MEMBER CAPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN country_name_de
      ELSE country_name
    END
  MEMBER DESCRIPTION
    CASE
      WHEN sys_context('USERENV','LANGUAGE') LIKE 'GERMAN%'
        THEN country_name_de
      ELSE country_name
    END
  ORDER BY country_name
  DETERMINES(region_id)
LEVEL STATE_PROVINCE
  KEY state_province_id
  -- Note:  State/Province names only provided in English.
  MEMBER NAME state_province_name
  MEMBER CAPTION state_province_name
  MEMBER DESCRIPTION state_province_name
  ORDER BY state_province_name
  DETERMINES(country_id)
ALL MEMBER NAME 'ALL CUSTOMERS';</code>

<p>Geography hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY geography_hier
  CLASSIFICATION caption VALUE 'Geography Hierarchy'
  CLASSIFICATION description VALUE 'Geography Hierarchy'
  CLASSIFICATION caption VALUE 'Geographische Hierarchie' LANGUAGE 'GERMAN'
  CLASSIFICATION description VALUE 'Geographische Hierarchie' LANGUAGE 'GERMAN'
USING geography_attr_dim
 (state_province
  CHILD OF country
  CHILD OF region);</code>

<p>Set NLS_LANGUAGE = AMERICAN</p>

<code>ALTER SESSION SET nls_language = AMERICAN;</code>

<p>Query attributes and hierarchical attributes from the PRODUCT_HIER hierarchy.  Note that both English and German are available from attributes and the hierarchical attributes return English values.</p>

<code>SELECT
  department_name,
  category_name,
  department_name_de,
  category_name_de,
  member_name,
  member_caption,
  member_description
FROM product_hier
WHERE level_name = 'CATEGORY'</code>;

<p>Set NLS_LANGUAGE = GERMAN</p>

<code>ALTER SESSION SET nls_language = GERMAN;</code>

<p>Query attributes and hierarchical attributes from the PRODUCT_HIER hierarchy.  Note that the attributes continue to return English and German values and the hierarchical attributes now return German values.</p>

<code>SELECT
  department_name,
  category_name,
  department_name_de,
  category_name_de,
  member_name,
  member_caption,
  member_description
FROM product_hier
WHERE level_name = 'CATEGORY'</code>;",02-FEB-17 08.03.14.388960000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-NOV-19 06.27.42.376827000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
97371311146065714514166543120564103062,97371311146062087736707699233039984534,"The Basics",10,"<p>
The basis of this module is the EMP table of the SCOTT sample schema, particularly these columns.
</p>
<code>select deptno
     , ename
     , sal
  from emp
order by deptno
       , sal;</code>
<p>
Most aggregate functions exist in an analytic variant too. The analytic function call is characterized by the keyword OVER followed by a set of parentheses, optionally containing one or more of three different analytic clauses.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/yq85bcswaw424ev/syntax1.gif""/>
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/4ncgrit85jltgt2/syntax2.gif""/>
</p><p>
In the simplest form an empty set of parentheses can be used after OVER, signifying the function is to be applied on all rows of the output. For example using the SUM function to produce a grand total of salaries. (Or try using COUNT or AVG or other functions.)
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over () sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
The first analytic clause is PARTITION BY. Similar to GROUP BY for aggregate functions, the PARTITION BY clause for analytic functions can be used to signify the function is to be applied to groups (partitions) of data. For the SUM function creating subtotals by for example department. (Or try partitioning by JOB or other columns.)
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
The other two analytic clauses are the ORDER BY clause and the so-called WINDOWING clause. The WINDOWING clause specifies a ""window"" of rows (relative to the current row of the output) for the function, like for example the window ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROWS. That window specifies to calculate the function on the rows from the start (unbounded) until the current row - and to give meaning to what is the ""start"" you use the ORDER BY clause to give the rows an ordering (otherwise the ordering is indeterminate and the ""start"" could be any row.) With an ORDER BY and the mentioned windowing clause used on the SUM function we get a running total.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          order by sal
          rows between unbounded preceding
                   and current row
       ) sum_sal
  from emp
order by sal;</code>
<p>
In the example above, the ORDER BY in the analytic clause and the ORDER BY of the query itself is identical, but they need not be. You can have the ordering (and thereby windowing) of the analytic clause be completely different from the final query ORDER BY. (Try different combinations and observe the results.)
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          order by sal
          rows between unbounded preceding
                   and current row
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
All three analytic clauses can be combined if you wish, for example using PARTITION BY to execute the analytics separately for each DEPTNO, which then makes the running total work inside each partition, so the ""start row"" (unbounded preceding) is the row with the lowest salary within each department. The running total is performed completely separately for each partition.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between unbounded preceding
                   and current row
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
The windowing clause can be specified very detailed allowing much more complex windows than a simple running total. Rows can be specified with a certain offset either PRECEDING or FOLLOWING, or the offset may be UNBOUNDED in either direction, or CURRENT ROW is specified. For example a slight variation of the running total can be to replace CURRENT ROW with 1 PRECEDING, which means to calculate the function on all rows from the ""start"" and up to and including the row <em>before</em> the current row - in other words a total of all <em>previous</em> rows.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between unbounded preceding
                   and 1 preceding
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
The previous examples have all used PRECEDING to specify a window of rows that came <em>before</em> the current row in the ordering. We can also ""look ahead"" to rows that come <em>after</em> the current row by using FOLLOWING, for example a kind of ""reversed"" running total.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between current row
                   and unbounded following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
With FOLLOWING we can also use an offset rather than CURRENT ROW, so we for example can calculate the total for all the rows <em>yet to come</em>.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between 1 following
                   and unbounded following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
It is also allowed a window that is unbounded in both ends, like this example. But it is not really sensible, as the result is identical to skipping the ORDER BY and the windowing clauses alltogether.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between unbounded preceding
                   and unbounded following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
The window can also be spefied with offset bounds both for the start and the end of the window. The start and end bounds may both be either preceding or following, or you can have the start be preceding and the end following to get for example a total sum of the previous row, the current row and the following row. (Try different offsets, either preceding or following.)
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between 1 preceding
                   and 1 following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
When we use ROWS BETWEEN, the offset used for PRECEDING and FOLLOWING specifies a number of rows in the output. If you are using a single column in the ORDER BY clause and it is numeric or date/timestamp, you can instead use RANGE BETWEEN. Using RANGE BETWEEN the window is not specified counting rows, but instead the window is those rows who have a value in the ordering column that is the specified offset less than or greater than the value in the current row. For example we can have a total of salaries for those who earn between 500 less and 500 more than the employee in the current row.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          range between 500 preceding
                    and 500 following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
Just like ROWS BETWEEN, RANGE windows also do not need to include the current row. The window is always <em>relative</em> to the current row, but it may be specified for example as those who earn between 300 more and 3000 more than the employee in the current row.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          range between  300 following
                    and 3000 following
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
When you specify RANGE BETWEEN and use CURRENT ROW, then CURRENT ROW actually means <em>rows with the same value</em> in the ordering column. This means that if we do the <em>running total</em> using RANGE instead of ROWS, the analytic function output can include data from ""following"" rows if they have the same value - even though CURRENT ROW was specified as the end of the window.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          range between unbounded preceding
                    and current row
       ) sum_sal
  from emp
order by deptno
       , sal;</code>
<p>
It is easier to see what happens if we make two running totals side by side - one with RANGE and one with ROWS. Look at employees that earn the same (within same department) - you'll see the difference between RANGE and ROWS for SCOTT/FORD and MARTIN/WARD.
</p>
<code>select deptno
     , ename
     , sal
     , sum(sal) over (
          partition by deptno
          order by sal
          range between unbounded preceding
                    and current row
       ) range_sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between unbounded preceding
                   and current row
       ) rows_sal
  from emp
order by deptno
       , sal;</code>
<p>
The windowing clause needs not be specified. But if you use an ORDER BY clause without any windowing clause (for functions that support windowing clause, some do not), then the default windowing clause is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW, as you can observe by comparing the two explicit windowing clauses with no windowing clause.
</p>
<code>select deptno, ename, sal
     , sum(sal) over (
          partition by deptno
          order by sal
          range between unbounded preceding
                    and current row
       ) range_sal
     , sum(sal) over (
          partition by deptno
          order by sal
          rows between unbounded preceding
                   and current row
       ) rows_sal
     , sum(sal) over (
          partition by deptno
          order by sal
          /* no window - rely on default */
       ) def_sal
  from emp
order by deptno
       , sal;</code>
<p>
As a large majority of use cases for analytic functions call for ROWS BETWEEN, a best practice is that whenever you use an ORDER BY clause, then also always use an explicit windowing clause (most often ROWS and once in a while RANGE) even if it happens to match the default.
</p><p>
Another best practice is to ensure consistent results for ROWS BETWEEN by using a column combination for ORDER BY that is unique (within the partition, if any). Many of the examples above using an ordering of SAL together with ROWS BETWEEN could potentially return different results between calls, because it would be indeterminate whether SCOTT or FORD was first in the ordering. So SCOTT could be included in the running total of FORD or vice versa. By adding EMPNO to the ORDER BY, we ensure consistent results.
</p>
<code>select deptno, empno, ename, sal
     , sum(sal) over (
          partition by deptno
          order by sal, empno
          rows between unbounded preceding
                   and current row
       ) sum_sal
  from emp
order by deptno
       , sal
       , empno;</code>
<p>
You now know the basics of the three analytic clauses within OVER () - the partitioning, ordering and windowing clauses. You can play around with changing the examples above and observe the effects.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",24-FEB-17 07.50.59.433500000 AM,"KIBEHA@GMAIL.COM",11-JUN-17 08.58.59.641535000 AM,"KIBEHA@GMAIL.COM"
97932625407649901985365647638624138525,97399059242426665206102153060921522170,"What to include in MEASURES clause",70,"<p>Based on the query that was in the original post I think the following syntax would make it easier to understand what is happening within the pattern matching process and provide useful information about the data that matches the pattern:</p>
<code>SELECT symbol, tstamp, price, first_date, last_date, first_price, last_price, m_n, classi 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES
   FIRST(b.tstamp) AS first_date,
   LAST(b.tstamp) AS last_date,
   FIRST(b.price) AS first_price,
   LAST(b.price) AS last_price,
   match_number() AS m_n,
 classifier() AS classi
 ALL ROWS PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
);
</code>
<p>...returns a resultset that includes all the columns from the source TICKER table.</p>",01-MAR-17 03.18.59.746030000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.25.50.207719000 PM,"SHARON.KENNEDY@ORACLE.COM"
98773055496726019889767213655600550927,98773055494155843597266512030175220751,"Find fraudulent transactions",40,"<p>The input into MATCH_RECOGNIZE is a SELECT statement that returns the JSON data in a standard relational table format
</p>
<code>SELECT user_id, first_t, last_t, big_amount 
FROM (SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer')
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS first_t, 
   LAST(y.time_id) AS last_t, 
   y.amount AS big_amount
 ONE ROW PER MATCH
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10); </code>
<p>How does this statement breakdown?</p>",09-MAR-17 04.27.56.261022000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 02.16.33.051171000 PM,"KEITH.LAKER@ORACLE.COM"
98774265128760584508111939323130738798,98773055494155843597266512030175220751,"Check pattern correctly applied",50,"<p>We can check if out pattern has found all the relevant matches by using two of the built-in measures:
</p>
<ul>
<li>MATCH_NUMBER()</li>
<li>CLASSIFIER</li>
</ul>
<h3>MATCH_NUMBER()</h3>
<p>You might have a large number of matches for your pattern inside a given row partition. How do you tell these matches apart? This is done with the MATCH_NUMBER() function. Matches within a row pattern partition are numbered sequentially starting with 1 in the order they are found. This numbering starts at 1 within each row pattern partition, because there is no linked ordering across row pattern partitions.
</p>
<h3>CLASSIFIER()</h3>
<p>Along with knowing which MATCH_NUMBER you are seeing, you may want to know which component of a pattern applies to a specific row. This is done using the CLASSIFIER() function. The classifier of a row is the pattern variable that the row is mapped to by a row pattern match. The CLASSIFIER() function returns a character string whose value is the name of the pattern variable defined within the PATTERN clause.
</p>
<p>The last step we need to do is change the amount of information returned from summary to detailed. Therefore, we need to use <strong>ALL ROWS PER MATCH WITH UNMATCHED ROW</strong> - <em>there is a separate livesql tutorial that covers this topic</em>
</p>
<code>SELECT user_id, time_id, amount, first_t, last_t, first_small_amount, last_small_amount, big_amount, mn, classifier 
FROM (SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer')
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS first_t, 
   LAST(y.time_id) AS last_t,
   FIRST(x.amount) AS first_small_amount,
   LAST(x.amount) AS last_small_amount,  
   y.amount AS big_amount,
   match_number() AS mn,
   classifier() AS classifier
 ALL ROWS PER MATCH WITH UNMATCHED ROWS
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10); </code>
<p>As you can now see, we have correctly identified the three small transfers on 02-Jan, 10-Jan and 20-Jan, the difference between 02-Jan and 20-Jan is less than 30 days. The large transfer is within our range of 10,000 or greater and the time difference between the last small transfer on Jan 20 and the large transfer on 27-Jan is less than 10 days.
</p>",09-MAR-17 04.28.55.394829000 PM,"KEITH.LAKER@ORACLE.COM",04-APR-17 09.41.04.104081000 AM,"KEITH.LAKER@ORACLE.COM"
98860391606982943292445382337830233966,98773055494155843597266512030175220751,"Overview",5,"<p>In Database 12c there is a new SQL construct MATCH_RECOGNIZE for finding rows matching a pattern  across a sequence of rows, using a regular expressions.
</p>
<p>Pattern matching using SQL is based around four logical concepts:</p>
<ul>
<li>Logically partition and order the data/li>
<li>Define pattern using regular expression and pattern variables/li>
<li>Regular expression is matched against a sequence of rows/li>
<li>Each pattern variable is defined using conditions on rows and aggregates</li>
</ul>
</p>
<h3>Business Problem: Finding Suspicious Money Transfers</h3>
<p>In this tutorial we want to explore how to use MATCH_RECOGNIZE to find potentially fraudulent transactions.
</p>
<p>Let's define a suspicious money transfer pattern for an account as:
<p>
<ul>
<li>3 or more small (<2K) money transfers within 30 days</li>
<li>Large transfer (>=1M) within 10 days of last small transfer</li>
</ul> 
<p>When we find this pattern we need to report the following information: account, date of first small transfer, date of last large transfer and the amount of the large transfer.
</p>",10-MAR-17 12.16.11.331953000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.16.11.332020000 PM,"KEITH.LAKER@ORACLE.COM"
98860391607384306664557439223832684398,98773055494155843597266512030175220751,"Breaking down the query",45,"<h3>Set the PARTITION BY and ORDER BY clauses</h3>
<p>Need to group and order the data to make the pattern â€œvisibleâ€ within the sequence of rows...,therefore, we need to group and partition the data by each user_id and then sort the data within each partition by the transaction date.
</p>

<h3>Define output MEASURES</h3>
<p>The MEASURES clause allows us to list the columns that will be returned. These can be existing columns from the input table/view and/or calculated columns. In this case we need to return the account/user id along with the value of the large amount and we need to calculate the date of first small transfer and the date of last large transfer
</p>
<h3>Report Type</h3>
<p>For this query we only need a summary report so the ONE ROW PER MATCH clause is used.
</p>
<h3>PATTERN clause</h3>
<p>We have two pattern variables: X and Y. In this example we are searching for three or more occurrences of X followed by a single occurrence of Y.
<h3>Describing each pattern variable</h3>
<p>For a match on variable X we are searching for small transfers of amounts less than 2K and all three transfers must occur within a 30 day window
</p>
<p>For a match on variable Y we are searching for a large transfer of more than 10K. The large transfer must occur within 10 days of last small transfer.
</p>",10-MAR-17 12.34.58.550771000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.34.58.550841000 PM,"KEITH.LAKER@ORACLE.COM"
97934027152928436980268722088489689271,97399059242426665206102153060921522170,"Create my dataset",20,"<h3>Creating our ticker data set</h3>
<p>First step is to setup our data table and then populate it with data</p>

<code>CREATE TABLE ticker (SYMBOL VARCHAR2(10), tstamp DATE, price NUMBER);

BEGIN
INSERT INTO ticker VALUES('ACME', '01-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '17-Apr-11', 8);
INSERT INTO ticker VALUES('GLOBEX', '01-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '20-Apr-11', 9);
INSERT INTO ticker VALUES('ACME', '02-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '19-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '03-Apr-11', 19);
INSERT INTO ticker VALUES('GLOBEX', '03-Apr-11', 13);
INSERT INTO ticker VALUES('OSCORP', '18-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '02-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '04-Apr-11', 21);
INSERT INTO ticker VALUES('GLOBEX', '04-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '17-Apr-11', 14);
INSERT INTO ticker VALUES('OSCORP', '15-Apr-11', 12);
INSERT INTO ticker VALUES('OSCORP', '14-Apr-11', 15);
INSERT INTO ticker VALUES('OSCORP', '16-Apr-11', 16);
INSERT INTO ticker VALUES('ACME', '05-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '05-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '06-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '06-Apr-11', 10);
INSERT INTO ticker VALUES('ACME', '07-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '07-Apr-11', 9);
INSERT INTO ticker VALUES('GLOBEX', '08-Apr-11', 8);
INSERT INTO ticker VALUES('ACME', '08-Apr-11', 20);
INSERT INTO ticker VALUES('OSCORP', '13-Apr-11', 11);
INSERT INTO ticker VALUES('ACME', '13-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '10-Apr-11', 25);
INSERT INTO ticker VALUES('ACME', '11-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '09-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '09-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '12-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '10-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '11-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '11-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '10-Apr-11', 15);
INSERT INTO ticker VALUES('ACME', '12-Apr-11', 15);
INSERT INTO ticker VALUES('GLOBEX', '12-Apr-11', 9);
INSERT INTO ticker VALUES('OSCORP', '09-Apr-11', 16);
INSERT INTO ticker VALUES('GLOBEX', '13-Apr-11', 10);
INSERT INTO ticker VALUES('OSCORP', '08-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '14-Apr-11', 25);
INSERT INTO ticker VALUES('GLOBEX', '14-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '07-Apr-11', 17);
INSERT INTO ticker VALUES('OSCORP', '06-Apr-11', 20);
INSERT INTO ticker VALUES('ACME', '15-Apr-11', 14);
INSERT INTO ticker VALUES('GLOBEX', '15-Apr-11', 12);
INSERT INTO ticker VALUES('ACME', '17-Apr-11', 14);
INSERT INTO ticker VALUES('ACME', '16-Apr-11', 12);
INSERT INTO ticker VALUES('GLOBEX', '16-Apr-11', 11);
INSERT INTO ticker VALUES('OSCORP', '05-Apr-11', 17);
INSERT INTO ticker VALUES('ACME', '18-Apr-11', 24);
INSERT INTO ticker VALUES('GLOBEX', '18-Apr-11', 7);
INSERT INTO ticker VALUES('OSCORP', '04-Apr-11', 18);
INSERT INTO ticker VALUES('OSCORP', '03-Apr-11', 19);
INSERT INTO ticker VALUES('ACME', '19-Apr-11', 23);
INSERT INTO ticker VALUES('GLOBEX', '19-Apr-11', 5);
INSERT INTO ticker VALUES('OSCORP', '02-Apr-11', 22);
INSERT INTO ticker VALUES('ACME', '20-Apr-11', 22);
INSERT INTO ticker VALUES('GLOBEX', '20-Apr-11', 3);
INSERT INTO ticker VALUES('OSCORP', '01-Apr-11', 22);

commit;
END;
</code>",01-MAR-17 03.03.36.968621000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.24.31.678923000 PM,"SHARON.KENNEDY@ORACLE.COM"
97934027152929645906088336717664395447,97399059242426665206102153060921522170,"Viewing my data",30,"<h3>Quick review of our ticker data</h3>
<p>Now let's check to see how many rows are in our dataset</p>
<code>SELECT count(*) FROM ticker;
SELECT symbol, min(tstamp), max(tstamp), count(*) FROM ticker GROUP BY symbol;
</code>
<p>You should have 60 rows of data spread across three symbols (ACME, GLOBEX, OSCORP) with 20 rows of data for each ticker symbol. Our ticker data for each symbol starts on April 1 and ends on April 20.</p>",01-MAR-17 03.03.55.155234000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.24.40.520836000 PM,"SHARON.KENNEDY@ORACLE.COM"
97930968340458179419317356770328451258,97399059242426665206102153060921522170,"MEASURES and ONE ROW PER MATCH",80,"<p>If we want to switch to using ONE ROW PER MATCH then we need to remove references to the columns tstamp and price and replace them with references to the pattern variable specific versions, or we can just remove the references all together. In this case as we only have two pattern variables we can NVL the references to return the required data:</p>
<code>SELECT symbol, o_tstamp, o_price, first_date, last_date, first_price, last_price, m_n, classi 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES
 nvl(a.tstamp, b.tstamp) as o_tstamp,
 nvl(a.price, b.price) as o_price,
   FIRST(b.tstamp) as first_date,
   LAST(b.tstamp) as last_date,
   FIRST(b.price) as first_price,
   LAST(b.price) as last_price,
   match_number() as m_n,
   classifier() as classi
 ONE ROW PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
);
</code> 
<p>
...the above query generates slightly fewer rows compared with the previous statement because we are only returning one for each match.  Note that this time we are referencing specific instances of tstamp and price.</p>",01-MAR-17 03.29.45.919914000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.26.09.033339000 PM,"SHARON.KENNEDY@ORACLE.COM"
97934027154033395179396493154171134135,97399059242426665206102153060921522170,"Summary",90,"<p>What have we learned:</p>
<ol>
<li>Check your PARTITION BY and ORDER BY clauses to ensure they make sense!</li>
<li>There is no need to list the source columns from your input table in the MEASURE clause because they are automatically included BUT ONLY when you use ALL ROWS PER MATCH.</li>
<li>Decide on your output method and match the columns listed in the SELECT clause with those returned by either ALL ROWS PER MATCH or ONE ROW PER MATCH.</li>
<li>Always a good idea to check your pattern is being applied correctly by using the built-in  MATCH_NUMBER and CLASSIFIER() measures.</li>
</ol>",01-MAR-17 03.31.13.034146000 PM,"KEITH.LAKER@ORACLE.COM",01-MAR-17 03.31.13.034213000 PM,"KEITH.LAKER@ORACLE.COM"
98773055494169141781282272951096988687,98773055494155843597266512030175220751,"Setup",10,"<p>First step is to create the JSON table that will hold our transaction log. The log will provide the details of the time, account/user id, type of event (deposit or transfer) and the transaction amount.
</p>
<p>Oracle Database 12c supports storing JSON documents inside the database. Use following code to create a table to store the transaction log which is in JSON format
</p>
<code>CREATE TABLE json_transactions 
(transaction_doc CLOB, 
 CONSTRAINT ""VALID_JSON"" CHECK (transaction_doc IS JSON) ENABLE
);</code>",09-MAR-17 04.01.43.994450000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.20.00.799352000 PM,"KEITH.LAKER@ORACLE.COM"
97399059242441172315937528611017996282,97399059242426665206102153060921522170,"Overview",10,"<p>This simple tutorial will explain how and why you get errors such as</p>
<ul>
<li>ORA-904 ""%s: invalid identifier""</li>
<li>ORA-918 ""column ambiguously defined""</li>
</ul>
<p>As with all the other pattern matching tutorials we will first setup the TICKER schema table and use the ticker table during this tutorial</p> 

<p>If you have any questions about this tutorial or the MATCH_RECOGNIZE clause then please contact me via email: keith.laker@oracle.com.</p>",24-FEB-17 12.08.45.257016000 PM,"KEITH.LAKER@ORACLE.COM",01-MAR-17 03.03.18.569035000 PM,"KEITH.LAKER@ORACLE.COM"
98774265127807950962255611533462272110,98773055494155843597266512030175220751,"Add data",20,"<p>Next step is to add some data to our JSON table using the normal JSON notation of key-value pairs.</p>
<code>INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-JAN-12"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":2000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""25-JAN-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1200000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":500000}');
COMMIT;</code>",09-MAR-17 04.02.28.814913000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.20.50.756146000 PM,"KEITH.LAKER@ORACLE.COM"
98773055494242886256278765330754065423,98773055494155843597266512030175220751,"View transaction data",30,"<p>Using the new JSON SQL notation we can query our data using a simple SELECT statement</p>
<code>SELECT 
  j.transaction_doc.time_id as time_id, 
  j.transaction_doc.user_id as user_id,
  j.transaction_doc.event_id as event_id,   
  j.transaction_doc.trans_amount as amount
FROM json_transactions j;</code>
<p>Here is the list of transfer transactions that we are of interest for our analysis</p>
<code>SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       to_number(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer';</code>",09-MAR-17 04.04.57.470845000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.36.20.888128000 PM,"KEITH.LAKER@ORACLE.COM"
95240136547122129956794124610378044738,95114558141387237177774563848501259509,"Creating the Analytic View",80,"<p>The analytic view inherits attributes from hierarchies so only the classifications need to be updated to support German.</p>

<p>CREATE OR REPLACE ANALYTIC VIEW sales_av with default and German classifications.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY
  (time_attr_dim
     KEY month_id REFERENCES month_id
     HIERARCHIES (
       time_hier DEFAULT),
   product_attr_dim
     KEY category_id REFERENCES category_id
     HIERARCHIES (
       product_hier DEFAULT),
   geography_attr_dim
     KEY state_province_id 
     REFERENCES state_province_id
     HIERARCHIES (
     geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales
    CLASSIFICATION caption VALUE 'Sales Analytic View'
    CLASSIFICATION description VALUE 'Sales Analytic View'
    CLASSIFICATION caption VALUE 'Vertrieb Analytische Sicht' LANGUAGE 'GERMAN'
    CLASSIFICATION description VALUE 'Vertrieb Analytische Sicht' LANGUAGE 'GERMAN',
 sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
   CLASSIFICATION caption VALUE 'Sales Analytic View'
   CLASSIFICATION description VALUE 'Sales Analytic View'
   CLASSIFICATION caption VALUE 'Vertrieb Analytische Sicht' LANGUAGE 'GERMAN'
   CLASSIFICATION description VALUE 'Vertrieb Analytische Sicht' LANGUAGE 'GERMAN',
 units FACT units
   CLASSIFICATION caption VALUE 'Units Sold'
   CLASSIFICATION description VALUE 'Units Sold'
   CLASSIFICATION caption VALUE 'VerkÃ¤ufe vor einem Jahr' LANGUAGE 'GERMAN'
   CLASSIFICATION description VALUE 'VerkÃ¤ufe vor einem Jahr' LANGUAGE 'GERMAN'
  )
DEFAULT MEASURE SALES;</code>

<p>Set NLS_LANGUAGE = AMERICAN</p>

<code>ALTER SESSION SET nls_language = AMERICAN;</code>

<p>Run a query against the analytic view.</p>

<code>SELECT time_hier.member_name AS Time,
 product_hier.member_name AS Product,
 geography_hier.member_name AS Geography,
 sales,
 sales_year_ago
FROM
 sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE time_hier.level_name in ('YEAR')
  AND product_hier.level_name in ('DEPARTMENT')
  AND geography_hier.level_name in ('REGION')
  AND time_hier.member_name in ('CY2014','CY2015')
ORDER BY time_hier.hier_order,
  product_hier.hier_order,
  geography_hier.hier_order;</code>

<p>Set NLS_LANGUAGE = GERMAN</p>

<code>ALTER SESSION SET nls_language = GERMAN;</code>

<p>Run the same query against the analytic view.</p>

<code>SELECT time_hier.member_name AS Time,
 product_hier.member_name AS Product,
 geography_hier.member_name AS Geography,
 sales,
 sales_year_ago
FROM
 sales_av HIERARCHIES (time_hier, product_hier, geography_hier)
WHERE time_hier.level_name in ('YEAR')
  AND product_hier.level_name in ('DEPARTMENT')
  AND geography_hier.level_name in ('REGION')
  AND time_hier.member_name in ('CY2014','CY2015')
ORDER BY time_hier.hier_order,
  product_hier.hier_order,
  geography_hier.hier_order;</code>",03-FEB-17 08.24.43.111800000 PM,"WILLIAM.ENDRESS@ORACLE.COM",03-FEB-17 08.57.00.147609000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
97932625407408116821442721803682903325,97399059242426665206102153060921522170,"ORA-918 errors",40,"<p>
This post is the result of reviewing a post on stackoverflow.com. Here is my version of the code that was posted, which includes the same issues/errors as the original:</p>
<code>SELECT symbol, tstamp, price 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY symbol, tstamp
 MEASURES
   a.symbol AS a_symbol,
   a.tstamp AS a_date,
   a.price AS a_price
 ALL ROWS PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
);
</code>
<p>The above example will not run because of the following error:</p>
<blockquote>
<CODE>
ORA-00918: column ambiguously defined
</CODE>
</blockquote>

<p>So what is wrong with our code?</p>
<p>The ORA-918 error is easy to resolve if you stare long enough at the ORDER BY clause! Itâ€™s pointless to include the SYMBOL column as the partition by key and the order by key. </p>
<p>If we change the ORDER BY clause as shown here then the code will run:</p>
<code>SELECT symbol, tstamp, price 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 MEASURES
   a.symbol AS a_symbol,
   a.tstamp AS a_date,
   a.price AS a_price
 ALL ROWS PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
); 
</code>
<p>This will now return the resultset contain all 60 rows from our source ticker table</p>",01-MAR-17 03.09.37.975399000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.25.11.118408000 PM,"SHARON.KENNEDY@ORACLE.COM"
97932625407425041782917326612128789789,97399059242426665206102153060921522170,"No MEASURES clause",50,"<p>Okay so our code is running now what?</p> 
<p>What happens if we omit the MEASURES clause? Well itâ€™s optional so the code should still runâ€¦</p>
<code>SELECT symbol, tstamp, price 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 ALL ROWS PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
);
</code> 
<p>...sure enough we get the same result - all 60 rows from our source ticker table</p>",01-MAR-17 03.12.09.717022000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.25.23.373438000 PM,"SHARON.KENNEDY@ORACLE.COM"
97932625407566486103812238225569412381,97399059242426665206102153060921522170,"ORA-904 Errors",60,"<p>From the above we can assume that there is no need to list the source columns from your input table in the MEASURE clause because they are automatically included in the output.
</p> 
<p><strong>BUT</strong> this is ONLY true when you use <strong>ALL ROWS PER MATCH</strong>. If we change the output control to ONE ROW PER MATCH:</p>
<code>SELECT symbol, tstamp, price 
FROM ticker 
MATCH_RECOGNIZE(
PARTITION BY symbol
ORDER BY tstamp
ONE ROW PER MATCH
PATTERN(A B*)
DEFINE
B AS (price < PREV(price))
); 
</code>
<p>..you will now get an error:</p>
<blockquote>
<CODE>
ORA-00904: ""PRICE"": invalid identifier
</CODE>
</blockquote>
<p>...because when using ONE ROW PER MATCH the only columns that are automatically returned are those listed in the PARTITION BY clause.
</p>
<p>Therefore, we need to use either â€œSELECT * FROM â€¦..â€ or â€œSELECT symbol FROMâ€¦â€ and list all the columns we want to be returned to get a working version of our code. Using â€œSELECT * FROMâ€¦â€ as follows:</p>
<code>SELECT * 
FROM ticker 
MATCH_RECOGNIZE(
 PARTITION BY symbol
 ORDER BY tstamp
 ONE ROW PER MATCH
 PATTERN(A B*)
 DEFINE
   B AS (price < PREV(price))
); 
</code> 
<p>....actually returns only one column (symbol) from the ticker table. </p>",01-MAR-17 03.16.03.868741000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.25.39.002535000 PM,"SHARON.KENNEDY@ORACLE.COM"
98864663131747105078119269379202484548,98773055494155843597266512030175220751,"Changing the Requirements",60,"<p>Having found some fraudulent transfers the business has now come back with an additional requirement:
<p>
<ol>
<li>Check for transfers across different accounts</li>
<li>Total sum of small transfers must be less than 20K</li>
</ol>
<p>The application developers have modified the JSON model to include the id of the person receiving the transfer so let's update our table with this new information:
</p>
<code>TRUNCATE TABLE json_transactions;
INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":100}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Bob"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-JAN-12"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":2000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Allen"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""25-JAN-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1200000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-JAN-12"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-12"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":500000}');
COMMIT;</code>",10-MAR-17 12.51.12.812037000 PM,"KEITH.LAKER@ORACLE.COM",04-APR-17 10.08.23.621721000 AM,"KEITH.LAKER@ORACLE.COM"
103562494808954049384531671400875478924,103563632879277014483527974609264228592,"Reset my tutorial environment",30,"<h3>Just in case...</h3>
<p>If at anytime you need to restart this tutorial then simply run the following statement to reset your environment</p>
<code>DROP TABLE json_sessionization PURGE;
</code>",24-APR-17 12.54.51.287020000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.49.25.621696000 PM,"SHARON.KENNEDY@ORACLE.COM"
103566264885586305671026633050751142868,103563632879277014483527974609264228592,"Add data",50,"<p>Next step is to add some data to our JSON table using the normal JSON notation of key-value pairs.</p>
<code>INSERT INTO json_sessionization VALUES ('{""time_id"":""1"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""2"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""11"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""12"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""22"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""23"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""32"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""34"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""43"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""44"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""47"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""48"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""53"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""59"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""60"",""user_id"":""Sam""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""63"",""user_id"":""Mary""}');
INSERT INTO json_sessionization VALUES ('{""time_id"":""68"",""user_id"":""Sam""}');
COMMIT;</code>",24-APR-17 12.56.40.866609000 PM,"KEITH.LAKER@ORACLE.COM",26-APR-17 01.26.22.686864000 PM,"KEITH.LAKER@ORACLE.COM"
103563827416702946435283121656683977480,103563632879277014483527974609264228592,"SKIP TO where?",110,"<p>After we have found a match (and this actually includes empty matches as well!) we need to decide where to start searching for the next match. This is controlled by the AFTER MATCH SKIP clause. There are five possible options for controlling the start point and the default for the clause is AFTER MATCH SKIP PAST LAST ROW.
</p>
<p>In this case we are going to use the default.</p>
<code>SELECT 
 time_id,
 userid,
 session_id
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j)
MATCH_RECOGNIZE(         
   PARTITION BY userid ORDER BY time_id
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   AFTER MATCH SKIP PAST LAST ROW
   PATTERN (b s+)
   DEFINE
       s as (time_id - PREV(time_id) <=10)
 );
</code>
<p>Note the position of the SKIP TO syntax within the MATCH_RECOGNIZE code - it comes just before the PATTERN clause</p>
<p>The results from the above code are exactly the same as the results from the previous example which indicates that although the previous example did not include a SKIP TO clause the default processing was implemented.
</p>",24-APR-17 01.04.03.867340000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.52.14.203078000 PM,"SHARON.KENNEDY@ORACLE.COM"
108031870563034668228881722562738585156,97371311146062087736707699233039984534,"FIFO Picking",40,"<p>
The basis of this module is a table containing inventory of items - how big a quantity of each item is located at each location in the warehouse and when was that particular quantity purchased.
</p>
</p>
<code>select *
  from inventory
 order by item, loc;</code>
<p>
Location codes consists of three parts - warehouse, aisle and position - in the 2 warehouses shown here.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/luz0rqwfzwsj0w0/image1.png""/>
</p><p>
Also we have a table of orderlines specifying for each order how many of each item we must deliver to the customer.
</p>
<code>select *
  from orderline
 order by ordno, item;</code>
<p>
The forklift operator in the warehouse needs to pick the items for delivering one of the orders, order number 42, but as the items have a limited shelf-life he needs to pick from the oldest quantities first (the principle of FIFO: First-In-First-Out.)
</p><p>
If we join the orderlines to the inventory and order each item by the purchase date, we can visually easily see that he needs to pick from the locations with the oldest quantities and move on the the second-oldest and continue until he has picked the quantity ordered.
</p>
<code>select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
  from orderline o
  join inventory i
      on i.item  = o.item
 where o.ordno = 42
 order by o.item, i.purch, i.loc;</code>
<p>
The trick is now to build a query to perform what we easily can see visually. Keep picking (adding quantities) until a certain quantity is reached sounds rather like doing a running total until it reaches a desired total. Let us try doing a running total.
</p>
<code>select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
     , sum(i.qty) over (
         partition by i.item
         order by i.purch, i.loc
         rows between unbounded preceding and current row
       ) sum_qty
  from orderline o
  join inventory i
      on i.item  = o.item
 where o.ordno = 42
 order by o.item, i.purch, i.loc;</code>
<p>
And then we can try keeping only those rows where the running total is less than the desired ordered quantity - will that give us the desired result?
</p>
<code>select s.*
  from (
   select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
        , sum(i.qty) over (
            partition by i.item
            order by i.purch, i.loc
            rows between unbounded preceding and current row
          ) sum_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_qty < s.ord_qty
 order by s.item, s.purch, s.loc;</code>
<p>
Nope, it did not work. The problem being that the location quantity of the row where the desired total is reached is included in the running total of that row, so our WHERE clause filters away the last row of the rows we need.
</p><p>
What we need is a running total of all the <em>previous</em> rows, which is easy with analytic functions, just change the windowing clauses to use 1 PRECEDING.
</p>
<code>select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
     , sum(i.qty) over (
         partition by i.item
         order by i.purch, i.loc
         rows between unbounded preceding and 1 preceding
       ) sum_prv_qty
  from orderline o
  join inventory i
      on i.item  = o.item
 where o.ordno = 42
 order by o.item, i.purch, i.loc;</code>
<p>
Now we can do a WHERE clause that keeps all rows where the sum of the <em>previous</em> rows is less than the ordered quantity. This means that when the previous rows running total is greater than or equal to the ordered quantity, then the current row is not needed for the picking and is filtered away. Note we use an NVL to make the <em>first</em> SUM_PRV_QTY value 0 instead of null.
</p><p>
The quantity our operator needs to pick at each location is then either all of the location quantity or the quantity not yet picked (ordered qty minus the previous running total), whichever is the smallest.
</p>
<code>select s.*
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.purch, i.loc
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.item, s.purch, s.loc;</code>
<p>
During development of the query, we've kept selecting all the columns - now we can clean up the query a bit and only keep the necessary columns to be shown to the operator as a picking list, which we order by location rather than the item/purchase date order we've used before.
</p>
<code>select s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.purch, i.loc
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Taking advantage of how the analytic ORDER BY can be different than the final query ORDER BY, we can switch the principles of picking just by changing the analytic ORDER BY. For example instead of FIFO principle, we can choose inventory quantities based on location order - always try to find the nearest locations for fastest picking.
</p>
<code>select s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.loc   -- << only line changed
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Or we can choose inventory quantities by locations with the largest quantities first - which will yield the smallest number of picks to be performed (but then leave a lot of small quantities scattered in the warehouse for later picking.)
</p>
<code>select s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.qty desc, i.loc   -- << only line changed
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Or we can use a space optimizing principle of cleaning out small quantities first - that way we'll free most locations for use by other items (but then have to drive a lot around in the warehouse to perform many picks.)
</p>
<code>select s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.qty, i.loc   -- << only line changed
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Now we know how to pick an order by FIFO or other principle in a single SQL statement in location order. But location order is not really the most efficient way for our operator to drive his forklift. He would work more efficiently if the picking list was ordered such, that the picks in the first aisle in the warehouse he needs to visit will be ordered in one direction, the picks in the second aisle he needs to visit will be ordered in the other direction, and so on, switching directions every other aisle he visits.
</p><p>
To demonstrate doing that, we'll use the picking principle of cleaning out small quantities first rather than FIFO, as it gives us the most locations to visit, thus demonstrating the results most clearly. 
</p><p>
The location code in the inventory table is split into 3 parts - warehouse number, aisle letter, and position number (in true use cases you might get that information from a lookup table based on a location pseudo primary key.)
</p>
<code>select to_number(substr(s.loc,1,1)) warehouse
     , substr(s.loc,3,1) aisle
     , to_number(substr(s.loc,5,2)) position
     , s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.qty, i.loc
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Showing those results in the warehouse, we can see that location order is not the optimal way.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/rac2yvnlolgc0jg/image3.png""/>
</p><p>
Within an analytic DENSE_RANK call we can use an ORDER BY containing warehouse and aisle. This means that ""ties"" (rows with identical values for warehouse and aisle) will get the same rank. And since we use DENSE_RANK instead of RANK, the ranking numbers will be consecutive. So column AISLE_NO will number the aisles our operation has to visit with numbers 1, 2, 3...
</p>
<code>select to_number(substr(s.loc,1,1)) warehouse
     , substr(s.loc,3,1) aisle
     , dense_rank() over (
          order by to_number(substr(s.loc,1,1))     -- warehouse
                 , substr(s.loc,3,1)                -- aisle
       ) aisle_no
     , to_number(substr(s.loc,5,2)) position
     , s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
            partition by i.item
            order by i.qty, i.loc
            rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderline o
     join inventory i
         on i.item  = o.item
    where o.ordno = 42
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
Wrapping the DENSE_RANK calculation in an inline view enables us to use the AISLE_NO column in the final ORDER BY clause, where we then can order the picks in odd-numbered aisles (1,3,...) by position ascending and even-numbered aisles (2,4,...) by position descending.
</p>
<code>select s2.warehouse, s2.aisle, s2.aisle_no, s2.position, s2.loc, s2.item, s2.pick_qty
  from (
   select to_number(substr(s.loc,1,1)) warehouse
        , substr(s.loc,3,1) aisle
        , dense_rank() over (
             order by to_number(substr(s.loc,1,1))     -- warehouse
                    , substr(s.loc,3,1)                -- aisle
          ) aisle_no
        , to_number(substr(s.loc,5,2)) position
        , s.loc, s.item
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
           , nvl(sum(i.qty) over (
               partition by i.item
               order by i.qty, i.loc
               rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
        from orderline o
        join inventory i
            on i.item  = o.item
       where o.ordno = 42
          ) s
    where s.sum_prv_qty < s.ord_qty
       ) s2
 order by s2.warehouse
        , s2.aisle_no
        , case
             when mod(s2.aisle_no,2) = 1 then s2.position
             else                            -s2.position
          end;</code>
<p>
And now with the use of DENSE_RANK our forklift driver will be shown a much more efficient route.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/vf2i5hq9pif3szn/image4.png""/>
</p><p>
Supposing the two warehouses do not have doors between them at both end of the aisles, but only one door at one end, then we need the AISLE_NO count to ""restart"" in the second warehouse, so the first aisle visited in the second warehouse always will be odd-numbered (1) rather than either odd or even depending on how many aisles he visits in the first warehouse.
<p></p>
That is easily accomplished in the DENSE_RANK analytic clauses - instead of having both warehouse and aisle in the ORDER BY clause, we PARTITION by warehouse and ORDER BY aisle. Then the ranking is performed individually for each warehouse.
</p>
<code>select s2.warehouse, s2.aisle, s2.aisle_no, s2.position, s2.loc, s2.item, s2.pick_qty
  from (
   select to_number(substr(s.loc,1,1)) warehouse
        , substr(s.loc,3,1) aisle
        , dense_rank() over (
             partition by to_number(substr(s.loc,1,1))  -- warehouse
             order by     substr(s.loc,3,1)             -- aisle
          ) aisle_no
        , to_number(substr(s.loc,5,2)) position
        , s.loc, s.item
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
           , nvl(sum(i.qty) over (
               partition by i.item
               order by i.qty, i.loc
               rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
        from orderline o
        join inventory i
            on i.item  = o.item
       where o.ordno = 42
          ) s
    where s.sum_prv_qty < s.ord_qty
       ) s2
 order by s2.warehouse
        , s2.aisle_no
        , case
             when mod(s2.aisle_no,2) = 1 then s2.position
             else                            -s2.position
          end;</code>
<p>
With the PARTITION BY in the DENSE_RANK function, we easily get a better route for this warehouse layout too.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/v3f4oxhgyw5ccny/image5.png""/>
</p><p>
You have now seen step by step the development of a query that using two analytic functions (SUM and DENSE_RANK) solve a problem efficiently in SQL that without analytic functions would either be very hard and inefficient or need a slow procedural approach.
</p><p>
In the next module we'll continue adding more to this FIFO picking query to show how to go even further in SQL complexity.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",06-JUN-17 07.14.59.494714000 AM,"KIBEHA@GMAIL.COM",11-JUN-17 08.59.23.678406000 AM,"KIBEHA@GMAIL.COM"
108031870563050384264536712742009765444,97371311146062087736707699233039984534,"FIFO Batch Picking",50,"<p>
This continues the previous module, based on the same tables.
</p>
<p>
So far our forklift operator has picked a single order (order number 42) - now we would like to use him more efficiently and have him batch-pick multiple orders simultaneously, for example order numbers 51, 62 and 73.
</p><p>
For a first shot at the query needed, we can simply group the desired order numbers by item giving us the total quantities of each item we need to pick. This aggregated dataset we can pass to the FIFO picking query and get a picking list just as if it was a single order.
</p>
<code>with orderbatch as (
   select o.item, sum(o.qty) qty
     from orderline o
    where o.ordno in (51, 62, 73)
    group by o.item
)
select s.loc
     , s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
   select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
             partition by i.item
             order by i.purch, i.loc
             rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
     from orderbatch o
     join inventory i
        on i.item = o.item
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.loc;</code>
<p>
This gives us the correct total amount to be picked at each location, which is OK as such, but the operator cannot tell how much of each pick he should pack with each order. We need to add something more for this.
</p><p>
First we can add another rolling sum SUM_QTY besides our SUM_PRV_QTY, with SUM_QTY including the current row (while SUM_PRV_QTY only has the previous rows.) SUM_QTY could also have been calculated as SUM_PRV_QTY + LOC_QTY, but as this is an analytic function tutorial, we'll use the two analytic calls just to demonstrate the difference in windowing clauses.
</p><p>
Using the two rolling sums, we get the FROM_QTY and TO_QTY that specifies, that out of the total 48 ALE we need to pick, in the first location we pick number 1 to 18, in the second location we pick number 19 to 42, and so on. These intervals we can use later on.
</p>
<code>with orderbatch as (
   select o.item, sum(o.qty) qty
     from orderline o
    where o.ordno in (51, 62, 73)
    group by o.item
)
select s.loc, s.item
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
     , sum_prv_qty + 1 from_qty
     , least(sum_qty, ord_qty) to_qty
  from (
   select o.item, o.qty ord_qty, i.purch, i.loc, i.qty loc_qty
        , nvl(sum(i.qty) over (
             partition by i.item
             order by i.purch, i.loc
             rows between unbounded preceding and 1 preceding
          ),0) sum_prv_qty
        , nvl(sum(i.qty) over (
             partition by i.item
             order by i.purch, i.loc
             rows between unbounded preceding and current row
          ),0) sum_qty
     from orderbatch o
     join inventory i
        on i.item = o.item
       ) s
 where s.sum_prv_qty < s.ord_qty
 order by s.item, s.purch, s.loc;</code>
<p>
Similar to calculating the intervals that each <em>location</em> represents out of the total picked quantity of an item, so we can calculate intervals that each <em>order</em> represents of the total. Using the same technique with two running totals each with a different windowing clause, we see that out of the total 48 ALE, the first order receives number 1 to 24, the second order receives number 25 to 32, and so on.
</p>
<code>select o.ordno, o.item, o.qty
     , nvl(sum(o.qty) over (
          partition by o.item
          order by o.ordno
          rows between unbounded preceding and 1 preceding
       ),0) + 1 from_qty
     , nvl(sum(o.qty) over (
          partition by o.item
          order by o.ordno
          rows between unbounded preceding and current row
       ),0) to_qty
  from orderline o
 where ordno in (51, 62, 73)
 order by o.item, o.ordno;</code>
<p>
Having these two sets of quantity intervals calculated, we can join on ""overlapping"" intervals.
-- Now join on ""overlapping"" qty intervals

</p>
<code>with orderlines as (
   select o.ordno, o.item, o.qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and 1 preceding
          ),0) + 1 from_qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and current row
          ),0) to_qty
     from orderline o
    where ordno in (51, 62, 73)
), orderbatch as (
   select o.item, sum(o.qty) qty
     from orderlines o
    group by o.item
), fifo as (
   select s.loc, s.item, s.purch
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
        , sum_prv_qty + 1 from_qty
        , least(sum_qty, ord_qty) to_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and current row
             ),0) sum_qty
        from orderbatch o
        join inventory i
           on i.item = o.item
          ) s
    where s.sum_prv_qty < s.ord_qty
)
select f.loc, f.item, f.purch, f.pick_qty, f.from_qty f_from_qty, f.to_qty f_to_qty
     , o.ordno, o.qty, o.from_qty o_from_qty, o.to_qty o_to_qty
  from fifo f
  join orderlines o
       on o.item = f.item
      and o.to_qty >= f.from_qty
      and o.from_qty <= f.to_qty
 order by f.item, f.purch, o.ordno;</code>
<p>
Looking at the joined intervals, we can see that the 18 ALE picked in the first location all go to the first order. The 24 we pick in the second location overlaps with all three order quantity intervals, so the first order get 6 of the 24, the second order get 8 of the 24, and the third order get the last 10. Using LEAST and GREATEST we can calculate this overlap as column PICK_ORD_QTY.
</p>
<code>with orderlines as (
   select o.ordno, o.item, o.qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and 1 preceding
          ),0) + 1 from_qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and current row
          ),0) to_qty
     from orderline o
    where ordno in (51, 62, 73)
), orderbatch as (
   select o.item, sum(o.qty) qty
     from orderlines o
    group by o.item
), fifo as (
   select s.loc, s.item, s.purch, s.loc_qty
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
        , sum_prv_qty + 1 from_qty
        , least(sum_qty, ord_qty) to_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and current row
             ),0) sum_qty
        from orderbatch o
        join inventory i
           on i.item = o.item
          ) s
    where s.sum_prv_qty < s.ord_qty
)
select f.loc, f.item, f.purch, f.pick_qty, f.from_qty f_from_qty, f.to_qty f_to_qty
     , o.ordno, o.qty, o.from_qty o_from_qty, o.to_qty o_to_qty
     , least(
          f.loc_qty
        , least(o.to_qty, f.to_qty)
               - greatest(o.from_qty, f.from_qty) + 1
       ) pick_ord_qty
  from fifo f
  join orderlines o
       on o.item = f.item
      and o.to_qty >= f.from_qty
      and o.from_qty <= f.to_qty
 order by f.item, f.purch, o.ordno;</code>
<p>
We've now developed the multi-order batch-picking FIFO query with all the columns showing the intermediate calculations, so now we clean up the query to contain only the needed columns for the picking list in order of location.
</p>
<code>with orderlines as (
   select o.ordno, o.item, o.qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and 1 preceding
          ),0) + 1 from_qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and current row
          ),0) to_qty
     from orderline o
    where ordno in (51, 62, 73)
), orderbatch as (
   select o.item, sum(o.qty) qty
     from orderlines o
    group by o.item
), fifo as (
   select s.loc, s.item, s.loc_qty
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
        , sum_prv_qty + 1 from_qty
        , least(sum_qty, ord_qty) to_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and current row
             ),0) sum_qty
        from orderbatch o
        join inventory i
           on i.item = o.item
          ) s
    where s.sum_prv_qty < s.ord_qty
)
select f.loc, f.item, f.pick_qty pick_at_loc, o.ordno
     , least(
          f.loc_qty
        , least(o.to_qty, f.to_qty)
               - greatest(o.from_qty, f.from_qty) + 1
       ) qty_for_ord
  from fifo f
  join orderlines o
       on o.item = f.item
      and o.to_qty >= f.from_qty
      and o.from_qty <= f.to_qty
 order by f.loc, o.ordno;</code>
<p>
And as a last addition to make our forklift operator really efficient, we can add on the odd/even-numbered aisle ordering to this query too, ending up with this ultimate query.
</p>
<code>with orderlines as (
   select o.ordno, o.item, o.qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and 1 preceding
          ),0) + 1 from_qty
        , nvl(sum(o.qty) over (
             partition by o.item
             order by o.ordno
             rows between unbounded preceding and current row
          ),0) to_qty
     from orderline o
    where ordno in (51, 62, 73)
), orderbatch as (
   select o.item, sum(o.qty) qty
     from orderlines o
    group by o.item
), fifo as (
   select s.loc, s.item, s.loc_qty
        , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
        , sum_prv_qty + 1 from_qty
        , least(sum_qty, ord_qty) to_qty
     from (
      select o.item, o.qty ord_qty, i.loc, i.qty loc_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and 1 preceding
             ),0) sum_prv_qty
           , nvl(sum(i.qty) over (
                partition by i.item
                order by i.purch, i.loc
                rows between unbounded preceding and current row
             ),0) sum_qty
        from orderbatch o
        join inventory i
           on i.item = o.item
          ) s
    where s.sum_prv_qty < s.ord_qty
), pick as (
   select to_number(substr(f.loc,1,1)) warehouse
        , substr(f.loc,3,1) aisle
        , dense_rank() over (
             order by
             to_number(substr(f.loc,1,1)),    -- warehouse
             substr(f.loc,3,1)                -- aisle
          ) aisle_no
        , to_number(substr(f.loc,5,2)) position
        , f.loc, f.item, f.pick_qty pick_at_loc, o.ordno
        , least(
             f.loc_qty
           , least(o.to_qty, f.to_qty)
                  - greatest(o.from_qty, f.from_qty) + 1
          ) qty_for_ord
     from fifo f
     join orderlines o
          on o.item = f.item
         and o.to_qty >= f.from_qty
         and o.from_qty <= f.to_qty
)
select p.loc, p.item, p.pick_at_loc, p.ordno, p.qty_for_ord
  from pick p
 order by p.warehouse
        , p.aisle_no
        , case
             when mod(p.aisle_no,2) = 1 then p.position
             else                           -p.position
          end;</code>
<p>
Over these two modules you now have seen the process of developing a real-life use case for analytic functions step by step by adding a little more analytic functions one at a time viewing the intermediate results and gradually letting the query grow more complex as you go along. Often this can be a typical way of developing analytic queries.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",06-JUN-17 07.15.38.692839000 AM,"KIBEHA@GMAIL.COM",06-JUN-17 07.15.38.692939000 AM,"KIBEHA@GMAIL.COM"
101460138563957158416127717054089936031,98773055494155843597266512030175220751,"Does previous query still run?",75,"<p>Now that we have modified our original JSON model to now include the id of the person receiving the transfer it is still possible to run the original pattern matching query which is based on the original business rules:
</p>
<code>SELECT user_id, first_t, last_t, big_amount 
FROM (SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer')
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS first_t, 
   LAST(y.time_id) AS last_t, 
   y.amount AS big_amount
 ONE ROW PER MATCH
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10); </code>
<p>As we can see, our original query does in fact still run even though we have added new fields to our JSON data.</p>",04-APR-17 09.35.55.663835000 AM,"KEITH.LAKER@ORACLE.COM",04-APR-17 09.36.07.365684000 AM,"KEITH.LAKER@ORACLE.COM"
103563827414966928958316514161805908744,103563632879277014483527974609264228592,"Business Problem",20,"<h3>Sessionization analysis</h3>
<p>
This tutorial is designed to help show you how you can run sessionization analysis on application logs, web logs, etc. that are in JSON format.</p>
<p>
For this tutorial we are going to define a session as a sequence of one or more events where the inter-timestamp gap is less than 10 minutes between events.
</p>
<p>
Using our simplified JSON table of click data, which will create in the next step, we will create a sessionization data set which tracks each session, the duration of the session and the number of clicks/events using the new 12c MATCH_RECOGNIZE feature.
</p>",24-APR-17 12.54.27.397454000 PM,"KEITH.LAKER@ORACLE.COM",24-APR-17 12.54.27.397519000 PM,"KEITH.LAKER@ORACLE.COM"
103563827414979018216512660453552970504,103563632879277014483527974609264228592,"Setup",40,"<p>First step is to create the JSON table that will hold our data from our session log file. The log will provide the details of the time and account/user id.
</p>
<p>Oracle Database 12c supports storing JSON documents inside the database. Use following code to create a table to store the transaction log which is in JSON format
</p>
<code>CREATE TABLE json_sessionization 
(session_doc CLOB, 
 CONSTRAINT ""VALID_JSON"" CHECK (session_doc IS JSON) ENABLE
);</code>",24-APR-17 12.56.13.036828000 PM,"KEITH.LAKER@ORACLE.COM",24-APR-17 01.08.26.935229000 PM,"KEITH.LAKER@ORACLE.COM"
103563827415000778881265723778697681672,103563632879277014483527974609264228592,"View session log data",60,"<p>Using the new JSON SQL notation we can query our data using a simple SELECT statement</p>
<code>SELECT 
  TO_NUMBER(j.session_doc.time_id) as time_id, 
  j.session_doc.user_id as user_id
FROM json_sessionization j;</code>",24-APR-17 12.58.10.731947000 PM,"KEITH.LAKER@ORACLE.COM",02-JUN-17 12.26.16.709324000 PM,"KEITH.LAKER@ORACLE.COM"
103563827416654589402498536489695730440,103563632879277014483527974609264228592,"Building my first sessionization report",70,"<h3>Some important keywords</h3>
<p>Running our first sessionization report using only the main keywords within MATCH_RECOGNIZE clause</p>
<p>The keywords that we are going to use are:</p>
<blockquote>
<p>
<strong>ALL ROWS PER MATCH</strong><br>
You will sometimes want summary data about the matches and other times need details. For this report we are going to return a detailed report.
</p>
<p>
<strong>PATTERN</strong><br>
The PATTERN clause lets you define which pattern variables must be matched, the sequence in which they must be matched, and the quantity of rows which must be matched. The PATTERN clause specifies a regular expression for the match search.
<p>
<strong>DEFINE</strong><br>
The PATTERN clause depends on pattern variables, therefore, you must have a clause to define these variables. They are specified in the DEFINE clause. DEFINE is a required clause, used to specify the conditions that a row must meet to be mapped to a specific pattern variable.
</p>
</blockquote>
<p>
So let's write our first MATCH_RECOGNIZE statement based on our business requirements:</p>
<code>SELECT 
 time_id,
 userid
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j) 
MATCH_RECOGNIZE(         
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (time_id - PREV(time_id) <= 10));
</code>",24-APR-17 01.01.37.099961000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.50.20.535400000 PM,"SHARON.KENNEDY@ORACLE.COM"
103563827416658216179957380377219848968,103563632879277014483527974609264228592,"More about PATTERN and DEFINE  keywords",80,"<h3>How do we build the PATTERN and DEFINE clauses?</h3>
<p>
As we mentioned earlier, the <strong>PATTERN</strong> defines a regular expression, which is a highly expressive way to search for patterns. <strong>PATTERN (b s+)</strong> says that the pattern we are searching for has two pattern variables: <strong>b</strong> and <strong>s</strong>. The star sign <strong>(*)</strong> after <strong>s</strong> means that zero or more rows must be mapped to confirm a match.
</p>
<p>The plus sign <strong>(+)</strong> is part of a library of quantifiers that can be used to describe your pattern. We will explore these in a separate tutorial</p>

<p><strong>DEFINE</strong> gives us the conditions that must be met for a row to map to your row pattern variables <strong>b</strong> and <strong>s</strong>. Because there is no condition for b, any row can be mapped to b. Why have a pattern variable with no condition? You use it as a starting point for testing for matches. S takes advantage of the PREV() function, which lets them compare the timestamp in the current row to the timestamp in the prior row. S is matched when a row has a timestamp than is within 10 minutes of the row that preceded it.
</p>",24-APR-17 01.02.23.748897000 PM,"KEITH.LAKER@ORACLE.COM",28-APR-17 10.28.20.793850000 AM,"KEITH.LAKER@ORACLE.COM"
98870667916069531440668747432528077238,98773055494155843597266512030175220751,"Summary",100,"<p>This tutorial has explained how to use the new JSON feature to store data inside the database and query it using familiar SQL syntax. 
</p>
<p>We have designed a fraud pattern matching SQL statement based on a set of business rules and then enhanced the pattern matching SQL statement in response to changing business rules.
</p>
<p>Finally we have used some of the built-in measures to validate that our pattern matching statement is working correctly (using the MATCH_NUMBER() and CLASSIFIER() functions) along with the FIRST() and LAST() functions to retrieve specific data points within our pattern.
</p>",10-MAR-17 02.02.27.071775000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 02.02.27.071839000 PM,"KEITH.LAKER@ORACLE.COM"
103563827416670305438153526668966910728,103563632879277014483527974609264228592,"Calculating the session number",90,"<h3>Finding the session number</h3>
<p>Congratulations your report runs! We have a list of events where each event is within 10 minutes of the previous event</p>
<p>But we don't actually know how many sessions each user logged, so how can we find out this information?</p>
<p>We can use the internal <strong>MATCH_NUMBER()</strong> function to return the session id for each user - assuming that a session contains clicks that are within a 10 minute interval of previous click</p>

<p>What does <strong>MATCH_NUMBER()</strong> actually do?<br>
You might have a large number of matches for your pattern inside a given row partition. How do you tell these matches apart? This is done with the <strong>MATCH_NUMBER()</strong> function. Matches within a row pattern partition are numbered sequentially starting with 1 in the order they are found. This numbering starts at 1 within each row pattern partition, because there is no linked ordering across row pattern partitions.
</p>

<code>SELECT 
 time_id,
 userid,
 session_id
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j) 
MATCH_RECOGNIZE(         
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (time_id - PREV(time_id) <= 10)
 );
</code>
<h5>That does not look correct...</h5>
<p>What you will notice is that each row has a session id of 1 which means that we are only finding one instance of our pattern within our data set.</p>
<p>Obviously this is not correct because just looking at the raw data we can see that Mary has logged more than one session</p>
<code>SELECT 
  TO_NUMBER(j.session_doc.time_id) as time_id, 
  j.session_doc.user_id as user_id
FROM json_sessionization j
WHERE j.session_doc.user_id = 'Mary'
</code>",24-APR-17 01.03.03.503687000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.51.07.890885000 PM,"SHARON.KENNEDY@ORACLE.COM"
103563827416696901806185048510810446600,103563632879277014483527974609264228592,"Partitioning and ordering data",100,"<h3>Some more important keywords</h3>
<p>What we need to do is use the PARTITION BY and ORDER BY key phrases to correctly divide up our data set by each user and then within each user make sure the data is ordered by our timestamp so we can determine if the clicks recorded in our log by each user are within 10 minutes of their previous click.
</p>
<p>We need to expand our MATCH_RECOGNIZE() clause as shown here:</p>
<code>SELECT 
 time_id,
 userid,
 session_id
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j)
MATCH_RECOGNIZE(         
   PARTITION BY userid ORDER BY time_id
   MEASURES match_number() as session_id
   ALL ROWS PER MATCH
   PATTERN (b s+)
   DEFINE
       s as (time_id - PREV(time_id) <=10)
 );
</code>
<p>Now you should see that Mary has logged 3 sessions and Sam has logged three sessions. Which means that have now correctly specified our MATCH_RECOGNIZE clause to create our sessionization report.</p>",24-APR-17 01.03.36.823476000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.51.42.228855000 PM,"SHARON.KENNEDY@ORACLE.COM"
103563632879285476964265277013487171824,103563632879277014483527974609264228592,"Overview of SQL Pattern Matching",10,"<h3>Finding patterns with SQL</h3>
<p>
Recognizing patterns in a sequence of rows has been a capability that was widely desired, but not really possible with SQL until now. There were many workarounds, but these were difficult to write, hard to understand, and inefficient to execute.</p> 
<p>
With Oracle Database 12c you can use the MATCH_RECOGNIZE clause to perform pattern matching in SQL to do the following:</p>
<ol>
<li>Logically group and order the data that is used in the MATCH_RECOGNIZE clause using the PARTITION BY and ORDER BY clauses.</li>
<li>Define business rules/patterns using the PATTERN clause. These patterns use regular expressions syntax, a powerful and expressive feature and applied to the pattern variables.</li>
<li>Specify the logical conditions required to map a row to a row pattern variable using the DEFINE clause.</li>
<li>Define output measures, which are expressions within the MEASURES clause.</li>
<li>Control the output (summary vs. detailed) from the pattern matching process.</li>
</ol>",24-APR-17 12.53.40.750824000 PM,"KEITH.LAKER@ORACLE.COM",24-APR-17 12.53.40.750896000 PM,"KEITH.LAKER@ORACLE.COM"
98865671820685737011654779696199662194,98773055494155843597266512030175220751,"Viewing new data set",70,"<p>As before we can use the new JSON SQL notation to query our data using a simple SELECT statement
</p>
<code>SELECT 
  j.transaction_doc.time_id as time_id, 
  j.transaction_doc.user_id as user_id,
  j.transaction_doc.event_id as event_id,   
  j.transaction_doc.transfer_id as transfer_id,   
  j.transaction_doc.trans_amount as amount
FROM json_transactions j;</code>
<p>Here is the list of the transfer transactions that we are of interest for our analysis which includes the new column: transfer_id
</p>
<code>SELECT 
 TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
 j.transaction_doc.user_id as user_id,
 j.transaction_doc.event_id as event_id,   
 j.transaction_doc.transfer_id as transfer_id,   
 to_number(j.transaction_doc.trans_amount) as amount
FROM json_transactions j
WHERE  j.transaction_doc.event_id = 'Transfer';</code>",10-MAR-17 12.53.56.171571000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 12.53.56.171635000 PM,"KEITH.LAKER@ORACLE.COM"
98865671820713542305505916167217904242,98773055494155843597266512030175220751,"Using new business rules",80,"<p>Let's change the definitions of the two pattern variables to include the new business requirements:
</p>
<p>1) Check for transfers across different accounts</p>
<blockquote>
         PREV(transfer_id) <> transfer_id
</blockquote>
<p>2) Total sum of small transfers must be less than 20K</p>
<blockquote>
         SUM(x.amount) < 20000)
</blockquote>

<code>SELECT user_id, first_t, last_t, big_amount 
FROM (SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer')
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS first_t, 
   LAST(y.time_id) AS last_t, 
   y.amount AS big_amount
 ONE ROW PER MATCH
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         PREV(transfer_id) <> transfer_id AND
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10 AND
         SUM(x.amount) < 20000);</code>",10-MAR-17 12.55.40.671218000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 02.15.43.194047000 PM,"KEITH.LAKER@ORACLE.COM"
98865671820742556525176667267410852466,98773055494155843597266512030175220751,"Extracting more value...",90,"<p>We can extract a lot more information from our pattern using some of the built-in functions. For example we can extract data for each of the three small transfers by using the FIRST() and LAST() functions. Notice that we can use the FIRST(value, x) syntax to extract information about the 2nd transfer. 
</p>
<p>Therefore, <strong>FIRST(x.transfer_id,1)</strong> will return the transfer_id for the second match.
</p>
<code>SELECT user_id, first_t, amount_1, transfer_1, amount_2, transfer_2, amount_3, transfer_3, last_t, big_amount,transfer_4 
FROM (SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
     FROM json_transactions j
     WHERE  j.transaction_doc.event_id = 'Transfer')
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS first_t, 
   LAST(y.time_id) AS last_t, 
   FIRST(x.amount) AS amount_1,
   FIRST(x.transfer_id) AS transfer_1,
   FIRST(x.amount,1) AS amount_2,
   FIRST(x.transfer_id,1) AS transfer_2,
   LAST(x.amount) AS amount_3,
   LAST(x.transfer_id) AS transfer_3,
   y.amount AS big_amount,
   y.transfer_id AS transfer_4
 ONE ROW PER MATCH
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         PREV(transfer_id) <> transfer_id AND
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10 AND
         SUM(x.amount) < 20000);</code>",10-MAR-17 12.56.50.865842000 PM,"KEITH.LAKER@ORACLE.COM",10-MAR-17 02.15.13.455797000 PM,"KEITH.LAKER@ORACLE.COM"
103568285807913712937324988253600394690,103567330442136148468768570832363109098,"Overview",10,"<p>This is an interesting problem that has come up a few times in discussions (and I think it has been mentioned on the SQL forums as well).  When using LISTAGG on very large data sets you can sometimes create a list that is too long and consequently get an:</p> 
<CODE>ORA-01489: result of string concatenation is too long </CODE> 
<p>error. Wouldnâ€™t it be great if there was a simple yet elegant way to resolve this issue?</p> 
<p>Actually there is and we can use a few of the most recent analytical SQL functions. If you are using Database 12c you can make use of the MATCH_RECOGNIZE function to effectively create chunks of strings that do not exceed the VARCHAR2 limit.</p>",24-APR-17 01.50.39.953858000 PM,"KEITH.LAKER@ORACLE.COM",24-APR-17 01.50.39.953935000 PM,"KEITH.LAKER@ORACLE.COM"
103568285807931846824619207691220987330,103567330442136148468768570832363109098,"Our test data",20,"<p>For example, letâ€™s assume that we have the following statement (to keep things relatively simple letâ€™s use the EMP table in the schema SCOTT)</p>
<code>SELECT
 deptno,
 LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno) AS namelist
FROM scott.emp
GROUP BY deptno;</code>
<p>generates a nicely concatenated list of employee names within each department.</p>
<p>Now letâ€™s assume that the above statement does not run and that we have a limit of 15 characters that can be returned by each row in our LISTAGG function.</p>",24-APR-17 01.52.47.068398000 PM,"KEITH.LAKER@ORACLE.COM",26-APR-17 10.52.10.722028000 AM,"KEITH.LAKER@ORACLE.COM"
103568285807952398563552656387190992322,103567330442136148468768570832363109098,"Chopping up the list of values",30,"<p>We can use the Database 12c SQL pattern matching function, MATCH_RECOGNIZE, to return a list of values that does not exceed 15 characters. First step is to wrap the processing in a view so that we can then get data from this view to feed our LISTAGG function. Here is the view that contains the MATCH_RECOGNIZE clause:</p>
<code>CREATE OR REPLACE VIEW emp_mr AS
 SELECT * FROM scott.emp MATCH_RECOGNIZE(
    PARTITION BY deptno ORDER BY empno
    MEASURES 
     match_number() AS mno,
     classifier() as pattern_vrb
    ALL ROWS PER MATCH
    AFTER MATCH SKIP PAST LAST ROW
    PATTERN (S B+)
    DEFINE B AS LENGTHB(S.ename) + SUM(LENGTHB(CONCAT(b.ename, ';'))) + LENGTHB(';') < = 15
 );
</code>
<p>Let's view the data returned by MATCH_RECOGNIZE and using the analytic function version of SUM() to calculate a running total to show the overall length of the concatenated string of names within each department:</p>
<code>SELECT
 deptno,
 empno,
 ename,
 mno, 
 pattern_vrb,
 sum(LENGTH(ename)) OVER (PARTITION BY deptno, mno ORDER BY empno) AS str_length
FROM emp_mr;</code>",24-APR-17 01.54.17.396655000 PM,"KEITH.LAKER@ORACLE.COM",28-APR-17 09.39.02.786348000 AM,"KEITH.LAKER@ORACLE.COM"
103568285808221989021326718693150469570,103567330442136148468768570832363109098,"Changing the cut-off point",50,"<p>You can change the cut-off point for the string truncation process by changing the value shown in bold on the last line of the code...<em>in this version I have changed the truncation point to 25 characters</em>:</p>
<code>CREATE OR REPLACE VIEW emp_mr AS
 SELECT * FROM scott.emp MATCH_RECOGNIZE(
    PARTITION BY deptno ORDER BY empno
    MEASURES 
     match_number() AS mno,
     classifier() as pattern_vrb
    ALL ROWS PER MATCH
    AFTER MATCH SKIP PAST LAST ROW
    PATTERN (S B*)
    DEFINE B AS LENGTHB(S.ename) + SUM(LENGTHB(CONCAT(b.ename, ';'))) + LENGTHB(';') < = 25
 );
</code>
<p>Let's view the data returned by MATCH_RECOGNIZE:</p>
<code>SELECT
 deptno,
 empno,
 ename,
 mno, 
 pattern_vrb,
 sum(LENGTH(ename)) OVER (PARTITION BY deptno, mno ORDER BY empno) AS str_length
FROM emp_mr;</code>
<p>The above code now returns groups of strings for each department where the total length of each group is less than 25 characters:
<code>SELECT
  deptno,
 LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno) AS namelist,
 LENGTH(LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno)) AS how_long
 FROM emp_mr
 GROUP BY deptno, mno;
</code>",24-APR-17 02.06.29.993002000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 03.07.26.132617000 PM,"SHARON.KENNEDY@ORACLE.COM"
106709019627103350739255409127610558352,97371311146062087736707699233039984534,"TOP-N and Ratios",20,"<p>
The basis of this module is a table of items, each item within an item group GRP.
</p>
<code>select *
  from items
 order by grp, item;</code>
<p>
And a table of monthly sales quantities for each item.
</p>
<code>select *
  from sales
 order by item, mth;</code>
<p>
The base query for our TOP-N calculation is a classic join with group by to get the sales quantity for each item in the year 2014. That will be the basis for determining a TOP-3 best selling items within each item group.
</p>
<code>select i.grp, i.item, max(i.name) name, sum(s.qty) qty
  from items i
  join sales s
    on s.item = i.item
 where s.mth between date '2014-01-01' and date '2014-12-01'
 group by i.grp, i.item
 order by i.grp, sum(s.qty) desc, i.item;</code>
<p>
For TOP-N we can use three different analytic functions, DENSE_RANK, RANK and ROW_NUMBER. All three assigns integer numbers to the rows in the ordering specified by the ORDER BY analytic clause - the difference is how they handle ties (duplicates) in the order:<br>
<ul>
<li>DENSE_RANK assigns the same rank to ties and the next rank will be 1 higher (i.e. ranks will be consecutive without gaps.)</li>
<li>RANK also assigns the same rank to ties, but the next rank will then be incremented by the number of tied rows rather than 1 (i.e. there will be gaps in the ranks where there are ties, just like how the olympics assign medals.)</li>
<li>ROW_NUMBER never assigns the same number to any rows, it simply assigns consecutive numbers according to the ordering specified, so if there are for example 2 rows tied for 3rd place, it will be indeterminate which row gets number 3 and which gets number 4. (Therefore a best practice for ROW_NUMBER is to make the ordering unique within each partition to ensure consistent results.)
</ul><br>
So we can take our base GROUP BY query in an inline view and then calculate all three analytic functions on the result and compare them to see how the ties are handled by each function.
</p>
<code>select g.grp, g.item, g.name, g.qty
     , dense_rank() over (
          partition by g.grp
          order by g.qty desc
       ) drnk
     , rank()       over (
          partition by g.grp
          order by g.qty desc
       ) rnk
     , row_number() over (
          partition by g.grp
          order by g.qty desc, g.item
       ) rnum
  from (
   select i.grp, i.item, max(i.name) name, sum(s.qty) qty
     from items i
     join sales s
       on s.item = i.item
    where s.mth between date '2014-01-01' and date '2014-12-01'
    group by i.grp, i.item
) g
 order by g.grp, g.qty desc, g.item;</code>
<p>
It is possible to avoid the inline view in this case, because analytic functions are evaluated almost at the very end of subquery evaluation (normally just before the final ORDER BY.) That means that the WHERE, GROUP BY and HAVING clauses of a query (and thus all aggregates) are evaluated before the analytic functions are applied. Hence we can make a query with a GROUP BY clause and aggregate functions, and in the select list we can use analytic functions that within the various analytic clauses can use all the grouped columns as well as aggregates (the same expressions that we can SELECT in a GROUP BY query.) So for example we can use the aggregate expression SUM(S.QTY) in the ORDER BY clause of our analytic functions.
</p>
<code>select i.grp, i.item, max(i.name) name, sum(s.qty) qty
     , dense_rank() over (
          partition by i.grp
          order by sum(s.qty) desc
       ) drnk
     , rank()       over (
          partition by i.grp
          order by sum(s.qty) desc
       ) rnk
     , row_number() over (
          partition by i.grp
          order by sum(s.qty) desc, i.item
       ) rnum
  from items i
  join sales s
    on s.item = i.item
 where s.mth between date '2014-01-01' and date '2014-12-01'
 group by i.grp, i.item
 order by i.grp, sum(s.qty) desc, i.item;</code>
<p>
But since the analytic functions are evaluated almost at the end of query evaluation, that also means analytic functions are not allowed within for example the WHERE clause. This query may look like it queries a TOP-3, but it is syntactically wrong and will raise error: ORA-30483: window functions are not allowed here.
</p>
<code>select i.grp, i.item, max(i.name) name, sum(s.qty) qty
     , rank()       over (
          partition by i.grp
          order by sum(s.qty) desc
       ) rnk
  from items i
  join sales s
    on s.item = i.item
 where s.mth between date '2014-01-01' and date '2014-12-01'
   and rank() over (          /* Raises ORA-30483 */
          partition by i.grp
          order by sum(s.qty) desc
       ) <= 3
 group by i.grp, i.item
 order by i.grp, sum(s.qty) desc, i.item;</code>
<p>
So instead we have to wrap the query with the analytic functions inside an inline view and then filter with WHERE on the results of the inline view. This is very classic when you work with analytic functions and you can easily create a query with several layers of inline views. (Therefore it can also be nice to use techniques like this one where we do analytic functions directly on aggregates to avoid more inline views than we have to - not that it matters to performance, it is mostly about readability when the queries become large.)
</p><p>
So here we can see a TOP-3 query using RANK - RANK being equivalent to olympic ranking in that when we have 2 gold medals (rank 1) we skip the silved medal and then number 3 will get a bronze medal. Observe that using RANK a TOP-3 may return more than 3 rows as the top 3, that can happen if there are ties for bronze medal or if there are more than 3 tied for gold medal.
</p>
<code>select g.grp, g.item, g.name, g.qty, g.rnk
  from (
    select i.grp, i.item, max(i.name) name, sum(s.qty) qty
         , rank() over (
              partition by i.grp
              order by sum(s.qty) desc
           ) rnk
      from items i
      join sales s
        on s.item = i.item
     where s.mth between date '2014-01-01' and date '2014-12-01'
     group by i.grp, i.item
) g
 where g.rnk <= 3
 order by g.grp, g.rnk, g.item;</code>
<p>
Using DENSE_RANK for TOP-3 can also (and in more cases than RANK) return more than 3 rows, as you can see when comparing the results of this query with the previous one.
</p>
<code>select g.grp, g.item, g.name, g.qty, g.rnk
  from (
    select i.grp, i.item, max(i.name) name, sum(s.qty) qty
         , dense_rank() over (
              partition by i.grp
              order by sum(s.qty) desc
           ) rnk
      from items i
      join sales s
        on s.item = i.item
     where s.mth between date '2014-01-01' and date '2014-12-01'
     group by i.grp, i.item
) g
 where g.rnk <= 3
 order by g.grp, g.rnk, g.item;</code>
<p>
Using ROW_NUMBER on the other hand will never return more than 3 rows for a TOP-3, as ties are never assigned the same number. But it also means that of two items that sold the same (iGloves and Cover), only one will be in the output. Which one it will be is indeterminate, so the ensure we always get consistent results it is a good idea to add something to the ORDER BY (in this case I.ITEM) to make it unique and therefore determinate results, otherwise a user might run the report and get iGloves included sometimes and Cover other times, which users will tend to see as a bug (even if both results are correct.)
</p>
<code>select g.grp, g.item, g.name, g.qty, g.rnk
  from (
    select i.grp, i.item, max(i.name) name, sum(s.qty) qty
         , row_number() over (
              partition by i.grp
              order by sum(s.qty) desc, i.item
           ) rnk
      from items i
      join sales s
        on s.item = i.item
     where s.mth between date '2014-01-01' and date '2014-12-01'
     group by i.grp, i.item
) g
 where g.rnk <= 3
 order by g.grp, g.rnk, g.item;</code>
<p>
We would like to add to the TOP-3 report a couple columns showing for the top items how great a percentage the item sales is out of the item group total sales respectively out of the grand total sales. That is not hard to do using two analytic SUM function calls - one using PARTITION BY to get the item group total, one using the empty () to get the grand total. Again here we take advantage of the fact that aggregates can be used within analytics, so the ""nested"" SUM calls are an <em>aggregate</em> SUM within an <em>analytic</em> SUM.
</p>
<code>select g.grp, g.item, g.name, g.qty, g.rnk
     , round(g.g_pct,1) g_pct
     , round(g.t_pct,1) t_pct
  from (
    select i.grp, i.item, max(i.name) name, sum(s.qty) qty
         , rank() over (
              partition by i.grp
              order by sum(s.qty) desc
           ) rnk
         , 100 * sum(s.qty) / sum(sum(s.qty)) over (partition by i.grp) g_pct
         , 100 * sum(s.qty) / sum(sum(s.qty)) over () t_pct
      from items i
      join sales s
        on s.item = i.item
     where s.mth between date '2014-01-01' and date '2014-12-01'
     group by i.grp, i.item
) g
 where g.rnk <= 3
 order by g.grp, g.rnk, g.item;</code>
<p>
But there is an easier way rather than taking the row value and divide by the SUM total, we can instead simply use the analytic RATIO_TO_REPORT function, which calculates the desired ratio in a single call rather than doing the division manually. It helps the code being self-documenting as the function name clearly states what we are calculating. Also it helps for those (very rare border-line) cases where the sum total becomes negative (if we are summing values that can be both negative and positive), where the division would raise an error but RATIO_TO_REPORT just becomes NULL to signify the ratio is indeterminate in such cases.
</p>
<code>select g.grp, g.item, g.name, g.qty, g.rnk
     , round(g.g_pct,1) g_pct
     , round(g.t_pct,1) t_pct
  from (
    select i.grp, i.item, max(i.name) name, sum(s.qty) qty
         , rank() over (
              partition by i.grp
              order by sum(s.qty) desc
           ) rnk
         , 100 * ratio_to_report(sum(s.qty)) over (partition by i.grp) g_pct
         , 100 * ratio_to_report(sum(s.qty)) over () t_pct
      from items i
      join sales s
        on s.item = i.item
     where s.mth between date '2014-01-01' and date '2014-12-01'
     group by i.grp, i.item
) g
 where g.rnk <= 3
 order by g.grp, g.rnk, g.item;</code>
<p>
You now have the techniques for creating TOP-N queries of the three different types. When your users ask you to develop a TOP-N query, be sure to ask them how they want ties handled, as that will determine which ranking function you want to use.
</p><p>
<em>Note:</em> In Oracle version 12.1 came a shorter notation for doing TOP-N queries, where you do not need analytic functions and inline views but simply can add FETCH FIRST 3 ROWS ONLY or FETCH FIRST 3 ROWS WITH TIES. This FETCH FIRST syntax executes analytic functions and filters on them behind the scenes, just like the queries in this module. FETCH FIRST syntax supports two methods with the same results as using ROW_NUMBER or RANK, but if you want DENSE_RANK results, you still need to do it manually like shown here. Also FETCH FIRST cannot do PARTITION BY, it can only do a TOP-3 for <em>all</em> items, not a TOP-3 <em>within each group</em> (partition). So queries like these are still often needed.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",24-MAY-17 03.19.03.359981000 PM,"KIBEHA@GMAIL.COM",06-JUN-17 07.13.28.493916000 AM,"KIBEHA@GMAIL.COM"
108967592094154564018617374424462675912,108572075684787330976836028944125402228,"Hierarchical child count",50,"<p>
The basis of this module is the standard SCOTT.EMP table, where we want a hierarchical output with a column for each employee showing how many subordinates that employee has. With a scalar subquery we can find this with a fairly simple query.
</p>
<code>select empno
     , lpad(' ', (level-1)*2) || ename as ename
     , (
         select count(*)
           from emp sub
          start with sub.mgr = emp.empno
          connect by sub.mgr = prior sub.empno
       ) subs
  from emp
 start with mgr is null
 connect by mgr = prior empno
 order siblings by empno;</code>
<p>
But that requires accessing the table many times - not very efficient. We can make a much more efficient query using row pattern matching.
</p>
<p>
Since row pattern matching is very dependent on something to order the data by, we need to be able to preserve the hierarchical ordering for use in the MATCH_RECOGNIZE clause. For that purpose we create a subquery factoring WITH clause where we put the hierarchical query in an inline view, so we can use ROWNUM and give it an alias RN. This WITH clause will be the basis of our row pattern queries.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select *
  from hierarchy
 order by rn;</code>
<p>
If we order the data by the hierarchy (order by rn) and start at any given employee, then following employees with a higher level are subordinates - until we reach a point where the level is the same or less. In MATCH_RECOGNIZE we use DEFINE to define a HIGHER row to be one where the level is greater than the starting row of the match. Then the pattern simply is that a given row must be followed by zero or more HIGHER rows. When we reach a row with the same or lower level than the starting row, the match stops.
</p><p>
Counting the number of HIGHER rows then is the number of subordinates we want. But once we've reached the end of the match, normally we'd search from there for a new match - in this case instead we want to go back to row after the beginning of the match and start searching for a new match from there. This is accomplished by AFTER MATCH SKIP TO NEXT ROW.
</p><p>
ALL ROWS PER MATCH allows us to see the details of each match in the output, so we can observe what actually is happening here.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select match_no, rn
     , empno, lpad(' ', (lvl-1)*2) || ename as ename
     , rolling_cnt, subs, class
     , strt_no, strt_name, high_no, high_name
  from hierarchy
match_recognize (
   order by rn
   measures
      match_number() as match_no
    , classifier() as class
    , strt.empno as strt_no
    , strt.ename as strt_name
    , higher.empno as high_no
    , higher.ename as high_name
    , count(higher.lvl) as rolling_cnt
    , final count(higher.lvl) as subs
   all rows per match
   after match skip to next row
   pattern ( strt higher* )
   define
      higher as higher.lvl > strt.lvl
)
 order by match_no, rn;</code>
<p>
Just as a visual aid, we can take the match numbers of the previous query and PIVOT it to visualize which rows are in match number 1, which are in match number 2, and so on.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select rn, empno, ename
     , case ""1""  when 1 then 'XX' end ""1"" 
     , case ""2""  when 1 then 'XX' end ""2"" 
     , case ""3""  when 1 then 'XX' end ""3"" 
     , case ""4""  when 1 then 'XX' end ""4"" 
     , case ""5""  when 1 then 'XX' end ""5"" 
     , case ""6""  when 1 then 'XX' end ""6"" 
     , case ""7""  when 1 then 'XX' end ""7"" 
     , case ""8""  when 1 then 'XX' end ""8"" 
     , case ""9""  when 1 then 'XX' end ""9"" 
     , case ""10"" when 1 then 'XX' end ""10""
     , case ""11"" when 1 then 'XX' end ""11""
     , case ""12"" when 1 then 'XX' end ""12""
     , case ""13"" when 1 then 'XX' end ""13""
     , case ""14"" when 1 then 'XX' end ""14""
  from (
   select match_no, rn, empno
        , lpad(' ', (lvl-1)*2) || ename as ename
     from hierarchy
   match_recognize (
      order by rn
      measures match_number() as match_no
      all rows per match
      after match skip to next row
      pattern ( strt higher* )
      define
         higher as higher.lvl > strt.lvl
   )
  )
 pivot (
   count(*)
   for match_no in (
      1,2,3,4,5,6,7,8,9,10,11,12,13,14
   )
 )
 order by rn;</code>
<p>
So if we change to using ONE ROW PER MATCH, we can get an output just like the original scalar subquery based query shown in the beginning of the module. But this query does not access the table multiple times - for larger datasets this scales much better and uses far fewer ressources.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select empno
     , lpad(' ', (lvl-1)*2) || ename as ename
     , subs
  from hierarchy
match_recognize (
   order by rn
   measures
      strt.rn as rn
    , strt.lvl as lvl
    , strt.empno as empno
    , strt.ename as ename
    , final count(higher.lvl) as subs
   one row per match
   after match skip to next row
   pattern ( strt higher* )
   define
      higher as higher.lvl > strt.lvl
)
 order by rn;</code>
<p>
Of course even when using ONE ROW PER MATCH we can still include more data if we wish, using FIRST or LAST or aggregate functions, like demonstrated here with MAX.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select empno, lpad(' ', (lvl-1)*2) || ename as ename
     , subs, high_from, high_to, high_max
  from hierarchy
match_recognize (
   order by rn
   measures
      strt.rn as rn
    , strt.lvl as lvl
    , strt.empno as empno
    , strt.ename as ename
    , count(higher.lvl) as subs
    , first(higher.ename) as high_from
    , last(higher.ename) as high_to
    , max(higher.lvl) as high_max
   one row per match
   after match skip to next row
   pattern ( strt higher* )
   define
      higher as higher.lvl > strt.lvl
)
 order by rn;</code>
<p>
It could be that we only want to output those employees that have subordinates. One way could be to put the query above in an inline view and filter on ""subs > 0"", but a simpler method is to simply change the PATTERN clause. If we change HIGHER* to HIGHER+, we indicate that a match only can be found if there is at least one HIGHER row, which means at least one subordinate.
</p>
<code>with hierarchy as (
   select lvl, empno, ename, rownum as rn
   from (
      select level as lvl, empno, ename
        from emp
       start with mgr is null
       connect by mgr = prior empno
       order siblings by empno
   )
)
select empno
     , lpad(' ', (lvl-1)*2) || ename as ename
     , subs
  from hierarchy
match_recognize (
   order by rn
   measures
      strt.rn as rn
    , strt.lvl as lvl
    , strt.empno as empno
    , strt.ename as ename
    , count(higher.lvl) as subs
   one row per match
   after match skip to next row
   pattern ( strt higher+ )
   define
      higher as higher.lvl > strt.lvl
)
 order by rn;</code>
<p>
Here you have seen a use case for AFTER MATCH SKIP TO NEXT ROW - a clause that not often is used, so can be easy to forget. But if you have a use case for it, it is highly efficient.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",15-JUN-17 06.34.30.159734000 AM,"KIBEHA@GMAIL.COM",15-JUN-17 06.34.30.159826000 AM,"KIBEHA@GMAIL.COM"
108967592094169071128452749974559150024,108572075684787330976836028944125402228,"Bin fitting with fixed number of bins",70,"<p>
The basis of this module is a simple table of items with a certain value (indicating for example weight or volume or some other measure).
</p>
<code>select *
  from items
 order by item_value desc;</code>
<p>
We have 3 bins that we want to fill as equally as possible - meaning that the sum of the item values in each bin should be as near equal as we can get it. There is a fairly simple algorithm for this purpose:
</p><ul>
<li>First, order the items by value in descending order.</li>
<li>In that order, assign each item to whatever bin has the smallest sum so far.</li>
</ul><p>
In the following MATCH_RECOGNIZE clause we accomplish this with the use of the OR operator | to define a pattern, that matches zero or more occurences of either a BIN1 row or a BIN2 row or a BIN3 row.
</p><p>
So when a row is examined to find out which classifier the row matches, it will first try to find out if it matches the definition of BIN1. When testing if it matches, it is always assumed that it does, which means that in the formula used in the BIN1 definition, the COUNT and SUM will <em>include</em> the row we are testing to find out if it is a BIN1 row or not. So when we test <b>COUNT(bin1.*) = 1</b>, we actually test <em>will this row be the first row in BIN1</em>. Similarly, the <b>SUM(bin1.item_value)</b> is the sum of all BIN1 rows, <em>which includes this row</em>, so we need to subtract this row from the sum to get sum of all the <em>previous</em> rows. That way we can test if the sum of all previous rows in BIN1 is less than or equal to the smallest of the sums of BIN2 and BIN3 - if it is, then BIN1 is the bin with the smallest sum so far, so the row will be assigned to BIN1.
</p><p>
If the row was not assigned to BIN1, then BIN1 was not the bin with the smallest sum so far. Then we test for BIN2 if the row would be the first row in BIN2 or if the sum of previous rows in BIN2 is smaller than or equal to the BIN3 sum - if yes, then BIN2 is the the bin with the smallest sum so far, so we assign the row to BIN2.
</p><p>
if the row was not assigned to BIN2, then it must go in BIN3. We do not need to do any define for BIN3, as any classification used in the PATTERN but not defined in DEFINE is automatically true for any row (if the row has not matched any of the other defined classifications.)
</p><p>
As we defined the pattern with the * meaning zero or more occurences, the match will actually continue and include all the rows in a single match (so in a sense we are not really ""searching for a pattern"", as the pattern matches all rows.) But doing that allows the running COUNT and SUM in the DEFINE clauses to keep adding to the running totals all the way to the last row.
</p>
<code>select item_name, item_value, bin#, bin1, bin2, bin3
  from items
match_recognize (
   order by item_value desc
   measures
      to_number(substr(classifier(),4)) bin#,
      sum(bin1.item_value) bin1,
      sum(bin2.item_value) bin2,
      sum(bin3.item_value) bin3
   all rows per match
   pattern (
      (bin1|bin2|bin3)*
   )
   define
      bin1 as count(bin1.*) = 1
           or sum(bin1.item_value) - bin1.item_value <= least(sum(bin2.item_value), sum(bin3.item_value))
    , bin2 as count(bin2.*) = 1
           or sum(bin2.item_value) - bin2.item_value <= sum(bin3.item_value)
);</code>
<p>
With a little bit of simplification, the output gives us very easily the result of which items are distributed in each bin.
</p>
<code>select bin#, item_value, bin_total, item_name
  from items
match_recognize (
   order by item_value desc
   measures
      to_number(substr(classifier(),4)) bin#
    , case classifier()
         when 'BIN1' then final sum(bin1.item_value)
         when 'BIN2' then final sum(bin2.item_value)
         when 'BIN3' then final sum(bin3.item_value)
      end bin_total
   all rows per match
   pattern (
      (bin1|bin2|bin3)*
   )
   define
      bin1 as count(bin1.*) = 1
           or sum(bin1.item_value) - bin1.item_value <= least(sum(bin2.item_value), sum(bin3.item_value))
    , bin2 as count(bin2.*) = 1
           or sum(bin2.item_value) - bin2.item_value <= sum(bin3.item_value)
)
 order by bin#, item_value desc;</code>
<p>
We can observe that the match actually includes all the rows if we change ALL ROWS PER MATCH to ONE ROW PER MATCH. But then we lose information which items are in each bin.
</p>
<code>select *
  from items
match_recognize (
   order by item_value desc
   measures
      sum(bin1.item_value) bin1_total,
      sum(bin2.item_value) bin2_total,
      sum(bin3.item_value) bin3_total
   one row per match
   pattern (
      (bin1|bin2|bin3)*
   )
   define
      bin1 as count(bin1.*) = 1
           or sum(bin1.item_value) - bin1.item_value <= least(sum(bin2.item_value), sum(bin3.item_value))
    , bin2 as count(bin2.*) = 1
           or sum(bin2.item_value) - bin2.item_value <= sum(bin3.item_value)
);</code>
<p>
You now know that not just the patterns may be complex in row pattern matching - the DEFINE clause can also incorporate a logic in them with a higher complexity than merely comparing values. With techniques like this, you can make your row pattern matching solve very complex problems.
</p><p>
In total in these modules you have seen various examples of very different use cases for row pattern matching. It is a quite different way of thinking, but when you get into the habit, it can solve cases where even the use of analytic functions still make for complex queries. It can also be more efficient by being designed for working with multiple rows rather than one row at a time. When you learn the syntax you can write highly declarative analytic queries, where (if you choose good names in the DEFINE clause) you write the data patterns you search for almost in plain readable English.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",15-JUN-17 06.35.57.091399000 AM,"KIBEHA@GMAIL.COM",15-JUN-17 06.35.57.091474000 AM,"KIBEHA@GMAIL.COM"
108139481219833384132005527017562451108,97371311146062087736707699233039984534,"Forecasting",60,"<p>
The basis of this module is a table of monthly sales data for three years for two items, snowchain and sunshade. The snowchain sells well in the winter and hardly anything in the summer with a trend of selling more and more each year. While the the sunshade sells well in the summer with a downward trend less and less each year.
</p>
<code>select *
  from monthly_sales
 order by item, mth;</code>
<p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/lc772gn1x0rgvo9/figure1.PNG""/>
</p><p>
We will be forecasting with a time series model using seasonal adjustment and exponential smoothing very similar to the model explained here: <a href=""http://people.duke.edu/~rnau/411outbd.htm"">http://people.duke.edu/~rnau/411outbd.htm</a> (link given for your perusal later, not necessary to read now to complete the hands-on-lab.)
</p>
First we build a 48 month time series for each item, using an inline view to create 48 months (the 3 years we have sales data for + the 1 year we will be forecasting.) With a partitioned outer join to the sales data, this creates a row source with exactly 48 rows per item numbered 1 to 48 in the TS (Time Serie) column.
</p>
<code>select ms.item, mths.ts, mths.mth, ms.qty
     , extract(year from mths.mth) yr
     , extract(month from mths.mth) mthno
  from (
   select add_months(date '2011-01-01', level-1) mth, level ts --time serie
     from dual
   connect by level <= 48
       ) mths
  left outer join monthly_sales ms
      partition by (ms.item)
      on ms.mth = mths.mth
 order by ms.item, mths.mth;</code>
<p>
We take the previous query and put it in a WITH subquery factoring clause. This will be the method used for the rest of this module - build one step at a time and then tack it onto the chain of WITH clauses for building the next step.
</p><p>
In this step we wish to calculate a socalled <em>centered moving average</em> (CMA). For each month we use analytic AVG to create a moving average with a window one year in length, first we use a window of -5 to +6 months, and then a window of -6 to +5 months. By taking the average of those 2 (add them and divide by 2) we get the <em>centered</em> moving average. But it only makes sense to calculate that CMA for those months where we actually have a years worth of data ""surrounding"" the month, hence we only calculate it for months 7 through 30.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
)
select s1.*
     , case when ts between 7 and 30
          then
             (nvl(avg(qty) over (
                partition by item
                order by ts
                rows between 5 preceding and 6 following
             ),0) + nvl(avg(qty) over (
                partition by item
                order by ts
                rows between 6 preceding and 5 following
             ),0)) / 2
          else
             null
       end cma -- centered moving average
  from s1
 order by item, ts;</code>
<p>
We put the CMA calculation in a second WITH clause and continue with calculating the <em>seasonality</em> of each month. It is a factor that shows how much that particular month is selling compared to the average month and we calculate it by taking the quantity of the month and divide by the CMA. (The model will fail further on if this becomes zero, so we change all zero quantities to a very small quantity instead - that allows the model to work.)
</p><p>
So we've calculated a seasonality for each individual month by the simple division. But there might be small differences in seasonality throughout the years (some winters might be more snow than others), so for a better result we want a seasonality factor that is the average per month, which we achieve by PARTION BY ITEM, MTHNO (where mthno is 1..12). This means that we get an average seasonality for january, an average for february and so on. Because of the way the analytic function with PARTITION clause works, this means that the average january seasonality value is repeated for all the 4 januaries per item - <em>also those in the year we are trying to forecast!</em>
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
)
select s2.*
     , nvl(avg(
          case qty when 0 then 0.0001 else qty end / nullif(cma,0)
       ) over (
          partition by item, mthno
       ),0) s -- seasonality
  from s2
 order by item, ts;</code>
<p>
Having calculated the seasonality, for the 3 years we have sales data we divide the sales with the seasonality factor and thereby get a deseasonalized quantity. The deseasonalization ""smoothes"" the graph of the sales into something approaching a straight line, as we'll see in the next step.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
), s3 as (
   select s2.*
        , nvl(avg(
             case qty when 0 then 0.0001 else qty end / nullif(cma,0)
          ) over (
             partition by item, mthno
          ),0) s -- seasonality
     from s2
)
select s3.*
     , case when ts <= 36 then
          nvl(case qty when 0 then 0.0001 else qty end / nullif(s,0), 0)
       end des -- deseasonalized
  from s3
 order by item, ts;</code>
<p>
Time to use some more analytic functions. The deseasonalized quantity approaches a straight line - with functions REGR_INTERCEPT and REGR_SLOPE we can do a linear regression and get the defining parameters of the straight line that is the best fit to our deseasonalized sales. With the parameters (des,ts) we perform linear regression on a graph with <em>des</em> on the Y axis and <em>ts</em> on the X axis.
</p><p>
REGR_INTERCEPT gives us the point where the line intercepts the Y axis (meaning the Y value for X=0), and REGR_SLOPE gives us the slope (meaning how much Y goes either up or down when X increases by 1.) Since our X axis is the month number 1 to 48, we can get the Y value of the straight line (the <em>trend</em> line) by taking the intercept point and adding the month number multiplied by the slope.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
), s3 as (
   select s2.*
        , nvl(avg(
             case qty when 0 then 0.0001 else qty end / nullif(cma,0)
          ) over (
             partition by item, mthno
          ),0) s -- seasonality
     from s2
), s4 as (
   select s3.*
        , case when ts <= 36 then
             nvl(case qty when 0 then 0.0001 else qty end / nullif(s,0), 0)
          end des -- deseasonalized
     from s3
)
select s4.*
     , regr_intercept(des,ts) over (partition by item)
       + ts*regr_slope(des,ts) over (partition by item) t -- trend
  from s4
 order by item, ts;</code>
<p>
Viewed on a graph I can get a feeling about how perfect the seasonal variations for the item is. The closer the deseasonalized graph is to a straight line, the closer the item is to having the exact same seasonal pattern every year. For snowchain here we can see some ""hick-ups"" which are due to months with very low or no sales, but the hick-ups even out and the general trend follows the trend line nicely.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/qpxe03ri22ytfst/figure2.PNG""/>
</p><p>
So the straight trend line in the graph extends out into the year we want to forecast. This straight line in essence is the forecast of <em>deseasonalized sales</em>, so to get the forecast we want, we simply have to <em>reseasonalize</em> it again by multiplying with the seasonality factor. This is easy, since we saw above that the seasonality factor is available in all the months, also the ones for the forecast year - we simply have to multiply the trend line with the seasonality, and the result is our forecast.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
), s3 as (
   select s2.*
        , nvl(avg(
             case qty when 0 then 0.0001 else qty end / nullif(cma,0)
          ) over (
             partition by item, mthno
          ),0) s -- seasonality
     from s2
), s4 as (
   select s3.*
        , case when ts <= 36 then
             nvl(case qty when 0 then 0.0001 else qty end / nullif(s,0), 0)
          end des -- deseasonalized
     from s3
), s5 as (
   select s4.*
        , regr_intercept(des,ts) over (partition by item)
          + ts*regr_slope(des,ts) over (partition by item) t -- trend
     from s4
)
select s5.*
     , t * s forecast --reseasonalized
  from s5
 order by item, ts;</code>
<p>
Checking the graphs we can see whether the reseasonalized sales match the actual sales fairly well for the three years we have sales data. If they match well, then our model is fairly good and the reseasonalized sales for the forecast year is probably a fairly good forecast. We can also see that the forecast for both items keep the ""shape"" of the graph for the previous years, but the shape just is a bit bigger for snowchain (upward trend) and a bit smaller for sunshade (downward trend.)
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/pf588xuqfavq28n/figure3.PNG""/>
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/3ehuxxh0tq5pmxz/figure4.PNG""/>
</p><p>
The previous query showed all the columns of the calculations along the way. Now let us just keep the columns of interest, and then we can use analytic SUM function partitioned by YR on both the actual quantity sold as well as the reseasonalized (forecast) quantity. This gives us a ""quick-and-dirty"" way to get a feel for how well the model matches the reality.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
), s3 as (
   select s2.*
        , nvl(avg(
             case qty when 0 then 0.0001 else qty end / nullif(cma,0)
          ) over (
             partition by item, mthno
          ),0) s -- seasonality
     from s2
), s4 as (
   select s3.*
        , case when ts <= 36 then
             nvl(case qty when 0 then 0.0001 else qty end / nullif(s,0), 0)
          end des -- deseasonalized
     from s3
), s5 as (
   select s4.*
        , regr_intercept(des,ts) over (partition by item)
          + ts*regr_slope(des,ts) over (partition by item) t -- trend
     from s4
)
select item, mth, qty
     , t * s forecast --reseasonalized
     , sum(qty) over (partition by item, yr) qty_yr
     , sum(t * s) over (partition by item, yr) fc_yr
  from s5
 order by item, ts;</code>
<p>
Having satisfied ourselves that the model matches reality ""good enough"", we can present the data simplified for the end users, so they just get the information about actual sales and forecast sales that they are interested in. Using analytic SUM to give them a yearly sum also helps the users to get a feel for the data.
</p>
<code>with s1 as (
   select ms.item, mths.ts, mths.mth, ms.qty
        , extract(year from mths.mth) yr
        , extract(month from mths.mth) mthno
     from (
      select add_months(date '2011-01-01', level-1) mth, level ts --time serie
        from dual
      connect by level <= 48
          ) mths
     left outer join monthly_sales ms
         partition by (ms.item)
         on ms.mth = mths.mth
), s2 as (
   select s1.*
        , case when ts between 7 and 30
             then
                (nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 5 preceding and 6 following
                ),0) + nvl(avg(qty) over (
                   partition by item
                   order by ts
                   rows between 6 preceding and 5 following
                ),0)) / 2
             else
                null
          end cma -- centered moving average
     from s1
), s3 as (
   select s2.*
        , nvl(avg(
             case qty when 0 then 0.0001 else qty end / nullif(cma,0)
          ) over (
             partition by item, mthno
          ),0) s -- seasonality
     from s2
), s4 as (
   select s3.*
        , case when ts <= 36 then
             nvl(case qty when 0 then 0.0001 else qty end / nullif(s,0), 0)
          end des -- deseasonalized
     from s3
), s5 as (
   select s4.*
        , regr_intercept(des,ts) over (partition by item)
          + ts*regr_slope(des,ts) over (partition by item) t -- trend
     from s4
)
select item
     , mth
     , case
          when ts <= 36 then qty
          else round(t * s)
       end qty
     , case
          when ts <= 36 then 'Actual'
          else 'Forecast'
       end type
     , sum(
          case
             when ts <= 36 then qty
             else round(t * s)
          end
       ) over (
          partition by item, yr
       ) qty_yr
  from s5
 order by item, ts;</code>
<p>
And that final query can then be represented in an easily readable graph.
</p><p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/sp9u2cuv00i5b3i/figure5.PNG""/>
</p><p>
You have in this final module seen a complex calculation broken down into small individually simpler calculations with a series of WITH clauses. Doing analytic statement development step for step helps a lot in modularization of a large statement, so each part becomes relatively simple to calculate. Even the more advanced analytic functions like the linear regression functions are not so difficult when used as a single simple step in the more complex whole. That way it becomes easier to keep track of what's happening in each step.
</p><p>
In the 6 modules of this Hands-On-Lab, methods of using analytic functions have been covered - from the simple to the more complex. Many analytic functions exist that have not been covered, but once you master the three analytic clauses (partition, order and windowing clauses) and get used to the way of thinking (that data from other rows - few or many - can be part of calculations on <em>this</em> row), you will start to use analytic functions more in your daily life as a developer. Just start using them - the more you do, the more you'll recognize more and more opportunities for using them in your code, thus making your applications more efficient and faster and your users (and boss) happier.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",07-JUN-17 07.53.45.264171000 AM,"KIBEHA@GMAIL.COM",11-JUN-17 09.06.51.331263000 AM,"KIBEHA@GMAIL.COM"
117123875509310626938698188922393619580,117123875509271941312470520788803021948,"Creating a Table",10,"<p>To create a table, you need to define three things:</p>

<ul><li>Its name</li>
<li>Its columns</li>
<li>The data types of these columns</li></ul>

<p>The basic syntax to create a table is:</p>

<pre>create table &lt;table_name&gt; (
  &lt;column1_name&gt; &lt;data_type&gt;,
  &lt;column2_name&gt; &lt;data_type&gt;,
  &lt;column3_name&gt; &lt;data_type&gt;,
  ...
)</pre>

<p>For example, to create a table to store the names and weights of toys, run the following statement:</p>

<code>create table toys (
  toy_name varchar2(100),
  weight   number
);</code>",01-SEP-17 08.19.01.326369000 AM,"CHRIS.SAXON@ORACLE.COM",25-SEP-18 07.21.45.164111000 AM,"CHRIS.SAXON@ORACLE.COM"
103566264885718078585364627630794116052,103563632879277014483527974609264228592,"Building a summary report",130,"<h3>Summarizing our results</h3>
<p>What we really want is a report that shows one line for each session, shows the number of events, the start time and the session duration.</p>
<p>To return a summary report we only need to change one line of code!</p>
<p>We just need to change the output type from <strong>ALL ROWS PER MATCH</strong> to <strong>ONE ROW PER MATCH</strong> and remove the reference to <strong>tstamp</strong> within the SELECT clause.</p>
<code>SELECT
 userid,
 session_id,
 no_of_events,
 start_time,
 end_time,
 session_duration
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j) 
MATCH_RECOGNIZE(
   PARTITION BY userid ORDER BY time_id
   MEASURES match_number() as session_id,
            COUNT(*) as no_of_events,
            FIRST(b.time_id) start_time,
            LAST(s.time_id) end_time,
            LAST(s.time_id) - FIRST(b.time_id) session_duration    
   ONE ROW PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (time_id - PREV(time_id) <= 10)
 );
</code>
<p>...and there is our finished sessionization report.</p>",24-APR-17 01.05.14.625859000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.53.09.634728000 PM,"SHARON.KENNEDY@ORACLE.COM"
108969316191388214960932956957764654328,108572075684787330976836028944125402228,"Merge DATE ranges",30,"<p>
The basis of this module is a table defining employee hire periods with a START_DATE and an END_DATE and TITLE of the job they had in that period. Sometimes they have more than one job in a period. NULL as END_DATE indicates the period is still on-going and currently valid.
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , title
from emp_hire_periods
order by emp_id, start_date;</code>
<p>
The table is defined using <b>temporal validity</b> with the PERIOD FOR clause. You can see the DDL here (no need to execute it, the setup code has done that):
<pre>create table emp_hire_periods (
   emp_id         integer           not null
 , name           varchar2(20 char) not null
 , start_date     date              not null
 , end_date       date
 , title          varchar2(20 char) not null
 , constraint emp_hire_periods_pk primary key (emp_id, start_date)
 , period for employed_in (start_date, end_date)
);</pre><p>
When using temporal validity, the valid period of the row goes from and <i>including</i> START_DATE, but to and <i>excluding</i> END_DATE. This is called a <i>half-open interval</i> (as opposed to closed or open intervals) and is very practical to use for date ranges.
</p><p>
As a side note, having temporal validity enabled on the table allows us to use AS OF PERIOD FOR in a query to view hire periods data <i>as of</i> a specific date. For example there were not so many employees in 2010:
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , title
from emp_hire_periods as of period for employed_in date '2010-07-01'
order by emp_id, start_date;</code>
<p>
In 2016 there were more employees and some of the existing had new job titles:
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , title
from emp_hire_periods as of period for employed_in date '2016-07-01'
order by emp_id, start_date;</code>
<p>
What we want to output is a list where we have merged adjoining or overlapping date ranges, so that we can see for example that Mogens Juel has been employed from 2010-07-01 until current time (NULL as END_DATE) and in that period has had 4 jobs.
</p><p>
First we can attempt to do similar to how we found consecutive numbers. We can define ADJOIN_OR_OVERLAP as rows where the START_DATE is smaller than or equal to the END_DATE of the <i>previous</i> row, where we order the rows by START_DATE, END_DATE. And then we look for a pattern of any row followed by zero or more adjoining or overlapping rows.
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , jobs
from emp_hire_periods
match_recognize (
   partition by emp_id
   order by start_date, end_date
   measures
      max(name)         as name
    , first(start_date) as start_date
    , last(end_date)    as end_date
    , count(*)          as jobs
   pattern (
      strt adjoin_or_overlap*
   )
   define
      adjoin_or_overlap as
         start_date <= prev(end_date)
)
order by emp_id, start_date;</code>
<p>
It does merge <i>some</i> of the date ranges when you check the output, but not all. For example Mogens Juel is not completely merged, there should have been a single row only for him with 4 jobs. The problem is that when we order his rows by start_date, the ""Code Tester"" and ""IT Manager"" rows are compared and <i>not</i> found overlapping. A comparison like this to the previous row fails to discover that <i>both</i> rows are adjoining or overlapping to ""Sys Admin"".
</p><p>
Would it help to order by END_DATE first instead of START_DATE? Let's try it:
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , jobs
from emp_hire_periods
match_recognize (
   partition by emp_id
   order by end_date, start_date
   measures
      max(name)         as name
    , first(start_date) as start_date
    , last(end_date)    as end_date
    , count(*)          as jobs
   pattern (
      strt adjoin_or_overlap*
   )
   define
      adjoin_or_overlap as
         start_date <= prev(end_date)
)
order by emp_id, start_date;</code>
<p>
The output has changed, but Mogens Juel still wrongly is shown twice. With the changed ordering, the first attempt at finding a match for Mogens Juel will try to compare the ""IT Technician"" row with the ""Code Tester"" row and fail to find an overlap.
</p><p>
No matter which ordering we choose, we <i>cannot</i> get <i>all</i> the overlaps in a single match by simply comparing a row to the previous row. We need a different way to handle this.
</p><p>
Take a closer look at the rows of Mogens Juel:
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , title
from emp_hire_periods
where emp_id = 143
order by emp_id, start_date;</code>
<p>
A better approach will be to compare the START_DATE of a row with the <i>highest END_DATE that we have found so far</i> in the match.
</p><p>
We could attempt to do this with a definition that says START_DATE <= MAX(END_DATE), but this will not work. MAX(END_DATE) would <i>not</i> be the highest END_DATE of all the previous rows, it would be the highest of <i>all the previous rows plus the current row</i>. If this had been analytic functions, we would have liked a window of ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING, but we are getting ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. But we cannot specify windows like that in our definition.
</p><p>
Also if we <i>try</i> such a definition, we will at minimum get an error and possibly crash our session. Depending on client and DB version, we might get ""ORA-03113: end-of-file on communication channel"" or ""java.lang.NullPointerException"". Here on LiveSQL you will get a generic ""Error"" after hanging some time and your session will be unusable afterwards - you will need to close your browser and start over logging in again in a new session.
</p><p>
So <b>do not call this statement</b>, it just illustrates a point:
</p>
<pre>/*
select
   emp_id
 , name
 , start_date
 , end_date
 , jobs
from emp_hire_periods
match_recognize (
   partition by emp_id
   order by start_date, end_date
   measures
      max(name)         as name
    , first(start_date) as start_date
    , max(end_date)     as end_date
    , count(*)          as jobs
   pattern (
      strt adjoin_or_overlap*
   )
   define
      adjoin_or_overlap as
         start_date <= max(end_date)
)
order by emp_id, start_date;
*/</pre><p>
Instead we need to reverse the logic. Rather than trying to compare this row to the max of all previous rows (which can't work), we compare the max of all previous rows + current row with the value of the <i>next</i> row:
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , jobs
from emp_hire_periods
match_recognize (
   partition by emp_id
   order by start_date, end_date
   measures
      max(name)         as name
    , first(start_date) as start_date
    , max(end_date)     as end_date
    , count(*)          as jobs
   pattern (
      adjoin_or_overlap* last_row
   )
   define
      adjoin_or_overlap as
         next(start_date) <= max(end_date)
)
order by emp_id, start_date;</code>
<p>
Explanation of the pieces of that query:
</p>
<ul>
<li>We go back to ordering by START_DATE.</li>
<li>The definition checks if START_DATE of the <i>next</i> row is less than or equal to the highest END_DATE seen so far in the match â€“ <i>including the current row</i>, because the MAX call will assume the current row is part of the match when it is evaluated. That means that when a row is classified as ADJOIN_OR_OVERLAP, that row <i>should</i> be merged with the next row.</li>
<li>The pattern looks for zero or more ADJOIN_OR_OVERLAP rows followed by one single row classified LAST_ROW. As that classification is undefined, any row can match it - but since the row <i>before</i> LAST_ROW was classified ADJOIN_OR_OVERLAP, we <i>know</i> that the LAST_ROW should be merged too.</li>
<li>If I find <i>no</i> ADJOIN_OR_OVERLAP rows, the row will become classified LAST_ROW because of the * in the pattern that says that zero ADJOIN_OR_OVERLAP rows are acceptable in the pattern. This means that when a row is <i>not</i> overlapping with any other rows, it will become a match of a single row classified as LAST_ROW and thus <i>unmerged</i> be part of the output.</li>
<li>The measure END_DATE is calculated as the largest END_DATE of the match. Because we are <i>not</i> qualifying the END_DATE in the MAX call with either ADJOIN_OR_OVERLAP or LAST_ROW, MAX here is applied to <i>all rows of the match</i> no matter what classification the rows got.</li>
</ul>
<p>
The output of the query above is much improved, but still not quite correct.
</p><p>
Firstly several of the employees (including Mogens Juel) have a wrong value in the measure END_DATE. Those that are still employed should have NULL in the END_DATE column, and in this output that is only true for those with just a single hire period. For those that have had more than one job, the <i>highest non-null</i> END_DATE is wrongly displayed.
</p><p>
Secondly notice that Zoe Thorston also has overlapping rows â€“ the problem here is just that the END_DATE of <i>both</i> rows are NULL, meaning both rows are current and she has both job functions. With the NULL values, the simple comparison to MAX(END_DATE) will not be true.
</p><p>
Both of these problems are because we are not handling the NULL values in END_DATE. This we will do now.
</p><p>
In this table we have set START_DATE to NOT NULL, but for the sake of being complete, let us <i>pretend</i> it could include NULL.
</p><ul>
<li>So NULL in START_DATE (if we had allowed it) means the employee was employed since the Big Bang at the beginning of time.</li>
<li>And NULL in END_DATE means the employee is employed until the end of time (or at least until someone updates the column with a non-null value ;-).</li>
</ul><p>
We cannot simply use IS NULL logic, as we want MAX to consider a NULL to be higher than any date. So instead we just change the definition to use NVL on both START_DATE and END_DATE, turning NULL in START_DATE into -4712-01-01 (smallest possible DATE in Oracle) and NULL in END_DATE into 9999-12-31 (highest possible DATE in Oracle).
</p><p>
Similarly in the calculation of the END_DATE measure, we use NVL in the MAX call to turn NULL in END_DATE into 9999-12-31, and then if the MAX <i>does</i> return 9999-12-31, we use NULLIF to turn that back into a NULL again.
</p>
<code>select
   emp_id
 , name
 , start_date
 , end_date
 , jobs
from emp_hire_periods
match_recognize (
   partition by emp_id
   order by start_date nulls first, end_date nulls last
   measures
      max(name)         as name
    , first(start_date) as start_date
    , nullif(
         max(nvl(end_date, date '9999-12-31'))
       , date '9999-12-31'
      )                 as end_date
    , count(*)          as jobs
   pattern (
      adjoin_or_overlap* last_row
   )
   define
      adjoin_or_overlap as
         nvl(next(start_date), date '-4712-01-01')
            <= max(nvl(end_date, date '9999-12-31'))
)
order by emp_id, start_date;</code>
<p>
This is a somewhat tricky MATCH_RECOGNIZE clause to understand, but it ends up giving us the desired merged date ranges.
<p></p>
You have seen that date comparison between rows in a pattern classification definition can ""match up"" periods for merging. Unlike how we grouped sequential numbers, this compares not simply to previous row, but to largest value of previous rows. This required us to reverse the logic and look ahead with NEXT.
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",15-JUN-17 06.33.19.819563000 AM,"KIBEHA@GMAIL.COM",25-JUN-21 09.44.24.397941000 AM,"KIBEHA@GMAIL.COM"
108969316191394259590031030103638185208,108572075684787330976836028944125402228,"Tablespace growth spurts",40,"<p>
The basis of this module is a table that every midnight samples the used space in each tablespace in the database. The dates are continuous (without gaps.)
</p>
<code>select tabspace, sampledate, gigabytes
  from space
 order by tabspace, sampledate;</code>
<p>
We want to discover where there has been ""spurts"" of growth in a tablespace, but spurts can be two different things - either a ""fast"" spurt where a tablespace grew at least 25% in a single day, or a ""slow"" spurt where the tablespace grew between 10% and 25% every day for at least 3 days.
</p><p>
As the samples are at midnight, if we compare the value of the current row with the value of the next row, we get the growth for the current day. This is easily accomplished in the DEFINE clauses by using the NEXT function to read the gigabyte values of the next row (= next date). So we create two classifiers, FAST and SLOW, each with a formula that fits the growth percentage we want to search for. Then in the PATTERN clause we use the operator | which means OR, so that we search for a pattern of <em>either</em> one or more FAST rows <em>or</em> three or more consecutive SLOW rows.
</p>
<code>select tabspace, spurttype, match_no, sampledate, gigabytes
     , end_of_day_gb, end_of_day_gb - gigabytes as growth
     , round(100 * (end_of_day_gb - gigabytes) / gigabytes, 1) as pct
  from space
match_recognize (
   partition by tabspace
   order by sampledate
   measures
      classifier() as spurttype
    , match_number() as match_no
    , next(gigabytes) as end_of_day_gb
   all rows per match
   after match skip past last row
   pattern ( fast+ | slow{3,} )
   define
      fast as next(fast.gigabytes) / fast.gigabytes >= 1.25
    , slow as next(slow.gigabytes) / slow.gigabytes >= 1.10 and
              next(slow.gigabytes) / slow.gigabytes <  1.25
)
 order by tabspace, sampledate;</code>
<p>
Having observed with ALL ROWS PER MATCH that the pattern gives us what we want, we can now transform it to ONE ROW PER MATCH to get a simpler output for our growth spurt report.
</p><p>
Note in particular here, that in ONE ROW PER MATCH output, aggregates and PREV/NEXT functions get their values from the <em>last</em> row of the match. When we use ALL ROWS PER MATCH we could get the same value for DAYS by using FINAL COUNT(*), since FINAL keyword specifies to get the last (final) value. But if we tried FINAL NEXT(gigabytes) we would have gotten an error, as the combination of FINAL and NEXT is for some reason not allowed. However, here in a ONE ROW PER MATCH output, we can achieve the result <em>as if</em> we had used FINAL NEXT, since the NEXT is from the last row and therefore in effect FINAL (just without FINAL keyword.)
</p>
<code>select tabspace, spurttype, startdate, startgb, enddate, endgb, days
     , (endgb - startgb) / days as avg_growth
     , round(100 * ((endgb - startgb) / days) / startgb, 1) as avg_pct
  from space
match_recognize (
   partition by tabspace
   order by sampledate
   measures
      classifier() as spurttype
    , first(sampledate) as startdate
    , first(gigabytes) as startgb
    , last(sampledate) as enddate
    , next(gigabytes) as endgb
    , count(*) as days
   one row per match
   after match skip past last row
   pattern ( fast+ | slow{3,} )
   define
      fast as next(fast.gigabytes) / fast.gigabytes >= 1.25
    , slow as next(slow.gigabytes) / slow.gigabytes >= 1.10 and
              next(slow.gigabytes) / slow.gigabytes <  1.25
)
 order by tabspace, startdate;</code>
<p>
In the previous query we calculated the AVG_GROWTH and AVG_PCT columns in the SELECT list. But it is also allowed to put complex expressions in the MEASURES clause, so we could just as well place the calculations in the measures like this.
</p>
<code>select tabspace, spurttype, startdate, startgb, enddate, endgb, days
     , avg_growth, avg_pct
  from space
match_recognize (
   partition by tabspace
   order by sampledate
   measures
      classifier() as spurttype
    , first(sampledate) as startdate
    , first(gigabytes) as startgb
    , last(sampledate) as enddate
    , next(gigabytes) as endgb
    , count(*) as days
    , (next(gigabytes) - first(gigabytes)) / count(*) as avg_growth
    , round(100 * ((next(gigabytes) - first(gigabytes)) / count(*)) / first(gigabytes), 1) as avg_pct
   one row per match
   after match skip past last row
   pattern ( fast+ | slow{3,} )
   define
      fast as next(gigabytes) / gigabytes >= 1.25
    , slow as next(slow.gigabytes) / slow.gigabytes >= 1.10 and
              next(slow.gigabytes) / slow.gigabytes <  1.25
)
 order by tabspace, startdate;</code>
<p>
You have now seen how to use multiple classifiers and | (OR) in the expressions to search for multiple row patterns simultaneously, if what you are searching is multiple conditions.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",15-JUN-17 06.33.57.620755000 AM,"KIBEHA@GMAIL.COM",25-JUN-17 01.13.02.047056000 AM,"KIBEHA@GMAIL.COM"
108969316191409975625686020282909365496,108572075684787330976836028944125402228,"Bin fitting with limited kapacity",60,"<p>
The basis of this module is a table with results of scientific studies of ""sites"" (like genomes or similar). Our task is assign the study sites to groups in such a manner, that the sum of the CNT column does not exceed 65000 - but the sites in each group has to be consecutive. In other words we need to perform a rolling sum of CNT in order of STUDY_SITE, but stop the rolling sum at the row before it exceeds 65000. Let us try a simple analytic rolling sum.
</p>
<code>select study_site, cnt
     , sum(cnt) over (
          order by study_site
          rows between unbounded preceding and current row
       ) as rolling_sum
  from sites
 order by study_site;</code>
<p>
We can spot that at site 1023 the rolling sum is 73946, which is too much, so our first group (bin) will have to be from site 1001 to site 1022, and then the rolling sum will have to be restarted from site 1023 so we can begin ""filling a new bin"".
</p><p>
The analytic sum cannot be ""restarted"" like that, but pattern matching can do it. Here we make a definition of classifier A that a row is classified A as long as the sum of all the rows in the match so far (<em>including</em> the current row) is less than or equal to 65000. The pattern simply states that a match is one or more consecutive rows classified A. So the rolling sum will continue up to site 1022, then in site 1023 the row will no longer be classified as an A row, so the match stops. With AFTER MATCH SKIP PAST LAST ROW, a new match is then started at the next row after the last row of the previous match, so the new match starts at site 1023. With ALL ROWS PER MATCH we can see how the rolling sum restarts whenever needed, so the MATCH_NO identifies which ""bin"" we group the rows in.
</p>
<code>select study_site, cnt, rolling_sum, match_no
  from sites
match_recognize (
   order by study_site
   measures
      sum(cnt) rolling_sum
    , match_number() match_no
   all rows per match
   after match skip past last row
   pattern ( a+ )
   define a as sum(cnt) <= 65000
)
 order by study_site;</code>
<p>
Of course we could then group by MATCH_NO to get the set of ""bins"" and which sites go where, but we can easily get the same result by changing the query to use ONE ROW PER MATCH and then use aggregates in the MEASURES clause.
</p>
<code>select first_site, last_site, cnt_sum
  from sites
match_recognize (
   order by study_site
   measures
      first(study_site) first_site
    , last(study_site) last_site
    , sum(cnt) cnt_sum
   one row per match
   after match skip past last row
   pattern ( a+ )
   define a as sum(cnt) <= 65000
)
 order by first_site;</code>
<p>
In the previous modules, formulas in DEFINE have accessed the current row, the previous row, following row, first row, or some combination thereof. In this module you have now learned that the DEFINE clause can contain aggregates too, just like MEASURES, enabling you to define a row classification based on data from a whole set of rows.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",15-JUN-17 06.35.09.159124000 AM,"KIBEHA@GMAIL.COM",15-JUN-17 06.35.09.159207000 AM,"KIBEHA@GMAIL.COM"
151905700197617671236850503090237646910,151905700197614044459391659202713528382,"Introduction",10,"<p>This tutorial shows you how to summarize your data using aggregate functions and group by. It uses the following table as its source:</p>

<code>select * from bricks;</code>",31-JUL-18 08.48.32.384409000 AM,"CHRIS.SAXON@ORACLE.COM",31-JUL-18 08.56.21.073042000 AM,"CHRIS.SAXON@ORACLE.COM"
117123875509657588648927587495534292092,117123875509271941312470520788803021948,"External Tables",30,"<p>You use external tables to read non-database files on the database server. For example, comma-separated values (CSV) files. To do this, you need to:</p>
<ul>
<li>Create a directory pointing to the location of the file on the server</li>
<li>Use the organization external clause</li>
<li>State the directory and name of the file you want to read</li>
</ul>

<p>For example:</p>

<code>create or replace directory toy_dir as '/path/to/file';

create table toys_ext (
  toy_name varchar2(100)
) organization external (
  default directory tmp
  location ('toys.csv')
);</code>

<p><i><strong>Note:</strong> LiveSQL doesn't support external tables. So these statements will fail!</i></p>

<p>When you query this table, it will read from the file:</>

<pre>/path/to/file/toys.csv</pre>

<p>This file must be accessible to the database server. You cannot use external tables to read files on your machine!</p>",01-SEP-17 08.32.41.440260000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 07.42.19.360177000 AM,"CHRIS.SAXON@ORACLE.COM"
117463246204537119846605169766032886335,117463246204528657365867867361809943103,"Defining Columns",10,"<p>A table in Oracle Database can have up to 1,000 columns. You define these when you create a table. You can also add them to existing tables. </p>

<p>Every column has a data type. The data type determines the values you can store in the column and the operations you can do on it. The following statement creates a table with three columns. One varchar2, one number, and one date:</p>

<code>create table this_table_has_three_columns (
  this_is_a_character_column varchar2(100),
  this_is_a_number_column    number,
  this_is_a_date_column      date
);</code>",04-SEP-17 02.42.49.657847000 PM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 08.12.31.612268000 AM,"CHRIS.SAXON@ORACLE.COM"
117465572170966954644044556117853473401,117463246204528657365867867361809943103,"Viewing Column Information",20,"<p>You can find details about the columns in your user's tables by querying user_tab_columns. </p>
<p>This query finds the name, type, and limits of the columns in this schema:</p>

<code>select table_name, column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns;</code>",04-SEP-17 02.44.46.712263000 PM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 08.13.25.443436000 AM,"CHRIS.SAXON@ORACLE.COM"
117462377369435004023961807400607589627,117463246204528657365867867361809943103,"Adding Columns to Existing Tables",85,"<p>You add columns to an existing table with alter table. You can add as many as you want (up to the 1,000 column table limit):</p>

<p>The following adds two columns to the table this_table_has_three_columns. One timestamp and one blob:</p>

<code>alter table this_table_has_three_columns add (
  this_is_a_timestamp_column    timestamp, 
  this_is_a_binary_large_object blob
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'THIS_TABLE_HAS_THREE_COLUMNS';</code>",04-SEP-17 02.45.49.383910000 PM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 08.36.42.845138000 AM,"CHRIS.SAXON@ORACLE.COM"
117462377369634476784198221214434108667,117463246204528657365867867361809943103,"Character Data Types",50,"<p>Oracle Database has three key character types:</p>

<ul>
<li>varchar2</li>
<li>char</li>
<li>clob</li>
</ul>

<p>You use these to store general purpose text.</p>

<h3>Varchar2</h3>

<p>This stores variable length text. You need to specify an upper limit for the size of these strings. In Oracle Database 11.2 and before, the maximum you can specify is 4,000 bytes. From 12.1 you can increase this length to 32,767.</p>

<h3>Char</h3>

<p>These store fixed-length strings. If the text you insert is shorter than the max length for the column, the database right pads it with spaces.</p>

<p>The maximum size of char is 2,000 bytes.</p>

<p>Only use char if you need fixed-width data. In the vast majority of cases, you should use varchar2 for short strings.</p>

<h3>Clob</h3>

<p>If you need to store text larger than the upper limit of a varchar2, use a clob. This is a character large object. It can store data up to (4 gigabytes - 1) * (database block size). In a default Oracle Database installation this is 32Tb!</p>

<p>The following statement creates a table with various character columns:</p>

<code>create table character_data (
  varchar_10_col   varchar2(10),
  varchar_4000_col varchar2(4000),
  char_10_col      char(10),
  clob_col         clob
);

select column_name, data_type, data_length
from   user_tab_columns
where  table_name = 'CHARACTER_DATA';</code>

<p>Each of these types also has an N variation; nchar, nvarchar2, and nclob. These store Unicode-only data. It's rare you'll use these data types.</p>",04-SEP-17 03.00.50.017760000 PM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 09.08.32.543629000 AM,"CHRIS.SAXON@ORACLE.COM"
108578384087149817318250136163437936681,108572075684787330976836028944125402228,"Stock Ticker (The Basics)",10,"<p>
The basis of this module is a ticker table storing stock prices per day, in this case for the imaginary stock PLCH (PL/SQL Challenge).
</p>
<code>select *
  from ticker
 order by symbol, day;</code>
<p>
<img style=""max-width:80%; height:auto;"" src=""https://dl.dropboxusercontent.com/s/hbjk2q0czme9zbt/ticker.PNG""/>
</p><p>
A classic example of row pattern matching is to search to V shapes in the graph. Here is an example of that taken from the Data Warehousing Guide manual. Just try to execute it, then afterwards we'll go through the parts of it.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , final last(down.day) as bottom_day
    , final last(up.day) as end_day
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip to last up
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up   as up.price > prev(up.price)
)
 order by symbol, match_num, day;</code>
<p>
So we'll try different statements each showing parts of the statement above until we end with the same statement.
</p><p>
At first in the next statement we'll try to search for the ""down"" part of the V shape first, using the different parts of the MATCH_RECOGNIZE clause:
<ul><li>
PARTITION BY works like in analytic functions, it splits the data into partitions where the pattern matching is executed independently in each partition. Here we do it for each ticker symbol (our data only has one symbol, but this way the statement works even with multiple symbols.)
</li><li>
ORDER BY is a cornerstone in pattern matching, you cannot recognize a pattern in the data if they are not ordered.
</li><li>
DEFINE is a method of classifying the rows depending on conditions. If the condition right side of AS is true, the row will be classified with the classifier variable name left side of AS. There can be multiple classification conditions, which will be tested one by one in order, until the row satisfies one of the conditions. In the conditions can be used various navigational functions like PREV, NEXT, FIRST, LAST that return values from other rows. Using PREV here we test for whether a row has a price less than the price of the previous row - if yes, then the row is classified as a DOWN row.
</li><li>
Having classified the rows using DEFINE, we can then use the classification names we have defined in the PATTERN clause to state what sequence of row classifications we are looking for. Here we will look for <b>STRT DOWN+</b>, which is a syntax similar to regular expressions, in that we state we look for exactly 1 row classified STRT followed by <em>1 or more</em> rows classified DOWN. But we have not created a STRT classification definition in the DEFINE clause? We could have done it with something like ""strt as 1=1"", but that is not necessary, because an ""undefined"" classifier simply becomes the default classifier that a row gets if it does not satisfy any of the classifications of the DEFINE clause.
</li><li>
When the next row no longer fullfills the pattern, the <em>match</em> is completed and the pattern searching can start over to try and find if there are more matches of the pattern. With the AFTER MATCH clause we tell it where to start the next search for a match. In this case we use the default SKIP PAST LAST ROW, which will start the next search beginning at the row following the final DOWN row of the previous match.
</li><li>
Each match we find, we can choose between either getting all the rows or just a single aggregate row. Here we choose ALL ROWS PER MATCH.
</li><li>
And finally we specify what output we want in the MEASURES clause, which can contain row column values, aggregates, navigation functions, or special functions like MATCH_NUMBER() and CLASSIFIER(). With a SELECT * we get first the columns in PARTITION BY and ORDER BY, then the columns of the MEASURES, and finally the ""rest"" of the table columns (those not used in PARTITION or ORDER BY.)
</li></ul>
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , running last(down.day) as down_day
    , final last(down.day) as bottom_day
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip past last row
   pattern ( strt down+ )
   define
      down as down.price < prev(down.price)
)
 order by symbol, match_num, day;</code>
<p>
We can do the exact same thing defining UP to search for ""up"" parts of the V shape. Just like in the statement above, we can observe the difference between RUNNING and FINAL keyword in front of the navigational functions - in this case LAST. RUNNING (which is the default if nothing is specified) evaluates on the row itself, while FINAL evaluates on the last row of the match.
</p><p>
MATCH_NUMBER and CLASSIFIER are very useful to add to your query while you are developing it and testing out your pattern. When you go to production it may often not be necessary to include these functions, but they are very instructive to observe how the pattern matching works.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , running last(up.day) as up_day
    , final last(up.day) as top_day
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip past last row
   pattern ( strt up+ )
   define
      up as up.price > prev(up.price)
)
 order by symbol, match_num, day;</code>
<p>
Just like we can use RUNNING and FINAL on the navigational functions (used on LAST in the previous statement), we can also use them on aggregate/analytic functions. Here we use a RUNNING and FINAL COUNT, where RUNNING works like analytic ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW and FINAL works like ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      final count(up.*) as days_up
    , up.price - prev(up.price) as up_day
    , (up.price - strt.price) / running count(up.*) as running_up_avg
    , (final last(up.price) - strt.price) / final count(up.*) as total_up_avg
    , up.price - strt.price as up_total
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip to last up
   pattern ( strt up+ )
   define
      up as up.price > prev(up.price)
)
 order by symbol, match_num, day;</code>
<p>
So we can combine the search for downs and the search for ups in a single statement, where we have two DEFINE classifications and use both in the PATTERN clause, so we specify we want any one row, followed by at least one DOWN row, followed by at least one UP row - thereby finding V shapes in the graph.
</p><p>
We use the FINAL LAST twice, since by specifying <em>down.day</em> respectively <em>up.day</em>, we get the last DAY value of those rows classified DOWN respectively UP. If we had only used <em>last(day)</em> without specifying any classification variably in front, we would get the DAY value of the last row in the match, whichever classification that row might have. In this case the last row of the match will always be an UP row, but that is not always the case. You can try running this query as is and then edit and change <em>last(down.day)</em> to <em>last(day)</em> and see what happens.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , final last(down.day) as bottom_day
    , final last(up.day) as end_day
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip past last row
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up as up.price > prev(up.price)
)
 order by symbol, match_num, day;</code>
<p>
But the above query is not quite identical to the example from the manual given at the top of the module, and it does also not quite give the same result. The query from the manual does not use AFTER MATCH SKIP PAST LAST ROW, it uses AFTER MATCH SKIP TO LAST UP. The difference is that when a match has been found, it will begin searching for a new match from the last row of the old match, not the row that <em>follows</em> the last row. For our V shape search, this is important, as after the last UP row can very easily be a DOWN row.
</p><p>
So here we'll repeat the manual query with the change to SKIP TO LAST UP, so you can observe the difference between the output of the previous query and this one, where you'll notice two rows with 2011-04-10, one as last UP row in MATCH_NUM 1, then again as STRT row in MATCH_NUM 2. And this is perfectly OK that a row can become part of more than one match, and hence be output more than once.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , final last(down.day) as bottom_day
    , final last(up.day) as end_day
    , match_number() as match_num
    , classifier() as var_match
   all rows per match
   after match skip to last up -- Changed here
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up as up.price > prev(up.price)
)
 order by symbol, match_num, day;</code>
<p>
All the examples so far have used ALL ROWS PER MATCH. But we can also specify ONE ROW PER MATCH, which in effect is somewhat like changing from using analytic functions (ALL ROWS PER MATCH) to aggregating with GROUP BY (ONE ROW PER MATCH.) So we can get an aggregated row for each match like this.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , strt.price as start_price
    , final last(down.day) as bottom_day
    , final last(down.price) as bottom_price
    , final last(up.day) as end_day
    , final last(up.price) as end_price
    , match_number() as match_num
   one row per match
   after match skip to last up
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up as up.price > prev(up.price)
)
 order by symbol, match_num;</code>
<p>
The use of FINAL can be nice as sort of ""self-documenting"", but when we use ONE ROW PER MATCH it does not matter that much, as RUNNING in effect gives us the same result. Date in measures are evaluated on the last row when we use ONE ROW PER MATCH, so using RUNNING will be evaluated on the last row and hence give the same result as FINAL.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , strt.price as start_price
    , running last(down.day) as bottom_day
    , running last(down.price) as bottom_price
    , running last(up.day) as end_day
    , running last(up.price) as end_price
    , match_number() as match_num
   one row per match
   after match skip to last up
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up as up.price > prev(up.price)
)
 order by symbol, match_num;</code>
<p>
But in these examples we can even simplify it further when we use ONE ROW PER MATCH, since when it evaluates on the last row, we don't even need to use LAST navigational function. We can get the same result by simply using the classification variables.
</p>
<code>select *
  from ticker
match_recognize (
   partition by symbol
   order by day
   measures
      strt.day as start_day
    , strt.price as start_price
    , down.day as bottom_day
    , down.price as bottom_price
    , up.day as end_day
    , up.price as end_price
    , match_number() as match_num
   one row per match
   after match skip to last up
   pattern ( strt down+ up+ )
   define
      down as down.price < prev(down.price)
    , up as up.price > prev(up.price)
)
 order by symbol, match_num;</code>
<p>
However, using FINAL LAST can be nice as self-documenting code, plus it has the advantage of making it quicker to switch between ONE ROW PER MATCH and ALL ROWS PER MATCH, which often is needed as a debugging device for finding out if you have problems with your pattern definitions.
</p>
<p>
You have now seen the basic elements of the MATCH_RECOGNIZE clause, exemplified with stock ticker data. The next modules will give various use-cases, some where the thinking of ""patterns"" make sense, some where it is more obscure how it is a ""pattern"" but where the MATCH_RECOGNIZE clause is a very efficient solution.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",11-JUN-17 12.36.21.240876000 PM,"KIBEHA@GMAIL.COM",25-JUN-17 01.11.33.390891000 AM,"KIBEHA@GMAIL.COM"
108578384087182458315379731151155003433,108572075684787330976836028944125402228,"Grouping Sequences",20,"<p>
The basis of this module is a simple table of integers.
</p>
<code>select *
  from numbers
 order by numval;</code>
<p>
Our task is to group all consecutive numbers (sequences). Whenever there is a gap, a new group should be started.
</p><p>
To start with, in the MATCH_RECOGNIZE we order by NUMVAL, so we can define a row classifier B with a condition that the value should be precisely 1 larger than the previous value.
</p><p>
The pattern A B* then states, that we start with any row, then must come zero or more occurences of a row classified as B. So the first row will be A, then the match will go on matching B rows until we reach a ""gap"" where the row no longer will be classified B. At that point the match ends, and the following row (which will be the first in the new group) will be an A row and starts a new match. We can observe the behaviour with ALL ROWS PER MATCH.
</p>
<code>select *
  from numbers
match_recognize (
   order by numval
   measures
      match_number() match_no
    , classifier() class
    , a.numval a_numval
    , b.numval b_numval
    , first(numval) firstval
    , final last(numval) lastval
    , final count(*) cnt
   all rows per match
   pattern ( a b* )
   define
      b as numval = prev(numval) + 1
)
 order by numval;</code>
<p>
Having tested the behaviour with ALL ROWS PER MATCH, it is simple to change to ONE ROW PER MATCH (which is default so we can simply omit the PER MATCH specification) to get the desired groups of sequences.
</p>
<code>select *
  from numbers
match_recognize (
   order by numval
   measures
      first(numval) firstval
    , last(numval) lastval
    , count(*) cnt
   pattern (
      a b*
   )
   define
      b as numval = prev(numval) + 1
)
 order by firstval;</code>
<p>
The method can easily be adapted to for example find groups of consecutive dates. Also the condition could be more complex than simply ""value must be exactly one higher than the previous"".
</p><p>
For example we can say that we can accept a gap of <em>one</em> integer missing from the sequence and still allow it to be a group by a simple change to the DEFINE clause. Or you can try it out with ""+ 3"" and see what happens.
</p>
<code>select *
  from numbers
match_recognize (
   order by numval
   measures
      first(numval) firstval
    , last(numval) lastval
    , count(*) cnt
   pattern (
      a b*
   )
   define
      b as numval <= prev(numval) + 2 -- Just change here
)
 order by firstval;</code>
<p>
This module has shown a basic, but useful, pattern matching technique to group rows as long as some condition is met. A pattern of A B* and a definition of B with some condition (simple or complex) using PREV to compare the row to the previous row, that is a ""template"" that can be applied to several use cases.
</p>
<p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",11-JUN-17 12.38.32.603979000 PM,"KIBEHA@GMAIL.COM",25-JUN-17 01.12.05.292063000 AM,"KIBEHA@GMAIL.COM"
117468047066758533880172006975040295929,117463246204528657365867867361809943103,"Binary Data Types",80,"<p>You use binary data to store in its original format. These are usually other files, such as graphics, sound, video or Word documents. There are two key binary types: raw and blob.</p>

<h3>Raw</h3>

<p>Like with character data, raw is for smaller items. You specify the maximum length of data for each column. It has a maximum limit of 2,000 bytes up to 11.2 and 32,767 from 12.1.</p>

<h3>Blob</h3>

<p>Blob stands for binary large object. As with clob, the maximum size you can store is (4 gigabytes - 1) * (database block size).</p>

<p>The following creates a table with binary data type columns:</p>

<code>create table binary_data (
  raw_col  raw(1000),
  blob_col blob
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'BINARY_DATA';</code>",04-SEP-17 03.34.33.188392000 PM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 09.05.49.443544000 AM,"CHRIS.SAXON@ORACLE.COM"
117467040461687726819683265594666938645,117463246204528657365867867361809943103,"Numeric Data Types",60,"<p>The built-in numeric data types for Oracle Database are:</p>

<ul>
<li>number</li>
<li>float</li>
<li>binary_float</li>
<li>binary_double</li>
</ul>

<p>You use these to store numeric values, such as prices, weights, etc.</p>

<h3>Number</h3>

<p>This is the most common numeric data type. The format of it is:</p>
<pre>number ( precision, scale )</pre>
<p>The precision states the number of significant figures allowed. Scale determines the digits from the decimal point. The database rounds values that exceed the scale.</p>

<p>For example:</p>

<table>
  <tr>
    <th></th>
    <th>Min Value</th>
    <th>Max Value</th>
  </tr>
  <tr>
    <td>number ( 3, 2 )</td>
    <td>-9.99</td>
    <td>9.99</td>
  </tr>
  <tr>
    <td>number ( 3, -2 )</td>
    <td>-99900</td>
    <td>99900</td>
  </tr>
  <tr>
    <td>number ( 5 ) </td>
    <td>-99999</td>
    <td>99999</td>
  </tr>
</table>

<p>If you omit the precision and scale, the number defaults to the maximum range and precision.</p>

<h3>Float</h3>

<p>This is a subtype of number. You can use it to store floating-point numbers. But we recommend that you use binary_float or binary_double instead.</p>

<h3>Binary_float &amp; Binary_double</h3>

<p>These are floating point numbers. They can have any number of digits after the decimal point.</p>

<p>Binary_float is a 32-bit, single-precision floating-point number. Binary_double is a 64-bit, double-precision floating-point. The limits for these data types are:</p>

<table>
  <tr>
    <th>Value</th>
    <th>binary_float</th>
    <th>binary_double</th>
  </tr>
  <tr>
    <td>Maximum positive value</td>
    <td>3.40282E+38F</td>
    <td>1.79769313486231E+308</td>
  </tr>
  <tr>
    <td>Minimum positive value</td>
    <td>1.17549E-38F</td>
    <td>2.22507485850720E-308</td>
  </tr>
</table>

<p>These also allow you to store the special values infinity and NaN (not a number).</p>

<h3>ANSI Numeric Types</h3>

<p>Oracle Database also supports ANSI numeric types, which map back to built-in types. For example:</p>

<ul>
<li>integer => number(*, 0)</li>
<li>real => float(63)</li>
</ul>

<p>The following creates a table with various numeric data types:</p>

<code>create table numeric_data (
  number_3_sf_2_dp  number(3, 2),
  number_3_sf_2     number(3, -2),
  number_5_sf_0_dp  number(5, 0),
  integer_col       integer,
  float_col         float(10),
  real_col          real,
  binary_float_col  binary_float,
  binary_double_col binary_double
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'NUMERIC_DATA';</code>

<p>Note that the columns defined with ANSI types (integer_col & real_col) are mapped to the Oracle type.</p>",04-SEP-17 03.21.51.989085000 PM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 09.09.02.173726000 AM,"CHRIS.SAXON@ORACLE.COM"
117468047066687207256814743853732631545,117463246204528657365867867361809943103,"Datetime and Interval Data Types",70,"<p>Oracle Database has the following datetime data types:</p>

<ul>
<li>date</li>
<li>timestamp</li>
<li>timestamp with time zone</li>
<li>timestamp with local time zone</li>
</ul>

<p>You use these to store when events happened or are planned to happen. Always use one of the above types to store datetime values. Not numeric or string types!</p>

<h3>Date</h3>

<p>Dates are granular to the second. These always include the time of day. There is no ""day"" data type which stores calendar dates with no time in Oracle Database.</p>

<p>You can specify date values with the keyword date, followed by text in the format YYYY-MM-DD. For example the following is the date 14 Feb 2018:</p>

<pre>date'2018-02-14'</pre>

<p>This is a date with a time of midnight. If you need to state the time of day too, you need to use to_date. This takes the text of your date and a format mask. For example, this returns the datetime 23 July 2018 9:00 AM:</p>

<pre>to_date ( '2018-07-23 09:00 AM', 'YYYY-MM-DD HH:MI AM' )</pre>

<p>When you store dates, the database converts them to an internal format. The client controls the display format.</p>

<h3>Timestamp</h3>

<p>If you need greater precision than dates, use timestamps. These can include up to nine digits of fractional seconds. The precision states how many fractional seconds the column stores. By default you get six digits (microseconds).</p>

<p>You can specify timestamp values like dates. Either use the timestamp keyword or to_timestamp with a format mask:</p>

<pre>timestamp '2018-02-14 09:00:00.123'
to_timestamp ( '2018-07-23 09:00:00.123 AM', 'YYYY-MM-DD HH:MI:SS.FF AM' )</pre>

<p>Timestamps have another advantage over dates. You can store time zone information in them. You can't store time zone details in a date.</p>

<p>A timestamp with time zone column stores values passed as-is. When you query a timestamp with time zone, the database returns the value you stored.</p>

<p>The database converts values in local time zones to its time zone. When you fetch these columns, the database returns it in the time zone of the session.</p>

<h3>Time Intervals</h3>

<p>You can store time durations with intervals. Oracle Database has two interval types: year to month and day to second.</p>

<p>You can add or subtract intervals from dates, timestamps or equivalent intervals. But the intervals are incompatible! You can't combine a day to second interval with a year to month one. This is because the number of days varies between months and years.</p>

<p>The following creates a table with the various datetime data types:</p>

<code>create table datetime_data (
  date_col                      date,
  timestamp_with_3_frac_sec_col timestamp(3),
  timestamp_with_tz             timestamp with time zone,
  timestamp_with_local_tz       timestamp with local time zone,
  year_to_month_col             interval year to month,
  day_to_second_col             interval day to second
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'DATETIME_DATA';</code>",04-SEP-17 03.29.12.401031000 PM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 09.18.53.689371000 AM,"CHRIS.SAXON@ORACLE.COM"
117465572170970581421503400005377591929,117463246204528657365867867361809943103,"Removing Columns from a Table",95,"<p>You can also remove columns from a table. To get rid of a column from a table, alter the table again, this time with the drop clause.</p>

<p>The following removes the columns you added to this_table_has_three_columns in the previous step:</p>

<code>alter table this_table_has_three_columns drop (
  this_is_a_timestamp_column, 
  this_is_a_binary_large_object
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'THIS_TABLE_HAS_THREE_COLUMNS';</code>

<p>Note this is a one-way operation! After you drop a column there is no ""undrop"" command. If you want to get the column back, you have to restore it from a backup.</p> 

<p>Dropping columns is an expensive operation. It can take a long time to complete on large tables. So always triple, quadruple check before running this!</p>",04-SEP-17 02.48.18.749702000 PM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 08.38.26.102601000 AM,"CHRIS.SAXON@ORACLE.COM"
151284617274700295632259567885046524105,117675390209390608523249818520886328918,"Try It!",48,"<p>Complete the following query to find the rows where:</p>

<ul><li>The colour is red or blue</li>
<li>The price is greater than or equal to 6 and strictly less than 14.22</li></ul>
<code>select *
from   toys
where  /* TODO */</code>
<p>This should return the following row:</p>
<pre><b>TOY_NAME             COLOUR   PRICE   </b> 
Miss Smelly_bottom   blue         6 
</pre>",25-JUL-18 09.18.21.467411000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.19.46.965708000 AM,"CHRIS.SAXON@ORACLE.COM"
151905700197832860032741907083335346238,151905700197614044459391659202713528382,"Try It!",50,"<p>Complete the following query to return the total weight for each shape stored in the bricks table:</p>

<code>select shape, /* TODO */ shape_weight
from   bricks
/* TODO */;</code>

<p>The query should return the following rows:</p>

<pre><b>SHAPE    SHAPE_WEIGHT   </b>
cube	            5
cuboid	            1
pyramid	            4 </pre>",31-JUL-18 08.51.57.421004000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.17.25.005443000 AM,"CHRIS.SAXON@ORACLE.COM"
151905700197849784994216511891781232702,151905700197614044459391659202713528382,"Try It!",35,"<p>Complete the following query to return the: </p>

<ul><li>Number of different shapes</li>
<li>The standard deviation (stddev) of the unique weights</li></ul>

<code>select /* TODO */ number_of_shapes,
       /* TODO */ distinct_weight_stddev
from   bricks;</code>

<p>The output of this query should be:</p>

<pre><b>NUMBER_OF_SHAPES   DISTINCT_WEIGHT_STDDEV   </b>
               3                        1</pre>",31-JUL-18 08.52.18.181989000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.08.29.170630000 AM,"CHRIS.SAXON@ORACLE.COM"
151905700197994856092570267392745973822,151905700197614044459391659202713528382,"Try It!",70,"<p>Complete the following query to find the shapes which have a total weight less than four:</p>

<code>select shape, sum ( weight )
from   bricks
group  by /* TODO */;</code>

<p>The query should return the following row:</p>

<pre><b>SHAPE    SUM(WEIGHT)   </b>
cuboid             1</pre>",31-JUL-18 08.53.49.125387000 AM,"CHRIS.SAXON@ORACLE.COM",30-AUG-18 09.27.53.570880000 AM,"CHRIS.SAXON@ORACLE.COM"
103563827416706573212741965544208096008,103563632879277014483527974609264228592,"Counting the number of events",120,"<h3>Adding more measures</h3>
<p>The next stage is to count the number of events within each session and calculate the duration of each session</p>
<p>How do we do that? We can use some of the other built-in measures such as <strong>FIRST()</strong> and <strong>LAST()</strong> to extract values from our resultset and we can calculate new values such as the duration of a session.</p>
<p>In our code we will add some new measures</p>
<ul>
<li>count(*) returns the number of events within a session</li>
<li>first(tstamp) returns the start time of each session</li>
<li>last(tstamp) returns the end time of each session</li>
<li>last(tstamp) - first(tstamp) calculates the duration of each session</li>    
</ul>
<br>
<code>SELECT
 time_id,
 userid,
 session_id,
 no_of_events,
 start_time,
 end_time,
 session_duration
FROM (SELECT 
       TO_NUMBER(j.session_doc.time_id) as time_id, 
       j.session_doc.user_id as userid
     FROM json_sessionization j) 
MATCH_RECOGNIZE(
   PARTITION BY userid ORDER BY time_id
   MEASURES match_number() as session_id,
            COUNT(*) as no_of_events,
            FIRST(b.time_id) start_time,
            LAST(s.time_id) end_time,    
            LAST(s.time_id) - FIRST(b.time_id) session_duration    
   ALL ROWS PER MATCH
   PATTERN (b s+)    
   DEFINE
       s as (time_id - PREV(time_id) <= 10)
 ); 
</code>
<p>This is looking good but it would it would be better if we could create a summary report</p>",24-APR-17 01.04.23.536181000 PM,"KEITH.LAKER@ORACLE.COM",16-MAY-18 01.52.42.952872000 PM,"SHARON.KENNEDY@ORACLE.COM"
103570655252003027420259362109032938896,103567330442136148468768570832363109098,"New string groupings",40,"<p>You might well ask: why donâ€™t we put the LISTAGG function inside the measure clause? At the moment it is not possible to include analytical functions such as LISTAGG in the measure clause. Therefore, we have put the LISTAGG function in a separate SQL statement:<p>
<code>SELECT
  deptno,
 LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno) AS namelist
 FROM emp_mr
 GROUP BY deptno, mno;
</code>
<p>The above code now returns groups of strings for each department where the total length of each group is less than 15 characters:

<code>SELECT
  deptno,
 LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno) AS namelist,
 length(LISTAGG(ename, ';') WITHIN GROUP (ORDER BY empno)) AS how_long
 FROM emp_mr
 GROUP BY deptno, mno;
</code>",24-APR-17 01.57.06.640681000 PM,"KEITH.LAKER@ORACLE.COM",26-APR-17 03.06.51.018527000 PM,"KEITH.LAKER@ORACLE.COM"
117467662623284693125876943380856607408,117463246204528657365867867361809943103,"Try It!",100,"<p>Complete the alter table statements to:</p>

<ul>
<li>Add the weight as a number with precision 8 and scale 1</li>
<li>Remove cuddliness factor</li>
</ul>

<code>drop table toys;
create table toys (
  toy_id            integer,
  toy_name          varchar2(100),
  cuddliness_factor integer
);

alter table toys add /* TODO */;

alter table toys drop ( /* TODO */ );

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'TOYS';</code>

<p>The query at the end should return the following rows:</p>

<pre><b>COLUMN_NAME   DATA_TYPE   DATA_LENGTH   DATA_PRECISION   DATA_SCALE  </b> 
TOY_ID        NUMBER                 22           &lt;null&gt;            0 
TOY_NAME      VARCHAR2              100           &lt;null&gt;       &lt;null&gt; 
WEIGHT        NUMBER                 22                8            1 </pre>",04-SEP-17 03.40.12.131632000 PM,"CHRIS.SAXON@ORACLE.COM",05-FEB-19 04.12.56.021850000 PM,"CHRIS.SAXON@ORACLE.COM"
108031505147082566973604955061876355356,97371311146062087736707699233039984534,"Group Consecutive Data",30,"<p>
The basis of this module is three different tables. First we'll look at a simple table of numbers.
</p>
<code>select *
  from numbers
 order by numval;</code>
<p>
The task is to group these numbers such that consecutive numbers are grouped together, while wherever there is a ""gap"" in the numbers, a new group is started. One way to do this is the socalled ""Tabibitosan"" method, which simply compares the number values with a truly consecutive number (such as can be generated with analytic function ROW_NUMBER) - as long as the difference is identical, the tested number values are consecutive (just like the generated ROW_NUMBER values), but where there is a ""gap"", the difference increases. We can use this difference as a GRP column.
</p>
<code>select numval
     , row_number() over (order by numval) as rn
     , numval - row_number() over (order by numval) as grp
  from numbers
 order by numval;</code>
<p>
Then we simply wrap in an inline view and GROUP BY the GRP column, and we get the desired grouping.
</p>
<code>select min(numval) firstval
     , max(numval) lastval
     , count(*) cnt
     , grp
  from (
   select numval
        , numval-row_number() over (order by numval) as grp
     from numbers
  )
 group by grp
 order by firstval;</code>
<p>
This method can be adapted to other things than just integer numbers increasing by 1. For example let us look at a simple table of dates.
</p>
<code>select *
  from dates
 order by dateval;</code>
<p>
All we need is to create an expression that returns an integer number that should increase by 1 to meet ""consecutive"" criteria. In this case that can simply be done by subtracting a fixed DATE literal from the date value, since that returns a numeric value where ""1"" equals one day. As long as we can create some integer expression with a scale of 1, we can apply Tabibitosan method by subtracting ROW_NUMBER.
</p>
<code>select dateval
     , row_number() over (order by dateval) as rn
     , (dateval - date '2016-01-01') - row_number() over (order by dateval) as grp
  from dates
 order by dateval;</code>
<p>
And again this can simply be used in a GROUP BY.
</p>
<code>select min(dateval) firstval
     , max(dateval) lastval
     , count(*) cnt
     , grp
  from (
   select dateval
        , (dateval - date '2016-01-01') - row_number() over (order by dateval) as grp
     from dates
  )
 group by grp
 order by firstval;</code>
<p>
Sometimes it is not so much ""gaps"" in the consecutive data we are after, but rather consecutive data where we wish to group rows that have identical values in another column, <em>as long as they are consecutive</em>. When a non-identical value appears, we start a new group. If the original value appears again later, this is a new group, as it was not consecutive with the previous appearances. To examine methods for this, we use a table that stores daily gasoline prices (in Danish Kroner per Liter) every day.
</p>
<code>select *
  from gas_price_log
 order by logged_date;</code>
<p>
We want to group rows of consecutive days with identical prices. Multiple methods can be used for this.
</p><p>
The first method we start by using LAG in a CASE statement to make a column PERIOD_START. It will contain the period start date for those rows where the price is different than the previous row (this is the beginning of each group), while those rows that have same price as the previous date get NULL value.
</p>
<code>select logged_date
     , logged_price
     , case lag(logged_price) over (order by logged_date)
          when logged_price then null
                            else logged_date
       end period_start
  from gas_price_log
 order by logged_date;</code>
<p>
We can fill out the NULL values in PERIOD_START by using LAST_VALUE with IGNORE NULLS. This ""carries down"" the PERIOD_START value downwards as long as there are NULLs.
</p>
<code>select logged_date
     , logged_price
     , period_start
     , last_value(period_start ignore nulls) over (
          order by logged_date
          rows between unbounded preceding and current row
       ) period_group
  from (
   select logged_date
        , logged_price
        , case lag(logged_price) over (order by logged_date)
             when logged_price then null
                               else logged_date
          end period_start
     from gas_price_log
  )
 order by logged_date;</code>
<p>
Then if we want it, we can create a nice numeric ""group id"" with DENSE_RANK as a ""reporting friendly"" way of display rather than PERIOD_GROUP.
</p>
<code>select logged_date
     , logged_price
     , period_start
     , period_group
     , dense_rank() over (
          order by period_group
       ) logged_group_id
  from (
   select logged_date
        , logged_price
        , period_start
        , last_value(period_start ignore nulls) over (
             order by logged_date
             rows between unbounded preceding and current row
          ) period_group
     from (
      select logged_date
           , logged_price
           , case lag(logged_price) over (order by logged_date)
                when logged_price then null
                                  else logged_date
             end period_start
        from gas_price_log
     )
  )
 order by logged_date;</code>
<p>
Or of course we can GROUP BY the PERIOD_GROUP to show the rows in a grouped fashion.
</p>
<code>select min(logged_date) start_date
     , max(logged_date) end_date
     , count(*) days
     , min(logged_price) logged_price
  from (
   select logged_date
        , logged_price
        , period_start
        , last_value(period_start ignore nulls) over (
             order by logged_date
             rows between unbounded preceding and current row
          ) period_group
     from (
      select logged_date
           , logged_price
           , case lag(logged_price) over (order by logged_date)
                when logged_price then null
                                  else logged_date
             end period_start
        from gas_price_log
     )
  )
 group by period_group
 order by start_date;</code>
<p>
An alternative to using LAG to create the PERIOD_START column with NULLs in it (and filling the NULLs afterwards), is to use the LAG in CASE comparison to create a column specifying 1 if the row price is identical to the previous row, and 0 otherwise.
</p>
<code>select logged_date
     , logged_price
     , case lag(logged_price) over (order by logged_date)
          when logged_price then 1
          else 0
       end identical
  from gas_price_log
 order by logged_date;</code>
<p>
Then we apply a kind of ""reverse Tabibitosan"". The rolling sum of the ""identical"" column increases by 1 whenever the row has identical price to the previous one. Which means that if we subtract that rolling sum from the consecutive numbers generated by ROW_NUMBER, we get a new number whenever the rolling sum is <em>not</em> increasing, and the same number whenever the rolling sum <em>is</em> increasing. The result is a nice numeric group id just like we got above with DENSE_RANK.</p>
<code>select logged_date
     , logged_price
     , identical
     , sum(identical) over (
          order by logged_date
          rows between unbounded preceding and current row
       ) sum_identical
     , row_number() over (
          order by logged_date
       ) - sum(identical) over (
          order by logged_date
          rows between unbounded preceding and current row
       ) logged_group_id
  from (
   select logged_date
        , logged_price
        , case lag(logged_price) over (order by logged_date)
             when logged_price then 1
             else 0
          end identical
     from gas_price_log
  )
 order by logged_date;</code>
<p>
But this numeric method can be simplified by reversing the logic. Rather than the IDENTICAL column, which had a 1 value for identically priced rows, we can turn it around with a NON_IDENTICAL column, which has a 1 value for rows that are <em>not</em> identically priced.
</p>
<code>select logged_date
     , logged_price
     , case lag(logged_price) over (order by logged_date)
          when logged_price then 0
          else 1
       end non_identical
  from gas_price_log
 order by logged_date;</code>
<p>
This enables us simply to get the desired group id by a simple rolling sum of the NON_IDENTICAL column. You can think of NON_IDENTICAL as being a marker for whenever there has been a change in the value, and the rolling sum then becomes a ""count of changes so far"". Where the prices are identical, the ""count of changes"" does not increase.
</p>
<code>select logged_date
     , logged_price
     , non_identical
     , sum(non_identical) over (
          order by logged_date
          rows between unbounded preceding and current row
       ) logged_group_id
  from (
   select logged_date
        , logged_price
        , case lag(logged_price) over (order by logged_date)
             when logged_price then 0
             else 1
          end non_identical
     from gas_price_log
  )
 order by logged_date;</code>
<p>
Most importantly this module has shown that you can use multiple methods to achieve the same results. Tabibitosan and the last query of summing ""change markers"" are quite efficient, but the alternatives might be good approaches for different use cases. Keep an open mind and try out different approaches to your problems, as different solutions may exists - some more efficient than others.
</p><p align=""right"">
<a align=""right"" href=""#R1033440518617548126""><em>Return to module list at the top</em></a>
</p>",06-JUN-17 07.14.07.416296000 AM,"KIBEHA@GMAIL.COM",20-JUN-17 08.08.54.182212000 PM,"KIBEHA@GMAIL.COM"
117123180204204286304759775459382295112,117123875509271941312470520788803021948,"Index-Organized Tables",20,"<p>Unlike a heap table, an index-organized table (IOT) imposes order on the rows within it. It physically stores rows sorted by its primary key. To create an IOT, you need to:</p>
<ul>
<li>Specify a primary key for the table</li>
<li>Add the organization index clause at the end</li>
</ul>

<p>For example:</p>

<code>create table toys_iot (
  toy_id   integer primary key,
  toy_name varchar2(100)
) organization index;</code>

<p>You can find IOT in the data dictionary by looking at the column IOT_TYPE. This will return IOT if the table is index-organized:</p>

<code>select table_name, iot_type
from   user_tables
where  table_name = 'TOYS_IOT';</code>",01-SEP-17 08.27.52.796512000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 07.57.47.436415000 AM,"CHRIS.SAXON@ORACLE.COM"
117123180204368700216227365027142335048,117123875509271941312470520788803021948,"Table Clusters",60,"<p>A table cluster can store rows from many tables in the same physical location. To do this, first you must create the cluster:</p>

<code>create cluster toy_cluster (
  toy_name varchar2(100)
);</code>

<p>Then place your tables in it using the cluster clause of create table:</p>

<code>create table toys_cluster_tab (
  toy_name varchar2(100)
) cluster toy_cluster ( toy_name );

create table toy_owners_cluster_tab (
  owner    varchar2(20),
  toy_name varchar2(100)
) cluster toy_cluster ( toy_name );</code>

<p>Rows that have the same value for toy_name in toys_clus_tab and toy_owners_clus_tab will be in the same place. This can make it faster to get a row for a given toy_name from both tables. </p>

<p>You can view details of clusters by querying the *_clusters views. If a table is in a cluster, cluster_name of *_tables tells you which cluster it is in:</p>

<code>select cluster_name from user_clusters;

select table_name, cluster_name
from   user_tables
where  table_name in ( 'TOYS_CLUSTER_TAB', 'TOY_OWNERS_CLUSTER_TAB' );</code>

<p><b>Note:</b> Clustering tables is an advanced topic. They have some restrictions. So make sure you read up on these before you use them!</p>",01-SEP-17 08.46.04.173547000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 08.05.43.961820000 AM,"CHRIS.SAXON@ORACLE.COM"
117123180204450907171961159811022355016,117123875509271941312470520788803021948,"Viewing Table Information",11,"<p>The data dictionary stores information about your database. You can query this to see which tables it contains. There are three key views with this information:</p>

<ul><li>user_tables - all tables owned by the current database user</li>
<li>all_tables - all tables your database user has access to</li>
<li>dba_tables - all tables in the database. Only available if you have DBA privileges</li></ul>

<p>The following query will show you the toys table you created in the previous step:</p>

<code>select table_name, iot_name, iot_type, external, 
       partitioned, temporary, cluster_name
from   user_tables;</code>

<p>The other columns display details of the properties of each table. The rest of this tutorial will explore these.</p>",01-SEP-17 08.47.23.604840000 AM,"CHRIS.SAXON@ORACLE.COM",06-AUG-18 09.24.51.122640000 AM,"CHRIS.SAXON@ORACLE.COM"
117123180205068668265784235319297210952,117123875509271941312470520788803021948,"Try It!",90,"<p>Complete the following statement to drop the toys table:</p>

<code>drop table /*TODO*/ ;

select table_name
from   user_tables
where  table_name = 'TOYS';</code>

<p>The query afterwards should return no rows.</p>",01-SEP-17 09.24.32.573236000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 07.54.44.833602000 AM,"CHRIS.SAXON@ORACLE.COM"
119766065531488157928501959082235282547,119767479666755979335390994148897767438,"Create table to store JSON data",10,"<p>Create new table that will hold transactions in JSON format. We can ensure the data is in the correct format by using the new IS JSON check constraint</p>
<code>CREATE TABLE json_transactions 
(transaction_doc CLOB, 
 CONSTRAINT ""VALID_JSON"" CHECK (transaction_doc IS JSON) ENABLE
);</code>
<p>Add data in JSON format. Note we could use external table to read directly from a JSON log file
but in this case we need to insert the data directly into a table because we don't have access
to external filesystem.</p>
<code>truncate table json_transactions;
INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":500000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""25-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1200000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-MAR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":3100400}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-JAN-17"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":2000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-MAR-17"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":12000}');

COMMIT;</code>",26-SEP-17 04.19.21.990305000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.49.09.329038000 PM,"KEITH.LAKER@ORACLE.COM"
119766065531835119638731357655375955059,119767479666755979335390994148897767438,"Simple pattern matching expression",40,"<p>Let's create a very basic pattern matching statement where we will accept the default values for as possible.</p>
<code>SELECT user_id
FROM transfers_view
 MATCH_RECOGNIZE(
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>This code will not run! It will return an error:</p>
<blockquote>ORA-00904: ""USER_ID"": invalid identifier</blockquote>
<p>Why? Because we have not defined the columns to be returned by MATCH_RECOGNIZE. So can we simply use 'SELECT *' to return the data we need?</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>This no generates a different error:</p>
<blockquote>ORA-30732: table contains no user-visible columns</blockquote>
<p>Answer: No! We need to define the information to be returned by using the MEASURES clause. To keep things simple, we will return only the user_id and the amount.
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 MEASURES
   user_id AS user_id,
   amount AS amount
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>This query returns no rows which does not seem correct because if we look at the source data it's clear that we have some suspicious transfers. So what is going on?</p>",26-SEP-17 04.30.23.017084000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.50.52.057752000 PM,"KEITH.LAKER@ORACLE.COM"
119774987331355089339339677722923903688,119767479666755979335390994148897767438,"Adding more transactions",60,"<p>To test whether our MATCH_RECOGNIZE statement is processing data correctly we need to add some more transactions that include ones for a different user_id:</p>
<code>truncate table json_transactions;
INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":10000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":500000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""25-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1200000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":50000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""03-JAN-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":12000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-MAR-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-MAR-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-MAR-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-MAR-17"",""user_id"":""Keith"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-MAR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":3100400}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""15-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""16-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""17-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-JAN-17"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":2000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-MAR-17"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":12000}');
COMMIT;</code>",26-SEP-17 05.19.08.985928000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.52.42.273106000 PM,"KEITH.LAKER@ORACLE.COM"
119774678074968818758567201174624813863,119767479666755979335390994148897767438,"Cleaning up the output",110,"<p>Let's clean-up the column selection within our SELECT statement</p>
<code>SELECT user_id,time_id, amount, mn, cf
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id
 ORDER BY time_id
 MEASURES
   match_number() AS mn,
   classifier() AS cf
 ALL ROWS PER MATCH WITH UNMATCHED ROWS  
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>Can we create a summary report that contains more information about each match?</p>
<code>SELECT user_id, start_small, end_small, small_amount, avg_small_amount, start_big, big_amount
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id
 ORDER BY time_id
 MEASURES
   FIRST(x.time_id) AS start_small,
   LAST(x.time_id) AS end_small,
   MAX(x.amount) AS small_amount,
   TRUNC(AVG(x.amount),2) AS avg_small_amount,
   FIRST(y.time_id) AS start_big,
   y.amount AS big_amount
 ONE ROW PER MATCH  
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>",26-SEP-17 05.31.20.380135000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.55.56.262620000 PM,"KEITH.LAKER@ORACLE.COM"
117678645575915422997720251465126809809,117675390209390608523249818520886328918,"Null",60,"<p>The price for Baby Turtle is null. This is neither equal to nor not equal to anything! The result of comparing a value to null is unknown.</p>

<p>Where clauses only return rows where the tests are true. So if you search for rows where the price equals null, you get no data:</p>

<code>select * from toys
where  price = null;</code>

<p>To find rows storing null values, you must use the ""is null"" condition:</p>

<code>select * from toys
where  price is null;</code>",06-SEP-17 03.52.43.031920000 PM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 09.12.55.064560000 AM,"CHRIS.SAXON@ORACLE.COM"
119769770187040166593268801716527738161,119767479666755979335390994148897767438,"Working with multiple user_ids",70,"<p>Now let's re-run the pattern matching but point to a specific occurrence of user_id by referencing our pattern matching variables.</p>
<p>In this case we want x.user_id which means the user_id when the pattern is matched for variable x</p>
<p>You will see that the resultset changes!</p>
<code>
SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 ORDER BY time_id
 MEASURES
   x.user_id AS user_id,
   amount AS amount
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10); 
</code>
<p>We still get two rows but the user_id in the first row has changed! It's now ""Keith"" and not ""John"". So something is still not correct.</p>",26-SEP-17 05.21.01.670325000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.53.04.542063000 PM,"KEITH.LAKER@ORACLE.COM"
120342174829114442430659846996404909599,119767479666755979335390994148897767438,"Adding Richer Analysis",140,"<p>Let's look at a new scenario where we want to track the location of credit card transactions to determine if card has been illegally replicated. We will use the geo-spatial features that are part of the Oracle Database.</p>
<p>First let's build a JSON table to hold our sample data set</p>
<code>CREATE TABLE json_transactions_geo 
(trans_geo_doc CLOB, 
 CONSTRAINT ""VALID_GEO_JSON"" CHECK (trans_geo_doc IS JSON) ENABLE
);</code>
<p>Now let's add some data</p>
<code>TRUNCATE TABLE json_transactions_geo;
INSERT INTO json_transactions_geo VALUES ('{""time_id"":""25-JAN-17 08:30:23"",""user_id"":""John"",""merchant_id"":""Baskin Robins"", ""item_ref"":""Ice Cream"",""trans_amount"":10.00, ""x"":-71.48923,""y"":42.72347,""l_x"":-71.48923,""l_y"":42.72347}');
INSERT INTO json_transactions_geo VALUES ('{""time_id"":""25-JAN-17 09:35:10"",""user_id"":""John"",""merchant_id"":""Costco"",""item_ref"":""Groceries"",""trans_amount"":55.00,""x"":-71.48923,""y"":42.72347,""l_x"":-71.48989,""l_y"":42.72347}');
INSERT INTO json_transactions_geo VALUES ('{""time_id"":""25-JAN-17 10:40:10"",""user_id"":""John"",""merchant_id"":""Starbucks"",""item_ref"":""Coffee"",""trans_amount"":5.00,""x"":-71.48923,""y"":42.72347,""l_x"":-71.48854,""l_y"":42.72347, }');
INSERT INTO json_transactions_geo VALUES ('{""time_id"":""25-JAN-17 16:36:10"",""user_id"":""John"",""merchant_id"":""BestBuy"",""item_ref"":""Apple Mac Pro"",""trans_amount"":4150.00,""x"":-91.48923,""y"":42.72347,""l_x"":-71.48923,""l_y"":42.72347}');
COMMIT;</code>",02-OCT-17 04.14.57.662291000 AM,"KEITH.LAKER@ORACLE.COM",03-OCT-17 02.55.54.725090000 PM,"SHARON.KENNEDY@ORACLE.COM"
117674707141417750480063339708239458486,117675390209390608523249818520886328918,"Negation",70,"<p>You can return the opposite of most conditions by placing NOT before it. For example, to find all the toys that aren't green, you can do:</p>

<code>select *
from   toys 
where  not colour = 'green';</code>

<p>You can get the same result by changing equals to either of the not equal conditions, != or <>:</p>

<code>select *
from  toys 
where colour <> 'green';</code>

<p>One exception to this is null. Searching for rows that are NOT equal to null still returns nothing:</p>

<code>select *
from   toys 
where  not colour = null;

select *
from   toys 
where  colour <> null;</code>

<p>To get all the rows with a non-null value, you must use the is not null condition:</p>

<code>select *
from   toys 
where  colour is not null;</code>",06-SEP-17 04.09.30.193893000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.26.04.575483000 PM,"CHRIS.SAXON@ORACLE.COM"
119767763764782448174773152682055742067,119767479666755979335390994148897767438,"Create a view",30,"<p>To make our pattern matching SQL statements easier to read we will create a view that will form the input into our pattern matching statement</p>
<code>CREATE VIEW transfers_view AS
SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
FROM json_transactions j
WHERE  j.transaction_doc.event_id = 'Transfer';</code>
<p>Note that we have converted the first column to a normal date format and the amount column is now a number format.</p>",26-SEP-17 04.28.28.522585000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.49.40.306659000 PM,"KEITH.LAKER@ORACLE.COM"
119767763764824760578459664703170458227,119767479666755979335390994148897767438,"Business Problem - Find Suspicious Transfers",5,"<p>The aim of this exercise is to find suspicious money transfers within a series of accounts. We define a suspicious money transfer pattern as:</p>
<ol>
<li>3 or more small (<2K) money transfers within 30 days</li>
<li>Large transfer (>=1M) within 10 days of last small transfer</li>
</ol>
<p>The pattern matching process needs to identify the following data points:</p>
<ol>
<li>Account 
<li>Date of first small transfer</li> 
<li>Date of last small transfer</li>
<li>Date of large transfer</li>
<li>Amount of large transfer</li>
</ol>",26-SEP-17 04.34.50.722019000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 04.34.50.722095000 PM,"KEITH.LAKER@ORACLE.COM"
119778527232363916872258756561892689612,119767479666755979335390994148897767438,"Coding the new requirements",130,"<p>The changes we need to make are all contained within the DEFINE clause. Firstly we need to check if the transfer_id changes for each small transaction:</p>
<blockquote>PREV(transfer_id) <> transfer_id</blockquote>
<p>Secondly we need to check the total of all the small transactions:<p>
<blockquote>SUM(x.amount) < 20000</blockquote>
</p>
<p>The full code looks like this:</p>

<code>SELECT user_id, start_small, end_small, sum_small, start_big, big_amount 
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id 
 ORDER BY time_id
 MEASURES 
   FIRST(x.time_id) AS start_small,
   LAST(x.time_id) AS end_small,
   SUM(x.amount) AS sum_small,
   FIRST(y.time_id) AS start_big,
   y.amount AS big_amount
 ONE ROW PER MATCH
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         PREV(transfer_id) <> transfer_id AND
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10 AND
         SUM(x.amount) < 20000);</code>",26-SEP-17 06.16.03.100652000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 06.32.43.000208000 PM,"KEITH.LAKER@ORACLE.COM"
120339453023109525064458420773915872669,119767479666755979335390994148897767438,"Check credit card data",150,"<p>Let's quickly look at out credit card data...note that at this point we simply have numeric values for our geospatial coordinates. We will convert these to geospatial datatypes later</p>
<code>SELECT 
       TO_DATE(j.trans_geo_doc.time_id, 'DD-MON-YYYY, HH:MI:SS') as time_id, 
       j.trans_geo_doc.user_id as user_id,
       j.trans_geo_doc.merchant_id as merchant_id,   
       j.trans_geo_doc.item_ref as item_ref,
       to_number(j.trans_geo_doc.trans_amount) as trans_amount,
       to_number(j.trans_geo_doc.x) as geo_x,
       to_number(j.trans_geo_doc.y) as geo_y,
       to_number(j.trans_geo_doc.l_x) as geo_lag_x,
       to_number(j.trans_geo_doc.l_y) as geo_lag_y
FROM json_transactions_geo j;</code>",02-OCT-17 04.17.05.882944000 AM,"KEITH.LAKER@ORACLE.COM",02-OCT-17 04.17.05.883022000 AM,"KEITH.LAKER@ORACLE.COM"
119767479667451111681669405924353818638,119767479666755979335390994148897767438,"View JSON data",20,"<p>Now we need to view the data that's been added. The data is returned in JSON format, i.e. key-value pairs, if we use a simple 'SELECT *' statement:</p>
<code>SELECT *
FROM json_transactions j;</code>
<p>We can also view the data in a transformed state where the resultset is a normal relational set of columns and rows. This is down using the new syntax for accessing JSON data: document_ref.attribute_name</p>
<code>SELECT 
  j.transaction_doc.time_id as time_id, 
  j.transaction_doc.user_id as user_id,
  j.transaction_doc.event_id as event_id,   
  j.transaction_doc.trans_amount as amount
FROM json_transactions j;</code>
<p>All the columns are returned as VARCHAR2!</p>

<p>Using SQL we can apply predicates to our query so that it returns all the transactions that are transfers.</p>
<code>SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,   
       to_number(j.transaction_doc.trans_amount) as amount
FROM json_transactions j
WHERE  j.transaction_doc.event_id = 'Transfer'
ORDER BY 2,1;</code>
<p>Important point is that you can see that we can do normal SQL operations on our JSON data</p>",26-SEP-17 04.25.14.615359000 PM,"KEITH.LAKER@ORACLE.COM",18-DEC-18 02.45.10.355326000 PM,"SHARON.KENNEDY@ORACLE.COM"
119772662573258537804635566563916402263,119767479666755979335390994148897767438,"Correct order is important",50,"<p>What we are missing the keyword ORDER BY which will sort the data within the pattern matching process.</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 ORDER BY time_id
 MEASURES
   user_id AS user_id,
   amount AS amount
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code> 
<p>Now we see that 2 matches have been found. But is this correct?</p>",26-SEP-17 04.44.59.030550000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.53.16.939929000 PM,"KEITH.LAKER@ORACLE.COM"
119777900923290744176043279186999796727,119767479666755979335390994148897767438,"New Requirements ",120,"<p>Now we are going to add some new business requirements to our definition of suspicious activity within an account. Two new rules need to be applied:</p>
<ol>
<li>Small transfers do not go to the same account twice in a row</li>
<li>Total sum of small transfers must be less than 20K</li>
</ol>
<p>Let's add some transactions which contain additional information:</p>
<code>TRUNCATE TABLE json_transactions;
INSERT INTO json_transactions VALUES ('{""time_id"":""01-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""25-JAN-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":1200000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Deposit"",""trans_amount"":500000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":100}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Bob"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Allen"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-JAN-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1000000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-MAR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Allen"",""trans_amount"":1200500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Allen"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-FEB-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"", ""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""02-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1000}');
INSERT INTO json_transactions VALUES ('{""time_id"":""27-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":3100400}');
INSERT INTO json_transactions VALUES ('{""time_id"":""20-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1200}');
INSERT INTO json_transactions VALUES ('{""time_id"":""10-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""15-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Tim"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""16-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""Bob"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""17-APR-17"",""user_id"":""John"",""event_id"":""Transfer"",""transfer_id"":""John"",""trans_amount"":1500}');
INSERT INTO json_transactions VALUES ('{""time_id"":""05-JAN-17"",""user_id"":""John"",""event_id"":""Withdrawal"",""trans_amount"":2000}');

COMMIT;</code>
<p>Question: has the addition of new sets of key-value pairs broken our original queries?</p>
<code>SELECT * FROM json_transactions;</code>
<p>Still lists all our JSON transactions and our view still works:</p>
<code>SELECT * FROM transfers_view;</code>
<p>But the view does not include the new data.</p>",26-SEP-17 06.10.50.022904000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 06.30.37.835703000 PM,"KEITH.LAKER@ORACLE.COM"
119775831120511072561833537057457873761,119767479666755979335390994148897767438,"Amend View",125,"<p>As we have added a new set of key-value pairs to our JSON table, we need to update our view to include this new information</p>
<code>CREATE OR REPLACE VIEW transfers_view AS
SELECT 
       TO_DATE(j.transaction_doc.time_id, 'DD-MON-YYYY') as time_id, 
       j.transaction_doc.user_id as user_id,
       j.transaction_doc.event_id as event_id,
       j.transaction_doc.transfer_id as transfer_id,   
       TO_NUMBER(j.transaction_doc.trans_amount) as amount
FROM json_transactions j
WHERE  j.transaction_doc.event_id = 'Transfer';</code>
<p>Note that we have converted the first column to a normal date format and the amount column is now a number format.</p>
<p>Does our original pattern matching query still work?</p>
Pattern matching query still works!
<code>SELECT user_id, start_small, end_small, small_amount, avg_small_amount, start_big, big_amount
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id
 ORDER BY time_id
 MEASURES
   FIRST(x.time_id) AS start_small,
   LAST(x.time_id) AS end_small,
   MAX(x.amount) AS small_amount,
   TRUNC(AVG(x.amount),2) AS avg_small_amount,
   FIRST(y.time_id) AS start_big,
   y.amount AS big_amount
 ONE ROW PER MATCH  
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>Yes it works!</p>",26-SEP-17 06.21.52.330637000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 06.32.17.658652000 PM,"KEITH.LAKER@ORACLE.COM"
119769433543930056629130707780621734443,119767479666755979335390994148897767438,"Using the PARTITION BY clause",80,"<p>What need to do is put each user_id in a separate block, or partition, and then process the transactions for each user_id where the transactions are sorted in data order. To do this we use the PARTITION BY and ORDER BY clauses. This query will now return three matches.</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id
 ORDER BY time_id
 MEASURES
   amount AS amount
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>",26-SEP-17 05.22.27.567666000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.53.45.501958000 PM,"KEITH.LAKER@ORACLE.COM"
119769433544011054659044887935327048235,119767479666755979335390994148897767438,"Determining the type of output ",90,"<p>Can we extract any more information about each match which will allow us to determine if our pattern matching query is returning the correct information?</p>
<p>We can control the output using ONE ROW PER MATCH or ALL ROWS PER MATCH.</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 PARTITION BY user_id
 ORDER BY time_id
 MEASURES
   amount AS t_amount
 ALL ROWS PER MATCH  
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>Now we can see the details of the rows that are matched against the pattern we have defined.</p>
<p>Also note that the amount column now appears twice because using ALL ROWS PER MATCH forces MATCH_RECOGNIZE to returns all the columns in the source table.</p>
<p>Therefore, if you add columns to your source table the new columns will automatically appear in the resultset - which may or may not be a problem.</p>
<p>Therefore, think very carefully whether you want to use SELECT * FROM... or whether you want to list the columns to be returned.</p>",26-SEP-17 05.27.13.705403000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.54.13.926431000 PM,"KEITH.LAKER@ORACLE.COM"
119769433544036442101256795147995877931,119767479666755979335390994148897767438,"Checking the results",100,"<p>Are there any debugging tools to help us understand how the pattern is being applied? Yes: MATCH_NUMBER() and CLASSIFIER() functions.</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 partition by user_id
 order by time_id
 MEASURES
   amount AS t_amount,
   match_number() AS mn,
   classifier() AS cf
 ALL ROWS PER MATCH  
 PATTERN (X{3,} Y)
 DEFINE
   X AS (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y AS (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>
<p>We can also see the rejected rows, i.e. the rows that were not matched by adding some more keywords: ALL ROWS PER MATCH WITH UNMATCHED ROWS</p>
<code>SELECT *
FROM transfers_view
MATCH_RECOGNIZE(
 partition by user_id
 order by time_id
 MEASURES
   match_number() as mn,
   classifier() as cf
 ALL ROWS PER MATCH WITH UNMATCHED ROWS  
 PATTERN (X{3,} Y)
 DEFINE
   X as (amount < 2000) AND 
         LAST(time_id) - FIRST(time_id) < 30,
   Y as (amount >= 1000000) AND 
         time_id - LAST(x.time_id) < 10);</code>",26-SEP-17 05.29.49.519216000 PM,"KEITH.LAKER@ORACLE.COM",26-SEP-17 05.59.21.677606000 PM,"KEITH.LAKER@ORACLE.COM"
117672495078510513550408644401277067993,117675390209390608523249818520886328918,"Lists of Values",40,"<p>Often you want to get rows where a column matches any value in a list. You can do this by ORing these conditions together. For example, the following finds rows where the colour is red or green:</p>

<code>select * from toys
where  colour = 'red' or
       colour = 'green';</code>

<p>But this is a pain to write if you have a large number of values!</p>

<p>Luckily you can simplify this with IN. Place the list of values in parentheses. Then check if the column is IN this list. So this query is the same as the one above, finding all the rows where the colour is red or green:</p>

<code>select * from toys
where  colour in ( 'red' , 'green' );</code>",06-SEP-17 03.40.27.697160000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.17.49.614305000 AM,"CHRIS.SAXON@ORACLE.COM"
117675752842227189356210852391121766259,117675390209390608523249818520886328918,"Wildcards",50,"<p>When searching strings, you can find rows matching a pattern using LIKE. This has two wildcard characters:</p>

<ul>
<li>Underscore (_) matches exactly one character</li> 
<li>Percent (%) matching zero or more characters</li>
</ul>

<p>You can place these either side of the characters you're searching for. So this finds all the rows that have a colour starting with b:</p>

<code>select * from toys
where  colour like 'b%';</code>

<p>And this all the rows with colours ending in n:</p>

<code>select * from toys
where  colour like '%n';</code>

<p>Underscore matches exactly one character. So the following finds all the rows with toy_names eleven characters long:</p>

<code>select * from toys
where  toy_name like '___________';</code>

<p>Percent is true even if it matches no characters. So the following tests to search for colours containing the letter ""e"" all return different results:</p>

<code>select * from toys
where  colour like '_e_';

select * from toys
where  colour like '%e%';

select * from toys
where  colour like '%_e_%';</code>

<p>This is because these searches work as follows:</p>

<ul><li>_e_ => any colour with exactly one character either side of e (red)</li>
<li>%e% => any colour that contains e anywhere in the string (red, blue, green)</li>
<li>%_e_% => any colour with at least one character either side of e (red, green)</li></ul>

<h3>Searching for Wildcard Characters</h3>

<p>You may want to find rows that contain either underscore or percent. For example, if you want to see all the rows that include underscore in the toy_name, you might try:</p>

<code>select * from toys
where  toy_name like '%_%';</code>

<p>But this returns all the rows!</p>

<p>This is because it sees underscore as the wildcard. So it looks for all rows that have at least one character in toy_name.</p>

<p>To avoid this, you can use escape characters. Place this character before the wildcard. Then state what it is in the escape clause after the condition. This can be any character. But usually you'll use symbols you're unlikely to search for. Such as backslash \ or hash #.</p>

<p>So both the following find Miss Smelly_bottom, the only toy_name that includes an underscore:</p>

<code>select * from toys
where  toy_name like '%\_%' escape '\';

select * from toys
where  toy_name like '%#_%' escape '#';</code>",06-SEP-17 03.48.12.876215000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.22.06.957942000 AM,"CHRIS.SAXON@ORACLE.COM"
117680833658183726585251495329252666238,117675390209390608523249818520886328918,"Try It!",80,"<p>Complete the following query to find the rows where:</p>

<ul><li>The colour is not green</li>
<li>The price is not equal to 6</li></ul>

<code>select toy_name
from   toys
where  /* TODO */</code>

<p>This should return the following rows:</p>

<pre><b>TOY_NAME  </b>        
Sir Stripypants   
Cuteasaurus       
Mr Bunnykins     </pre>

<p>How many ways can you think of to write this query?</p>",06-SEP-17 04.15.20.161752000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.24.13.811682000 AM,"CHRIS.SAXON@ORACLE.COM"
120344613688842771274837787771008642503,119767479666755979335390994148897767438,"Using spatial analytics",170,"<p>We can now used the SDO_GEOM.SDO_DISTANCE function to calculate the distance between transactions...</p>
<code>SELECT 
 l.day_id as day_id,
 l.time_id as time_id,
 l.lag_time as mins,
 l.user_id,
 l.merchant_id,
 l.item_ref,
 l.trans_amount,
 SDO_GEOM.SDO_DISTANCE(SDO_GEOMETRY(2001,8307,SDO_POINT_TYPE(l.geo_x, l.geo_y, NULL),NULL,NULL),
                      SDO_GEOMETRY(2001,8307,SDO_POINT_TYPE(l.geo_lag_x, l.geo_lag_y, NULL),NULL,NULL),
                      0.0001,'unit=km') AS DISTANCE_BETWEEN_TRANS
FROM VW_JSON_CC_TRANS l 
ORDER BY l.day_id, time_id;</code>
<p>Your task now is to use this information in MATCH_RECOGNIZE query that looks for 3 low value transactions, followed by a large transaction where the distance between the last small transaction and the large transaction is too large (i.e. suspicious)</p>
<p>Using the previous examples you should be able to construct this query yourself.</p>",02-OCT-17 04.22.04.197349000 AM,"KEITH.LAKER@ORACLE.COM",02-OCT-17 04.22.04.197426000 AM,"KEITH.LAKER@ORACLE.COM"
152139165451759477350044190653742354531,152139165451368994310308665430312259683,"Try It!",30,"<p>Complete the following query to insert a row into the toy table with the toy_id 3 and colour red:</p>

<code>insert into toys /* TODO */ values ( 3, 'red' );

select * from toys
where  toy_id = 3;</code>

<p>The query should return the following row:</p>

<pre><b>TOY_ID   TOY_NAME   COLOUR   </b>
       3 &lt;null&gt;     red</pre>

<p><i>If you make a mistake and need to reset the table, click ""Execute Prerequisite SQL"" again</i></p>",02-AUG-18 01.53.15.136381000 PM,"CHRIS.SAXON@ORACLE.COM",04-SEP-18 03.41.12.120215000 PM,"CHRIS.SAXON@ORACLE.COM"
152139165451780029088977639349712359523,152139165451368994310308665430312259683,"Saving DML Changes",50,"<p>To save changes to the database, you must issue a commit. So to ensure you preserve the row you inserted, commit afterwards:</p>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 6, 'Green Rabbit', 'green' );
  
commit;

select *
from   toys
where  toy_id = 6;</code>

<p>The row is only visible to other users after you commit. Before this point, only your session (database connection) can see the new data.</p>

<p>Many tools have an autocommit property. This will do the commit for you. Either after each call to the database. Or when the session ends.</p>

<p>LiveSQL commits after each click of the Run button completes.</p>

<p>Ensure you check how this is set. If autocommit is on and you think it isn't (or vice-versa) it can lead to saving unwanted changes!</p>

<p>How to do this is specific to your tools and programming language. Refer to their documentation to find out how to set the autocommit property.</p>",02-AUG-18 01.54.01.329530000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.19.17.382435000 PM,"CHRIS.SAXON@ORACLE.COM"
120339453023134912506670327986584702365,119767479666755979335390994148897767438,"Building A View",160,"<p>To keep our pattern matching code simple and readable, let's create a view over our JSON data...</p>
<code>CREATE OR REPLACE VIEW VW_JSON_CC_TRANS AS
SELECT 
    TO_DATE(j.trans_geo_doc.time_id, 'DD-MON-YYYY, HH24:MI:SS') as day_id, 
    TO_CHAR(TO_TIMESTAMP(j.trans_geo_doc.time_id, 'DD-MON-YYYY, HH24:MI:SS'), 'HH24:MI:SS') as time_id, 
    TRUNC((TO_DATE(j.trans_geo_doc.time_id, 'DD-MON-RR, HH24:MI:SS') - 
    (TO_DATE(LAG(j.trans_geo_doc.time_id,1) over (order by j.trans_geo_doc.time_id), 'DD-MON-RR, HH24:MI:SS'))) * 24, 0) AS LAG_TIME,
    j.trans_geo_doc.user_id as user_id,
    j.trans_geo_doc.merchant_id as merchant_id,   
    j.trans_geo_doc.item_ref as item_ref,
    to_number(j.trans_geo_doc.trans_amount) as trans_amount,
    to_number(j.trans_geo_doc.x) as geo_x,
    to_number(j.trans_geo_doc.y) as geo_y,
    to_number(j.trans_geo_doc.l_x) as geo_lag_x,
    to_number(j.trans_geo_doc.l_y) as geo_lag_y
FROM json_transactions_geo j;</code>",02-OCT-17 04.18.10.726821000 AM,"KEITH.LAKER@ORACLE.COM",03-OCT-17 02.55.15.550695000 PM,"SHARON.KENNEDY@ORACLE.COM"
139783754536485151081077599934870247607,139783754536471852897061839013948479671,"Is null condition",10,"<p>To find rows that have a null-value, use the <strong>""is null""</strong> condition.</p>

<p>This query finds all the rows storing null in volume_of_wood:</p>

<code>select *
from   toys
where  volume_of_wood is null;</code>

<p><strong>Try it!</strong></p>

<p>Insert this incomplete query the editor. Complete it to find the row where the times_lost is null ('Blue Brick'):</p>

<code>select *
from   toys
where  </code>",06-APR-18 09.24.41.828599000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-18 08.32.00.239346000 AM,"CHRIS.SAXON@ORACLE.COM"
142406337564529141101171336929621107903,142399856232522825415084831548294078164,"Inline Views",10,"<p>An inline view replaces a table in the from clause of your query. In this query, ""select * from bricks"" is an inline view:</p>

<code>select * from (
  select * from bricks
)</code>

<p>You use inline views to calculate an intermediate result set. For example, you can count the number of bricks you have of each colour using:</p>

<code>select * from (
  select colour, count(*) c
  from   bricks
  group  by colour
) brick_counts</code>

<p>You can then join the result of this to the colours table. This allows you to find out which colours you have fewer bricks of than the minimum needed defined in colours:</p>

<code>select * from (
  select colour, count(*) c
  from   bricks
  group  by colour
) brick_counts
right  join colours
on     brick_counts.colour = colours.colour_name
where  nvl ( brick_counts.c, 0 ) < colours.minimum_bricks_needed</code>

<p><i>The outer join is necessary to return colours for which you have no bricks (yellow, purple, and orange)</i></p>",01-MAY-18 09.20.37.718978000 AM,"CHRIS.SAXON@ORACLE.COM",15-DEC-22 10.54.45.857111000 AM,"CHRIS.SAXON@ORACLE.COM"
142406337571629162439768054072670479551,142399856232522825415084831548294078164,"Nested Subqueries",20,"<p>Nested subqueries go in your where clause. The query filters rows in the parent tables.</p>

<p>For example, to find all the rows in colours where you have a matching brick, you could write:</p>

<code>select * from colours c
where  c.colour_name in (
  select b.colour from bricks b
);</code>

<p>You can also use exists in a nested subquery. The following is the same as the previous query:</p>

<code>select * from colours c
where  exists (
  select null from bricks b
  where  b.colour = c.colour_name
);</code>

<p>You can filter rows in a subquery too. To find all the colours that have at least one brick with a brick_id less than 5, write:</p>

<code>select * from colours c
where  c.colour_name in (
  select b.colour from bricks b
  where  b.brick_id < 5
);</code>

<p>Or with exists:</p>

<code>select * from colours c
where  exists (
  select null from bricks b
  where  b.colour = c.colour_name 
  and    b.brick_id < 5
);</code>",01-MAY-18 10.15.07.808772000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 09.10.59.149419000 AM,"CHRIS.SAXON@ORACLE.COM"
139793525354520770608059904052364583500,139783754536471852897061839013948479671,"Is not null",20,"<p>To do the reverse, and find all the rows with a non-null value, use the <strong>""is not null""</strong> condition:</p>
<code>select *
from   toys
where  volume_of_wood is not null;
</code>",06-APR-18 09.28.09.456494000 AM,"CHRIS.SAXON@ORACLE.COM",17-APR-18 03.23.58.732254000 PM,"CHRIS.SAXON@ORACLE.COM"
139795759427000816615480228272833095569,139783754536471852897061839013948479671,"Nulls in range comparisons",30,"<p>If you want to find the toys with a volume of wood less than 15, you can write:</p>

<code>select * from toys
where  volume_of_wood < 15;</code>

<p>But this will exclude rows where the volume_of_wood is null. If you want to include these, you must also test for this:</p>

<code>select * from toys
where  volume_of_wood < 15 or
       volume_of_wood is null;</code>",06-APR-18 09.31.39.955911000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-18 08.33.15.080082000 AM,"CHRIS.SAXON@ORACLE.COM"
139795759428439438340821636990733445009,139783754536471852897061839013948479671,"Null functions",40,"<p>Oracle Database includes many functions to help you handle nulls. NVL and coalesce are two that map nulls to non-null values.</p>

<p><strong>NVL</strong></p>

<p>This takes two arguments. If the first is null, it returns the second:</p>

<code>select toy_name, volume_of_wood, nvl ( volume_of_wood , 0 ) mapped_volume_of_wood
from   toys;</code>

<p><strong>Coalesce</strong></p>

<p>This is like NVL. But it can take any number of arguments. It returns the first non-null value it finds:</p>

<code>select t.*,
       coalesce ( volume_of_wood , 0 ) coalesce_two,
       coalesce ( times_lost, volume_of_wood , quantity_of_stuffing, 0 ) coalesce_many
from   toys t;</code>

<p>You can use these functions in the where clause to map nulls to a real value. So you no longer need a separate ""or column is null"" test. </p>

<p>For example, these return the same rows as the final query in section 5:</p>

<code>select *
from   toys
where  nvl ( volume_of_wood , 0 ) < 15;

select *
from   toys
where  coalesce ( volume_of_wood , 0) < 15;</code>",06-APR-18 09.39.04.097938000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-18 08.27.29.271147000 AM,"CHRIS.SAXON@ORACLE.COM"
139795759429302611376026482221473654673,139783754536471852897061839013948479671,"Introduction",1,"<p>The table contains rows with several null values. Run the query below to view it:</p>

<code>select * from toys;</code>",06-APR-18 09.43.47.911007000 AM,"CHRIS.SAXON@ORACLE.COM",17-APR-18 03.16.31.136957000 PM,"CHRIS.SAXON@ORACLE.COM"
139795759429504501987902125293649586065,139783754536471852897061839013948479671,"Nothing equals null!",5,"<p>Null is neither equal to nor not equal to anything. The result of:</p>

<p>null = &lt;anything&gt;</p>

<p>is always unknown. So the following query will <strong>always</strong> return no rows:</p>

<code>select * from toys
where  volume_of_wood = null;</code>

<p>This is also the case when you check if a column is not equal to null:</p>

<code>select * from toys
where  volume_of_wood <> null;</code>",06-APR-18 09.45.20.417159000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-18 08.18.31.197089000 AM,"CHRIS.SAXON@ORACLE.COM"
139795759430391853539499263107883919249,139783754536471852897061839013948479671,"Try it!",50,"<p>Complete the following query to find all the rows where times_lost is less than 5 or null:</p>

<code>select *
from   toys
where  </code>

<p>Your query should return the rows for ""Red Brick"" and ""Blue Brick"".</p>

<p>How many different ways can you think of to write a query that returns these two rows?</p>",06-APR-18 09.48.51.606794000 AM,"CHRIS.SAXON@ORACLE.COM",17-APR-18 03.18.07.319671000 PM,"CHRIS.SAXON@ORACLE.COM"
142509292807311140225523788074516777377,142399856232522825415084831548294078164,"Try it!",25,"<p>Complete the subquery to find all the rows in bricks with a colour where colours.minimum_bricks_needed = 2:</p>

<code>select * from bricks b
where  b.colour in (

);</code>

<p>This should return</p>

<pre><b>BRICK_ID COLOUR</b>
       1 blue
       2 blue
       3 blue
       6 red
       7 red
       8 red</pre>",02-MAY-18 09.10.43.016639000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.04.47.361799000 PM,"CHRIS.SAXON@ORACLE.COM"
142401775233067557727912802662937838389,142399856232522825415084831548294078164,"Scalar Subqueries",30,"<p>Scalar subqueries return one column and at most one row. You can replace a column with a scalar subquery in most cases.</p>

<p>For example, to return a count of the number of bricks matching each colour, you could do the following:</p>

<code>select colour_name, (
         select count(*) 
         from   bricks b
         where  b.colour = c.colour_name
         group  by b.colour
       ) brick_counts
from   colours c;</code>

<p>Note the colours with no matching bricks return null. To show zero instead, you can use NVL or coalesce. This needs to go around the whole subquery:</p>

<code>select colour_name, nvl ( (
         select count(*) 
         from   bricks b
         where  b.colour = c.colour_name
         group  by b.colour
       ), 0 ) brick_counts
from   colours c;</code>

<p>You also need to join bricks to colours in the subquery. If you don't, it will return four rows (one for each different value for colour in bricks). This leads to an ORA-01427 error:</p>

<code>select c.colour_name, (
         select count(*) 
         from   bricks b
         group  by colour
       ) brick_counts
from   colours c;</code>

<p>Usually you will correlate a scalar subquery with a parent table to give the correct answer.</p>

<p>You can also use scalar subqueries in your having clause. So instead of a join, you could write the query in part 1 to find those bricks you have less than the minimum needed like so:</p>

<code>select colour, count(*) c
from   bricks b
group  by colour
having count(*) < (
  select c.minimum_bricks_needed 
  from   colours c
  where  c.colour_name = b.colour
);</code>",01-MAY-18 10.19.00.212111000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 02.51.10.627501000 PM,"CHRIS.SAXON@ORACLE.COM"
142401775233419355141420659752777335605,142399856232522825415084831548294078164,"Correlated vs. Uncorrelated",22,"<p>A subquery is correlated when it joins to a table from the parent query. If you don't, then it's uncorrelated.</p>

<p>This leads to a difference between IN and EXISTS. EXISTS returns rows from the parent query, as long as the subquery finds at least one row. So the following uncorrelated EXISTS returns all the rows in colours:</p>

<code>select * from colours
where  exists (
  select null from bricks
);</code>

<p>Note: When using EXISTS, what you select in the subquery does not matter because it is only checking the existence of a row that matches the where clause (if there is one).  You can ""select null"" or ""select 1"" or select an actual column. </p>

<p>This is because the query means:</p>

<p>Return all the in colours if there is <i>at least one row in bricks</i></p>

<p>To find all the colour rows which have at least one row in bricks <i>of the same colour</i>, you must join in the subquery. Normally you need to correlate EXISTS subqueries with a table in the parent.</p>",01-MAY-18 10.19.25.589196000 AM,"CHRIS.SAXON@ORACLE.COM",22-JAN-20 02.14.46.288121000 PM,"SHARON.KENNEDY@ORACLE.COM"
142401775233644215343868980779272684341,142399856232522825415084831548294078164,"Testing Subqueries",70,"<p>Another big advantage of the with clause is it makes your SQL easier to test and debug.</p>

<p>If you want to check that the brick counts from the previous query are correct, update the final from clause:</p>

<code>with brick_counts as (
  select b.colour, count(*) c
  from   bricks b
  group  by b.colour
), average_bricks_per_colour as (
  select avg ( c ) average_count
  from   brick_counts
)
  select * from brick_counts bc;</code>

<p>But with inline views you have to rip out the correct subquery, which can be hard to do!</p>",01-MAY-18 10.28.24.998610000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.11.57.644476000 PM,"CHRIS.SAXON@ORACLE.COM"
139797834698439849986568678989456246986,139783754536471852897061839013948479671,"Magic Values",60,"<p>Null complicates your where clause. So some people are tempted to use ""magic values"" instead of null for missing or not applicable information.</p>

<p>For example, Mr. Penguin is made of fluff, not wood! So to show the volume_of_wood doesn't apply, you could set this to the ""impossible"" value of minus one.</p>

<p>Run this update to do this:</p>

<code>update toys
set   volume_of_wood = -1
where volume_of_wood is null;

select * from toys;</code>

<p>You can now use standard comparison logic to find all the rows with a volume of wood less than 15 or where it doesn't apply:</p>

<code>select * from toys
where  volume_of_wood  < 15;</code>

<p>At first glance this seems to make your code simpler. But it brings complications elsewhere. For example, say you're analysing the volume of wood used in your toys. You want to find the mean, standard deviation and minimum values for this.</p>

<p>Only Blue Brick and Red Brick use wood. So these calculations should return 15, 7.07 (rounded), and 10 respectively.</p>

<p>But if you run the query below, you get different results:</p>

<code>select avg ( volume_of_wood ), 
       stddev ( volume_of_wood ), 
       min ( volume_of_wood )
from   toys;</code>

<p>This is because it includes the ""impossible"" value of -1 for Mr. Penguin.</p>

<p>To avoid this, you need to check that the volume_of_wood is greater than or equal to zero:</p>

<code>select avg ( volume_of_wood ), 
       stddev ( volume_of_wood ), 
       min ( volume_of_wood )
from   toys
where  volume_of_wood >= 0;</code>

<p>So while magic values appear to make life easier, they often bring bigger problems elsewhere.</p>",06-APR-18 10.00.53.282865000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-18 08.37.01.061570000 AM,"CHRIS.SAXON@ORACLE.COM"
142509292807155188794793500910979680673,142399856232522825415084831548294078164,"Try it!",35,"<p>Complete the scalar subquery below to find the minimum brick_id for each colour:</p>

<code>select c.colour_name, (

       ) min_brick_id
from   colours c
where  c.colour_name is not null;</code>

<p>The query should return the following rows:<p>

<pre><b>COLOUR_NAME   MIN_BRICK_ID   </b>
green                      4 
red                        6 
blue                       1 
purple                &lt;null&gt;
yellow                &lt;null&gt; 
orange                &lt;null&gt;</pre>",02-MAY-18 09.02.29.030870000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.06.59.059266000 PM,"CHRIS.SAXON@ORACLE.COM"
142504867728372329499221005498592303643,142399856232522825415084831548294078164,"Try it!",15,"<p>Complete the query below, using an inline view to find the min and max brick_id for each colour of brick:</p>

<code>select * from (

)</code>

<p>Hint: you need group by colour. Use min(brick_id) to find the minimum. The query should return the following rows (the order may be different):</p>

<pre><b>COLOUR   MIN(BRICK_ID)   MAX(BRICK_ID)   </b>
green                  4               5 
&lt;null&gt;                 9               9 
red                    6               8 
blue                   1               3</pre>",02-MAY-18 08.54.31.554215000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.07.37.985741000 PM,"CHRIS.SAXON@ORACLE.COM"
142509292807803173034106942148622191009,142399856232522825415084831548294078164,"NOT IN vs NOT EXISTS",24,"<p>You can do the reverse of IN & EXISTS by placing NOT in front of them. This returns you all the rows from the parent which don't have a match in the subquery.</p>

<p>For example to find all the rows in colours without a matching colour in bricks, you can use NOT EXISTS:</p>

<code>select * from colours c
where  not exists (
  select null from bricks b
  where  b.colour = c.colour_name
);</code>

<p>But do the same with NOT IN:</p>

<code>select * from colours c
where  c.colour_name not in (
  select b.colour from bricks b
);</code>

<p>And your query returns nothing!</p>

<p>This is because there is a row in bricks with a null colour. So the previous query is the same as:</p>

<code>select * from colours c
where c.colour_name not in (
  'red', 'green', 'blue', 
  'orange', 'yellow', 'purple',
  null
);</code>

<p>For the NOT IN condition to be true, comparing all its elements to the parent table must return false.</p>

<p>But remember that comparing anything to null gives unknown! So the whole expression is unknown and you get no data.</p>

<p>To resolve this, either use NOT EXISTS or add a where clause to stop the subquery returning null values:</p>

<code>select * from colours c
where c.colour_name not in (
  select b.colour from bricks b
  where  b.colour is not null
);</code>",02-MAY-18 09.18.23.841820000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 08.56.46.095572000 AM,"CHRIS.SAXON@ORACLE.COM"
142406337571972497372538608758287033535,142399856232522825415084831548294078164,"Common Table Expressions",50,"<p>Common table expressions (CTEs) enable you to name subqueries. You then refer to these like normal tables elsewhere in your query. This can make your SQL easier to write and understand later.</p>

<p>CTEs go in the with clause above the select statement. The following defines a CTE that counts how many rows of each colour there are in the bricks table:</p>

<code>with brick_colour_counts as (
  select colour, count(*) 
  from   bricks
  group  by colour
) 
  select * from brick_colour_counts ;</code>",01-MAY-18 10.23.10.684507000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 02.52.36.245907000 PM,"CHRIS.SAXON@ORACLE.COM"
142509292808794492206190938071881255329,142399856232522825415084831548294078164,"Try it!",80,"<p>Complete the following with clause to count how many rows there are in colours:</p>

<code>with colour_count as (

)
  select * from colour_count;</code>

<p>This should return</p>

<pre><b>COUNT(*)</b>
6</pre>",02-MAY-18 09.37.27.059092000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.11.36.474022000 PM,"CHRIS.SAXON@ORACLE.COM"
142637515221026819158513326345425825732,142628083376372792586797880077330433823,"Reversing the Order",30,"<p>You can invert the order by adding the desc keyword. This stands for descending.</p>

<p>So the following query sorts the rows from the most expensive toy to the cheapest:</p>

<code>select * from toys
order  by price desc;</code>",03-MAY-18 02.32.12.955863000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 02.32.12.955932000 PM,"CHRIS.SAXON@ORACLE.COM"
142634930253363278094570885794711249687,142628083376372792586797880077330433823,"Try It!",50,"<p>Complete the following query, so it sorts the rows by:</p>
<ol>
<li>Weight, lightest to heaviest</li>
<li>Toys with the same weight by purchased date from the last bought to the first:</li>
</ol>

<code>select toy_name, weight, purchased_date 
from   toys
</code>

<p>The query should return the rows in this order:</p>

<pre><b>TOY_NAME        WEIGHT   PURCHASED_DATE         </b>
Baby Turtle            1 01-SEP-2016
Miss Snuggles          4 01-FEB-2018
Purple Ninja           8 01-FEB-2018
Blue Dinosaur          8 01-JUL-2013
Kangaroo              10 01-MAR-2017</pre>",03-MAY-18 02.46.42.255908000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.35.15.997166000 AM,"CHRIS.SAXON@ORACLE.COM"
142637515221159800998670935554643505092,142628083376372792586797880077330433823,"Sorting Null",60,"<p>By default, Oracle Database sorts null last in the data:</p>

<code>select * from toys
order  by last_lost_date;</code>

<p>You can change this with the nulls clause. Set this to nulls first to get null-valued rows at the top:</p>

<code>select * from toys
order  by last_lost_date nulls first;</code>",03-MAY-18 02.48.24.210279000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 02.48.24.210337000 PM,"CHRIS.SAXON@ORACLE.COM"
142637515221165845627769008700517035972,142628083376372792586797880077330433823,"Try It!",70,"<p>Complete the query to sort the rows by:</p>
<ol>
<li>Price, cheapest to most expensive</li>
<li>Toys with same price by date last lost, from newest to oldest. Rows with a null last_lost_date should appear above any of the same price</li>
</ol>

<code>select toy_name, price, last_lost_date 
from   toys
</code>

<p>This should return the rows in this order:</p>
<pre><b>TOY_NAME        PRICE   LAST_LOST_DATE </b>
Baby Turtle           5 03-MAR-2017 
Miss Snuggles      9.99 01-JUN-2018 
Blue Dinosaur      9.99 01-NOV-2016 
Purple Ninja      29.99 <null>                 
Kangaroo          29.99 01-JUN-2018 </pre>",03-MAY-18 02.49.12.071272000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 04.35.38.677256000 PM,"CHRIS.SAXON@ORACLE.COM"
142634930253794864612173308410081354519,142628083376372792586797880077330433823,"Custom Sorting",80,"<p>Sometimes you want to order data in way that doesn't follow the standard sorting rules. For example, say you want to sort the toys by name. But Miss Snuggles is your favourite, so you want this to always appear at the top. All the toys after should appear alphabetically.</p>

<p>To do this, you need to map Miss Snuggles to a value lower than for all the other toys. For example, 1 for Miss Snuggles and 2 for everything else.</p>

<p>You can do this with a case expression in your order by. For example:</p>

<code>select * from toys
order  by case 
  when toy_name = 'Miss Snuggles' then 1 
  else 2
end, toy_name;</code>",03-MAY-18 03.03.25.394070000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 04.42.21.244172000 PM,"CHRIS.SAXON@ORACLE.COM"
142641886268412369071535506523417217001,142628083376372792586797880077330433823,"Try It!",120,"<p>Write a query to return the first three toys, ordered by toy_name</p>

<code>select toy_name 
from   toys</code>

<p>This should return the rows in this order:</p>

<pre><b>TOY_NAME        </b>
Baby Turtle     
Blue Dinosaur   
Kangaroo </pre>

<p>How many ways can you think of to write this query?</p>",03-MAY-18 04.17.59.365846000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 04.37.12.988022000 PM,"CHRIS.SAXON@ORACLE.COM"
142637515221651833807254089628748918724,142628083376372792586797880077330433823,"Top-N with Ties",110,"<p>When doing a top-N query sorting by non-unique values, you have an important question to answer:</p>

<p>Do you want exactly N <i>rows</i>, all the rows for the first N <i>values</i>, or N rows along with any that have the same value as the Nth?</p>

<p>For example, both Miss Snuggles and Blue Dinosaur have a price of 9.99. So if you fetch the three most expensive toys, you could get either of them!</p>

<p>You can guarantee you get both using fetch first. Swap only for ""with ties"". This will return you N rows, plus any that have the same value for the order by columns as the last. So you get both the 9.99 priced toys:</p>

<code>select toy_name, price from toys
order  by price desc
fetch  first 3 rows with ties;</code>

<p>You can get the same effect using a subquery by swapping row_number for rank:</p>

<code>select * from (
  select t.*, 
         rank() over ( order by price desc ) rn
  from   toys t
)
where  rn <= 3
order  by rn;</code>

<p>If you want all the rows for the first three values, use dense_rank in the subquery instead:</p>

<code>select * from (
  select t.*, 
         dense_rank() over ( order by price desc ) rn
  from   toys t
)
where  rn <= 3
order  by rn;</code>

<p>In all these examples you'll get at least three rows in your output. But it could be many more!</p>

<p>Usually you want to get exactly N rows. To do this and get the same rows every time, ensure there are no duplicated values in your order by columns.</p>

<p>If you want to learn more about the difference between rank, dense_rank and row_number, <a href=""https://www.youtube.com/watch?v=Tuxxbuywb1w"" target=""_blank"">watch this video</a>.</p>",03-MAY-18 03.37.47.422028000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.46.52.602960000 AM,"CHRIS.SAXON@ORACLE.COM"
142715333170857388230931573820504210347,142399856232522825415084831548294078164,"CTEs: Reusable Subqueries",55,"<p>Because you access CTEs in the same way as a regular table, you can use it many times in your query. This can help if you want to use this subquery many times. For example if you want to find:</p>
<ul>
<li>Which colours you have more bricks of than the minimum needed</li>
<li>The average number of bricks you have of each colour</li>
</ul>

<p>You need to group the bricks by colour. Then filter the colours table where this count is greater than the minimum_bricks_needed for that colour. And compute the mean of the counts.</p>

<p>You can do the filtering with a nested subquery. And show the average in a scalar subquery. This looks like:</p>

<code>select c.colour_name, 
       c.minimum_bricks_needed, (
         select avg ( count(*) )
         from   bricks b
         group  by b.colour
       ) mean_bricks_per_colour
from   colours c
where  c.minimum_bricks_needed < (
  select count(*) c
  from   bricks b
  where  b.colour = c.colour_name
  group  by b.colour
);</code>

<p>Note that ""group by colour"" appears twice in the statement. This creates maintenance problems. If you need to change this, say to join bricks to another table, you have to do this in two places.</p>

<p>Using CTEs, you can do the group by once. Then refer to it in your select:</p>

<code>with brick_counts as (
  select b.colour, count(*) c
  from   bricks b
  group  by b.colour
)
  select c.colour_name, 
         c.minimum_bricks_needed, (
           select avg ( bc.c )
           from   brick_counts bc
         ) mean_bricks_per_colour
  from   colours c
  where  c.minimum_bricks_needed < (
    select bc.c
    from   brick_counts bc
    where  bc.colour = c.colour_name
  );</code>

<p>So now if you need to join bricks to a new table to get the count you only have to edit the subquery brick_counts.</p>

<p>Oracle Database can also optimize queries that access the same CTE many times. This can make it more efficient than regular subqueries.</p>",04-MAY-18 08.23.40.300609000 AM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 03.09.36.879822000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784739506633693678562139840487172,143127379594936522093884822782011566106,"Windowing Clause",70,"<p>When you use order by, the database adds a default windowing clause of:</p>

<pre>range between unbounded preceding 
      and current row</pre>

<p>This means:</p>

<p>Include all the rows with a <b><i>value</i></b> less than or equal to that of the current row.</p>

<p>This can lead to the function including values from rows after the current!</p>

<p>For example, there are several rows with the same weight. So when you sort by this, all rows with the same weight have the same running count and weight:</p>

<code>select b.*, 
       count(*) over (
         order by weight
       ) running_total, 
       sum ( weight ) over (
         order by weight
       ) running_weight
from   bricks b
order  by weight;</code>

<p>Usually this isn't what you want. Normally running totals should only include values from previous rows in the data set.</p>

<p>To do this, you must specify a windowing clause of </p>

<pre>rows between unbounded preceding 
     and current row</pre>

<p>For example:</p>

<code>select b.*, 
       count(*) over (
         order by weight
         rows between unbounded preceding and current row
       ) running_total, 
       sum ( weight ) over (
         order by weight
         rows between unbounded preceding and current row
       ) running_weight
from   bricks b
order  by weight;</code>

<p>Note that this makes your results non-deterministic. Rows of the same weight could have their running totals in a different order each time you run the query.</p>

<p>To fix this, add columns to the order by until each set of values in the sort appears once in your results. This makes your results deterministic. Here that's the brick_id:</p>

<code>select b.*, 
       count(*) over (
         order by weight, brick_id
         rows between unbounded preceding and current row
       ) running_total, 
       sum ( weight ) over (
         order by weight, brick_id
         rows between unbounded preceding and current row
       ) running_weight
from   bricks b
order  by weight, brick_id;</code>",08-MAY-18 08.04.47.042061000 AM,"CHRIS.SAXON@ORACLE.COM",03-SEP-18 01.31.11.806039000 PM,"CHRIS.SAXON@ORACLE.COM"
143268521600957211554606776631780228329,143268521600193170436610330993365925097,"Introduction",10,"<p>This tutorial will show you how to pivot rows to columns and unpivot them back again. It uses this table of match results from a simple league:</p>

<code>select * from match_results</code>",09-MAY-18 04.15.10.470297000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 07.31.47.493374000 AM,"CHRIS.SAXON@ORACLE.COM"
143272175590483752543571928024718532694,143268521600193170436610330993365925097,"Manual Pivot",20,"<p>You want to see how many games have been played in each location. You can do this with group by:</p>

<code>select location, count (*)
from   match_results
group  by location;</code>

<p>But you want to show the locations as columns instead of rows. To do this you need to pivot the results.</p>

<p>You can do this by defining a column for each location. Each must only include rows where the location matches the column name.</p>

<p>Count only considers non-null values. So you can do this by checking the location inside the count. If this equals the column name, return a constant. Otherwise null.</p> 

<p>You can do this with a case expression like:</p>

<pre>count ( case when location = '&lt;location&gt;' then 1 end ) &lt;location&gt;</pre>

<p>The complete SQL query is:</p>

<code>select count ( case when location = 'Snowley' then 1 end ) snowley,
       count ( case when location = 'Coldgate' then 1 end ) coldgate, 
       count ( case when location = 'Dorwall' then 1 end ) dorwall,
       count ( case when location = 'Newdell' then 1 end ) newdell
from   match_results;</code>",09-MAY-18 04.16.02.705732000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 01.51.44.290009000 PM,"CHRIS.SAXON@ORACLE.COM"
143267768118213561783734513579957428233,143268521600193170436610330993365925097,"Implicit Group By",50,"<p>Any columns from the input table not in the pivot clause form an implicit group by. This can lead to the output having more rows than you expected.</p>

<p>For example, if you place pivot after the from clause, all the table's columns are inputs to it. So you get one row for every match<p>

<code>select * from match_results
pivot (
  count(*) for location in (
    'Snowley', 'Coldgate', 'Dorwall', 'Newdell' 
  )
);</code>

<p>To avoid this, use an inline view or CTE. This selects only the columns you'll use in the pivot or want in the final output</p>",09-MAY-18 04.17.06.594714000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 01.57.53.689048000 PM,"CHRIS.SAXON@ORACLE.COM"
143267768118223233190291430613355077641,143268521600193170436610330993365925097,"Dynamic Pivoting",110,"<p>Often you'll want to change the columns pivot adds. For example, in many leagues the teams change each season. So every year you'll need to change queries that pivot by location. It'd be handy if these changed dynamically.</p>

<p>Sadly regular pivot doesn't allow you to do this. The values in the IN list are fixed. You can't use subqueries or variables to change them.</p>

<p>Luckily you can have a dynamic IN list using XML pivoting. This enables you to pass values from a subquery. Or generate totals for every value in the pivot column using ANY:</p>

<code>with rws as (
  select location
  from   match_results
)
  select xmlserialize ( -- formats an XML document
           document location_xml as clob indent size=2
         ) location_xml
  from   rws
  pivot xml (
    count (*) matches for location in ( any )
  );</code>

<p>But you get the pivoted values as a single XML document! So to extract the values, you need to manipulate the XML.</p>",09-MAY-18 04.17.55.834532000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.07.45.575037000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784739466739141631279377075183364,143127379594936522093884822782011566106,"Order By",40,"<p>The order by clause enables you to compute running totals. For example, the following sorts the rows by brick_id. Then shows the total number of rows and sum of the weights for rows with a brick_id less than or equal to that of the current row:</p>

<code>select b.*, 
       count(*) over (
         order by brick_id
       ) running_total, 
       sum ( weight ) over (
         order by brick_id
       ) running_weight
from   bricks b;</code>",08-MAY-18 08.03.59.525430000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 10.16.33.678770000 AM,"CHRIS.SAXON@ORACLE.COM"
143121596518464197252673861135828259912,143127379594936522093884822782011566106,"Try it!",50,"<p>Complete the following query to get the running average weight, ordered by brick_id:</p>

<code>select b.brick_id, b.weight,
       round ( avg ( weight ) over (
         order /* TODO */
       ), 2 ) running_average_weight
from   bricks b
order  by brick_id;</code>

<p>The updated query should give this output:</p>

<pre><b>BRICK_ID   WEIGHT   RUNNING_AVERAGE_WEIGHT   </b>
         1        1                        1 
         2        2                      1.5 
         3        1                     1.33 
         4        2                      1.5 
         5        3                      1.8 
         6        1                     1.67</pre>",08-MAY-18 08.04.11.942099000 AM,"CHRIS.SAXON@ORACLE.COM",05-FEB-19 04.32.41.428990000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784739498171212941259735617543940,143127379594936522093884822782011566106,"Partition By + Order By",60,"<p>You can combine the partition by and order by clauses to get running totals within a group. For example, the following splits the rows by colour.  It then gets the running count and weight of rows for each colour, sorted by brick_id:</p>

<code>select b.*, 
       count(*) over (
         partition by colour
         order by brick_id
       ) running_total, 
       sum ( weight ) over (
         partition by colour
         order by brick_id
       ) running_weight
from   bricks b;</code>",08-MAY-18 08.04.29.196629000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 10.17.35.120754000 AM,"CHRIS.SAXON@ORACLE.COM"
143131784739336175153112899426206916356,143127379594936522093884822782011566106,"Introduction",10,"<p>Aggregate and analytic functions both enable you to do a calculation over many rows. Aggregate functions squash the output to one row per group. For example the following counts the total rows in the table. It returns one row:</p>

<code>select count(*) from bricks;</code>

<p>Adding the over clause converts it to an analytic. This preserves the input rows. So you get all six, each with the value six:</p>

<code>select count(*) over () from bricks;</code>

<p>This allows you to see the values from all the other columns, which you can't using group by:</p>

<code>select b.*, 
       count(*) over () total_count 
from   bricks b;</code>",08-MAY-18 08.03.13.076596000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 08.30.55.281634000 AM,"CHRIS.SAXON@ORACLE.COM"
143131784739348264411309045717953978116,143127379594936522093884822782011566106,"Partition By",20,"<p>The group by clause splits rows into groups of the same value. For example, the following get the number of rows and total weight for each colour:</p>

<code>select colour, count(*), sum ( weight )
from   bricks
group  by colour;</code>

<p>You can carve up the input to an analytic function like this with the partition by clause. The following returns the total weight and count of rows of each colour. It includes all the rows:</p>

<code>select b.*, 
       count(*) over (
         partition by colour
       ) bricks_per_colour, 
       sum ( weight ) over (
         partition by colour
       ) weight_per_colour
from   bricks b;</code>",08-MAY-18 08.03.26.287762000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 10.15.57.554670000 AM,"CHRIS.SAXON@ORACLE.COM"
143272175590530900650536898562532073558,143268521600193170436610330993365925097,"Expressions",60,"<p>Sometimes you may want to manipulate the values you're pivoting by. For example, you may want to see the number of games played each month, showing the months as columns.</p>

<p>So you need to convert the match dates to months. You can extract a month's abbreviation from a date using to_char with the format mask 'MON'.</p>

<p>But if you do this in your pivot clause, you'll get an error:</p>

<code>with rws as (
  select match_date from match_results
)
  select * from rws
  pivot (
    count (*) for to_char ( match_date, 'MON' ) in (
      'JAN', 'FEB', 'MAR'
    )
  );</code>

<p>To fix this, extract the month in an inline view or CTE. Alias the expression and use this alias in the pivot. For example:</p>
  
<code>with rws as (
  select to_char ( match_date, 'MON' ) match_month 
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_month in (
      'JAN', 'FEB', 'MAR'
    )
  );</code>

<p>Often you want to change values in the columns you're pivoting on. This and pivot's implicit grouping means it's a good idea to pivot the output of a CTE. This makes it easy to select and manipulate the columns you want.</p>",09-MAY-18 04.17.22.270562000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 01.59.24.382022000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784739532021135890469352509316868,143127379594936522093884822782011566106,"Sliding Windows",80,"<p>As well as running totals so far, you can change the windowing clause to be a subset of the previous rows.</p>

<p>The following shows the total weight of:</p>

<ol>
<li>The current row + the previous row</li>
<li>All rows with the same weight as the current + all rows with a weight one less than the current</li>
</ol>

<code>select b.*, 
       sum ( weight ) over (
         order by weight
         rows between 1 preceding and current row
       ) running_row_weight, 
       sum ( weight ) over (
         order by weight
         range between 1 preceding and current row
       ) running_value_weight
from   bricks b
order  by weight, brick_id;</code>

<p>You can also change ""current row"" to rows or values after the current. Do this by specifying a number following. For example, this query computes the sliding weights including rows or values either side of the current row:</p>

<code>select b.*, 
       sum ( weight ) over (
         order by weight
         rows between 1 preceding and 1 following
       ) sliding_row_window, 
       sum ( weight ) over (
         order by weight
         range between 1 preceding and 1 following
       ) sliding_value_window
from   bricks b
order  by weight;</code>

<p>You can also offset the window, so it excludes the current row! You can do this either side of the current row.</p>

<p>For example, the following query has two counts. The first shows the number of rows with a weight one or two less than the current. The second counts those with weights greater than the current. So if the current weight = 2, the first count includes rows with the weight 0 or 1. The second rows with weight 3 or 4:</p>

<code>select b.*, 
       count (*) over (
         order by weight
         range between 2 preceding and 1 preceding 
       ) count_weight_2_lower_than_current, 
       count (*) over (
         order by weight
         range between 1 following and 2 following
       ) count_weight_2_greater_than_current
from   bricks b
order  by weight;</code>",08-MAY-18 08.05.03.820486000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 10.21.38.601201000 AM,"CHRIS.SAXON@ORACLE.COM"
143131784739570706762118137486099914500,143127379594936522093884822782011566106,"More Analytic Functions",105,"<p>You can add the over clause to aggregate functions to make them an analytic. But there are many functions which need the over clause.</p>

<p>Here is an overview of common analytic functions</p>

<h3>Row numbering</h3>

<p>The analytic functions rank, dense_rank and row_number all return an increasing counter, starting at one.</p>

<ul>
<li>Rank - Rows with the same value in the order by have the same rank. The next row after a tie has the value N, where N is its position in the data set.</li>
<li>Dense_rank - Rows with the same value in the order by have the same rank, but there are no gaps in the ranks</li>
<li>Row_number - each row has a new value</li>
</ul>

<p>For example:</p>

<code>select brick_id, weight, 
       row_number() over ( order by weight ) rn, 
       rank() over ( order by weight ) rk, 
       dense_rank() over ( order by weight ) dr
from   bricks;</code>

<h3>Previous and Next Values</h3>

<p>Lag and lead enable you to get values from rows backwards and forwards in your results.</p>

<code>select b.*,
       lag ( shape ) over ( order by brick_id ) prev_shape,
       lead ( shape ) over ( order by brick_id ) next_shape
from   bricks b;</code>

<h3>First and Last Values</h3>

<p>You can get the first or last value in an ordered set with first_value and last_value:</p>

<code>select b.*,
       first_value ( weight ) over ( 
         order by brick_id 
       ) first_weight_by_id,
       last_value ( weight ) over ( 
         order by brick_id 
       ) last_weight_by_id
from   bricks b;</code>

<p>Note the result of first_value stays the same. But for last_value it changes for each row. This is because the default windowing clause stops at the current row. To find the value from the last row in the data set, you change the end of the window to ""unbounded following"". For example:</p>

<code>select b.*,
       first_value ( weight ) over ( 
         order by brick_id 
       ) first_weight_by_id,
       last_value ( weight ) over ( 
         order by brick_id 
         range between current row and unbounded following
       ) last_weight_by_id
from   bricks b;</code>

<p>To learn more about these and other pure analytic functions in Oracle Database, check <a href=""https://docs.oracle.com/en/database/oracle/oracle-database/18/sqlrf/Analytic-Functions.html"">the docs</a>.</p>",08-MAY-18 08.05.37.921585000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 10.30.45.619700000 AM,"CHRIS.SAXON@ORACLE.COM"
143272175590718284152577166084611530838,143268521600193170436610330993365925097,"Filtering Pivoted Rows",65,"<p>You can filter the output of pivot using a where clause. This goes after the pivot.</p>

<p>For example, say you want to show a table of the number of matches played each month in each location. You can do this with:</p>

<code>with rws as (
  select location, to_char ( match_date, 'MON' ) match_month 
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_month in (
      'JAN', 'FEB', 'MAR'
    )
  )</code>

<p>If you want to restrict this to those locations which had at least one match in January, add the where clause <b>""'JAN'"" > 0</b> at the end:</p>

<code>with rws as (
  select location, to_char ( match_date, 'MON' ) match_month 
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_month in (
      'JAN', 'FEB', 'MAR'
    )
  )
  where  ""'JAN'"" > 0</code>",09-MAY-18 04.19.47.167146000 PM,"CHRIS.SAXON@ORACLE.COM",15-MAY-18 09.59.59.398044000 AM,"CHRIS.SAXON@ORACLE.COM"
143263422229191455755699307605176114376,143268521600193170436610330993365925097,"Unpivot Clause",140,"<p>Instead you could use the unpivot clause. This was also introduced in Oracle Database 11g and makes unpivoting easier.</p>

<ol>
<li>A new column storing the pivoted values</li>
<li>Another new column stating the source column of these values</li>
<li>The columns to become rows</li>
</ol>

<p>This gives the following statement:</p>

<code>select match_date, location, home_or_away, team 
from   match_results
unpivot (
  team for home_or_away in ( 
    home_team_name as 'HOME', away_team_name as 'AWAY'
  )
)
order  by match_date, location, home_or_away;</code>",09-MAY-18 04.18.16.924320000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.36.12.157923000 PM,"CHRIS.SAXON@ORACLE.COM"
142401775233551128055758654332820308789,142399856232522825415084831548294078164,"Literate SQL",60,"<p><a href=""https://en.wikipedia.org/wiki/Literate_programming"">Literate programming</a> is a concept introduced by Donald Knuth. The idea is to write code that makes sense if you read it like a book: from start to finish.</p>

<p>Simple SQL queries follow this principle. For example the query <i>""Get me the brick_ids of all the bricks that are red or blue""</i> is the following statement:</p>

<code>select brick_id 
from   bricks 
where  colour in ('red', 'blue');</code>

<p>But subqueries usually break this human readable flow. For example, if you want to find which bricks you have less of than the average number of each colour, you need to:</p>
<ol>
<li>Count the bricks by colour</li>
<li>Take the average of these counts</li>
<li>Return those rows where the value in step 1 is less than in step 2</li>
</ol>

<p>Without the with clause, you need to write something like the following:</p>

<code>select colour
from   bricks
group  by colour  
having count (*) < (
  select avg ( colour_count ) 
  from   (
    select colour, count (*) colour_count
    from   bricks
    group  by colour  
  )
);</code>

<p>Step one is at the bottom of the query! Using CTEs, you can order the subqueries to match the logic above:</p>

<code>with brick_counts as ( 
  -- 1. Count the bricks by colour
  select b.colour, count(*) c
  from   bricks b
  group  by b.colour
), average_bricks_per_colour as ( 
  -- 2. Take the average of these counts
  select avg ( c ) average_count
  from   brick_counts
)
  select * from brick_counts bc  
  join   average_bricks_per_colour ac
  -- 3. Return those rows where the value in step 1 is less than in step 2
  on     bc.c < average_count; </code>

<p>This makes it easier to figure out what a SQL statement does when you return to it later.</p>",01-MAY-18 10.26.08.474809000 AM,"CHRIS.SAXON@ORACLE.COM",25-APR-23 09.05.56.018440000 AM,"CHRIS.SAXON@ORACLE.COM"
142628083375753822567155189939880871711,142399856232522825415084831548294078164,"Introduction",5,"<p>This tutorial shows you how to write subqueries. It uses the bricks and colours table. Run the queries below to see their contents:</p>

<code>select * from bricks;

select * from colours;</code>

<p>Note: This tutorial makes heavy use of group by. If you want a refresher on this, check out module 7 of <a href=""https://devgym.oracle.com/devgym/database-for-developers.html"">Databases for Developers: Foundations</a></p>",03-MAY-18 12.58.13.995067000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 02.46.20.480156000 PM,"CHRIS.SAXON@ORACLE.COM"
142636704917386740106919386869985455505,142628083376372792586797880077330433823,"Unordered Data",10,"<p>Queries without an order by can return data in any sequence. When accessing a single table, this is often the order you inserted the rows into the table.</p>

<p>For example, the rows in the toys table are out-of-sequence for all its columns:</p>

<code>select * from toys;</code>",03-MAY-18 02.26.54.620515000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.32.35.122626000 AM,"CHRIS.SAXON@ORACLE.COM"
142636704917740955372066473218174365073,142628083376372792586797880077330433823,"The Order By Clause",20,"<p>To guarantee the rows appear in a given sequence, you must use an order by. This sorts numbers from smallest to largest. So to sort the toys from cheapest to most expensive, order by price:</p>

<code>select * from toys
order  by price;</code>

<p>Dates sort from oldest to newest. So the following sorts the toys with the most recently purchased last:</p>

<code>select * from toys
order  by purchased_date;</code>

<p>And character data sorts alphabetically. So this sorts by the toy's names:</p>

<code>select * from toys
order  by toy_name;</code>",03-MAY-18 02.30.51.038122000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.33.06.811556000 AM,"CHRIS.SAXON@ORACLE.COM"
142636704917849758695831789843897920913,142628083376372792586797880077330433823,"Resolving Ties",40,"<p>There are two toys priced at 9.99 (Miss Snuggles & Blue Dinosaur) and two at 29.99 (Kangaroo & Purple Ninja). If you sort by only price, the database can return each of these either way round.</p>

<p>To avoid this and guarantee a particular sequence, add more columns to your order by. Do this until each set of values appear in only one row.</p>

<p>You can do this here by adding toy_name:</p>

<code>select toy_name, price from toys
order  by price, toy_name;</code>",03-MAY-18 02.33.35.391038000 PM,"CHRIS.SAXON@ORACLE.COM",03-MAY-18 04.32.41.403349000 PM,"CHRIS.SAXON@ORACLE.COM"
142636704919582149395339553451251871121,142628083376372792586797880077330433823,"Positional Notation vs. Aliases",90,"<p>It can be tricky to debug custom sorting functions. To help with this, include the expression in your select:</p>

<code>select t.*, 
       case 
         when toy_name = 'Miss Snuggles' then 1 
         else 2
       end
from   toys t
order  by case 
         when toy_name = 'Miss Snuggles' then 1 
         else 2
       end, toy_name;</code>

<p>But this means you have the function in two places! This makes code maintenance harder.</p>

<p>You can avoid this by using positional notation or aliases.</p>

<b>Positional notation</b>

<p>This is where you put the number of the column in the select you want to order by (working from left to right in the output). So the following sorts by the sixth column then the first. i.e. the case expression then toy_name:</b>

<code>select t.*, 
       case 
         when toy_name = 'Miss Snuggles' then 1 
         else 2
       end
from   toys t
order  by 6, 1;</code>

<p>But this makes it hard to spot which column you're sorting by. Particularly if the query uses ""select *"". And if you need to change the columns you're selecting, it's easy to overlook the positional order by. Leading to wrong results!</p>

<b>Aliases</b>

<p>It's better to give the function an alias. Then refer to this alias in the order by clause:</p>

<code>select t.*, 
       case 
         when toy_name = 'Miss Snuggles' then 1 
         else 2
       end custom_sort
from   toys t
order  by custom_sort, toy_name;</code>

<p>This makes it clearer what you're doing. And your code more resilient to changes!</p>",03-MAY-18 03.04.19.511491000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.38.35.456552000 AM,"CHRIS.SAXON@ORACLE.COM"
142640817288855692638604167338997343845,142628083376372792586797880077330433823,"Top-N Queries",100,"<p>A top-N query returns the first N rows in a sorted data set. For example, to find the three cheapest toys.</p> 

<p>There are several way to do this in Oracle Database</p>

<b>Rownum</b>

<p>Rownum is an Oracle-specific function. It assigns an increasing number to each row you fetch.</p>

<p>But if you use it in a where clause before the order by, you'll get unexpected results. For example, the following tries to get the three most expensive toys:</p>

<code>select * from toys
where  rownum <= 3
order  by price desc;</code>

<p>But it includes the cheapest, Baby Turtle! This is because the database processes order by after where. So this gets <i>any</i> three rows. Then sorts them.</p>

<p>To fix this, sort the data in a subquery. Then filter the results of this:</p>

<code>select * from (
  select *
  from   toys t
  order  by price desc
)
where  rownum <= 3;</code>

<b>Row_number</b>

<p>Row_number is an analytic function. Like rownum, it assigns an incrementing counter. This is determined by the sort defined in the order by in the over clause.</p>

<p>To use this in a top-N query, you must also use a subquery:</p>

<code>select * from (
  select t.*, row_number() over (order by price desc) rn
  from   toys t
)
where  rn <= 3
order  by rn;</code>

<b>Fetch first</b>

<p>Oracle Database 12c introduced the ANSI compliant fetch first clause. This goes after the order by and removes the need to use a subquery:</p>

<code>select * from toys
order  by price desc
fetch  first 3 rows only;</code>",03-MAY-18 03.22.41.741453000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.41.10.655338000 AM,"CHRIS.SAXON@ORACLE.COM"
142645128603093290806615628054419789470,142628083376372792586797880077330433823,"Try It!",95,"<p>Complete the query, so:</p>
<ul>
<li>Kangaroo is top</li>
<li>Blue Dinosaur is second</li>
<li>The remaining toys are ordered by price, cheapest to most expensive</li>
</ul>

<code>select t.toy_name, t.price,
       case
         when toy_name = 'Kangaroo'
         when toy_name = 'Blue Dinosaur'
         else 
       end custom_sort
from   toys t
order  by </code>

<p>This should return the rows in this order:</p>
<pre><b>TOY_NAME        PRICE   </b>
Kangaroo          29.99      
Blue Dinosaur      9.99      
Baby Turtle           5      
Miss Snuggles      9.99      
Purple Ninja      29.99      </pre>",03-MAY-18 04.15.11.508011000 PM,"CHRIS.SAXON@ORACLE.COM",04-MAY-18 05.39.45.939173000 AM,"CHRIS.SAXON@ORACLE.COM"
143131784739354309040407118863827508996,143127379594936522093884822782011566106,"Try it!",30,"<p>Complete the following query to return the count and average weight of bricks for each shape:</p>

<code>select b.*, 
       count(*) over (
         partition /* TODO */
       ) bricks_per_shape, 
       median ( weight ) over (
         partition /* TODO */
       ) median_weight_per_shape
from   bricks b
order  by shape, weight, brick_id;</code>

<p>The updated query should give this output:</p>

<pre><b>BRICK_ID   COLOUR   SHAPE     WEIGHT   BRICKS_PER_SHAPE   MEDIAN_WEIGHT_PER_SHAPE   </b>
         1 blue     cube             1                  3                         1 
         3 red      cube             1                  3                         1 
         4 red      cube             2                  3                         1 
         6 green    pyramid          1                  3                         2 
         2 blue     pyramid          2                  3                         2 
         5 red      pyramid          3                  3                         2 </pre>",08-MAY-18 08.03.45.484814000 AM,"CHRIS.SAXON@ORACLE.COM",05-FEB-19 04.32.22.438610000 PM,"CHRIS.SAXON@ORACLE.COM"
143445658330119123169524953204361330577,143268521600193170436610330993365925097,"Unpivoting",130,"<p>Unpivoting is the process of taking columns and converting them to rows. For example, you may want to convert the home & away team names to a single team column.</p>

<p>You can do a DIY unpivot using union all. This will query the source table once for each column you want to become a row.</p>

<p>For example, to get a row for the home and away teams for each match, you need two queries like so:</p>

<code>select match_date, location, 'HOME' home_or_away, home_team_name team
from   match_results
union  all
select match_date, location, 'AWAY' home_or_away, away_team_name team
from   match_results
order  by match_date, location, home_or_away;</code>",11-MAY-18 08.21.05.726751000 AM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.09.37.784100000 PM,"CHRIS.SAXON@ORACLE.COM"
144380801214809597616436736835021755346,144380801214805970838977892947497636818,"Union All",30,"<p>Often when combining tables you want to see all the rows. Including duplicates. Not the list of distinct values.</p>

<p>You can do this by sticking all after union:</p>

<code>select colour, shape from my_brick_collection
union all 
select colour, shape from your_brick_collection;</code>

<p>A standard union is the same as the following:</p>

<code>select distinct * from (
  select colour, shape from my_brick_collection
  union all 
  select colour, shape from your_brick_collection
);</code>

<p>The distinct operator adds an extra sorting step to your SQL. So in most cases you'll want to use union all. Save plain union for when you know you want to remove duplicate rows.</p>",20-MAY-18 12.40.55.497585000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 03.02.52.523422000 PM,"CHRIS.SAXON@ORACLE.COM"
143272175591090633305018471870421033046,143268521600193170436610330993365925097,"Pivoting Many Values",80,"<p>You can pivot many values. For example, say for each month you want columns showing:</p>
<ul>
<li>The number of matches played</li>
<li>The total points scored by the home team</li>
<li>The total points scored by the away team</li>
</ul>

<p>You can do this by adding:</p>

<ul>
<li>count(*)</li>
<li>sum ( home_team_points )</li>
<li>sum ( away_team_points )</li>
</ul>

<p>To the first part of the pivot. Note you must alias all except one of these. The generated column names are the aliases appended to the IN list headings:</p>

<code>with rws as (
  select location, to_char ( match_date, 'MON' ) match_month ,
         home_team_points, away_team_points
  from   match_results
)
  select * from rws
  pivot (
    count (*) matches, 
    sum ( home_team_points ) home_points,
    sum ( away_team_points ) away_points
    for match_month in (
      'JAN' jan, 'FEB' feb, 'MAR' mar
    )
  );</code>

<p>Each new aggregate will have a new column for every value in the IN list. So the total number of new columns is:</p>

<pre>Number of aggregate columns * Number of IN list values</pre>",09-MAY-18 04.21.15.323935000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.04.54.021184000 PM,"CHRIS.SAXON@ORACLE.COM"
143263422229197500384797380751049645256,143268521600193170436610330993365925097,"Try It!",150,"<p>Complete the following query to unpivot the home and away points for each match:</p>

<code>select match_date, location, home_or_away, points 
from   match_results
unpivot (
  
  home_or_away in ( 
    home_team_points as 'HOME', away_team_points as 'AWAY'
  )
)
order  by match_date, location, home_or_away;</code>

<p>This should give this output:</p>

<pre><b>MATCH_DATE    LOCATION   HOME_OR_AWAY   POINTS   </b>
01-JAN-2018   Coldgate   AWAY                  4 
01-JAN-2018   Coldgate   HOME                  1 
01-JAN-2018   Snowley    AWAY                  0 
01-JAN-2018   Snowley    HOME                  2 
01-FEB-2018   Dorwall    AWAY                  1 
01-FEB-2018   Dorwall    HOME                  0 
01-MAR-2018   Coldgate   AWAY                  3 
01-MAR-2018   Coldgate   HOME                  3 
02-MAR-2018   Newdell    AWAY                  0 
02-MAR-2018   Newdell    HOME                  8</pre>",09-MAY-18 04.19.27.597123000 PM,"CHRIS.SAXON@ORACLE.COM",15-MAY-18 10.17.58.657950000 AM,"CHRIS.SAXON@ORACLE.COM"
143263422229185411126601234459302583496,143268521600193170436610330993365925097,"Try it!",40,"<p>Complete this query to show the locations as columns, with date of the last match played in each:</p>

<p><i>Hint: to find the last date in a set, use max</i><p>

<code>with rws as (
  select location, match_date from match_results
)
  select * from rws
  pivot (
    
    for location in (
      'Snowley', 'Coldgate', 'Dorwall', 'Newdell' 
    )
  );</code>

<p>The output of this query should be:</p>
<pre><b>'Snowley'     'Coldgate'    'Dorwall'     'Newdell'     </b>
01-JAN-2018   01-MAR-2018   01-FEB-2018   02-MAR-2018</pre>

<p>Complete this query to show the team names as columns, with the number of home games each has played:</p>

<code>with rws as (
  select home_team_name from match_results
)
  select * from rws
  pivot (
    count (*)
    for 
    in (

    )
  );</code>

<p>The team names are:</p>
<ul>
<li>Underrated United</li>
<li>Average Athletic</li>
<li>Terrible Town</li>
<li>Champions City</li></ul>
<p>And the output of this query should be:</p>
<pre><b>'Underrated United'   'Average Athletic'   'Terrible Town'   'Champions City' </b>  
                    1                    2                 1                  1 
</pre>",09-MAY-18 04.16.46.748705000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 01.56.25.584767000 PM,"CHRIS.SAXON@ORACLE.COM"
144493783695822550064666151320109746878,144380801214805970838977892947497636818,"Symmetric Difference with Group By",80,"<p>There are a couple of drawbacks to both the previous methods. Firstly you have to read all the rows from both tables twice. Secondly minus and intersect return distinct values. So you only see the values only in one table. Not all the rows. </p>

<p>So if you get an extra red cube, you have one more than me. But the queries don't show this:</p>

<code>insert into your_brick_collection values ( 4, 4, 4, 'red', 'cube' );

select * from ( 
  select colour, shape from your_brick_collection
  minus
  select colour, shape from my_brick_collection
) union all (
  select colour, shape from my_brick_collection
  minus
  select colour, shape from your_brick_collection
);</code>

<p>Luckily there's an alternative which solves both of these issues.</p>

<p>Check whether each table has the same number of rows for each set of values.</p>

<p>You can do this by union alling the two tables together with a couple of extra columns. One to count the rows from the first table, the other for the second. By returning the values 1 or 0 you can sum these up to get the count.</p>

<p>To see the different rows, return those where these sums are not equal in the having clause.</p>

<p>This gives the following query:</p>

<code>select colour, shape, sum ( your_bricks ), sum ( my_bricks ) 
from (
  select colour, shape, 1 your_bricks, 0 my_bricks
  from   your_brick_collection
  union all
  select colour, shape, 0 your_bricks, 1 my_bricks 
  from   my_brick_collection
)
group  by colour, shape
having sum ( your_bricks ) <> sum ( my_bricks );</code>

<p>Using this method you only read the rows in each table once. So this can be notably faster than the classic way. And you can see how many rows have each set of values in each table.</p>

<p>This leads to another benefit of this method: it enables you to see which table has more rows. You can do this by comparing the sum of bricks for each colour and shape. Then return the name of the table that has more. You can also get the number of extra rows. Do this by finding the absolute value of the difference between these two sums.</p>

<p>For example:</p>

<code>select colour, shape, 
       case
         when sum ( your_bricks ) < sum ( my_bricks ) then 'ME'
         when sum ( your_bricks ) > sum ( my_bricks ) then 'YOU'
         else 'EQUAL'
       end who_has_extra,
       abs ( sum ( your_bricks ) - sum ( my_bricks ) ) how_many
from (
  select colour, shape, 1 your_bricks, 0 my_bricks
  from   your_brick_collection
  union all
  select colour, shape, 0 your_bricks, 1 my_bricks 
  from   my_brick_collection
)
group  by colour, shape;</code>",21-MAY-18 08.59.02.448182000 AM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 03.11.30.542071000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784739574333539576981373624033028,143127379594936522093884822782011566106,"Try It!",90,"<p>Complete the windowing clauses to return:</p>

<ul>
<li>The minimum colour of the two rows before (but not including) the current row</li>
<li>The count of rows with the same weight as the current and one value following</li>
</ul>

<code>select b.*, 
       min ( colour ) over (
         order by brick_id
         rows /* TODO */
       ) first_colour_two_prev, 
       count (*) over (
         order by weight
         range /* TODO */
       ) count_values_this_and_next
from   bricks b
order  by weight;</code>

<p>The output of the updated query should be:</p>

<pre><b>BRICK_ID   COLOUR   SHAPE     WEIGHT   FIRST_COLOUR_TWO_PREV   COUNT_VALUES_THIS_AND_NEXT 
</b>         1 blue     cube             1 &lt;null&gt;                                             5 
         3 red      cube             1 blue                                               5 
         6 green    pyramid          1 red                                                5 
         4 red      cube             2 blue                                               3 
         2 blue     pyramid          2 blue                                               3 
         5 red      pyramid          3 red                                                1</pre>",08-MAY-18 08.05.54.015682000 AM,"CHRIS.SAXON@ORACLE.COM",05-FEB-19 04.34.25.319280000 PM,"CHRIS.SAXON@ORACLE.COM"
143131784740947673270659200116090248964,143127379594936522093884822782011566106,"Filtering Analytic Functions",100,"<p>Often you want to filter rows using the result of an aggregate. For example, to find all the colours you have two or more bricks of. You can do this with group by using the having clause:</p>

<code>select colour from bricks
group  by colour
having count(*) >= 2;</code>

<p>But you can't see the other columns! One way to include these is to partition the count by colour.</p>

<p>But the database processes analytic functions after the where clause. So you can't use this in the where. The following raises an error:</p>

<code>select colour from bricks
where  count(*) over ( partition by colour ) >= 2;</code>

<p>To resolve this, you must use the analytic in a subquery. Then filter it in the outer query. For example:</p>

<code>select * from (
  select b.*,
         count(*) over ( partition by colour ) colour_count
  from   bricks b
)
where  colour_count >= 2;</code>",08-MAY-18 08.23.09.866046000 AM,"CHRIS.SAXON@ORACLE.COM",10-MAY-18 08.46.05.980861000 AM,"CHRIS.SAXON@ORACLE.COM"
143341653863966066440059080768532280636,143127379594936522093884822782011566106,"Try It!",102,"<p>Complete the following query to find the rows where </p>

<ul>
<li>The total weight for the shape</li>
<li>The running weight by brick_id</li>
</ul>

<p>are both greater than four:</p>

<code>with totals as (
  select b.*,
         sum ( weight ) over ( 
           /* TODO */
         ) weight_per_shape,
         sum ( weight ) over ( 
           /* TODO */
         ) running_weight_by_id
  from   bricks b
)
  select * from totals
  where  /* TODO */
  order  by brick_id</code>

<p>The updated query should give the following output:</p>

<pre><b>BRICK_ID   COLOUR   SHAPE     WEIGHT   WEIGHT_PER_SHAPE   RUNNING_WEIGHT_BY_ID   </b>
         5 red      pyramid          3                  6                      9 
         6 green    pyramid          1                  6                     10 
</pre>",10-MAY-18 08.52.18.226937000 AM,"CHRIS.SAXON@ORACLE.COM",05-FEB-19 04.31.52.616962000 PM,"CHRIS.SAXON@ORACLE.COM"
143272175590493423950128845058116182102,143268521600193170436610330993365925097,"Pivot Clause",30,"<p>A manual pivot is clunky. It's easy to make mistakes and hard to read when you have many columns.</p>
<p>The pivot clause, introduced in Oracle Database 11g, allows you to do the same more easily.</p>

<p>To use it, specify three things:</p>

<ol>
<li>The values to go in the location columns. You want a count, so this is count(*)</li>
<li>The column containing values you want to become new columns. Here that's location</li>
<li>The list of values to become columns. This is the names of the locations.</li>
</ol>

<p>Altogether this gives:</p>

<code>with rws as (
  select location from match_results
)
  select * from rws
  pivot (
    count(*) for location in (
      'Snowley', 'Coldgate', 'Dorwall', 'Newdell' 
    )
  );</code>

<p>Now if you need more locations, add them to the IN list!</p>",09-MAY-18 04.16.25.762800000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 01.55.54.666233000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171339295068215780569602904565,144380801214805970838977892947497636818,"Set Difference",40,"<p>The set difference operation returns all the rows in one table not in another. You can do this with not exists. For example:</p>

<code>select colour, shape from your_brick_collection ybc
where  not exists (
  select null from my_brick_collection mbc
  where  ybc.colour = mbc.colour
  and    ybc.shape = mbc.shape
);
</code>

<p>If you're comparing many columns this is a lot of typing. And tricky to understand later.</p>

<p>But there's a bigger issue. It handles nulls incorrectly! Because null = null => unknown, it returns rows where shape or colour is null. So the null-coloured cuboid is in the output. Even though this row exists in both tables!</p>

<p>To fix this, test if the columns are equal or both are null:</p>

<code>select colour, shape from your_brick_collection ybc
where  not exists (
  select null from my_brick_collection mbc
  where  ( ybc.colour = mbc.colour or 
           ( ybc.colour is null and mbc.colour is null ) 
         )
  and    ( ybc.shape = mbc.shape or
           ( ybc.shape is null and mbc.shape is null ) 
         )
);</code>",20-MAY-18 12.41.16.950935000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 03.03.25.341875000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171357428955510000007223497205,144380801214805970838977892947497636818,"Finding Common Values",50,"<p>You can find values that are in both table with exists. As with not exists, do to this correctly you need to test for null in the subquery:</p>

<code>select colour, shape from your_brick_collection ybc
where  exists (
  select null from my_brick_collection mbc
  where  ( ybc.colour = mbc.colour or ( ybc.colour is null and mbc.colour is null ) )
  and    ybc.shape = mbc.shape
);
</code>",20-MAY-18 12.41.28.963401000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 02.50.13.922781000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171333250439117707423729373685,144380801214805970838977892947497636818,"Introduction",10,"<p>The set operators union, intersect, and minus allow you to combine many tables into one. This tutorial will use these two tables for the queries:</p>

<code>select * from my_brick_collection;

select * from your_brick_collection;</code>",20-MAY-18 12.40.28.358430000 PM,"CHRIS.SAXON@ORACLE.COM",22-MAY-18 02.55.49.313776000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171334459364937322052904079861,144380801214805970838977892947497636818,"Union",20,"<p>The union operator combines two or more tables into a single result set. To use it, the tables must have the same number of columns with matching data types in each position.</p>

<p>The brick collection tables have different columns. So combining these with select * leads to an error:</p>

<code>select * from my_brick_collection
union 
select * from your_brick_collection;</code>

<p>To resolve this, select the common columns. Here that's the colour and shape. So this query returns a list of all the ( colour, shape ) values in the tables:</p>

<code>select colour, shape from my_brick_collection
union 
select colour, shape from your_brick_collection;</code>",20-MAY-18 12.40.40.088195000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 03.04.56.039057000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171369518213706146298970558965,144380801214805970838977892947497636818,"Try It!",35,"<p>Complete this query to return a list of all the colours in the two tables. Each colour must only appear once:</p>

<code>select colour from my_brick_collection
/*TODO*/
select colour from your_brick_collection
order by colour;
</code>

<p>The output of this query should be:</p>
<pre><b>COLOUR   </b>
blue     
green    
red      
&lt;null&gt;</pre>

<p>Complete the following query to return a list of all the shapes in both tables. There must show one row for each row in the source tables:</p>

<code>select shape from my_brick_collection
/*TODO*/
select shape from your_brick_collection
order  by shape;
</code>

<p>This query should return the following rows:</p>
<pre><b>SHAPE     </b>  
cube      
cube      
cube      
cuboid    
cuboid    
cuboid    
pyramid   
pyramid </pre>",20-MAY-18 12.41.48.969373000 PM,"CHRIS.SAXON@ORACLE.COM",31-MAR-22 08.23.29.156209000 AM,"CHRIS.SAXON@ORACLE.COM"
144404117171423919875588804611832336885,144380801214805970838977892947497636818,"Try It!",60,"<p>Complete the following query to return a list of all the shapes in my collection not in yours:</p>

<code>select shape from my_brick_collection
/*TODO*/
select shape from your_brick_collection;</code>
<p>This should return the following row:</p>
<pre><b>SHAPE     </b>
pyramid  </pre>

<p>Complete the following query to return a list of all the colours that are in both tables:</p>

<code>select colour from my_brick_collection
/*TODO*/
select colour from your_brick_collection
order  by colour;</code>
<p>This should return the following rows:</p>
<pre><b>COLOUR   </b>
blue     
red      
&lt;null&gt;   </pre>",20-MAY-18 12.42.30.618878000 PM,"CHRIS.SAXON@ORACLE.COM",31-MAR-22 08.23.52.185816000 AM,"CHRIS.SAXON@ORACLE.COM"
144724609165194232647071268117775391190,144380801214805970838977892947497636818,"Intersect",55,"<p>There is also an operator to find the common values: intersect</p>

<p>You use this in the same way as union and minus: place it between a select from each table:</p>

<code>select colour, shape from your_brick_collection
intersect
select colour, shape from my_brick_collection;</code>

<p>As with minus, the database considers null values to be the same and applies a distinct operator to the results. Oracle Database 21c added the all clause to intersect as well as minus.</p>",23-MAY-18 02.27.18.220561000 PM,"CHRIS.SAXON@ORACLE.COM",31-MAR-22 08.26.25.182220000 AM,"CHRIS.SAXON@ORACLE.COM"
144727362172854241063448254892298998910,144380801214805970838977892947497636818,"Distinct",25,"<p>There are two red cube rows in the tables, one in each table. But union only displays one!</p>

<p>This is because union applies the distinct operator to your results. This discards duplicate rows. So your results only have one row for each set of column values.</p>

<p>You can use distinct by placing it after select and before the column list. This squashes out duplicates. So you only get one row for each set of values in your columns.</p>

<p>For example, there are two green pyramid rows in my collection. If you do a distinct *, you only get one copy of this brick:</p>

<code>select distinct * from my_brick_collection;</code>

<p>You can also use distinct on a subset of a table's columns. For example, to get one row for each shape in your collection, select ""distinct shape"":</p>

<code>select distinct shape from your_brick_collection;</code>",23-MAY-18 02.51.05.339889000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 02.55.02.564486000 PM,"CHRIS.SAXON@ORACLE.COM"
144905567362682786887626052970543353640,144905567362239111111827484063426187048,"Try It!",95,"<p>Complete the following query to cycle on job_id. Define a cycle column is_repeat which defaults to N. When accessing the same job_id, set it to Y</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id, job_id
) as (
  select employee_id, first_name, last_name, manager_id , job_id
  from   employees
  where  employee_id = 102
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id, e.job_id
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) cycle /*TODO*/
  select * from org_chart;</code>
<p>This should return the following rows:</p>
<pre><b>EMPLOYEE_ID   FIRST_NAME   LAST_NAME   MANAGER_ID   JOB_ID    IS_REPEAT   </b>
          102 Lex          De Haan              100 AD_VP     N           
          103 Alexander    Hunold               102 IT_PROG   N           
          104 Bruce        Ernst                103 IT_PROG   Y           
          105 David        Austin               103 IT_PROG   Y           
          106 Valli        Pataballa            103 IT_PROG   Y           
          107 Diana        Lorentz              103 IT_PROG   Y</pre>",25-MAY-18 08.20.20.816129000 AM,"CHRIS.SAXON@ORACLE.COM",12-DEC-19 05.20.45.899267000 PM,"CHRIS.SAXON@ORACLE.COM"
143865597122470221903813235518865202604,143268521600193170436610330993365925097,"New Column Names",70,"<p>By default, the names of the new columns are the values from the IN list, wrapped in quotes. This can make your SQL fiddly. To reference the new columns, you must use double AND single quotes.</p>

<p>To simplify this, alias each value in the list:</p>

<code>with rws as (
  select location, to_char ( match_date, 'MON' ) match_month 
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_month in (
      'JAN' jan, 'FEB' feb, 'MAR' mar
    )
  );</code>

<p>This makes it easier to filter the values from the previous list:</p>

<code>with rws as (
  select location, to_char ( match_date, 'MON' ) match_month 
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_month in (
      'JAN' jan, 'FEB' feb, 'MAR' mar
    )
  )
  where jan > 0;</code>",15-MAY-18 09.04.03.993772000 AM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 07.59.51.933688000 AM,"CHRIS.SAXON@ORACLE.COM"
143865597126711133679021354663734468012,143268521600193170436610330993365925097,"Combining Pivot and Unpivot",160,"<p>You can combine pivot and unpivot a single statement. The output from the first becomes the input to the second.</p>

<p>For example, say you want to produce a league table from the match results. For each team this will show the number of games they've won, drawn, and lost.</p>

<p>To do this, you need several steps:</p>

<p>First, compare the home and away team points to find out whether each won, lost or drew.</p>

<p>Then, to count up the number of outcomes for each team, you need one column with all the names. To do this, you need to unpivot home and away, giving single column of team names.</p>

<p>Finally, to get the league table, you need to pivot the number of wins, losses, and draws for each team.</p>

<p>This gives a statement like:</p>

<code>with rws as (
  select home_team_name, away_team_name, 
         case
           when home_team_points > away_team_points then 'WON'
           when home_team_points < away_team_points then 'LOST'
           else 'DRAW'
         end home_team_result,
         case
           when home_team_points < away_team_points then 'WON'
           when home_team_points > away_team_points then 'LOST'
           else 'DRAW'
         end away_team_result
  from   match_results
)
  select team, w, d, l 
  from   rws
  unpivot (
    ( team, result ) for home_or_away in ( 
      ( home_team_name, home_team_result ) as 'HOME', 
      ( away_team_name, away_team_result ) as 'AWAY'
    )
  )
  pivot (
    count (*), min ( home_or_away ) dummy
    for result in (
      'WON' W, 'DRAW' D, 'LOST' L
    )
  )
  order  by w desc, d desc, l;</code>",15-MAY-18 09.10.31.390869000 AM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.16.17.915885000 PM,"CHRIS.SAXON@ORACLE.COM"
144177112310976508363013879918911871417,143268521600193170436610330993365925097,"Try It!",75,"<p>Complete the following query to show:</p>
<ul>
<li>For each location, the number of games played on each day of the week</li>
<li>The three letter abbreviation of each day as columns</li>
<li>The column headings without quotes</li>
<li>Those locations that had one or more games played on Monday</li>
</ul>
<code>with rws as (
  select location,
         to_char ( match_date, 'DY' ) match_day
  from   match_results
)
  select * from rws
  pivot (
    count (*) for match_day in (
      'MON' , 'TUE' , 'WED' , 'THU' ,
      'FRI' , 'SAT' , 'SUN' 
    )
  )
  where  mon
  order  by location </code>
<p><i>to_char ( match_date, 'DY' ) returns the three-letter abbreviation of the day for the date.</i></p>
<p>The output of this query should be:</p>
<pre><b>LOCATION   MON   TUE   WED   THU   FRI   SAT   SUN   </b>
Coldgate       1     0     0     1     0     0     0 
Snowley        1     0     0     0     0     0     0</pre>",18-MAY-18 08.13.25.941360000 AM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.02.07.511544000 PM,"CHRIS.SAXON@ORACLE.COM"
143272175590544198834552659483453841494,143268521600193170436610330993365925097,"Try It!",100,"<p>Complete the following query to show for each location:</p>

<ul>
<li>A column counting the number of games played</li>
<li>The total number of points scored at each location ( home_team_points + away_team_points )</li>
<li>The count columns should have the matches suffix. The total points scored the suffix points</li>
</ul>

<code>with rws as (
  select location, home_team_points, away_team_points
  from   match_results
)
  select * from rws
  pivot (

    for location in (
      'Snowley' snowley, 'Coldgate' coldgate, 
      'Dorwall' dorwall, 'Newdell' newdell
    )
  );</code>

<p>The output of this query should be:</p>
<pre><b>SNOWLEY_MATCHES   SNOWLEY_POINTS   COLDGATE_MATCHES   COLDGATE_POINTS   DORWALL_MATCHES   DORWALL_POINTS   NEWDELL_MATCHES   NEWDELL_POINTS   </b>
                1                2                  2                11                 1                1                 1                8 
</pre>",09-MAY-18 04.17.39.313608000 PM,"CHRIS.SAXON@ORACLE.COM",18-MAY-18 02.33.30.911989000 PM,"CHRIS.SAXON@ORACLE.COM"
144404117171837372505897007789581849077,144380801214805970838977892947497636818,"Finding the Difference Between Two Tables (Symmetric Difference)",70,"<p>You can combine set operators to implement a classic use case:</p>

<p>Comparing two tables, returning a list of all the values that only exist in one table.</p>

<p>This is also known as the symmetric difference. There isn't a native operator that does this. But you can do it by:</p>

<ul>
<li>Finding the rows in table one not in table two with minus</li>
<li>Finding the rows in table two not in table one with minus</li>
<li>Combining the output of these two operations with union (all)</li>
</ul>

<p>My collection has a blue cuboid and two green pyramids not in yours. And you have a blue cube I don't. So these should appear in the output. But when you chain the operators as described above:</p>

<code>select colour, shape from your_brick_collection
minus
select colour, shape from my_brick_collection
union all
select colour, shape from my_brick_collection
minus
select colour, shape from your_brick_collection;</code>

<p>You only get blue cuboid and green pyramid!</p>

<p>This is because the set operators all have the same priority in Oracle Database. To fix this and do the minuses before union, you need parentheses. Place them around the operations that should happen first:</p>

<code>select * from ( 
  select colour, shape from your_brick_collection
  minus
  select colour, shape from my_brick_collection
) union all (
  select colour, shape from my_brick_collection
  minus
  select colour, shape from your_brick_collection
);</code>

<p>Or, you could use the following method:</p>

<ul>
<li>Combine all the rows from the two tables with union (all)</li>
<li>Find the values that exist in both tables with intersect</li>
<li>Minus the second query from the first</li>
</ul>

<p>As before, you need brackets to ensure correct order of processing. Which gives:</p>

<code>select * from ( 
  select colour, shape from your_brick_collection
  union all
  select colour, shape from my_brick_collection
) minus (
  select colour, shape from my_brick_collection
  intersect
  select colour, shape from your_brick_collection
);</code>",20-MAY-18 12.46.57.432433000 PM,"CHRIS.SAXON@ORACLE.COM",23-MAY-18 03.08.34.058728000 PM,"CHRIS.SAXON@ORACLE.COM"
144724609164715498022503874964591745494,144380801214805970838977892947497636818,"Minus",45,"<p>Oracle Database includes an operator that implements set difference: minus</p>

<p>This is easier to use than not exists. All you need to do is select the relevant columns from each table with minus between them. And set operators are one of the rare cases where the database considers null values to be equal.</p>

<p>So the following query returns the same rows as not exists with null checking:</p>

<code>select colour, shape from your_brick_collection
minus
select colour, shape from my_brick_collection;</code>

<p>But there is another subtle difference. Like union, minus applies a distinct to the output. There are two green pyramids in my collection not in yours. But minus only returns one of these:</p>

<code>select colour, shape from my_brick_collection
minus
select colour, shape from your_brick_collection</code>

<p>To see both, you need to use not exists:</p>

<code>select colour, shape from my_brick_collection mbc
where  not exists (
  select null from your_brick_collection ybc
  where  ( ybc.colour = mbc.colour or ( ybc.colour is null and mbc.colour is null ) )
  and    ybc.shape = mbc.shape
);</code>

Oracle Database 21c added the all option for minus. It also included except as a synonym for minus.",23-MAY-18 02.21.57.823840000 PM,"CHRIS.SAXON@ORACLE.COM",31-MAR-22 08.25.29.658888000 AM,"CHRIS.SAXON@ORACLE.COM"
144904245953884366835974258068636339323,144905567362239111111827484063426187048,"Introduction",10,"<p>You can use hierarchical queries to travel along parent-child relationships in your data. For example, family trees, computer directory structures, and company organization charts.</p>

<p>This tutorial shows methods to build org charts using the following sample company:</p>

<code>select * from employees</code>",25-MAY-18 08.17.13.203682000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 09.43.20.755387000 AM,"CHRIS.SAXON@ORACLE.COM"
144905567362557058602386131536373911336,144905567362239111111827484063426187048,"Try It!",30,"<p>Complete the following query to build a ""reverse"" org chart.  Begin with employee 107 and go up the chain to Steven King. This switches the parent-child relationship so the employee is the parent row for their manager.</p>

<code>select employee_id, first_name, last_name, manager_id
from  employees
start with employee_id /*TODO*/
connect by /*TODO*/</code>

<p>This should give the following output:</p>

<pre><b>EMPLOYEE_ID   FIRST_NAME   LAST_NAME   MANAGER_ID   </b>
          107 Diana        Lorentz              103 
          103 Alexander    Hunold               102 
          102 Lex          De Haan              100 
          100 Steven       King              &lt;null&gt;</pre>",25-MAY-18 08.17.43.353649000 AM,"CHRIS.SAXON@ORACLE.COM",12-DEC-19 05.19.02.323869000 PM,"CHRIS.SAXON@ORACLE.COM"
146207965397322365472919548283637737333,146191602965377274749803460241482600523,"Use Table Function in TABLE Clause",30,"<p>
In the FROM clause of your query, right where you would have the table name, type in:
</p>
<p>
<pre>TABLE (your_function_name (parameter list))</pre>
</p>
<p>
You can (and should) give that TABLE clause a table alias. You can, starting in Oracle Database 12c, also use named notation when invoking the function. You can also call built-in functions for the value returned by the table function. All this is demonstrated below.
</p>
<p>
Notice that Oracle Database automatically uses the string ""COLUMN_VALUE"" as the name of the single column returned by the table function. You can, as shown, rename it using a column alias.
</p>
<code>SELECT rs.COLUMN_VALUE my_string FROM TABLE (random_strings (5)) rs
/

SELECT COLUMN_VALUE my_string FROM TABLE (random_strings (count_in => 5))
/

SELECT SUM (LENGTH (COLUMN_VALUE)) total_length,
       AVG (LENGTH (COLUMN_VALUE)) average_length
  FROM TABLE (random_strings (5))
/

/* On 12.1 and higher, don't bother with the TABLE clause */

SELECT rs.COLUMN_VALUE no_table FROM random_strings (5) rs
/
</code>
<p>
Pretty cool, right?
</p>",06-JUN-18 06.49.42.964033000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",13-JUN-18 01.23.59.542433000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206403583296232888889622914061345711,146206374662252057210273830044561075244,"Introduction",10,"<p>
In my introduction to table functions, I showed how to build and ""query"" from a table function that returns a collection of scalars (number, date, string, etc.). If that's all you need to do, well, lucky you!
</p>
<p>
Most of the time, however, you need to pass back a row of data consisting of more than one value, just as you would with ""regular"" tables when, say, you needed the ID and last name of all employees in department 10, as in:
</p>
<code>SELECT employee_id, last_name
  FROM hr.employees
 WHERE department_id = 10
/
</code>
<p>
This module explores how you can go about doing that with table functions.
</p>",06-JUN-18 07.04.21.070616000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUN-18 08.35.45.486428000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146300688548011764681395910841713069883,146218766402242523928587654762999856560,"Two-Step Transformation",60,"<p>
In my two-step transformation, I will create a function that returns a nested table of elements that match the stocks table. So I will need an object type and nested table type.
</p>
<code>CREATE OR REPLACE TYPE stock_ot AUTHID DEFINER IS OBJECT 
(
   ticker VARCHAR2 (20),
   trade_date DATE,
   open_price NUMBER,
   close_price NUMBER
)
/

CREATE OR REPLACE TYPE stocks_nt AS TABLE OF stock_ot;
/
</code>
<p>
Now I create a table function accepts a cursor variable each of whose rows contains ticker data, and returns a nested table, each of whose elements looks like a row in the stocks table. As it follows precisely the same pattern as the doubled function, I will not describe the individual lines.
</p>
<code>CREATE OR REPLACE FUNCTION singled (tickers_in IN stock_mgr.tickers_rc)
   RETURN stocks_nt
   AUTHID DEFINER
IS
   TYPE tickers_aat IS TABLE OF tickers%ROWTYPE INDEX BY PLS_INTEGER;
   l_tickers    tickers_aat;

   l_singles        stocks_nt := stocks_nt ();
BEGIN
   LOOP
      FETCH tickers_in BULK COLLECT INTO l_tickers LIMIT 100;

      EXIT WHEN l_tickers.COUNT = 0;
      
      FOR indx IN 1 .. l_tickers.COUNT
      LOOP
         l_singles.EXTEND;
         l_singles (l_singles.LAST) :=
            stock_ot (l_tickers (indx).ticker,
                       l_tickers (indx).pricedate,
                       l_tickers (indx).price,
                       l_tickers (indx).price * .5);
      END LOOP;
   END LOOP;

   RETURN l_singles;
END;
/
</code>
<p>
Now let's do a two-step transformation: stocks -> tickers -> more stocks!
</p>
<code>CREATE TABLE more_stocks
AS
   SELECT *
     FROM TABLE (
             singled (
                CURSOR (
                   SELECT * 
                     FROM TABLE (doubled (
                                   CURSOR (SELECT * FROM stocks))))))
/

SELECT COUNT (*)
  FROM more_stocks
/
</code>",07-JUN-18 04.18.14.914236000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 06.59.47.173752000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
144905567362587281747876497265741565736,144905567362239111111827484063426187048,"Try It!",50,"<p>Complete the following query to build the reverse org chart from step three again. But this time using recursive with. It should start with employee_id 107 and go up the company to the CEO. As in module 3, the employee is the parent row, their manager the child.</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id
) as (
  select employee_id, first_name, last_name, manager_id 
  from   employees
  where  /*TODO*/
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id 
  from   org_chart oc
  join   employees e
  on     /*TODO*/
)
  select * from org_chart;</code>
<p>This should give the following output:</p>
<pre><b>EMPLOYEE_ID   FIRST_NAME   LAST_NAME   MANAGER_ID   </b>
          107 Diana        Lorentz              103 
          103 Alexander    Hunold               102 
          102 Lex          De Haan              100 
          100 Steven       King              &lt;null&gt; </pre>",25-MAY-18 08.18.09.230514000 AM,"CHRIS.SAXON@ORACLE.COM",12-DEC-19 05.19.56.454870000 PM,"CHRIS.SAXON@ORACLE.COM"
146052530931229030015633536741041543223,144905567362239111111827484063426187048,"Sorting Output: Recursive With",75,"<p>Recursive with allows you to choose whether you want to traverse the tree using depth-first or breadth-first search. You define this in the search clause.</p>

<h3>Depth-First Search</h3>

<p>This starts at the root. Then picks one of its children. It then gets the child's child. And so on, down the tree accessing child nodes first. When it hits a leaf, it goes back up the tree until it finds an unvisited child.</p>

<p>So it always goes as far down the tree it can before accessing another row at the same level.</p>

<p>To use depth-first search, specify this in the search clause. The columns you sort by defines which order the database returns siblings. And the set clause defines a new column storing this sequence. It starts with 1 at for the first root. For each new row it increments by 1.</p>

<p>So to return employees in a depth first tree, sorting employees with the same manager by the date they were hired (first to last), use: </p>
<pre>search depth first by hire_date set hire_seq</pre>

<p>Which gives the following complete query:</p>

<code>with org_chart (
  employee_id, first_name, last_name, hire_date, manager_id, lvl
) as (
  select employee_id, first_name, last_name, hire_date, manager_id, 1 lvl
  from   employees
  where  manager_id is null
  union  all
  select e.employee_id, e.first_name, e.last_name, e.hire_date, e.manager_id, oc.lvl + 1
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) search depth first by hire_date set hire_seq
  select * from org_chart
  order  by hire_seq;</code>

<h3>Breadth-First Search</h3>

<p>Instead of travelling down the tree, you can go across it. This is breadth-first search.</p>

<p>Again, this starts with the root. But it accesses all the rows at the same level before going down to any children. So if you have many roots, it visits all these first. Then all the first generation children before going down to the second generation. And so on.</p>

<p>The sorting columns define which order you access nodes at the same depth.Unlike depth-first search, successive rows at the same level may alternate between parent rows.</p>

<p>So the following returns all the employees at the same rank next to each other. It sorts these by their hire date, first to last:</p>

<pre>search breadth first by hire_date set hire_seq</pre>

<p>Which completes the query like so:</p>

<code>with org_chart (
  employee_id, first_name, last_name, hire_date, manager_id, lvl
) as (
  select employee_id, first_name, last_name, hire_date, manager_id, 1 lvl
  from   employees
  where  manager_id is null
  union  all
  select e.employee_id, e.first_name, e.last_name, e.hire_date, e.manager_id, oc.lvl + 1
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) search breadth first by hire_date set hire_seq
  select * from org_chart
  order  by hire_seq;</code>",05-JUN-18 08.00.20.598091000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 02.59.28.505700000 PM,"CHRIS.SAXON@ORACLE.COM"
144905567362606624560990331332536864552,144905567362239111111827484063426187048,"Sorting Output: Connect By",70,"<p>When you build a hierarchical query, the database returns the rows in an order matching the tree structure.</p>

<p>Connect by returns rows in depth-first search order. If you use a regular order by you'll lose this sort.</p>

<p>But you can preserve the depth-first tree and sort rows with the same parent. You do this with the siblings clause of order by.</p>

<p>So to show a manager's reports after them, sorting employees with the same manager by hire date (first to last), you can do:</p>

<code>select level, employee_id, first_name, last_name, hire_date, manager_id 
from   employees
start  with manager_id is null
connect by prior employee_id = manager_id
order siblings by hire_date;</code>",25-MAY-18 08.18.51.107375000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 02.56.35.076991000 PM,"CHRIS.SAXON@ORACLE.COM"
144905567362632012003202238545205694248,144905567362239111111827484063426187048,"Detecting Loops",90,"<p>It's possible to store loops in your hierarchy. Usually this is a data error. But some structures may contain loops by design.</p>

<p>For example, the following sets the CEO's manager to be a lowly programmer:</p>

<code>update employees
set    manager_id = 107
where  employee_id = 100;</code>

<p>This leads to a circle in your data. So you could get stuck in an infinite loop. Luckily Oracle Database has cycle detection to stop this.</p>

<h3>Connect By</h3>

<p>If you try and build a hierarchy that contains a loop, connect by throws an ORA-1436 error:</p>

<code>select * from employees
start with employee_id = 100
connect by prior employee_id = manager_id;</code>

<p>You can avoid this using the nocycle keyword. This spots when the query returns to the same row. The database hides the repeated row and continues processing the tree.</p>

<p>To use it, place nocycle after connect by:</p>

<code>select * from employees
start with employee_id = 100
connect by nocycle prior employee_id = manager_id;</code>

<h3>Recursive With</h3>

<p>You control loop detection using the cycle clause of recursive with. Here you state which columns mark a loop. The database keeps track of the values it sees in these columns. If the current row's values for these appear in one of it's ancestors, you have a loop.</p>

<p>The syntax for this is:</p>

<pre>cycle &lt;columns&gt; set &lt;loop_column&gt; to &lt;loop_value&gt; default &lt;default_value&gt;</pre>

<p>The &lt;loop_column&gt; is a new one you define. When the database detects a cycle, it changes this from the default value to the &lt;loop_value&gt;</p>

<p>The org chart has a loop if you visit the same employee_id twice. So you want to check this column for cycles.</p>

<p>The snippet below does this and adds the column looped to your results. This starts with the value N. If the database finds a cycle, it sets it to Y:</p>

<pre>cycle employee_id set looped to 'Y' default 'N'</pre>

<p>You place this after your with clause, which gives the following complete query:</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id, department_id
) as (
  select employee_id, first_name, last_name, manager_id, department_id
  from   employees
  where  employee_id = 100
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id, e.department_id 
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) cycle employee_id set looped to 'Y' default 'N'
  select * from org_chart;</code>

<p>Unlike connect by, this includes the rows you visit twice. So the CEO, Steven King, appears twice in the results. If you want to exclude these, filter them out in your final where clause using the loop column you defined.</p>

<p>Using recursive with you can choose any columns in your query to mark a ""loop"". This allows you to stop processing before you get back to the same row.</p> 

<p>For example, you can cycle on department_id. The CEO's direct reports, Neena Kochhar and Lex De Haan, are in the same department as him. So if you cycle on department_id, like so:</p>
<pre>cycle department_id set looped to 'Y' default 'N'</pre>

<p>The database considers these two employees to form a cycle! So the hierarchy only contains three rows:</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id, department_id
) as (
  select employee_id, first_name, last_name, manager_id, department_id
  from   employees
  where  employee_id = 100
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id, e.department_id 
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) cycle department_id set looped to 'Y' default 'N'
  select * from org_chart;</code>

<p>This can be useful to detect rows with two parents. Without cycle detection, you may access that row and all its children twice. The cycle clause allows you to avoid this. Connect by doesn't have an inbuilt way to do this.</p>

<p>Remove the loop before continuing the tutorial:</p>

<code>update employees
set    manager_id = null
where  employee_id = 100;</code>",25-MAY-18 08.19.27.219829000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 03.14.31.816116000 PM,"CHRIS.SAXON@ORACLE.COM"
149111946068766293633342040521438953835,149075222712493902096666428584818910723,"Isolation Levels",35,"<p>To help you manage which read problems you're exposed to, the SQL standard defines four isolation levels. These state which phenomena are possible, as shown by this table:</p>

<table border=""0"" cellpadding=""1"" cellspacing=""1"">
	<tbody>
		<tr>
			<td>&nbsp;</td>
			<td style=""text-align:center""><strong>Dirty Reads</strong></td>
			<td style=""text-align:center""><strong>Non-repeatable Reads</strong></td>
			<td style=""text-align:center""><strong>Phantom Reads</strong></td>
		</tr>
		<tr>
			<td><strong>Read Uncommitted</strong></td>
			<td style=""text-align:center"">âœ”</td>
			<td style=""text-align:center"">âœ”</td>
			<td style=""text-align:center"">âœ”</td>
		</tr>
		<tr>
			<td><strong>Read Committed</strong></td>
			<td style=""text-align:center"">âœ˜</td>
			<td style=""text-align:center"">âœ”</td>
			<td style=""text-align:center"">âœ”</td>
		</tr>
		<tr>
			<td><strong>Repeatable Reads</strong></td>
			<td style=""text-align:center"">âœ˜</td>
			<td style=""text-align:center"">âœ˜</td>
			<td style=""text-align:center"">âœ”</td>
		</tr>
		<tr>
			<td><strong>Serializable</strong></td>
			<td style=""text-align:center"">âœ˜</td>
			<td style=""text-align:center"">âœ˜</td>
			<td style=""text-align:center"">âœ˜</td>
		</tr>
	</tbody>
</table>

<p>Oracle Database supports these as follows:</p>

<h3>Read Uncommitted</h3>

<p>As you can't have dirty reads in Oracle Database, this isolation level is not relevant and not implemented.</p>

<h3>Read Committed</h3>

<p>This is the default mode for Oracle Database. Using read committed, you have statement-level consistency. This means that each DML command (select, insert, update, or delete) can see all the data saved before it begins. Any changes saved by other sessions after it starts are hidden.</p>

<p>It does this using multiversion concurrency control (MVCC). When you update or delete a row, this stores the row's current state in undo. So other transactions can use this undo to view data as it existed in the past.</p>

<p>So none of the read phenomena are possible in a single statement in Oracle Database. Only within a transaction.</p>

<h3>Repeatable Read</h3>

<p>The intent of repeatable read in the SQL standard is to provide consistent results from a query. But Oracle Database already has this in read committed! So it has no use for this level and doesn't implement it.</p>

<p>If you're using a database without MVCC you may need this mode to get correct results.</p>

<h3>Serializable</h3>

<p>None of the three read phenomena are possible using serializable. You use this in Oracle Database to get transaction-level consistency. You can only view changes committed in the database at the time your transaction starts. Any changes made by other transactions after this are hidden from your transaction.</p>",04-JUL-18 04.43.24.968844000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 12.38.44.976030000 PM,"CHRIS.SAXON@ORACLE.COM"
151068387646240303138701925463542775446,117123875509271941312470520788803021948,"Table Organization",16,"<p>Create table in Oracle Database has an organization clause. This defines how it physically stores rows in the table.</p>

<p>The options for this are:</p>

<ul><li>Heap</li>
<li>Index</li>
<li>External</li></ul>

<p>By default, tables are heap-organized. This means the database is free to store rows wherever there is space. You can add the ""organization heap"" clause if you want to be explicit:</p>

<code>create table toys_heap (
  toy_name varchar2(100)
) organization heap;

select table_name, iot_name, iot_type, external, 
       partitioned, temporary, cluster_name
from   user_tables
where  table_name = 'TOYS_HEAP';</code>

<p>These are good general purpose tables and are the most common type in Oracle Database installations.</p>",23-JUL-18 08.29.59.404577000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 08.02.20.553562000 AM,"CHRIS.SAXON@ORACLE.COM"
151280682521620543952945409064740594945,117675390209390608523249818520886328918,"Try It!",15,"<p>Complete this query so it returns the price and colour columns of the toys, displaying price first:</p>

<code>select /* TODO */
from   toys; </code>

<p>The query should return the following rows:</p>

<pre><b>PRICE    COLOUR   </b>
    0.01 red      
       6 blue     
   17.22 blue     
   14.22 red      
  &lt;null&gt; green</pre>",25-JUL-18 08.28.45.735238000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.16.31.591782000 AM,"CHRIS.SAXON@ORACLE.COM"
151280682522651757677076687750764963073,117675390209390608523249818520886328918,"Try It!",32,"<p>Complete the following query to find the rows that have:</p>

<ul><li>The toy_name Sir Stripypants or the colour blue</li>
<li>And a price equal to 6</li></ul>
<code>select *
from   toys
where  /* TODO */</code>
<p>This should return the following row:</p>
<pre><b>TOY_NAME            COLOUR   PRICE   </b>
Miss Smelly_bottom  blue	 6
</pre>",25-JUL-18 09.02.59.784940000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.19.34.544480000 PM,"CHRIS.SAXON@ORACLE.COM"
146042842278767959299137505826286924279,144905567362239111111827484063426187048,"Displaying Tree Details: Recursive With",105,"<p>Recursive with doesn't have the built-in options like connect by. But you can emulate them.</p>

<h3>Values from the root (connect_by_root)</h3>

<p>To return a value from the parent row, select it in the base query. And in the recursive part, return this column from the with clause name.</p>

<h3>Showing the path from root to current row (sys_connect_by_path)</h3>

<p>Again, start by selecting the value you want in the base query. In the recursive part, append the values you want to add with an appropriate separator.</p>

<h3>Identifying the leaves (connect_by_isleaf)</h3>

<p>Displaying leaf rows is more complex with recursive with. To find these, add the depth (level) to the tree. Then sort it using depth-first search.</p>

<p>Depth-first search always goes as far down the tree it can. After hitting a leaf, it goes back up to the next unvisited child. So you know a row is a leaf if the next row is at the same or higher depth in the tree. So you need to check if the next row's level is less than or equal to the current.</p>

<p>You can test for this using lead. This gets a value from the next row in the results. Assuming seq follows a depth-first order, the following returns LEAF if the row is a leaf. And null otherwise:</p>

<pre>case 
  when lead ( lvl, 1, 1 ) over ( order by seq ) <= lvl then 'LEAF'
end is_leaf</pre>

<p>Putting these all together gives:</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id, root_emp, chart, lvl
) as (
  select employee_id, first_name, last_name, manager_id, 
         last_name root_emp, last_name chart, 1 lvl
  from   employees
  where  manager_id is null
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id, 
         oc.root_emp, oc.chart || ', ' || e.last_name, oc.lvl+1
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
) search depth first by employee_id set seq
  select oc.*, 
         case 
           when lead ( lvl, 1, 1 ) over ( order by seq ) <= lvl then 'LEAF'
         end is_leaf
  from   org_chart oc;</code>",05-JUN-18 08.03.49.318678000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 02.40.35.924214000 PM,"CHRIS.SAXON@ORACLE.COM"
146191602965455854928078411137838501963,146191602965377274749803460241482600523,"What Is a Table Function?",10,"<p>
A table function is a function that can be invoked inside the FROM clause of a SELECT statement. They return collections (usually nested tables or varrays), which can then be transformed with the TABLE clause into a dataset of rows and columns that can be processed in a SQL statement. Table functions come in very handy when you need to:
</p>
<ul>
<li>
Merge session-specific data with data from tables: you've got data, and lots of it sitting in tables. But in your session (and not in any tables), you have some data - and you need to ""merge"" these two sources together in an SQL statement. In other words, you need the set-oriented power of SQL to get some answers. With the TABLE operator, you can accomplish precisely that.
</li>
<li>
Programmatically construct a dataset to be passed as rows and columns to the host environment: 

Your webpage needs to display some data in a nice neat report. That data is, however, far from neat. In fact, you need to execute procedural code to construct the dataset. Sure, you could construct the data, insert into a table, and then SELECT from the table.

But with a table function, you can deliver that data immediately to the webpage, without any need for non-query DML.
</li>
<li>
Emulate a parameterized view: 
Oracle Database does not allow you to pass parameters to a view, but you can pass parameters to a function, and use that as the basis for what is in essence a parameterized view.
</li>
<li>
Improve performance of parallelized queries with pipelined table functions

Many data warehouse applications rely on Parallel Query to greatly improve performance of massive ETL operations. But if you execute a table function in the FROM clause, that query will serialize (blocked by the call to the function). Unless you define that function a a pipelined function and enable it for parallel execution.
</li>
<li>
Reduce consumption of Process Global Area (pipelined table functions): 

Collections (which are constructed and returned by ""normal"" table functions) can consume an  awful lot of PGA (Process Global Area). But if you define that table function as pipelined, PGA consumption becomes a non-issue.
</li>
</ul>
<p>
To call a function from within the FROM clause of a query, you need to:
</p>
<ul>
<li>
Define the RETURN datatype of the function to be a collection type (usually a nested table or a varray, but under some circumstances you can also use an associative array). The type must be defined at the schema level (CREATE [OR REPLACE] TYPE) or in package specification (for pipelined table functions only).
</li>
<li>
Make sure that all parameters to the function are of mode IN and have SQL-compatible datatypes. (You cannot, for example, call a function with a Boolean or record type argument inside a query.)
</li>
<li>
Embed the call to the function inside the TABLE clause. Note: as of 12.1, you can often invoke the function directly in the FROM clause with using TABLE.
</li>
</ul>
<p>
In this tutorial, I will show you how to build and query from a very simply table function, one that returns an array of strings (and, more generally, scalars). The next tutorial will show you how to work with table functions that return collections of multiple columns.
</p>
<p>
Note: if you are not yet familiar with PL/SQL collections, I suggest you check out my ""Working with Collections"" YouTube playlist - the link's at the bottom of this tutorial.
</p>",06-JUN-18 03.28.58.322702000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 07.58.09.062494000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206374661171277527538351562373753900,146191602965377274749803460241482600523,"Create Table Function",20,"<p>
First, let's create the nested table type to be returned by the function, then create a function that returns an array of random strings. Finally, demonstrate that the function works - inside a PL/SQL block.
</p>
<code>CREATE OR REPLACE TYPE strings_t IS TABLE OF VARCHAR2 (100);
/

CREATE OR REPLACE FUNCTION random_strings (count_in IN INTEGER)
   RETURN strings_t
   AUTHID DEFINER
IS
   l_strings   strings_t := strings_t ();
BEGIN
   l_strings.EXTEND (count_in);

   FOR indx IN 1 .. count_in
   LOOP
      l_strings (indx) := DBMS_RANDOM.string ('u', 10);
   END LOOP;

   RETURN l_strings;
END;
/

DECLARE
   l_strings   strings_t := random_strings (5);
BEGIN
   FOR indx IN 1 .. l_strings.COUNT
   LOOP
      DBMS_OUTPUT.put_line (l_strings (indx));
   END LOOP;
END;
/
</code>
<p>
OK, so the function does its job inside a PL/SQL block. Now let's use it as a table function.
</p>",06-JUN-18 06.45.48.467124000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 04.51.42.275915000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206374661558133789815032898279730220,146191602965377274749803460241482600523,"Blending Table Function and In-Table Data",40,"<p>
Now that the data from my function can be treated as rows and columns, I can use it just as I would any other dataset in my SELECT statement. Namely, I can join to this ""inline view"", perform set operations like UNION and INTERSECT, and more. Here are some examples for you to explore:
</p>
<code>SELECT e.last_name
  FROM TABLE (random_strings (3)) rs, hr.employees e
 WHERE LENGTH (e.last_name) <= LENGTH (COLUMN_VALUE)
/

SELECT COLUMN_VALUE last_name 
  FROM TABLE (random_strings (10)) rs
UNION ALL
SELECT e.last_name
  FROM hr.employees e
 WHERE e.department_id = 100
/
</code>
<p>
I can also call the table function inside a SELECT statement that is inside PL/SQL:
</p>
<code>BEGIN
   FOR rec IN (SELECT COLUMN_VALUE my_string FROM TABLE (random_strings (5)))
   LOOP
      DBMS_OUTPUT.put_line (rec.my_string);
   END LOOP;
END;
/
</code>",06-JUN-18 06.51.24.760627000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 03.04.54.170648000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206374661584730157846554740123266092,146191602965377274749803460241482600523,"Left Correlations and Table Functions",50,"<p>
A left correlation join occurs when you pass as an argument to your table function a column value from a table or view referenced to the left in the table clause. This technique is used with XMLTABLE and JSON_TABLE built-in functions, but also applied to your own table functions.
</p>
<p>
Here's the thing to remember: the function will be called <i>for each row</i> in the table/view that is providing the column to the function. Clearly, this could cause some performance issues, so be sure that is what you want and need to do. The following code demonstrates this behavior.
</p>
<code>CREATE TABLE things
(
   thing_id     NUMBER,
   thing_name   VARCHAR2 (100)
)
/

BEGIN
   INSERT INTO things VALUES (1, 'Thing 1');
   INSERT INTO things VALUES (2, 'Thing 2');
   COMMIT;
END;
/

CREATE OR REPLACE TYPE numbers_t IS TABLE OF NUMBER
/

CREATE OR REPLACE FUNCTION more_numbers (id_in IN NUMBER)
   RETURN numbers_t
IS
   l_numbers   numbers_t := numbers_t ();
BEGIN
   l_numbers.EXTEND (id_in * 5);

   FOR indx IN 1 .. id_in * 5
   LOOP
      l_numbers (indx) := indx;
   END LOOP;

   DBMS_OUTPUT.put_line ('more numbers');
   RETURN l_numbers;
END;
/

BEGIN
   FOR rec IN (SELECT th.thing_name, t.COLUMN_VALUE thing_number
                 FROM things th, TABLE (more_numbers (th.thing_id)) t)
   LOOP
      DBMS_OUTPUT.put_line ('more numbers ' || rec.thing_number);
   END LOOP;
END;
/
</code>",06-JUN-18 06.51.49.626365000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",13-JUN-18 01.18.58.751506000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206374661663310336121505636479167532,146191602965377274749803460241482600523,"Summary",100,"<p>
You should now have a solid grounding in what constitutes a table function, how to build a table function that returns a collection of scalars, and how to invoke that table function inside a SELECT statement.
</p>
<p>
Just remember:
</p>
<ul>
<li>
Define the RETURN datatype of the function to be a collection type (usually a nested table or a varray, but under some circumstances you can also use an associative array). The type must be defined at the schema level (CREATE [OR REPLACE] TYPE) or in package specification (for pipelined table functions only).
</li>
<li>
Make sure that all parameters to the function are of mode IN and have SQL-compatible datatypes. (You cannot, for example, call a function with a Boolean or record type argument inside a query.)
</li>
<li>
Embed the call to the function inside the TABLE clause. Note: as of 12.1, you can often invoke the function directly in the FROM clause with using TABLE.
</li>
</ul>",06-JUN-18 06.52.15.064395000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 09.26.44.005296000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146206374663811571517576701679932042284,146206374662252057210273830044561075244,"Just Use %ROWTYPE?",20,"<p>
You will undoubtedly be tempted, as I was tempted when first working with table functions, to use the %ROWTYPE as an attribute for the nested table type. This will not work. Let's take a quick look. Suppose I want my table function to return rows that could be inserted into this table: 
</p>
<code>CREATE TABLE animals
(
   name VARCHAR2 (10),
   species VARCHAR2 (20),
   date_of_birth DATE
)
/
</code>
<p>
The most straightforward way for a developer familiar with PL/SQL would be to do something like this:
</p>
<code>CREATE TYPE animals_nt IS TABLE OF animals%ROWTYPE;
/

CREATE OR REPLACE FUNCTION lots_of_animals RETURN animals_nt
...
/
</code>
<p>
Unfortunately, when you run this code you will see the following error:
</p>
<pre>
PLS-00329: schema-level type has illegal reference to <username>.ANIMALS
</pre>
<p>
That might be frustrating, but it sure makes a lot of sense. PL/SQL is a language that offers procedural ""extensions"" to SQL. So PL/SQL knows all bout SQL, but SQL doesn't know or recognize PL/SQL-specific constructs (for the most part). ""%ROWTYPE"" is not a part of SQL and the CREATE TYPE statement is a SQL DDL statement, not a PL/SQL statement. So that won't work.
</p>
<p>
So what's a developer supposed to do? Use object types!
</p>
</code>",06-JUN-18 07.10.57.833835000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUN-18 09.03.40.068660000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146209590824797403004714595596846788311,146206374662252057210273830044561075244,"Object Type Mimics Table",30,"<p>
Your table function's collection type must be an object type whose attributes look ""just like"" the columns of the dataset you want returned by the table function.
</p>
<p> 
Relatively few developers have used the object-oriented features of Oracle Database, which can make this step seem a bit intimidating. Don't worry! You will be using a very small portion of these features, and nothing actually that is object-oriented. If you'd like more information about our O-O features, though, check the Links at the bottom of this module.
</p>
<p>
So first I create an object type with attributes that match the table in both number and type. Then I create a nested table of those types.
</p>
<code>CREATE TYPE animal_ot IS OBJECT
(
   name VARCHAR2 (10),
   species VARCHAR2 (20),
   date_of_birth DATE
);
/

CREATE TYPE animals_nt IS TABLE OF animal_ot;
/
</code>
<p>
With this collection type in place, I can now build my table function. In the code below, I define a function that accepts two object types (the dad and the mom) and returns a collection with the whole family: mom, dad and kids. The number of kids varies according to the species. Rabbits have more babies, on average, than kangaroos.
</p>
<p>
Here's an explanation of the code below (line numbers are visible after you insert the code into the editor):
</p>
<ul>
<li>Lines 1-2: Two object type instances come in, one collection of those types comes out.</li>
<li>Line 5: Declare the local variable I will fill up and return for processing in the SELECT.
Initialize it with the mom and dad via the call to the constructor function.</li>
<li>Lines 7-12: Start the loop to fill up the collection. The CASE expression on the mother's
species determines the number of elements to go in the collection.</li>
<li>Line 14: Extend the collection, adding a new element with a value of NULL.</li>
<li>Lines 15-18: Put the babies in the collection in the new LAST row.
I do so by calling the constructor function for the object type and passing in a value for 
each attribute. Note: this logic has nothing to do <i>per se</i> with a table function. It's just how you work with object types. 
</li>
<li>Line 21: Return the collection to be used by the query.
</ul>
<code>CREATE OR REPLACE FUNCTION animal_family (dad_in IN animal_ot, mom_in IN animal_ot)
   RETURN animals_nt
   AUTHID DEFINER
IS
   l_family   animals_nt := animals_nt (dad_in, mom_in);
BEGIN
   FOR indx IN 1 ..
               CASE mom_in.species
                  WHEN 'RABBIT' THEN 12
                  WHEN 'DOG' THEN 4
                  WHEN 'KANGAROO' THEN 1
               END
   LOOP
      l_family.EXTEND;
      l_family (l_family.LAST) :=
         animal_ot ('BABY' || indx,
                 mom_in.species,
                 ADD_MONTHS (SYSDATE, -1 * DBMS_RANDOM.VALUE (1, 6)));
   END LOOP;

   RETURN l_family;
END;
/
</code>",06-JUN-18 07.14.23.507056000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUN-18 09.19.28.451949000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146218768157296956237835317097905532461,146206374662252057210273830044561075244,"Summary",50,"<p>
Most of the table functions I have written needed to return more than a single value in each collection element. Fortunately, Oracle Database makes it easy for us to accomplish this. Simply:
</p>
<ul>
<li>Define an object type whose attributes match the name, number and type of values you want to reference in the SELECT.</li>
<li>Define a schema-level nested table or varray type of those object types.</li>
<li>Create a function that returns a collection of that type, and inside the function use the constructor functions for both types to fill the collection as needed.</li>
</ul>",06-JUN-18 09.25.21.433476000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUN-18 09.25.21.433514000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146292661732581632942536314089934434505,146218766402242523928587654762999856560,"Setting Up Tables for Transformation",20,"<p>
In this tutorial, we will start with the stocks table, each row of which contains the open and close trading prices for each stock ticker symbol:
</p>
<code>CREATE TABLE stocks
(
   ticker        VARCHAR2 (20),
   trade_date    DATE,
   open_price    NUMBER,
   close_price   NUMBER
)
/
</code>
<p>
Let's load it up with some optimistic data!
</p>
<code>BEGIN
   FOR indx IN 1 .. 1000
   LOOP
      INSERT INTO stocks
           VALUES ('STK' || indx,
                   SYSDATE,
                   indx,
                   indx + 15);
   END LOOP;

   COMMIT;
END;
/
</code>
<p>
My transformation is simple: for each row in the stock table, generate two rows for the tickers table (one row each for the open and close prices):
</p>
<code>CREATE TABLE tickers
(
   ticker      VARCHAR2 (20),
   pricedate   DATE,
   pricetype   VARCHAR2 (1),
   price       NUMBER
)
/
</code>
<p>
Before continuing, I feel obligated to point out that for this particular transformation (one row in stocks to two rows in tickers), you don't need a table function to get the job done. For example, you can use INSERT ALL to insert into tickers twice:
</p>
<code>/* You do NOT need to run this for the tutorial! */

INSERT ALL
   INTO tickers (ticker, pricedate, pricetype, price) 
        values (ticker, trade_date, 'O', open_price)
   INTO tickers (ticker, pricedate, pricetype, price) 
        values (ticker, trade_date, 'C', close_price)
SELECT * FROM stocks
/
</code>
<p>
You could also use unpivot (thanks, Chris Saxon @chrisrsaxon, for making me aware of this technique!):
</p>
<code>/* You do NOT need to run this for the tutorial! */

INSERT INTO tickers (ticker,
                     pricedate,
                     pricetype,
                     price)
   SELECT *
     FROM stocks UNPIVOT (price
                 FOR price_type
                 IN (open_price AS 'O', close_price AS 'C'))
/
</code>
<p>
And if you <i>can</i> avoid the use of a table function, implementing your requirement in ""pure"" SQL instead, then you should by all means do so!
</p>
<p>
Please assume for the purposes of this tutorial that the transformation is much more complex and requires the use of PL/SQL.
</p>
<p>
And if you did happen to run either of those INSERTs above, you might want to clear out the table before proceeding:
</p>
<code>TRUNCATE TABLE tickers
/
</code>",07-JUN-18 02.35.29.341267000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUL-18 04.50.40.778121000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146297612867934045132532399828577825841,146218766402242523928587654762999856560,"Types and Package for Table Function",30,"<p>
As discussed in Module 2 of the Getting Started with Table Functions class (and tutorial), when you need that function to return more than one piece of data in each collection element, you need to create an object type and a collection of those object types. 
</p>
<p>
In this case, I want to move stock data to the tickers table, so I need an object type that ""looks like"" the tickers table.
</p>
<code>CREATE TYPE ticker_ot AUTHID DEFINER IS OBJECT 
(
   ticker VARCHAR2 (20),
   pricedate DATE,
   pricetype VARCHAR2 (1),
   price NUMBER
);
/

CREATE TYPE tickers_nt AS TABLE OF ticker_ot;
/
</code>
<p>
Since I am going to use the table function in a streaming process, I will also need to define a strong REF CURSOR type that will be used as the datatype of the parameter accepting the dataset inside the SQL statement.
</p>
<code>CREATE OR REPLACE PACKAGE stock_mgr AUTHID DEFINER
IS
   TYPE stocks_rc IS REF CURSOR RETURN stocks%ROWTYPE;

   TYPE tickers_rc IS REF CURSOR RETURN tickers%ROWTYPE;
END stock_mgr;
/</code>",07-JUN-18 03.26.08.586495000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 06.54.03.180211000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146295767362673348201358067911065662524,146218766402242523928587654762999856560,"Define the Table Function",40,"<p>
The main distinction with streaming table functions is that at least one parameter to that function is a cursor variable. The table function could have more than one cursor variable input, and other parameters of other types, such as a string or date). In this tutorial, we will stick with the minimum: a single cursor variable parameter.
</p>
<p>
Generally, the flow within a streaming table function is:
</p>
<ul>
<li>Fetch a row from the cursor variable.</li>
<li>Apply the transformation to each row.</li>
<li>Put the transformed data into the collection.</li>
<li>Return the collection when done.</li>
</ul>
<p>
Now let's see how this pattern unfolds in my doubled function (one row doubled to two). Line numbers may be seen in the worksheet window when you press the Insert into Editor button.
</p>
<ul>
<li>Line 1: Use the REF CURSOR type defined in the package for the rows passed in. Since we are selecting from the stocks table, we use the stocks_rc type.</li>
<li>Line 2: Return an array, each of whose elements looks <i>just like</i> a row in the tickers table.</li>
<li>Lines 5-6: Declare an associative array to hold rows fetched from the rows_in cursor variable.</li>
<li>Line 8: The local variable to be filled and then returned back to the SELECT statement.</li>
<li>Line 10: Start up a simple loop to fetch rows from the cursor variable. It's already open - the CURSOR expression takes care of that.</li>
<li>Lines 11-12: Use the BULK COLLECT feature to retrieve up to 100 rows with each fetch. We do this to avoid row-by-row processing, which is not efficient enough. Exit the loop when the associative array is empty.</li>
<li>Line 14: For each element in the array (row from the cursor variable)....</li>
<li>Lines 16-21: Use EXTEND to add another element at the end of the nested table, then call the object type constructor to create the first (""open"") of the two rows for tickers table, and put it in the new last index value of the collection.</li>
<li>Lines 23-28: Do the same for the second (""closed"") row of the tickers table.</li>
<li>Line 31: Close the cursor variable, now that all rows have been fetched. Note: this step is optional. When you use a CURSOR expression to pass in the result set, the cursor will be closed automatically when the function terminates.</li>
<li>Line 33: Send the nested table back to the SELECT statement for streaming.</li>
</ul>
<p>
<code>CREATE OR REPLACE FUNCTION doubled (rows_in stock_mgr.stocks_rc)
   RETURN tickers_nt
   AUTHID DEFINER
IS
   TYPE stocks_aat IS TABLE OF stocks%ROWTYPE INDEX BY PLS_INTEGER;
   l_stocks    stocks_aat;

   l_doubled   tickers_nt := tickers_nt ();
BEGIN
   LOOP
      FETCH rows_in BULK COLLECT INTO l_stocks LIMIT 100;
      EXIT WHEN l_stocks.COUNT = 0;

      FOR l_row IN 1 .. l_stocks.COUNT
      LOOP
         l_doubled.EXTEND;
         l_doubled (l_doubled.LAST) :=
            ticker_ot (l_stocks (l_row).ticker,
                       l_stocks (l_row).trade_date,
                       'O',
                       l_stocks (l_row).open_price);

         l_doubled.EXTEND;
         l_doubled (l_doubled.LAST) :=
            ticker_ot (l_stocks (l_row).ticker,
                       l_stocks (l_row).trade_date,
                       'C',
                       l_stocks (l_row).close_price);
      END LOOP;
   END LOOP;
   CLOSE rows_in;

   RETURN l_doubled;
END;
/
</code>
<p>
Regarding FETCH-BULK COLLECT-LIMIT
</p>
<p>
I used a value of 100 for the LIMIT clause; that's a decent default value - it's the number of rows retrieved by cursor FOR loops with each fetch. But if you are processing an extremely large number of rows and want to squeeze better performance out of your function, you might try a larger LIMIT value. Note, however, that this will consume more Process Global Area memory, and at some point your code will slow down due to excessive memory consumption.
</p>
<p>
You should also pass the LIMIT value as a parameter to give you the ability to modify the performance-memory profile without recompiling your function, as in:
</p>
<pre>
CREATE OR REPLACE FUNCTION doubled (
   rows_in stock_mgr.stocks_rc, limit_in IN INTEGER DEFAULT 100)
...
BEGIN
   LOOP
      FETCH rows_in BULK COLLECT INTO l_stocks LIMIT limit_in;
</pre>",07-JUN-18 04.01.30.289093000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",08-JUN-18 08.17.50.406714000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146308251126805873031939714825478529515,146191602965377274749803460241482600523,"Where Table Functions Can Be Defined",60,"To invoke a table function inside a SELECT statement, it must be defined at the schema level (CREATE OR REPLACE FUNCTION - you've seen this already) or in the specification of a package. It cannot be defined as a nested subprogram or a private subprogram.
</p>
<p>
Here's an example of a package-based table function:
</p>
<code>CREATE OR REPLACE TYPE strings_t IS TABLE OF VARCHAR2 (100);
/

CREATE OR REPLACE PACKAGE tf
IS
   FUNCTION strings RETURN strings_t;
END;
/

CREATE OR REPLACE PACKAGE BODY tf
IS
   FUNCTION strings RETURN strings_t
   IS
   BEGIN
      RETURN strings_t ('abc');
   END;
END;
/

SELECT COLUMN_VALUE my_string FROM TABLE (tf.strings)
/
</code>
<p>
But a reference to a nested or private subprogram cannot be resolved in the SQL layer, which is where of course the SELECT statement executes, so errors result, as shown below.
</p>
<code>DECLARE
   FUNCTION nested_strings (count_in IN INTEGER) RETURN strings_t
   IS
   BEGIN
      RETURN strings_t ('abc');
   END;
BEGIN
   FOR rec IN (SELECT * FROM TABLE (nested_strings()))
   LOOP
      DBMS_OUTPUT.PUT_LINE (rec.COLUMN_VALUE);
   END LOOP;
END;
/
</code>
<pre>
PLS-00231: function 'NESTED_STRINGS' may not be used in SQL 
</pre>
<p>
New to 12.1, you can now use the WITH clause to define functions directly inside a SELECT statement. Such a function can also be used as a table function (warning: as of July 2018, this syntax is not yet supported in LiveSQL; it <i>will</i> work in SQL Developer, SQLcl or SQL*Plus):
</p>
<code>WITH 
  FUNCTION strings RETURN strings_t 
  IS 
  BEGIN 
     RETURN strings_t ('abc'); 
  END; 
SELECT COLUMN_VALUE my_string FROM TABLE (strings) 
/
</code>",07-JUN-18 09.14.54.792718000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 07.09.21.517907000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
144904245953890411465072331214509870203,144905567362239111111827484063426187048,"Connect By",20,"<p>Connect by is an Oracle-specific way to create data trees using SQL. It has two key clauses, start with and connect by.</p>

<h3>Start With</h3>

<p>You state which rows are the roots here. These are the rows that appear at the ""top"" of the tree.</p>

<p>In a company org chart this is the CEO. Here that's employee_id 100, Steven King. So you can begin the chart with him using:</p>

<pre>start with employee_id = 100</pre>

<p>But if you do this, you need to change your query when a new CEO replaces him!</p>

<p>It's better to go with a more generic method. The CEO has no manager they report to. So their manager_id is null. So you could identify them with:</p>

<pre>start with manager_id is null</pre>

<h3>Connect By</h3>

<p>You state the parent-child relationship here. This links the columns that store the parent and child values. You access values from the parent row using the keyword prior.</p>

<p>In a company each employee's ""parent"" is their manager. Thus you need to join the parent row's employee_id to the child's manager_id. So you connect the prior employee_id to the current manager_id, like so:</p>

<pre>connect by prior employee_id = manager_id</pre>

<p>Put this all together and you get the following query:</p>

<code>select * from employees
start with manager_id is null
connect by prior employee_id = manager_id;</code>",25-MAY-18 08.17.24.572855000 AM,"CHRIS.SAXON@ORACLE.COM",15-MAY-20 04.18.08.365309000 PM,"CHRIS.SAXON@ORACLE.COM"
146218766402786540547414237891617635760,146218766402242523928587654762999856560,"Introduction",10,"<p>
Before diving into the details, here's an example of the use of a streaming table function:
</P>
<pre>
INSERT INTO tickers
   SELECT *
     FROM TABLE (doubled (CURSOR (SELECT * FROM stocks)))
/
</pre>
<p>
What's going on here? Let's take it step by step ""from the inside out"":
</p>
<ul>
<li>SELECT * FROM stocks: Get all the rows from the stocks table.</li>
<li>CURSOR (): Create a cursor variable with the CURSOR expression that points to the result set.</li>
<li>Pass that cursor variable to the doubled table function.</li>
<li>doubled (): The table function performs its transformation and returns a nested table of object type instances.</li>
<li>SELECT * FROM TABLE(...): Convert the collection into a relational set of rows and columns.</li>
<li>INSERT INTO tickers: Insert those rows into the tickers table.</li>
</ul>
<p>
Sometimes (often?) you will need to perform more than one transformation as part of the streaming process. No problem - you can certainly ""string together"" multiple invocations of table functions, as you see below (all the code to implement and demonstrate this statement follows in this tutorial).
</p>
<pre>
INSERT INTO tickers
SELECT *
  FROM TABLE (transform2 (
                CURSOR (SELECT *
                          FROM TABLE(transform1 (
                                        CURSOR (SELECT * FROM stocks
       ))))))
</pre>",06-JUN-18 09.55.11.478964000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 02.34.42.385164000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146214132714389536133612486652527944517,146206374662252057210273830044561075244,"Use the Table Function",40,"<p>
In my SELECT, I can reference the names of the attributes as the names of the columns in the dataset returned by the TABLE clause. Notice that I do not need a table alias (which is required when a column in your relational table is an object type and you want to reference an attribute of the type). The SQL engine simply hides all the work it is doing to convert each attribute of the object type in the array to a column. Thanks, SQL!
</p>
<code>SELECT name, species, date_of_birth
  FROM TABLE (
          animal_family (animal_ot ('Hoppy', 'RABBIT', SYSDATE - 500),
                         animal_ot ('Hippy', 'RABBIT', SYSDATE - 300)))
/ 
</code>
<p>
Here's an example of taking the result set from the function and inserted them directly into the table. This works so smoothly because the animal_ot object type attributes match the columns of the table. 
</p>
<code>INSERT INTO animals
SELECT name, species, date_of_birth
  FROM TABLE (
          animal_family (animal_ot ('Hoppy', 'RABBIT', SYSDATE - 500),
                         animal_ot ('Hippy', 'RABBIT', SYSDATE - 300)))
/ 
</code>
<p>
Of course, there's more to life than just rabbits, so let's make sure our function works (and works differently) for kangaroos.
</p>
<code>SELECT name, species, date_of_birth
  FROM TABLE (
          animal_family (animal_ot ('Bob',   'KANGAROO', SYSDATE - 1000),
                         animal_ot ('Sally', 'KANGAROO', SYSDATE - 700)))
/ 
</code>",06-JUN-18 09.00.21.288550000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",06-JUN-18 09.28.03.469055000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146290891075172896707338151794500761960,146218766402242523928587654762999856560,"Let's Do Some Streaming!",50,"<code>SELECT COUNT (*) FROM tickers
/

INSERT INTO tickers
   SELECT * 
     FROM TABLE (doubled (CURSOR (SELECT * FROM stocks)))
/

SELECT COUNT (*) FROM tickers
/

     SELECT *
       FROM tickers
FETCH FIRST 10 ROWS ONLY
/
</code>",07-JUN-18 04.18.03.871043000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 06.52.43.375625000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146290891075174105633157766423675468136,146218766402242523928587654762999856560,"Summary",70,"<p>
Streaming table functions play a crucial role in data warehouse ETLs (extract-transform-load) operations. Oracle Database makes building such functions easy through its implementation of PL/SQL cursor variables and the CURSOR expression.
</p>
<p>
Remember that the collection constructed and returned by a streaming table function will consume PGA memory, so very large data sets passed in to the function via the cursor variable could result in errors.
</p>
<p>
What can you do about that? Make that streaming table function a <i>pipelined</i> streaming table function - which we cover in the next module of the class.
</p>",07-JUN-18 04.18.34.958573000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",07-JUN-18 07.06.29.847712000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146702446915893409462858188183894294517,146417473425260160697587333524378916539,"A Very Simple Example",20,"<p>
Let's start our exploration of pipelined table functions (which I will also refer to as PTFs in this tutorial) with about as simple an example as you can get. Here are explanations for the steps taken in the code section:
</p>
<ol>
<li>Create a schema-level nested table type of strings.</li>
<li>Implement the pipelined table function. Notice that there 
are no parameters (unusual for a pipelined table function, but allowed) and the keyword PIPELINED. Two things of interest to note here: (1) the PIPE ROW statement is used to ""pipe"" the row of data back to the invoking SELECT, and (2) the RETURN statement does not return data, only control.</li>
<li>And a step <i>not</i> taken: </li>
<li>Call the table function inside a SELECT. Voila!</li>
</ol>
<code>CREATE OR REPLACE TYPE strings_t IS TABLE OF VARCHAR2 (100);
/

CREATE OR REPLACE FUNCTION strings 
   RETURN strings_t PIPELINED
   AUTHID DEFINER
IS
BEGIN
   PIPE ROW ('abc');
   RETURN;
END;
/

SELECT COLUMN_VALUE my_string FROM TABLE (strings())
/

/* And in 12.2 and higher, no need for TABLE */

SELECT COLUMN_VALUE my_string FROM strings()
/
</code>
<p>
What about executing this function in PL/SQL?
</p>
<code>
DECLARE
   l_strings strings_t := strings_t();
BEGIN
   l_strings := strings ();
END;
/
</code>
<p>
You will see this error:
</p>
<pre>PLS-00653: aggregate/table functions are not allowed in PL/SQL scope</pre>
<p>
 This makes a lot of sense. PL/SQL is not a multi-threaded language. It cannot accept rows that are ""piped"" back before the function terminates execution.
</p>
<p>
To be very clear: non-pipelined table functions <i>can</i> be invoked natively in PL/SQL, since they follow the standard model in PL/SQL: execute all code until hitting the RETURN statement or an exception is raised. But when you go with PIPELINED, you give up the ability to call the function in PL/SQL.
</p>
<p>
Of course, in this incredibly basic pipelined table function, there will be no issue of blocking or memory consumption. But it gets across the basic elements of a pipelined table function:
</p>
<ol>
<li>Add the PIPELINED keyword to the function header.</li>
<li>Use PIPE ROW to send the data back to the calling SELECT, instead of adding the data to a local collection.</li>
<li>RETURN nothing but control.</li>
</ol>",11-JUN-18 01.14.19.662079000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",24-AUG-18 07.17.33.718251000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
144905567362598162080253028928313921320,144905567362239111111827484063426187048,"Level",60,"<p>With the queries so far it's hard to tell how senior someone is in the company. Adding the generation they belong to makes this easier. This allows you to see who's at the same level in the hierarchy. How you do this depends on whether you're using connect by or recursive with.</p>

<h3>Connect By</h3>

<p>With connect by you can use the pseudo-column level. This returns the current depth in the tree, starting with 1 for the roots. Each new set of children increases the depth by one. So all a manager's reports are one level below them.</p>

<p>You can select this on its own to show the current row's depth in the tree:</p>

<code>select level, employee_id, first_name, last_name, manager_id 
from   employees
start  with manager_id is null
connect by prior employee_id = manager_id;</code>

<p>This helps. But it's still tricky to tell how senior someone is in the company. Indenting their name based on their position makes this easier. Combining level with lpad makes this easy.</p>

<p>The syntax of lpad is:</p>

<pre>lpad ( str1, N, str2 )</pre>

<p>It adds the characters in str2 before those in str1 until the string is N characters long. So you can use level to place spaces before each employee's name. This makes it easy to see where people place in the organization:</p>

<code>select level, employee_id, 
       lpad ( ' ', level, ' ' ) || first_name || ' ' || last_name name, manager_id 
from   employees
start  with manager_id is null
connect by prior employee_id = manager_id;</code>

<h3>Recursive With</h3>

<p>Recursive with doesn't have an in-built equivalent of level. You need to build your own. You can do this by selecting the value 1 in the base query. And incrementing it by one in the recursive part. For example:</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id, lvl
) as (
  select employee_id, first_name, last_name, manager_id, 1 lvl
  from   employees
  where  manager_id is null
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id, oc.lvl + 1
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
)
  select * from org_chart;</code>

<p>As with connect by, you can use this pad out values in the final select.</p>",25-MAY-18 08.18.32.625492000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 02.52.42.872145000 PM,"CHRIS.SAXON@ORACLE.COM"
144904245953921843536382311573052230779,144905567362239111111827484063426187048,"Try It!",80,"<p>Complete the following query to return employees in depth-first order. You should sort employees with the same manager by first_name:</p>

<code>select level, employee_id, first_name, last_name, hire_date, manager_id 
from   employees
start  with manager_id is null
connect by prior employee_id = manager_id
order /*TODO*/</code>
<p>This query should give the following output:</p>
<pre><b>LEVEL   EMPLOYEE_ID   FIRST_NAME    LAST_NAME   HIRE_DATE      MANAGER_ID   </b>
      1           100 Steven        King        17-JUN-2003          &lt;null&gt;
      2           102 Lex           De Haan     13-JAN-2001             100 
      3           103 Alexander     Hunold      03-JAN-2006             102 
      4           104 Bruce         Ernst       21-MAY-2007             103 
      4           105 David         Austin      25-JUN-2005             103 
      4           107 Diana         Lorentz     07-FEB-2007             103 
      4           106 Valli         Pataballa   05-FEB-2006             103 
      2           101 Neena         Kochhar     21-SEP-2005             100 
      3           108 Nancy         Greenberg   17-AUG-2002             101 
      4           109 Daniel        Faviet      16-AUG-2002             108 
      4           111 Ismael        Sciarra     30-SEP-2005             108 
      4           110 John          Chen        28-SEP-2005             108 
      4           112 Jose Manuel   Urman       07-MAR-2006             108 
      4           113 Luis          Popp        07-DEC-2007             108</pre>

<p>Can you write this using recursive with?</p>",25-MAY-18 08.19.04.391496000 AM,"CHRIS.SAXON@ORACLE.COM",12-DEC-19 05.20.29.144719000 PM,"CHRIS.SAXON@ORACLE.COM"
147831487722757000086703917394123568624,147831487722688091314985883531165316592,"Insert-then-update OR Update-then-insert",20,"<p>When adding rows to a table, sometimes you want to do insert-if-not-exists, update-if-exists logic. Aka an upsert. Writing this as separate insert and update statements is cumbersome.</p>

<p>For example, say you want to upsert a blue pyramid into the table of purchased_bricks. If it exists, you want to change its price. And if it doesn't, you want to add it.</p>

<p>You can do this using the following code block. The attribute sql%rowcount returns the number of rows affected by the last statement. You can use this to see if an update changed any rows. If it returns zero, it did nothing. So the row doesn't exist and you need to insert it:</p>

<code>declare
  l_colour varchar2(10) := 'blue';
  l_shape  varchar2(10) := 'pyramid';
  l_price  number(10, 2) := 9.99;
begin

  update purchased_bricks pb
  set    pb.price = l_price
  where  pb.colour = l_colour
  and    pb.shape = l_shape;

  if sql%rowcount = 0 then
  
    insert into purchased_bricks
    values ( l_colour, l_shape, l_price );
  
  end if;
  
end;
/

select * from purchased_bricks;</code>

<p>Purchased_bricks was empty before this code. So the update was wasted effort. It would have been better to do the insert first.</p>

<p>You can do this by flipping the statements around. And, instead of checking if the sql%rowcount is zero, trapping a unique key violation:</p>

<code>declare
  l_colour varchar2(10) := 'blue';
  l_shape  varchar2(10) := 'pyramid';
  l_price  number(10, 2) := 15.49;
begin

  insert into purchased_bricks
  values ( l_colour, l_shape, l_price );
  
exception
  when DUP_VAL_ON_INDEX then
  
    update purchased_bricks pb
    set    pb.price = l_price
    where  pb.colour = l_colour
    and    pb.shape = l_shape;
    
end;
/

select * from purchased_bricks;

commit;</code>

<p>But the code above ""added"" the same brick! You needed to update its price. So the insert was a waste here.</p>

<p>When writing upserts, in general it's hard to know whether insert or update is most likely. You can spend a lot of effort figuring this out. Luckily there's a better way: Merge!</p>",22-JUN-18 08.29.23.436454000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-18 09.25.21.640356000 AM,"CHRIS.SAXON@ORACLE.COM"
148144976606514311261804365794945834374,147831487722688091314985883531165316592,"Merging Two Tables",35,"<p>You may also want to upsert two whole tables, so all the rows in the source have a matching row in the target.</p>

<p>You can do this by writing an update-if-exists. Followed by an insert-if-not-exists (or vice-versa), like so:</p>

<code>update purchased_bricks pb
set    pb.price = (
  select bfs.price
  from   bricks_for_sale bfs
  where  pb.colour = bfs.colour 
  and    pb.shape = bfs.shape
)
where  exists (
  select null
  from   bricks_for_sale bfs
  where  pb.colour = bfs.colour 
  and    pb.shape = bfs.shape
);

insert into purchased_bricks ( 
  colour, shape, price
) 
  select bfs.colour, bfs.shape, bfs.price 
  from   bricks_for_sale bfs
  where  not exists (
    select null
    from   purchased_bricks pb
    where  pb.colour = bfs.colour 
    and    pb.shape = bfs.shape
  );

select * from purchased_bricks;

rollback;</code>

<p>But, as with upserting a single row, this a lot of typing and hard to follow. You can simplify the above into the following merge:</p>

<code>merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price;
  
select * from purchased_bricks;

commit;</code>

<p>This allows you to keep target rows in sync with those in the source. But this only affects rows with a match in the source.</p>

<p>For example, if you:</p>

<ul>
<li>Set the price of all bricks_for_sale to 0.99</li>
<li>Add a red pyramid to bricks_for_sale</li>
<li>Add a green cube to purchased_bricks</li>
</ul>

<p>Merge adds the red pyramid. But it will only change the price of the red cube and blue bricks. The green cube has no matching row in bricks_for_sale. So its price stays the same:</p>

<code>update bricks_for_sale
set    price = 0.99;

insert into bricks_for_sale values ( 'red', 'pyramid', 5.99 );
insert into purchased_bricks values ( 'green', 'cube', 9.95 );

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price;
  
select * from purchased_bricks;

rollback;</code>",25-JUN-18 09.34.18.459135000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 01.36.56.690127000 PM,"CHRIS.SAXON@ORACLE.COM"
147832521372468255358457765769633375972,147831487722688091314985883531165316592,"Try It!",90,"<p>Complete the following merge statement, so it removes matched rows from purchased_bricks that have a price less than 9:</p>

<code>select * from purchased_bricks;

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price
  delete /* TODO */ ;
  
select * from purchased_bricks;

rollback;</code>
<p>After the merge completes, the query of purchased_bricks should return the following rows:</p>
<pre><b>COLOUR   SHAPE     PRICE   </b>
blue     pyramid      9.99 </pre>",22-JUN-18 08.36.47.505951000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 09.29.44.525660000 AM,"CHRIS.SAXON@ORACLE.COM"
147831487723257495376024373872451925488,147831487722688091314985883531165316592,"Try It!",70,"<p>Complete the where clauses in the merge statement below, so that it:</p>

<ul><li>Only updates cubes</li>
<li>Only inserts green coloured rows</li></ul>

<code>update bricks_for_sale 
set    price = 49.99;

insert into bricks_for_sale values ( 'red', 'pyramid', 5.99 );
insert into bricks_for_sale values ( 'green', 'pyramid', 5.99 );

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
  where  /* TODO */
when matched then
  update set pb.price = bfs.price
  where  /* TODO */;

select * from purchased_bricks
order  by colour, shape;

rollback;</code>
<p>After running all the code above (update, inserts, and completed merge) the output of the query on purchased_bricks should be:</p>
<pre><b>COLOUR   SHAPE     PRICE   </b>
blue     cube        49.99 
blue     pyramid      9.99 
green    pyramid      5.99 
red      cube        49.99 </pre>",22-JUN-18 08.36.33.740644000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 01.39.38.265940000 PM,"CHRIS.SAXON@ORACLE.COM"
149111946068192053869025091663453520235,149075222712493902096666428584818910723,"Introduction",10,"<p>When two or more people read and write rows to the same table at the same time, you can get inconsistent results. This tutorial discusses these and how to handle them in Oracle Database.</p>

<p>It uses the following table to highlight the issues:</p>

<code>select * from toys;</code>",04-JUL-18 04.31.49.814016000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 08.05.36.814820000 AM,"CHRIS.SAXON@ORACLE.COM"
149117521792642367699476052291160426714,149075222712493902096666428584818910723,"Set Transaction Isolation Level",40,"<p>You can change the isolation level for a transaction using the set transaction statement. For example, to set it to read committed, run:</p>

<code>set transaction isolation level read committed;</code>

<p>This must be the first statement in the transaction. If you try and change it part way through, you'll hit an error:</p>

<code>insert into toys values ( 'Purple Ninja', 19.99 );
set transaction isolation level read committed;
rollback;
</code>",04-JUL-18 04.32.49.591159000 PM,"CHRIS.SAXON@ORACLE.COM",05-JUL-18 01.42.52.013691000 PM,"CHRIS.SAXON@ORACLE.COM"
144905567362664653000331833532922761000,144905567362239111111827484063426187048,"Displaying Tree Details: Connect By",100,"<p>Using level makes it possible to see how deep the current row is in the tree. But it can still be tricky to see how the rows relate to each other. Connect by has many options to help with this.</p>

<h3>Connect_by_root</h3>

<p>The operator connect_by_root returns the value of a column from the root row.</p>

<h3>Sys_connect_by_path </h3>

<p>It can be useful to see values from all the rows between the root and the current row. Sys_connect_by_path allows you to do this. It builds up a string, adding the value from the first argument for the current row to the end of the list. It separates these using the second argument.</p>

<h3>Connect_by_isleaf</h3>

<p>You can identify the leaf rows using connect_by_isleaf. This returns 1 if the current row is a leaf. Otherwise it returns 0.</p>

<p>Combining these allows you to display:</p>

<ul><li>The last_name of the CEO (the root row) on every row</li>
<li>A comma separated list of the management chain from the current employee up to the CEO</li>
<li>The employees who aren't managers (the leaves)</li></ul>

<p>The complete query for this is:</p>

<code>select employee_id, first_name, last_name, manager_id,
       connect_by_root last_name,
       sys_connect_by_path ( last_name, ', ') chart,
       connect_by_isleaf is_leaf
from   employees
start with manager_id is null
connect by prior employee_id = manager_id;</code>",25-MAY-18 08.20.03.127566000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 03.16.26.417661000 PM,"CHRIS.SAXON@ORACLE.COM"
147831487722779969677276595348442985968,147831487722688091314985883531165316592,"Conditional Merging",60,"<p>When merging tables, you may want to preserve values for some of the target's existing rows. Or stop users inserting certain new values.</p>

<p>You can do both of these using a where clause in the matched clauses. This is like a regular where clause and comes after the insert or update, e.g.:</p>

<pre>  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
  where  bfs.colour = 'blue'</pre>

<p>For example, the following code updates the price of all the bricks for sale to 100 and adds red pyramid. But the merge includes a filter to only affect blue rows in both clauses. So the price of the red cube remains 4.95 and the red pyramid is not added to purchased_bricks:</p>

<code>update bricks_for_sale 
set    price = 100;

insert into bricks_for_sale values ( 'red', 'pyramid', 5.99 );

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
  where  bfs.colour = 'blue'
when matched then
  update set pb.price = bfs.price
  where  bfs.colour = 'blue';
  
select * from purchased_bricks;

rollback;</code>",22-JUN-18 08.30.58.710217000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 01.39.11.435340000 PM,"CHRIS.SAXON@ORACLE.COM"
147831487722817446377684648852858877424,147831487722688091314985883531165316592,"Merge + Delete",80,"<p>You can also use merge to remove rows from the target table. This will only happen for rows in the target that have a matching row in the source.</p>

<p>To do this, add a delete clause after the update in the when matched clause. For example, to remove the matching blue rows, use:</p>

<pre>when matched then
  update set pb.price = bfs.price
  delete where pb.colour = 'blue'</pre>

<p>The delete uses values from the target table after applying the update.</p>

<p>Remember: this only changes existing rows. If the merge adds a new row that meets the criteria, it remains in the target table.</p>

<p>For example, at this point purchased_bricks should have two blue rows in it. If you delete rows with the colour blue in the when matched clause, only these are removed.</p>

<p>You can insert another blue brick into bricks_for_sale, which the merge will add. This remains in purchased_bricks after the merge completes, even though the delete removes blue rows:</p>

<code>insert into bricks_for_sale values ( 'blue', 'cuboid', 5.99 );

select * from purchased_bricks;

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price
  delete where pb.colour = 'blue' ;
  
select * from purchased_bricks;

rollback;</code>",22-JUN-18 08.31.18.242480000 AM,"CHRIS.SAXON@ORACLE.COM",03-SEP-18 01.28.54.769064000 PM,"CHRIS.SAXON@ORACLE.COM"
147831487723229690082173237401433683440,147831487722688091314985883531165316592,"Try It!",40,"<p>Replace the /* TODO */ sections below to complete the following merge. It should add the yellow cube to purchased_bricks. And update the price of the red brick to 5.55:</p>

<code>merge into purchased_bricks pb
using ( 
  select 'yellow' colour, 'cube' shape, 9.99 price from dual 
  union all
  select 'red' colour, 'cube' shape, 5.55 price from dual 
) bfs
on    (  /* TODO */  )
when not matched then
  insert /* TODO */
when matched then
  update /* TODO */;
  
select * from purchased_bricks
order  by colour, shape;

rollback;</code>
<p>The query should return the following rows:</p>
<pre><b>COLOUR	SHAPE	  PRICE</b>
blue	cube	   7.75
blue	pyramid    9.99
red	cube	   5.55
yellow	cube	   9.99</pre>",22-JUN-18 08.34.20.301276000 AM,"CHRIS.SAXON@ORACLE.COM",03-SEP-18 01.28.38.945712000 PM,"CHRIS.SAXON@ORACLE.COM"
146318517465076751119839789870039730192,146191602965377274749803460241482600523,"Valid Collection Types for Table Functions",70,"<p>
There are, basically, two things to keep in mind when it comes to the collection type used in the RETURN clause of a table function:
</p>
<ol>
<li>The collection type must be declared so that the SQL engine can resolve a reference to it.</li>
<li>The collection type (or the attributes within that type) must be SQL-compatible. You cannot, for example, return a collection of Booleans for a table function.</li>
</ol>
<p>
The SQL engine generally can resolve references to types and PL/SQL programs if they are defined at the schema level or within the specification of a package. When it comes to table functions, however, types defined within a package specification can only be used with <i>pipelined</i> table functions (explored in Module 4 of this class). This is demonstrated in the following code. The strings_sl and strings_pl functions can be invoked successfully as table functions. 
</p>
<code>CREATE OR REPLACE TYPE sl_strings_t IS TABLE OF VARCHAR2 (100);
/

CREATE OR REPLACE PACKAGE tf
IS
   TYPE strings_t IS TABLE OF VARCHAR2 (100);
   FUNCTION strings RETURN strings_t;
   FUNCTION strings_sl RETURN sl_strings_t;
   FUNCTION strings_pl RETURN strings_t PIPELINED;
END;
/

CREATE OR REPLACE PACKAGE BODY tf
IS
   FUNCTION strings RETURN strings_t
   IS
   BEGIN
      RETURN strings_t ('abc');
   END;
   
   FUNCTION strings_sl RETURN sl_strings_t
   IS
   BEGIN
      RETURN sl_strings_t ('abc');
   END;
   
   FUNCTION strings_pl RETURN strings_t PIPELINED
   IS
   BEGIN
      PIPE ROW ('abc');
      RETURN;
   END;
END;
/

SELECT COLUMN_VALUE my_string FROM TABLE (tf.strings)
/

SELECT COLUMN_VALUE my_string FROM TABLE (tf.strings_sl)
/

SELECT COLUMN_VALUE my_string FROM TABLE (tf.strings_pl)
/
</code>

<p>
But when I try to use the strings function, I get the ""ORA-00902: invalid datatype"" error, since it relies on a package-defined nested table type and it is not pipelined.
</p>
<code>SELECT COLUMN_VALUE my_string FROM TABLE (tf.strings)
/
</code>",07-JUN-18 09.26.30.504989000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 04.50.18.527521000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146417473427557119754855128956320650939,146417473425260160697587333524378916539,"Package-based Types Implicitly-Declared",50,"With pipelined table functions <i>only</i>, your types can be defined in the specification of a package, as opposed to in the schema with independent CREATE OR REPLACE statements. You can even declare your nested table as a collection of record types! 
</p>
<code>CREATE TABLE stocks2
(
   ticker        VARCHAR2 (20),
   trade_date    DATE,
   open_price    NUMBER,
   close_price   NUMBER
)
/

CREATE OR REPLACE PACKAGE pkg
   AUTHID DEFINER
AS
   TYPE stocks_nt IS TABLE OF stocks2%ROWTYPE;

   FUNCTION stock_rows
      RETURN stocks_nt
      PIPELINED;
END;
/

CREATE OR REPLACE PACKAGE BODY pkg
AS
   FUNCTION stock_rows
      RETURN stocks_nt
      PIPELINED
   IS
      l_stock   stocks2%ROWTYPE;
   BEGIN
      l_stock.ticker := 'ORCL';
      l_stock.open_price := 100;
      PIPE ROW (l_stock);
      RETURN;
   END;
END;
/

SELECT ticker, open_price FROM TABLE (pkg.stock_rows ())
/
</code>
<p>
This is more convenient, but behind the scenes, Oracle Database is creating types implicitly for you, as can see below.
</p>
<code>SELECT object_name, object_type
  FROM user_objects
 WHERE object_type IN ('TYPE', 'PACKAGE', 'PACKAGE BODY')
/
</code>",08-JUN-18 08.09.31.799350000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",11-JUN-18 08.55.12.837595000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
149117521792621815960542603595190421722,149075222712493902096666428584818910723,"Autonomous Transactions",20,"<p>Read consistency issues can happen when you have two sessions accessing the same data. But LiveSQL doesn't allow you to have two interacting sessions.</p>

<p>Luckily there's a workaround: autonomous transactions.</p>

<p>These are transactions that take place inside another.</p>

<p>You do this with the autonomous_transaction pragma. Place it in the declaration section of your PL/SQL:</p>

<pre>declare
  pragma autonomous_transaction;</pre>

<p>The transaction must complete within the block. That is, commit or rollback. This is independent from commits or rollbacks in the parent transaction.</p>

<p>For example, in the following the parent transaction inserts a row for Baby Turtle. At the end it removes it with a rollback. But between these the nested autonomous transaction adds a row for Blue Dinosaur. And commits it. So the final rollback removes Baby Turtle, but not Blue Dinosaur:</p>

<code>insert into toys values ( 'Baby Turtle', 7.95 );

declare
  pragma autonomous_transaction;
begin
  insert into toys values ( 'Blue Dinosaur', 15.95 );
  commit;
end;
/

select * from toys;
rollback;
select * from toys;</code>

<p>Note. It's rare you'll use autonomous transactions in real code. The major use-case for these is logging errors. Here you want to save the exception to a table. But may need to rollback the parent transaction. So you need an autonomous transaction. For almost all other uses they are the Wrong Method.</p>",04-JUL-18 04.32.06.202277000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 08.09.06.621645000 AM,"CHRIS.SAXON@ORACLE.COM"
146705939950851970195237419518889052349,146417473425260160697587333524378916539,"Impact of Switch to Pipelined Table Functions",30,"<p>
Pipelined table functions can help improve performance over a non-pipelined table function, and also reduce PGA memory consumption. Let's take a closer look.
</p>
Run the code below to create the database objects needed to culminate in a non-pipelined table function performing the stocks-to-tickers transformation covered in Module 3 (Streaming Table Functions).
</p>
<code>DROP TYPE ticker_ot FORCE;
DROP TYPE tickers_nt FORCE;
DROP TYPE stock_ot FORCE;
DROP TYPE stocks_nt FORCE;
DROP TABLE stocks;
DROP TABLE tickers;

CREATE TABLE stocks
(
   ticker        VARCHAR2 (20),
   trade_date    DATE,
   open_price    NUMBER,
   close_price   NUMBER
)
/

/* Load up 10000 rows - when running in your own database, you might want to
   use a higher volume of data here, to see a more dramatic difference in the
   elapsed time and memory utilization 
*/

INSERT INTO stocks
       SELECT 'STK' || LEVEL,
              SYSDATE,
              LEVEL,
              LEVEL + 15
         FROM DUAL
   CONNECT BY LEVEL <= 10000
/   

CREATE TABLE tickers
(
   ticker      VARCHAR2 (20),
   pricedate   DATE,
   pricetype   VARCHAR2 (1),
   price       NUMBER
)
/

CREATE TYPE ticker_ot AS OBJECT
(
   ticker VARCHAR2 (20),
   pricedate DATE,
   pricetype VARCHAR2 (1),
   price NUMBER
);
/

CREATE TYPE tickers_nt AS TABLE OF ticker_ot;
/

CREATE OR REPLACE PACKAGE stock_mgr
   AUTHID DEFINER
IS
   TYPE stocks_rc IS REF CURSOR RETURN stocks%ROWTYPE;
END stock_mgr;
/

CREATE OR REPLACE FUNCTION doubled_nopl (rows_in stock_mgr.stocks_rc)
   RETURN tickers_nt
   AUTHID DEFINER
IS
   TYPE stocks_aat IS TABLE OF stocks%ROWTYPE INDEX BY PLS_INTEGER;
   l_stocks    stocks_aat;

   l_doubled   tickers_nt := tickers_nt ();
BEGIN
   LOOP
      FETCH rows_in BULK COLLECT INTO l_stocks LIMIT 100;
      EXIT WHEN l_stocks.COUNT = 0;

      FOR l_row IN 1 .. l_stocks.COUNT
      LOOP
         l_doubled.EXTEND;
         l_doubled (l_doubled.LAST) :=
            ticker_ot (l_stocks (l_row).ticker,
                       l_stocks (l_row).trade_date,
                       'O',
                       l_stocks (l_row).open_price);

         l_doubled.EXTEND;
         l_doubled (l_doubled.LAST) :=
            ticker_ot (l_stocks (l_row).ticker,
                       l_stocks (l_row).trade_date,
                       'C',
                       l_stocks (l_row).close_price);
      END LOOP;
   END LOOP;

   RETURN l_doubled;
END;
/
</code>
<p>
Now let's created a pipelined version of the above function. Line numbers referenced in explanations below are visible after you Insert Into Editor.
</p>
<ul>
<li>Line 1: As a streaming table function, it should by now come as no surprise that the parameter to the function is a cursor variable passing in rows of data from the invoking SELECT.</li>
<li>Line 3: Specify that this is a pipelined table function.</li>
<li>Lines 6-7: Declare the associative array used to retrieve rows fetched from the cursor variable.</li>
<li>Lines 10-11: Fetch the next 100 rows, stop when collection is empty.</li>
<li>Line 13: For each row of data moved to the collection....</li>
<li>Lines 15-18: Use the object type constructor function to ""transfer"" open-price data from the element in the collection to the attributes of the object type instance. Then immediately send that data back to the calling query with PIPE ROW.</li>
<li>Lines 20-23: Do the same thing for the closed-price data.</li>
<li>Lines 27-29: Return control back to the query.</li>
</ul>
<p>
Note that you do not need to explicilty close the cursor variable; Oracle will automatically close a cursor variable created with the CURSOR expression when the table function terminates.
</p>
<code>CREATE OR REPLACE FUNCTION doubled_pl (rows_in stock_mgr.stocks_rc)
   RETURN tickers_nt
   PIPELINED
   AUTHID DEFINER
IS
   TYPE stocks_aat IS TABLE OF stocks%ROWTYPE INDEX BY PLS_INTEGER;
   l_stocks   stocks_aat;
BEGIN
   LOOP
      FETCH rows_in BULK COLLECT INTO l_stocks LIMIT 100;
      EXIT WHEN l_stocks.COUNT = 0;

      FOR l_row IN 1 .. l_stocks.COUNT
      LOOP
         PIPE ROW (ticker_ot (l_stocks (l_row).ticker,
                              l_stocks (l_row).trade_date,
                              'O',
                              l_stocks (l_row).open_price));

         PIPE ROW (ticker_ot (l_stocks (l_row).ticker,
                              l_stocks (l_row).trade_date,
                              'C',
                              l_stocks (l_row).close_price));
      END LOOP;
   END LOOP;

   RETURN;
END;
/
</code>
<p>
Before exploring the impact on performance and memory, let's verify that this pipelined table function can be used in a SELECT just like the non-pipelined version.
</p>
<code>SELECT COUNT(*) FROM tickers
/

INSERT INTO tickers
   SELECT * 
     FROM TABLE (doubled_pl (CURSOR (SELECT * FROM stocks)))
/

SELECT * FROM tickers
 WHERE ROWNUM < 20
/
</code>
<p>
So now let's prove to you that the SQL engine is able to take those piped rows and put them immediately to work! The line numbers referenced in the explanations below are visible after you load the code into the editor.
</p>
<ul>
<li>Lines 2-26: A small timer API - I use DBMS_UTILITY.get_time to return the number of hundredths of seconds that have elapsed since an arbitrary point in time. I store the start time in a local variable and the subtract the current time from the start time the amount of elapsed time.</li>
<li>Lines 30-33: I call the pipelined version of the doubled function to insert rows into the tickers table. But I add a ROWNUM clause to say ""I only want the first 9 rows.""</li>
<li>Lines 39-42: Do the same thing with the NON-pipelined version.</li>
</ul>
<code>CREATE OR REPLACE PACKAGE utils
IS
   PROCEDURE initialize (context_in IN VARCHAR2);

   PROCEDURE show_results (message_in IN VARCHAR2 := NULL);
END;
/

CREATE OR REPLACE PACKAGE BODY utils
IS
   last_timing   INTEGER := NULL;
   last_pga   INTEGER := NULL;

   FUNCTION pga_consumed
      RETURN NUMBER
   AS
      l_pga   NUMBER;
   BEGIN
      SELECT st.VALUE
        INTO l_pga
        FROM v$mystat st, v$statname sn
       WHERE st.statistic# = sn.statistic# AND sn.name = 'session pga memory';

      RETURN l_pga;
   END;

   PROCEDURE initialize (context_in IN VARCHAR2)
   IS
   BEGIN
      DELETE FROM tickers;
      COMMIT;
      DBMS_OUTPUT.put_line (context_in);
      last_timing := DBMS_UTILITY.get_time;
      last_pga := pga_consumed;
   END;

   PROCEDURE show_results (message_in IN VARCHAR2 := NULL)
   IS
      l_count   INTEGER;
   BEGIN
      SELECT COUNT (*) INTO l_count FROM tickers;

      DBMS_OUTPUT.put_line ('Ticker row count: ' || l_count);

      DBMS_OUTPUT.put_line (
         '""' || message_in || '"" completed in: ' || 
         TO_CHAR (DBMS_UTILITY.get_time - last_timing)||' centisecs; pga at: '||
         TO_CHAR (pga_consumed() - last_pga) || ' bytes');
   END;
END;
/
</code>
<code>BEGIN
   utils.initialize ('Pipelined');

   INSERT INTO tickers
      SELECT *
        FROM TABLE (doubled_pl (CURSOR (SELECT * FROM stocks)))
       WHERE ROWNUM < 10;

   utils.show_results ('First 9 rows');

   utils.initialize ('Not Pipelined');

   INSERT INTO tickers
      SELECT *
        FROM TABLE (doubled_nopl (CURSOR (SELECT * FROM stocks)))
       WHERE ROWNUM < 10;

   utils.show_results ('First 9 rows');
END;
/
</code>
<p>
The results shown below are for an initial load of 200,000 rows into the stocks table. That volume of data is not supported in LiveSQL sessions, so the results you see here will be less dramatic, but the pattern is the same.
</p>
<p>
The significantly faster response time with the pipelined function demonstrates clearly that the INSERT-SELECT statement was able to keep track of rows returned by the function. As soon as nine rows were passed back, the SQL engine terminated execution of the pipelined table function and inserted the rows.
</p>
<p>
With the non-pipelined version, we have to wait for 10,000 rows to be doubled into 20,000 rows (consuming lots of Process Global Area memory, as well). Then all those rows are passed back to the SELECT statement, at which point the SQL engine says ""Well, I just wanted the first 9."" and throws away the rest.
</p>
<p>
Very inefficient.
</p>
<p>From a memory standpoint, the non-pipelined table function consumes much more PGA memory then the pipelined version.
<pre>
Pipelined
""First 9 rows"" completed in: 8 centisecs; pga at: 528345 bytes
Ticker row count: 9

Not Pipelined
""First 9 rows"" completed in: 93 centisecs; pga at: 96206848 bytes
Ticker row count: 9
</pre>",11-JUN-18 02.02.30.243113000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",25-JUN-18 05.59.50.671933000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146824294818123678084006733519283600374,146417473425260160697587333524378916539,"Summary",60,"<p>
Pipelined table functions are something of an oddity in PL/SQL: they pass data back to the calling query, even before the function is completed; they don't pass back anything but control with the RETURN statement; you cannot call a PTF from within PL/SQL itself, only in the FROM clause of a SELECT.
</p>
<p>
But those oddities reflect the power of this special type of function: improved performance and reduced memory consumption, over normal (non-pipelined) table functions.
</p>
<p>
This tutorial covers the fundamental features and behavior of pipelined table functions. You may also want to explore parallel-enabling pipelined table functions. Follow the links at the bottom of this tutorial for more details on this topic.
</p>",12-JUN-18 05.55.14.666957000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-18 06.09.01.283025000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
148152729283380485867665647195403328880,147831487722688091314985883531165316592,"Single Operation Merge",75,"<p>Both the when matched and when not matched clauses of merge are optional. So you can have insert only or update only merges.</p>

<p>For example, you could split the previous merges into two:</p>

<code>merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price );

merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when matched then
  update set pb.price = bfs.price;
  
select * from purchased_bricks;

rollback;</code>

<p>Separating merges like this is a bad idea. In general you should combine SQL into as few statements as possible.</p>

<p>But an update only merge can be useful in some cases. Say you want to update the price of all the purchased_bricks with their current sale price. You can use the correlated update from part 4 of this tutorial:</p>

<pre>update purchased_bricks pb
set    pb.price = (
  select bfs.price
  from   bricks_for_sale bfs
  where  pb.colour = bfs.colour 
  and    pb.shape = bfs.shape
)
where  exists (
  select null
  from   bricks_for_sale bfs
  where  pb.colour = bfs.colour 
  and    pb.shape = bfs.shape
);</pre>

<p>You need the where exists clause to ensure you only update colours and shapes in both tables. Without this, if there you have bricks which are no longer for sale, the subquery in the set clause will return nothing. So the database will set the price of these to null!</p>

<p>This means the update will have to access bricks_for_sale at least twice. And it'll run the subquery to get the price once for each row in purchased_bricks. Whereas merge can access bricks_for_sale exactly once. This can be a big saving compared to the correlated update. So there can be situations where an update only merge is handy.</p>",25-JUN-18 09.59.21.893156000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 01.41.28.461289000 PM,"CHRIS.SAXON@ORACLE.COM"
146417473427538985867560909518700058299,146417473425260160697587333524378916539,"Introduction",10,"<p> 
Pipelined table functions are table functions that return or ""pipe"" rows back to the calling query as the function produces the data in the desired form - and <i>before</i> the function has completed all processing!
</p>
<p>
Before diving into the implementation and application of pipelined table functions, it is important to understand how unusual the above statement is. PL/SQL is not a multi-threaded language. Generally, when an PL/SQL block (anonymous, nested, procedure function, etc.) is invoked, further processing in that session is ""on hold"" (suspended) until the block returns control to the host that invoked the block - whether it be another PL/SQL block, a SQL statement, or a host language, such as Java.
</p>
<p>
Normal (non-pipelined) table functions act in precisely this way. Each time the table function is invoked (either once or with left correlations for each row of the table on the left in the FROM clause), SQL engine must wait until a RETURN statement is executed to pass back the collection to the SELECT for conversion into rows and columns.
</p>
<p>
This blocking behavior can have a negative impact on overall performance of the SELECT statement, especially in ETL (extract-transform-load) operations of a data warehouse. In addition, with each element added to the collection in the table function, more and more Process Global Area (PGA or session) memory is consumed. For very large datasets, this can lead to further performance degradation and even errors.
</p>
<p>
Pipelined table functions get around both these problems. Let's take a look!
</p>",08-JUN-18 08.08.40.382245000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",28-AUG-18 12.23.27.722000000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
146417473427547448348298211922923001531,146417473425260160697587333524378916539,"The NO_DATA_NEEDED Exception",40,"<p>
Sometimes (as in the performance and memory test in the previous section) you will want to terminate the pipelined table function before all rows have been piped back. Oracle will then raise the NO_DATA_NEEDED exception. This will terminate the function, but will <i>not</i> terminate the SELECT statement that called it. You <i>do</i> need to explicitly handle this exception if either of the following applies:
</p>
<ul>
<li>You include an OTHERS exception handler in a block that includes a PIPE ROW statement.</li>
<li>Your code that feeds a PIPE ROW statement must be followed by a clean-up procedure. Typically, the clean-up procedure releases resources that the code no longer needs.</li>
</ul>
<p>Let's explore this behavior in more detail. In this first section, I only use 1 row, so Oracle raises NO_DATA_NEEDED, but no exception is raised.
</p>
<code>CREATE OR REPLACE TYPE strings_t IS TABLE OF VARCHAR2 (100);
/

CREATE OR REPLACE FUNCTION strings
   RETURN strings_t
   PIPELINED
   AUTHID DEFINER
IS
BEGIN
   PIPE ROW (1);
   PIPE ROW (2);
   RETURN;
END;
/

SELECT COLUMN_VALUE my_string
  FROM TABLE (strings ())
 WHERE ROWNUM < 2
/
</code>
<p>
Now I add an OTHERS exception handler and nothing else.
</p>
<code>
CREATE OR REPLACE FUNCTION strings
   RETURN strings_t
   PIPELINED
   AUTHID DEFINER
IS
BEGIN
   PIPE ROW (1);
   PIPE ROW (2);
   RETURN;
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line ('Error: ' || SQLERRM);
      RAISE;
END;
/

SELECT COLUMN_VALUE my_string
  FROM TABLE (strings ())
 WHERE ROWNUM < 2
/

BEGIN
   DBMS_OUTPUT.put_line ('Flush output cache!');
END;
/
</code>
<p>
As you can see, the NO_DATA_NEEDED error is trapped by that handler, and the re-raise does not manifest as an error in the SELECT statement. The problem with taking this approach, though, is that your OTHERS handler might contain specific cleanup code that makes sense for unexpected failures, but not for an early termination of data piping. So the recommendation is to provide a specific handler for NO_DATA_NEEDED.
</p>
<code>CREATE OR REPLACE FUNCTION strings
   RETURN strings_t
   PIPELINED
   AUTHID DEFINER
IS
BEGIN
   PIPE ROW (1);
   PIPE ROW (2);
   RETURN;
EXCEPTION
   WHEN no_data_needed
   THEN
      RAISE;
   WHEN OTHERS
   THEN
      /* Clean up code here! */
      RAISE;
END;
/

SELECT COLUMN_VALUE my_string
  FROM TABLE (strings ())
 WHERE ROWNUM < 2
/
</code>
<p>
In the code below, I demonstrate that both NO_DATA_FOUND and NO_DATA_NEEDED are by default ""hidden"" from the calling query, but other exceptions like PROGRAM_ERROR result in termination of the SQL statement.
</p>
<code>CREATE OR REPLACE FUNCTION strings (err_in IN VARCHAR2)
   RETURN strings_t
   PIPELINED
   AUTHID DEFINER
IS
BEGIN
   PIPE ROW (1);

   CASE err_in
      WHEN 'no_data_found'
      THEN
         RAISE NO_DATA_FOUND;
      WHEN 'no_data_needed'
      THEN
         RAISE no_data_needed;
      WHEN 'program_error'
      THEN
         RAISE PROGRAM_ERROR;
   END CASE;
   RETURN;
END;
/

SELECT COLUMN_VALUE my_string FROM TABLE (strings ('no_data_found'))
/

SELECT COLUMN_VALUE my_string FROM TABLE (strings ('no_data_needed'))
/

SELECT COLUMN_VALUE my_string FROM TABLE (strings ('program_error'))
/
</code>
<p>
The basic takeaway regarding NO_DATA_NEEDED is: don't worry about it, unless you are providing a WHEN OTHERS handler in your pipelined table function. In that case, make sure to provide a handler for NO_DATA_NEEDED, in which you will simply re-raise the exception with a RAISE; statement.
</p>",08-JUN-18 08.09.00.877092000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",18-JUN-18 02.33.16.629590000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
144904245953896456094170404360383401083,144905567362239111111827484063426187048,"Recursive With",40,"<p>Oracle Database 11.2 introduced another method for accessing trees: recursive subquery factoring. Aka recursive with.</p>

<p>This is the ANSI compliant way to build hierarchies in SQL. It's composed of two queries. A base query and a recursive one.</p>

<h3>Base Query</h3>

<p>You use this to define the root rows in your tree. This is like the start with clause in connect by. So to begin the chart with the CEO, use:</p>

<pre>  select employee_id, first_name, last_name, manager_id 
  from   employees
  where  manager_id is null</pre>

<h3>Recursive Query</h3>

<p>This maps to the connect by clause. Here you join the source table to the with clause on the columns storing the parent-child values.</p>

<p>For the company chart, you need to join each employee to their manager. This is the employee from the previous row. So you're linking the org_chart to the employees table.</p>

<p>This gives the following query:</p>

<pre>  select e.employee_id, e.first_name, e.last_name, e.manager_id 
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id</pre>

<p>When you use recursive with, you must provide aliases for all the columns it returns. These go between the query name and ""as"":</p>

<pre>with org_chart (
  employee_id, first_name, last_name, manager_id
) as ( ...</pre>

<p>To complete the query you need to union all the base and recursive queries together. Put this all together and you get:</p>

<code>with org_chart (
  employee_id, first_name, last_name, manager_id
) as (
  select employee_id, first_name, last_name, manager_id 
  from   employees
  where  manager_id is null
  union  all
  select e.employee_id, e.first_name, e.last_name, e.manager_id 
  from   org_chart oc
  join   employees e
  on     e.manager_id = oc.employee_id
)
  select * from org_chart;</code>",25-MAY-18 08.17.57.786792000 AM,"CHRIS.SAXON@ORACLE.COM",19-JUN-18 02.46.34.773616000 PM,"CHRIS.SAXON@ORACLE.COM"
147832521371715094572837851793791428324,147831487722688091314985883531165316592,"Introduction",10,"<p>This tutorial will show you how to do update-or-insert logic in one statement using merge. It uses these two tables:</p>

<code>select * from purchased_bricks;
select * from bricks_for_sale;</code>",22-JUN-18 08.28.56.286534000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-18 09.19.43.658027000 AM,"CHRIS.SAXON@ORACLE.COM"
147832521371727183831033998085538490084,147831487722688091314985883531165316592,"Merging New Values",30,"<p>Merge is one statement that allows you to do either an insert or an update as needed. To use it, you need to state how values in the target table relate to those in the source in the join clause. Then add rows in the when not matched clause. And update them using when matched.</p>

<p>The target table is the one that you'll add or change the rows of. You merge the source data into this.</p>

<p>The source has to be a table. But remember: a query returns a table! So you can select the values you want to upsert. So you can merge a blue cube costing 15.95 by querying these values like so:</p>

<pre>  select 'blue' colour, 'cube' shape, 15.95 price 
  from   dual </pre>

<p>You then need to link these values to rows in the target. Each source row should each link to at most one row in the target table. The primary key of purchased_bricks is colour and shape. So you can guarantee this by joining using these columns:</p>

<pre>on ( pb.colour = bfs.colour and pb.shape = bfs.shape )</pre>

<p>You then define what to add or change in merge's matched clauses.</p>

<h3>When Matched</h3>

<p>This clause fires for each row in the source that links to a row in the target. So if there is a blue cube in the target table, you can change its price here, like so:</p>

<pre>when matched then
  update set pb.price = bfs.price;</pre>

<h3>When Not Matched</h3>

<p>For each row in the source that doesn't match one in the target, the when not matched clause fires. Here you state the values you'd like to insert into the target for which columns. If the source and target tables have the same column names, you must alias the columns in the values clause:</p>

<pre>when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )</pre>

<p>Putting together this gives the following statement to merge a blue cube:</p>

<code>merge into purchased_bricks pb
using ( 
  select 'blue' colour, 'cube' shape, 15.95 price 
  from   dual 
) bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price;
  
select * from purchased_bricks;</code>

<p>Note you can place the matched clauses in either order. Whether you do ""when match"" or ""when not matched"" first is a matter of personal preference.</p>",22-JUN-18 08.29.35.978264000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUN-18 01.35.13.589401000 PM,"CHRIS.SAXON@ORACLE.COM"
147832521371747735569967446781508495076,147831487722688091314985883531165316592,"Merge Restrictions",50,"<p>There are a couple of things you need to watch for in the when matched clause. You can only update:</p>

<ul>
<li>columns not in the join clause</li>
<li>each row once</li>
</ul>

<h3>Updating Join Columns</h3>

<p>If you try to set columns in the join clause, you'll get an error. For example, the following fails because it tries to update colour and shape, which are in the join clause:</p>

<code>merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour and pb.shape = bfs.shape )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.colour = bfs.colour, pb.shape = bfs.shape;</code>

<h3>Update Each Row Once</h3>

<p>The join should map each row in the source to at most one in the target. If there are two or more rows in the source which map to a row in the target, you'll get an error.</p>

<p>For example, the following joins the tables on colour alone. There are two rows in the source with the colour blue. So this will try and change blue rows in the target twice. This leads to an error:</p>

<code>merge into purchased_bricks pb
using bricks_for_sale bfs
on    ( pb.colour = bfs.colour )
when not matched then
  insert ( pb.colour, pb.shape, pb.price )
  values ( bfs.colour, bfs.shape, bfs.price )
when matched then
  update set pb.price = bfs.price;</code>",22-JUN-18 08.30.43.287166000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-18 09.55.59.479175000 AM,"CHRIS.SAXON@ORACLE.COM"
151905700197739772744631580636882970686,151905700197614044459391659202713528382,"Grouping Aggregates",40,"<p>As well as the overall total, you can split the results into separate groups. You do this with group by. This returns one row for each combination of values in the group by columns.</p>

<p>For example, the following returns the number of rows for each colour:</p>

<code>select colour, count (*) 
from   bricks
group  by colour;</code>

<p>You don't need to include all the columns in the group by in your select clause. The following splits the rows by colours as above. But excludes colour from the output:</p>

<code>select count (*) 
from   bricks
group  by colour;</code>

<p>This can be confusing, so it's a good idea to include all the grouping columns in your select.</p>

<p>But the reverse isn't true! All unaggregated values in your select clause must be in the group by.</p>

<p>So the following will raise an exception because shape is in the select, but not the group by:</p>

<code>select colour, shape, count (*) 
from   bricks
group  by colour;</code>

<p>You can group by many columns. The following returns the number of rows for each shape and weight:</p>

<code>select shape, weight, count (*) 
from   bricks
group  by shape, weight;</code>",31-JUL-18 08.51.03.750126000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.52.50.260557000 AM,"CHRIS.SAXON@ORACLE.COM"
192497261674752679927429620575628877306,192497261674369450442611783127247019514,"Opening a SODA collection",5,"This example uses PL/SQL function DBMS_SODA.open_collection to open the collection named myCollectionName and returns a SODA_COLLECTION_T instance that represents this collection. If the value returned is NULL then there is no existing collection named myCollectionName.
<code>
REM Open an existing collection
DECLARE
    collection  SODA_Collection_T;
BEGIN
    collection := dbms_soda.open_Collection('myCollectionName');

    IF collection IS NULL THEN
        dbms_output.put_Line('Collection does not exist');
    ELSE
        dbms_output.put_Line('Name: ' || collection.get_Name);
        dbms_output.put_Line('Metadata: ' || 
                         JSON_QUERY(collection.get_Metadata, '$' pretty));
    END IF;
END;
/
</code>",24-AUG-19 12.19.49.073596000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 11.50.28.479276000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192497261674761142408166922979851820538,192497261674369450442611783127247019514,"Discovering existing collections",6,"This example uses PL/SQL function DBMS_SODA.list_collection_names to obtain a list of the collection names. It then iterates over that list, printing out the names.
<code>
REM Discovering existing collections
DECLARE
    coll_List  SODA_CollName_List_T;
BEGIN
    coll_List := dbms_soda.list_Collection_Names;
    dbms_output.put_Line('Number of collections: '||to_Char(coll_List.count));
    dbms_output.put_Line('Collection List: ');

    -- Loop over the collection name list
    IF (coll_List.count > 0) THEN
        FOR i IN
            coll_List.first .. coll_List.last
        LOOP
            dbms_output.put_Line('['||i||']: ' || coll_List(i));
        END LOOP;  
    ELSE   
        dbms_output.put_Line('No collections found');
    END IF;
END;
</code>",24-AUG-19 12.20.19.149026000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",26-AUG-19 06.53.10.733924000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192497261674772022740543454642424176122,192497261674369450442611783127247019514,"Dropping a collection",7,"This example uses PL/SQL function DBMS_SODA.drop_collection to drop collection myCollectionName.

If the collection cannot be dropped because of uncommitted write operations then an exception is thrown. If the collection is dropped successfully, the returned status is 1; otherwise, the status is 0. In particular, if a collection with the specified name does not exist, the returned status is 0 â€” no exception is thrown.
<code>
REM Dropping a collection
DECLARE
    status  NUMBER := 0;
BEGIN
    status := dbms_soda.drop_Collection('myCollectionName');
    dbms_output.put_line('Status: ' || status);
    
    status := dbms_soda.drop_Collection('SODAPLSOPR1');
    dbms_output.put_line('Status: ' || status);
END;
</code>",24-AUG-19 12.21.11.942907000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 11.53.09.385485000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
213556296133148053050269397577519253703,210639064323015416218443333736849885877,"How Many Disk Reads and Writes Did an Operation Do?",60,"<p>When the database is unable to sort rows in memory it writes them to temporary disk. You can see the amount of disk needed in the Used-Tmp column with the MEMSTATS format.</p>

<p>To view the number of disk reads and writes, check the Reads and Writes columns with the IOSTATS format.</p>

<p>An easy way to display all this information is with the ALLSTATS format. This is shorthand for MEMSTATS IOSTATS ROWSTATS.</p>

<p>The alter session statements in this section limit the amount of memory available for joins and sorts. So these operations will need to write to disk:</p>

<code>alter session set workarea_size_policy = manual;
alter session set sort_area_size = 25000;

select /*+ gather_plan_statistics */ row_count from complex_query;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ALLSTATS LAST'));</code>

<p><em>It's rare you need to set the sort_area_size. The above is to show the principle on a small data set.</em></p>

<p>As with Buffers, the Reads and Writes column figures are cumulative. You may also see physical reads when querying rows that are not cached in memory (the buffer cache in Oracle Database).</p>",12-MAR-20 02.45.46.003178000 PM,"CHRIS.SAXON@ORACLE.COM",03-JUN-20 08.36.18.925201000 AM,"CHRIS.SAXON@ORACLE.COM"
211967105477724402865672084517152279778,210639799513641285543570732327153347316,"Correlated Columns",100,"<p>The row stats for BRICKS should now be up-to-date. But the row estimates for queries searching on both COLOUR_RGB_VALUE and SHAPE are still incorrect!</p>
<code>select /*+ gather_plan_statistics */count (*)
from   bricks
where  colour_rgb_value = 'FF0000'
and    shape = 'cylinder';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>

<p>This is because the colour and shape values are correlated. All the rows for each shape have the same colour. You can verify this with this query:</p>

<code>select colour_rgb_value, shape, count (*)
from   bricks
group  by colour_rgb_value, shape;</code>

<p>By default the optimizer doesn't know about this relationship. You improve row estimates in this case by creating extended statistics.</p>",26-FEB-20 09.30.29.834782000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.54.03.017509000 PM,"CHRIS.SAXON@ORACLE.COM"
152848903946588728138313802710847866509,117463246204528657365867867361809943103,"Other Data Types",110,"<p>Oracle Database includes other, specialized data types. These include XMLtype for XML documents. And spatial types to store location details.</p>

<p>For more details on these, read the <a href=""http://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-A3C0D836-BADB-44E5-A5D4-265BA5968483"">data type documentation</a>.</p>",09-AUG-18 08.45.10.519503000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 09.16.14.346971000 AM,"CHRIS.SAXON@ORACLE.COM"
117675752841671083479188122970756925299,117675390209390608523249818520886328918,"Selecting Rows",10,"<p>You access rows in a database table with the select statement. This returns data to the client. It has two core parts: select and from.</p>

<p>In the from clause you list the tables you want to get the rows from. And in select you state which columns you want to see the values of.</p>

<p>Placing asterisk (star) returns all the visible columns in the table. So to get all the rows and visible columns in the toys table, use:</br>

<code>select * from toys;</code>

<p>This excludes any invisible columns in the table. These are often system-generated columns. From Oracle Database 12c you can set the visibility of a column.</p>

<p>To view invisible columns you have to list them in the select clause. Here you can also restrict your query to fewer columns or change the order they appear. Do this by naming the columns you want like so:</p>

<code>select toy_name, price from toys;</code>

<p>It is good practice to only select the columns you need. This reduces the volume of data sent over the network. Which can make your application faster.</p> 

<p>It also makes your code more resilient. If you use *, your code may error if someone adds or removes columns from the table.</p>",06-SEP-17 03.22.52.791891000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.07.32.424578000 AM,"CHRIS.SAXON@ORACLE.COM"
192497261674853020770457634797129489914,192497261674369450442611783127247019514,"Replacing a Document in a Collection and getting the result Document",56,"This example replaces a document in a collection, given its key. It then gets (and prints) the key and the generated components from the result document. To obtain the components it uses SODA_DOCUMENT_T methods get_key(), get_created_on(), get_last_modified(), and get_version().
<code>

REM Replacing a Document in a Collection and getting the result Document
DECLARE
    collection  SODA_Collection_T;
    document    SODA_Document_T;
    rep_Doc     SODA_Document_T;
    key         VARCHAR2(255);
BEGIN
    collection := dbms_soda.open_Collection('myCollectionName');
    document := SODA_Document_T(b_Content => utl_raw.cast_to_raw('{""name"" : ""Sriky""}'));
    select ID into key from ""myCollectionName"" where rownum < 2;

    rep_Doc := collection.replace_One_And_Get(key, document);

    IF rep_Doc IS NOT NULL THEN
        dbms_output.put_line('Document components:');
        dbms_output.put_line('Key: ' || rep_Doc.get_Key);
        dbms_output.put_line('Creation timestamp: ' || rep_Doc.get_Created_On);
        dbms_output.put_line('Last modified timestamp: ' || rep_Doc.get_Last_Modified);
        dbms_output.put_line('Version: ' || rep_Doc.get_Version);
    END IF;
END;
</code>",24-AUG-19 12.27.20.090214000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.12.55.296746000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
214087169950966338114128094494397738136,210640540520278905448644863263597314383,"Incremental Materialized View Refreshes",70,"<p>Instead of re-running the whole query in an MV, you can apply the changes since the last refresh. This is called a fast refresh. To do this, first you must create a materialized view log on the table:</p>

<code>create materialized view log 
  on bricks 
  with primary key, rowid, sequence, commit scn ( 
    colour, shape, weight, insert_date 
  )
  including new values;</code>

<p>This captures details of all changes made to the rows. By default this is named MLOG$_&lt;tablename&gt;. This records all changes you make to the table's rows:</p>

<code>update bricks
set    colour = 'green'
where  brick_id = 1;

delete bricks
where  brick_id = 2;

insert into bricks values ( -2, 'red', 'cube', 100, sysdate, default );

commit;

select * from mlog$_bricks;</code>

<p>With the log in place, the database can bring an MV up-to-date by applying the changes in the log to the MV itself.</p>

<p>When defining an MV the refresh methods are:</p>
<ul>
<li>COMPLETE - refresh the MV by running its query</li>
<li>FAST- apply changes in the MV log to bring it up-to-date</li>
<li>FORCE - use FAST refresh if possible and COMPLETE if not</li>
</ul>

<p>The default refresh clause is:</p>

<pre>REFRESH FORCE ON DEMAND</pre>

<p>This means you have to refresh the MV manually. When you do so, the database will pick fast refresh if it can. To ensure the database always applies changes from the MV log, set the MV to REFRESH FAST.</p>

<p>You can get the database to do this automatically at the end of every transaction. Do this by setting the refresh to ON COMMIT. When you commit, the database will apply any changes in the log to the MV. To set this property the log must be empty, so first do a final complete refresh:</p>

<code>exec dbms_mview.refresh ( 'brick_colours_mv', 'C' );

alter materialized view brick_colours_mv
  refresh fast on commit;</code>

<p>Now any changes you make to the table are reflected in the MV during the commit:</p>

<code>select *
from   brick_colours_mv;

insert into bricks values ( -1, 'red', 'cube', 100, sysdate, default );
commit;

select *
from   brick_colours_mv;</code>

<p>When you create ENABLE QUERY REWRITE REFRESH FAST ON COMMIT MVs, the optimizer automatically uses them where possible. So these can give huge performance gains with no code changes!</p>",17-MAR-20 04.23.43.460487000 PM,"CHRIS.SAXON@ORACLE.COM",27-JUN-20 07.56.51.999978000 AM,"CHRIS.SAXON@ORACLE.COM"
214156118416261884131887781687446894297,210639064323069817880325992049711663797,"Top-N Queries",40,"<p>This query joins the tables and returns the first five rows:</p>

<code>select /*+ gather_plan_statistics */*
from   card_deck d1
join   card_deck d2
on     d1.suit = d2.suit
and    d1.val = d2.val
order  by d1.val
fetch  first 5 rows only;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Note that this reads <strong>52</strong> rows from both tables. And the join also returns 52 rows. You can see this from the A-rows column in the plan for lines 3-5:</p>

<pre>| Id  | Operation                | Name      | Starts | E-Rows | A-Rows |   A-Time   | Buffers |
...
|*  3 |    HASH JOIN             |           |      1 |     52 |     52 |00:00:00.01 |      30 |
|   4 |     TABLE ACCESS FULL    | CARD_DECK |      1 |     52 |     52 |00:00:00.01 |      15 |
|   5 |     TABLE ACCESS FULL    | CARD_DECK |      1 |     52 |     52 |00:00:00.01 |      15 |</pre>

<p>This is because are no indexes on the join column, so the optimizer chooses a hash join. This means it must read all the rows in one table <em>before</em> reading <strong>any</strong> rows from the second. You can make this query faster by adding an index on the join columns:</p>

<code>create index card_val_suit_i 
  on card_deck ( val, suit );</code>

<p><em>For a HASH JOIN, the optimizer can still use an index on the second table if you use its columns in the WHERE clause. It's indexes on the join columns that a HASH JOIN can't take advantage of.</em></p>

<p>This enables the optimizer to use nested loops to join the tables:</p>

<code>select /*+ gather_plan_statistics */ *
from   card_deck d1
join   card_deck d2
on     d1.suit = d2.suit
and    d1.val = d2.val
order  by d1.val
fetch first 5 rows only;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));
</code>

<p>Note this does a ""double nested loop"". It first joins the outer table to the index. Then joins matching rows to the inner table. This approach is common when using indexed nested loops.</p>

<p>The optimizer chooses nested loops over hash join because it knows the query will return at most five rows. Nested loops can search for rows in the inner table immediately after reading them from the outer table. So it can stop processing after reading just five rows from each table.</p>

<p>You can see that - unlike a hash join - the nested loops fetch and join at most five rows from each table:</p>

<pre>| Id  | Operation                       | Name            | Starts | E-Rows | A-Rows |   A-Time   | Buffers |
...
|   3 |    NESTED LOOPS                 |                 |      1 |      5 |      5 |00:00:00.01 |      24 |
|   4 |     NESTED LOOPS                |                 |      1 |      5 |      5 |00:00:00.01 |      19 |
|   5 |      TABLE ACCESS BY INDEX ROWID| CARD_DECK       |      1 |     52 |      5 |00:00:00.01 |      10 |
|   6 |       INDEX FULL SCAN           | CARD_VAL_SUIT_I |      1 |      5 |      5 |00:00:00.01 |       5 |
|*  7 |      INDEX RANGE SCAN           | CARD_VAL_SUIT_I |      5 |      1 |      5 |00:00:00.01 |       9 |
|   8 |     TABLE ACCESS BY INDEX ROWID | CARD_DECK       |      5 |      1 |      5 |00:00:00.01 |       5 |</pre>

<p>This highlights a key difference between hash joins and nested loops. A hash join must read all the rows in the first data set to build the hash table. Then start reading the second table.</p>

<p>Nested loops can read rows in the inner table after reading just one row from the outer table. Provided the lookup of the inner table is fast, this means it can start returning rows faster than a hash join.</p>

<p>This makes nested loop operations the best way to get a small fraction of large data sets. For example, when doing top-N queries or master-detail joins, such as orders to order items.</p>",18-MAR-20 09.43.11.194629000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.30.28.963111000 PM,"CHRIS.SAXON@ORACLE.COM"
192497261674885661767587229784846556666,192497261674369450442611783127247019514,"QBE: Count all Microsoft employees that are Managers",100,"The following example applies a filter on a collection and finds the count of matching documents using count() function. 
<code>
REM Count all Microsoft employees that are Managers

DECLARE
    collection SODA_Collection_T;
    operation  SODA_Operation_T;
    document   SODA_Document_T;
    cur        SODA_Cursor_T;
    qbe        VARCHAR2(400);
    rowCount   NUMBER;
BEGIN
    collection := dbms_soda.open_collection('Employees');
    oow_soda.show(collection,
                  setting => oow_soda.SHOW_COLL_NAME_ONLY);

    qbe := '{""$query"" : {
                 ""title"" : { ""$eq"" : ""Manager"" },
                 ""company"" : { ""$eq"" : ""Microsoft"" }
              }
            }';
    operation := collection.find().filter(qbe);
    rowCount := operation.count();
    dbms_output.put_line('No. of docs: ' || rowCount);
    COMMIT;
END;

</code>",24-AUG-19 12.31.04.650106000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.20.57.686021000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
152139165452725409079916279364332589155,152139165451368994310308665430312259683,"Savepoints",63,"<p>Your code will often add rows to many tables in one action. If there is an error part-way through, you may want to undo some - but not all - the changes since the last commit.</p>

<p>To help with this you can create savepoints. These are checkpoints. You can undo all changes made after it. And preserve those made beforehand.</p>

<p>To do this, first create the checkpoint with the savepoint command. Give it a name to refer to later:</p>

<code>exec savepoint save_this;</code> 

<p><i><b>Note</b>: exec is a requirement for LiveSQL. You do not need this prefix in other environments.</i></p>

<p>To undo the changes after the savepoint, add the clause ""to savepoint"" after rollback. Give the name of the savepoint you want to revert to. This undo will the changes you made after this:</p>

<code>exec savepoint save_this;
rollback to savepoint save_this;</code> 

<p>Note that savepoints do NOT commit! If you issue an unqualified rollback, you'll still reverse all changes since the last commit. Even those made before the savepoint.</p>

<p>For example, the code below:</p>

<ul>
<li>Adds a row for toy_id 8</li>
<li>Creates the savepoint after_six</li>
<li>Then inserts toy_id 9</li>
</ul>

<p>The rollback to savepoint only removes the row for toy_id 9. The final rollback at the end also removes toy_id 8:</p>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 8, 'Pink Rabbit', 'pink' );

exec savepoint after_six;

insert into toys ( toy_id, toy_name, colour ) 
  values ( 9, 'Purple Ninja', 'purple' );

select * from toys
where  toy_id in ( 8, 9 );

rollback to savepoint after_six;

select * from toys
where  toy_id in ( 8, 9 );

rollback;

select * from toys
where  toy_id in ( 8, 9 );</code>

<p>You can create many savepoints in one call. And rollback to any of them. If you rollback to a savepoint this destroys any made after the one you name. But you can still go back to the earlier points.</p>

<p>For example, the following creates three savepoints. But the first rollback undoes the changes after second_sp. So third_sp is lost and you can no longer go back to it:</p>

<code>exec savepoint first_sp;
exec savepoint second_sp;
exec savepoint third_sp;

rollback to savepoint second_sp;
/* Fails - third_sp no longer exists */
rollback to savepoint third_sp;
rollback to savepoint first_sp;</code>",02-AUG-18 02.10.08.836247000 PM,"CHRIS.SAXON@ORACLE.COM",04-SEP-18 03.50.39.079969000 PM,"CHRIS.SAXON@ORACLE.COM"
152139165451865862822170278021116498019,152139165451368994310308665430312259683,"Multi-table Insert",70,"<p>You can also insert rows into many tables in one statement. This is a multi-table insert. The general format of it is:</p>

<pre>insert all
  into tab1 ( col1, col2, ...) values ( 'val1', 'val2', ...)
  into tab2 ( col1, col2, ...) values ( 'val1', 'val2', ...)
  into ...
  select * from query;</pre>

<p>Like a multi-row insert, the source of these values must be a query. It adds a row to all the tables you list. You can even insert to the same table more than once!</p>

<p>The query below uses the dual table to select the value 0. Then insert it to bricks twice and toys once:</p>

<code>insert all
  into bricks ( brick_id ) values ( id )
  into bricks ( brick_id ) values ( id )
  into toys ( toy_id ) values ( id )
select 0 id from dual;

select * from toys
where  toy_id = 0;

select * from bricks
where  brick_id = 0;</code>

<p>Dual is a special one-row table in Oracle Database. You can use this to select values or functions not stored in a real table.</p>",02-AUG-18 01.55.29.991680000 PM,"CHRIS.SAXON@ORACLE.COM",04-SEP-18 03.54.36.693485000 PM,"CHRIS.SAXON@ORACLE.COM"
155483828806891549923332798107557334957,152139165451368994310308665430312259683,"Try It!",45,"<p>Run these inserts:</p>

<code>insert into toys ( toy_id, colour ) values ( 4, 'blue' );
insert into toys ( toy_id, colour ) values ( 5, 'green' );</code>

<p>Then complete this insert statement to add the rows for toy_ids 4 & 5 to bricks. It should store toy_id values in brick_id. And toy colours in brick colours.</p>

<code>insert /* TODO */
  select toy_id, colour
  from   toys
  where  toy_id in ( 4, 5 );

select * from bricks
where  brick_id in ( 4, 5 );</code>

<p>The query should return the following rows:</p>

<pre><b>BRICK_ID	COLOUR	SHAPE</b>
       4	blue	&lt;null&gt;
       5	green	&lt;null&gt;</pre>",03-SEP-18 03.06.44.036018000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.17.35.908202000 PM,"CHRIS.SAXON@ORACLE.COM"
151179144401608186467543491627455372123,117463246204528657365867867361809943103,"Try It!",82,"<p>Complete the following statement to create a table with the following columns:</p>

<ul><li>brick_id of type number(20, 0)</li>
<li>colour of type varchar2, max size 10</li>
<li>price of type number with precision 10 and scale 2</li>
<li>purchased_date of type date</li></ul>

<code>create table bricks (
  /* TODO */
);

select column_name, data_type, data_length, data_precision, data_scale
from   user_tab_columns
where  table_name = 'BRICKS';</code>

<p>When successful, the query should return the following rows:</p>

<pre><b>COLUMN_NAME      DATA_TYPE   DATA_LENGTH   DATA_PRECISION   DATA_SCALE   </b>
BRICK_ID         NUMBER                 22               20            0 
COLOUR           VARCHAR2               10           &lt;null&gt;       &lt;null&gt;
PRICE            NUMBER                 22               10            2 
PURCHASED_DATE   DATE                    7           &lt;null&gt;       &lt;null&gt;</pre>",24-JUL-18 09.35.25.248394000 AM,"CHRIS.SAXON@ORACLE.COM",09-AUG-18 08.35.28.919168000 AM,"CHRIS.SAXON@ORACLE.COM"
211135596249014260913896775853649042085,210639064319457547531317480075689609909,"Try it!",36,"<p>Replace TODO in this code to get the row statistics for every execution of the query:</p>
<code>select /*+ gather_plan_statistics */*
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => ' TODO '));</code>

<p>When this displays the stats for every execution, the Starts and A-rows figures will increase each time you run the query.</p>",18-FEB-20 09.59.30.727360000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.12.25.577801000 AM,"CHRIS.SAXON@ORACLE.COM"
155046904613270490799802611183397953157,155044276401612634730144596161551172634,"Introduction",10,"<p>This tutorial teaches you how to remove rows from a table using delete and truncate. It uses the following table to show how these work:</p>

<code>select * from toys</code>",30-AUG-18 09.51.11.721281000 AM,"CHRIS.SAXON@ORACLE.COM",03-SEP-18 08.11.42.368319000 AM,"CHRIS.SAXON@ORACLE.COM"
155046904613289833612916445250193251973,155044276401612634730144596161551172634,"Delete",20,"<p>Delete is another form of DML statement. You use it to remove rows from a table. An unqualified delete will erase all the rows from the table. So toys is empty after the delete statement:</p>

<code>delete from toys;

select * from toys;

rollback;

select * from toys;</code>

<p>The rollback undoes the delete, restoring the rows.</p>

<p>As with insert and update, you can supply a where clause. This only removes rows where the clause is true. So the following deletes the rows where the prices is less than one:</p>

<code>delete toys where price < 1;

select * from toys;

rollback;

select * from toys;</code>

<p>Note that the from clause is optional in delete.</p>

<p>Like update, delete locks the affected rows. This is from the time the statement starts until you commit or rollback the transaction. If someone else tries to update rows you're deleting, you block their update until your transaction ends.</p>",30-AUG-18 09.51.30.501027000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 07.57.12.900124000 AM,"CHRIS.SAXON@ORACLE.COM"
155046904613320056758406810979560906373,155044276401612634730144596161551172634,"Truncate",40,"<p>You can also remove all the rows from a table with truncate. Unlike delete, this is a Data Definition Language (DDL) statement in Oracle Database.</p>

<p>DDL statements (create table, alter table, etc.) commit. So you can't rollback a truncate!</p>

<p>Note how, even after the rollback, the table remains empty:</p>

<code>select * from toys;

truncate table toys;

select * from toys;

rollback;

select * from toys;</code>

<p>Truncate is also an all-or-nothing statement. You can't remove some rows using a where clause. If you add one, the statement throws an error:</p>

<code>truncate table toys where price < 1;</code>

<p>So with these gotchas, you may be wondering why you'd use truncate over delete. One key reason: performance.</p>

<p>Truncate is fast. It marks the table as empty without touching the data. So truncate operations are instant, no matter how many rows the table stores.</p>

<p>Whereas delete processes each row. So as the number of rows in the table increases, the time to delete them all will also increase. Deleting millions of rows can take hours to complete!</p>

<p>Truncate also deallocates a table's storage. But the space remains allocated to the table with delete.</p>

<p>If you want the space to stay assigned to the table after a truncate, use the ""reuse storage"" clause, like so:</p>

<code>truncate table toys reuse storage;</code>

<p>Keeping the storage is useful if you re-insert a similar number of rows soon after the truncate. For example, on a daily load from an external system. Leaving the space assigned to the table reduces the work the database does during the load.</p>",30-AUG-18 09.52.06.791675000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 07.43.39.384860000 AM,"CHRIS.SAXON@ORACLE.COM"
155046904613334563868242186529657380485,155044276401612634730144596161551172634,"Soft Deletes",50,"<p>Once you commit a delete or run a truncate, the rows are gone. To recover the data you need to restore from a backup*.</p>

<p>This is time-consuming and awkward. So it's hard to recover data deleted by accident. Many businesses are also subject to regulations stating that they can't remove certain types of data. For example, financial transactions.</p>

<p>So many applications implement a ""soft delete"" instead. This adds an ""is deleted"" flag to your tables. For example:</p>

<code>alter table toys add is_deleted varchar2(1) default 'N';</code>

<p>When adding new rows, ensure this value is N (No):</p>

<code>delete toys;

insert into toys values ('Baby Turtle', 0.01, 'N');
insert into toys values ('Miss Snuggles', 0.51, 'N');
insert into toys values ('Cuteasaurus', 10.01, 'N');
insert into toys values ('Sir Stripypants', 14.03, 'N');
insert into toys values ('Purple Ninja', 14.22, 'N');

select * from toys;

commit;</code>

<p>Now, to ""delete"" rows, you run an update. This sets the deleted flag to Yes:</p>

<code>update toys
set    is_deleted = 'Y'
where  toy_name = 'Cuteasaurus';

select * from toys;</code>

<p>But now you need to filter out the ""deleted"" values in most queries. This makes your code more complicated. To get only active rows, you need to add a where clause to all these queries. For example:</p>

<code>select * from toys
where is_deleted = 'N';</code>

<p>Luckily Oracle Database offers many ways to simplify this, including:</p>

<ul>
<li>Using views</li>
<li>Virtual Private Database (VPD)</li>
<li>In-Database Archiving</li>
</ul>

<h3>Views</h3>

<p>The most universal way is to create a view over the top of the table. This contains the query excluding ""deleted"" rows. You change your application to query the view instead of the table.</p>

<p>For example:</p>

<code>create or replace view active_toys as
  select * from toys
  where is_deleted = 'N';

select * from active_toys;</code>

<p>Ensuring all code uses the view can be hard to police. And it can be a lot of work to change your code to use the view. This makes it tough to add to existing applications.</p>

<h3>VPD</h3>

<p>You use VPD to control which users can see which rows. It does this by adding where clauses to your queries based on policies. This is primarily a security feature, allowing you to stop people seeing sensitive data without clearance. But you can also use it to manage soft-deletion.</p>

<p>For more on VPD, <a href=""https://oracle-base.com/articles/8i/virtual-private-databases"">read this article</a>.</p>

<h3>In-Database Archiving</h3>

<p>Oracle Database 12c introduced In-Database Archiving. This offers a new way to show or hide removed rows. It adds the invisible column ora_archive_state to each table you enable it for.</p>

<p>Do this with the following command:</p>

<code>alter table toys row archival;</code>

<p>You then ""delete"" rows by setting ora_archive_state to any value other than zero. For example:</p>

<code>update toys 
set    ora_archive_state = '1'
where  toy_name = 'Baby Turtle';

select * from toys;</code>

<p>You control which rows are visible in the session. If you set the row archival visibility to all, you see everything:</p>

<code>alter session set row archival visibility = all;

select * from toys;</code>

<p>To auto-filter out the deleted rows, set the visibility to active:</p>

<code>alter session set row archival visibility = active;

select * from toys;</code>

<p>Note that you can enable In-Database Archiving on a table AND your own soft-delete method. So ensure whichever method you use to do soft deletion, you document it well. Explain how it works to all new developers on your team. And use the same approach throughout your application!</p>

<p>*Oracle Flashback technologies enable you to <a href=""https://blogs.oracle.com/sql/how-to-recover-data-without-a-backup"">recover lost data without a backup</a>!</p>",30-AUG-18 09.52.26.189010000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 08.06.00.596318000 AM,"CHRIS.SAXON@ORACLE.COM"
153504682919317922616455028579481956318,153496376830076594111863106943541076337,"Calculating Weighted Average in the Analytic View",40,"<p>To calculate the weighted average in the analytic view, the same calculation steps are required as with the SELECT from tables.</p>

<ol>
<li> Calculate the weighting factor for each detail row.</li>
<li> Sum the weighting factor and the number of units sold</li>
<li> Divide the summed weighting factor by the summed units.</li>
</ol>

<p>To do this:</p>

<ol>
<li>Add a virtual column (PERCENT_MARGIN_WEIGHT) to the table to calculate the weighting factor for each detail row (or add a column to a view.)</li>
<li>Add the measure PERCENT_MARGIN_WEIGHT aggregated by SUM to the analytic view.</li>
<li>Add the WEIGHTED_AVERAGE_PERCENT_MARGIN calculated measure to the analytic view.</li>
</ol>

<p>Add PERCENT_MARGIN_WEIGHT to the SALES_FACT table:</p>

<code>
ALTER TABLE sales_fact ADD percent_margin_weight AS (units * percent_margin);</code>

<p>Query the fact table to view the new column:</p>

<code>
SELECT *
FROM sales_fact
WHERE
  rownum <= 50
  AND month_id = 'Apr-15'
  AND state_province_id = 'TEXAS_US';
</code>

<p>Create the analytic view (note the weighting factor and weighted average measures):</p>

<code>
CREATE OR REPLACE ANALYTIC VIEW sales_av
  CLASSIFICATION caption VALUE 'Sales AV'
  CLASSIFICATION description VALUE 'Sales Analytic View'
  CLASSIFICATION created_by VALUE 'George Jones'
USING sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales AGGREGATE BY SUM,
  units FACT units AGGREGATE BY SUM,
  -- Simple average.
  avg_percent_margin FACT percent_margin AGGREGATE BY AVG,
  -- Weighting factor.
  percent_margin_weight FACT percent_margin_weight AGGREGATE BY SUM,
  -- Weighted average.
  weighted_average_percent_margin AS (percent_margin_weight / units)
  )
DEFAULT MEASURE SALES;
</code>


<p>Query the analytic view:</p>

<code>
SELECT
  time_hier.member_name AS time,
  geography_hier.member_name AS geography,
  sales,
  units,
  ROUND(avg_percent_margin,3) AS average_price_per_unit,
  ROUND(weighted_average_percent_margin,3) AS weighted_average_percent_margin
FROM sales_av
  HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND geography_hier.level_name = 'REGION'
  AND time_hier.member_name = 'CY2015'
  AND geography_hier.member_name = 'South America'
ORDER BY
  time_hier.member_name,
  geography_hier.member_name;
</code>",15-AUG-18 03.36.46.062986000 PM,"WILLIAM.ENDRESS@ORACLE.COM",30-AUG-18 03.47.24.784424000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
152639219300584553877517643490496664773,152639219300562793212764580165351953605,"Update Statement",20,"<p>There are two core parts to an update:</p>

<ul>
<li>The name of the table you're changing. This goes after update</li>
<li>The columns you're changing and the values you set them to. These form a comma-separated list in the set clause</li>
</ul>

<p>So the general form of an update is:</p>

<pre>update table
set    col1 = 'value1',
       col2 = 'value2',
       ...</pre>

<p>For example, the following sets the quantity of all rows to 60:</p>

<code>update bricks
set    quantity = 60;

select * from bricks;</code>

<p>An update can change any number of columns from one up to the total in the table.</p>

<p>When you update a row, the database locks it. This stops other people from changing the row. The lock remains until you commit or rollback.</p> 

<p>If many people try to update the same rows at the same time, only the first person to start the update can complete it. All others must wait until they release the lock. If your application will run many updates at the same time, it can come grinding to a halt!</p>",07-AUG-18 08.45.37.272756000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.17.07.317690000 PM,"CHRIS.SAXON@ORACLE.COM"
189971655942533099870682829273099290949,188704173216096418598025228629326881042,"Perform location-based queries",50,"<p>
<strong>Query 1:</strong> Find the five customers closest to the warehouse whose warehouse ID is 3.
<code>SELECT  
   c.customer_id, 
   c.cust_last_name,
   c.GENDER
FROM warehouses w, 
   customers c
WHERE w.warehouse_id = 3
AND sdo_nn (c.cust_geo_location, w.wh_geo_location, 'sdo_num_res=5') = 'TRUE';
</code>
<p>
Notes on Query 1:
<ul>
<li>The SDO_NN operator returns the SDO_NUM_RES value of the customers from the CUSTOMERS table who are closest to warehouse 3. The first argument to SDO_NN (c.cust_geo_location in the example above) is the column to search. The second argument to SDO_NN (w.wh_geo_location in the example above) is the location you want to find the neighbors nearest to. No assumptions should be made about the order of the returned results. For example, the first row returned is not guaranteed to be the customer closest to warehouse 3. If two or more customers are an equal distance from the warehouse, then either of the customers may be returned on subsequent calls to SDO_NN.</li>
<li>When using the SDO_NUM_RES parameter, no other constraints are used in the WHERE clause. SDO_NUM_RES takes only proximity into account. For example, if you added a criterion to the WHERE clause because you wanted the five closest female customers, and four of the five closest customers are male, the query above would return one row. This behavior is specific to the SDO_NUM_RES parameter, and its results may not be what you are looking for. You will learn how to find the five closest female customers in the discussion of query 3.</li>
</ul>
<p>
<strong>Query 2</strong>: Find the five customers closest to warehouse 3 and put the results in order of distance
<p>
<code>SELECT 
   c.customer_id, 
   c.cust_last_name,
   c.GENDER,
   round( sdo_nn_distance (1), 2) distance_in_miles
FROM warehouses w, 
   customers c
WHERE w.warehouse_id = 3
AND sdo_nn 
(c.cust_geo_location, w.wh_geo_location, 'sdo_num_res=5  unit=mile', 1) = 'TRUE'
ORDER BY distance_in_miles;
</code>
<p>
Notes on Query 2:
<ul>
<li>The SDO_NN_DISTANCE operator is an ancillary operator to the SDO_NN operator; it can only be used within the SDO_NN operator. The argument for this operator is a number that matches the number specified as the last argument of SDO_NN; in this example it is 1. There is no hidden meaning to this argument, it is simply a tag. If SDO_NN_DISTANCE() is specified, you can order the results by distance and guarantee that the first row returned is the closest. If the data you are querying is stored as longitude and latitude, the default unit for SDO_NN_DISTANCE is meters.</li>
<li>The SDO_NN operator also has a UNIT parameter that determines the unit of measure returned by SDO_NN_DISTANCE.</li>
<li>The ORDER BY DISTANCE clause ensures that the distances are returned in order, with the shortest distance first.</li>
</ul>
<p>
<strong>Query 3:</strong> Find the five female customers closest to warehouse 3, put the results in order of distance, and give the distance in miles
<p>
<code>SELECT 
   c.customer_id, 
   c.cust_last_name,
   c.GENDER,
   round( sdo_nn_distance(1), 2) distance_in_miles
FROM warehouses w, 
   customers c
WHERE w.warehouse_id = 3
AND sdo_nn (c.cust_geo_location, w.wh_geo_location, 
   'sdo_batch_size =5 unit=mile', 1) = 'TRUE'
AND c.GENDER = 'F'
AND rownum < 6
ORDER BY distance_in_miles;
</code>
<p>
Notes on Query 3:
<ul>
<li>SDO_BATCH_SIZE is a tunable parameter that may affect your query's performance. SDO_NN internally calculates that number of distances at a time. The initial batch of rows returned may not satisfy the constraints in the WHERE clause, so the number of rows specified by SDO_BATCH_SIZE is continuously returned until all the constraints in the WHERE clause are satisfied. You should choose a SDO_BATCH_SIZE that initially returns the number of rows likely to satisfy the constraints in your WHERE clause.</li>
<li>The UNIT parameter used within the SDO_NN operator specifies the unit of measure of the SDO_NN_DISTANCE parameter. The default unit is the unit of measure associated with the data. For longitude and latitude data, the default is meters.</li>
<li>c.gender = 'F' and rownum < 6 are the additional constraints in the WHERE clause. The rownum < 6 clause is necessary to limit the number of results returned to fewer than 6.</li>
<li>The ORDER BY DISTANCE_IN_MILES clause ensures that the distances are returned in order, with the shortest distance first and the distances measured in miles.</li>
</ul>
<p>
<strong>Query 4:</strong> Find all the customers within 100 miles of warehouse 3
<code>SELECT 
   c.customer_id,
   c.cust_last_name,
   c.GENDER
FROM warehouses w, 
   customers c
WHERE w.warehouse_id = 3
AND sdo_within_distance (c.cust_geo_location,
   w.wh_geo_location,
   'distance = 100 unit=MILE') = 'TRUE';
</code>
<p>
Notes on Query 4:
<ul>
<li>The SDO_WITHIN_DISTANCE operator returns the customers from the customers table that are within 100 miles of warehouse 3. The first argument to SDO_WITHIN_DISTANCE (c.cust_geo_location in the example above) is the column to search. The second argument to SDO_WITHIN_DISTANCE (w.wh_geo_location in the example above) is the location you want to determine the distances from. No assumptions should be made about the order of the returned results. For example, the first row returned is not guaranteed to be the customer closest to warehouse 3.</li>
<li>The DISTANCE parameter used within the SDO_WITHIN_DISTANCE operator specifies the distance value; in this example it is 100.</li>
<li>The UNIT parameter used within the SDO_WITHIN_DISTANCE operator specifies the unit of measure of the DISTANCE parameter. The default unit is the unit of measure associated with the data. For longitude and latitude data, the default is meters; in this example, it is miles.</li>
</ul>
<p>
<strong>Query 5:</strong> Find all the customers within 100 miles of warehouse 3, put the results in order of distance, and give the distance in miles
<p>
<code>SELECT
   c.customer_id,
   c.cust_last_name,
   c.GENDER,
   round( 
    sdo_geom.sdo_distance (c.cust_geo_location, 
    w.wh_geo_location, 
    .005, 'unit=MILE'), 2) distance_in_miles 
FROM warehouses w, 
   customers c
WHERE w.warehouse_id = 3
AND sdo_within_distance (c.cust_geo_location,
   w.wh_geo_location,
   'distance = 100 unit=MILE') = 'TRUE'
ORDER BY distance_in_miles;
</code>
<p>
Notes on Query 5:
<ul>
<li>The SDO_GEOM.SDO_DISTANCE function computes the exact distance between the customer's location and warehouse 3. The first argument to SDO_GEOM.SDO_DISTANCE (c.cust_geo_location in the example above) contains the customer's location whose distance from warehouse 3 is to be computed. The second argument to SDO_WITHIN_DISTANCE (w.wh_geo_location in the example above) is the location of warehouse 3, whose distance from the customer's location is to be computed.</li>
<li>The third argument to SDO_GEOM.SDO_DISTANCE (0.005) is the tolerance value. The tolerance is a round-off error value used by Oracle Spatial. The tolerance is in meters for longitude and latitude data. In this example, the tolerance is 5 mm.</li>
<li>The UNIT parameter used within the SDO_GEOM.SDO_DISTANCE parameter specifies the unit of measure of the distance computed by the SDO_GEOM.SDO_DISTANCE function. The default unit is the unit of measure associated with the data. For longitude and latitude data, the default is meters. In this example it is miles.</li>
<li>The ORDER BY DISTANCE_IN_MILES clause ensures that the distances are returned in order, with the shortest distance first and the distances measured in miles.</li>
<ul>
<p>",30-JUL-19 06.51.23.218949000 PM,"DAVID.LAPP@ORACLE.COM",06-AUG-19 01.30.33.886938000 PM,"DAVID.LAPP@ORACLE.COM"
182241645422988204365864878912091860007,182241645422959190146194127811898911783,"SQL%ROWCOUNT and SQL%BULK_ROWCOUNT",90,"<p>SQL%ROWCOUNT returns the total number of rows modified by the FORALL, overall. SQL%BULK_ROWCOUNT is a pseudo-collection** that contains one element for each DML statement executed by FORALL. The element contains the number of rows modified by that specific DML statement.</p>
<p>
In the code below, I create a procedure to update salaries for specified name filters. Then I show the total number of rows modified. After that, I iterate through the pseudo-collection to show total number of rows modified. In addition, I apply a quality assurance check: the rule for this code is that if any filter does not change <i>any</i> rows, then I raise an exception. This is just one way that you might want to use SQL%BULK_ROWCOUNT.
</p>
<code>CREATE OR REPLACE TYPE filter_nt IS TABLE OF VARCHAR2(100)
/

CREATE OR REPLACE PROCEDURE update_by_filter (filter_in IN filter_nt)
IS
BEGIN
   FORALL indx IN 1 .. filter_in.COUNT
      UPDATE employees
         SET salary = salary * 1.1
       WHERE UPPER (last_name) LIKE filter_in (indx);

   DBMS_OUTPUT.put_line ('Total rows modified = ' || SQL%ROWCOUNT);

   FOR indx IN 1 .. filter_in.COUNT
   LOOP
      IF SQL%BULK_ROWCOUNT (indx) = 0
      THEN
         raise_application_error (
            -20000,
            'No rows found for filter ""' || filter_in (indx) || '""');
      ELSE
         DBMS_OUTPUT.put_line (
               'Number of employees with names like ""'
            || filter_in (indx)
            || '"" given a raise: '
            || SQL%BULK_ROWCOUNT (indx));
      END IF;
   END LOOP;

   ROLLBACK;
END;
/

BEGIN
   update_by_filter (filter_nt ('S%', 'E%', '%A%'));
END;
/
</code>
<p>
And now with a filter that finds no employees. 
</p>
<code>BEGIN
   update_by_filter (filter_nt ('S%', 'E%', '%A%', 'XXXXX'));
END;
/
</code>

<p>
** What's a ""pseudo-collection""? It's a data structure created implicitly by PL/SQL for us. The ""pseudo"" indicates that it doesn't have <i>all</i> the features of a ""normal"" collection. 
</p>
<p>
In the case of SQL%BULK_ROWCOUNT, it has no methods defined on it (COUNT, FIRST, LAST, etc.). It always has the same index values defined in it as the bind array, regardless of whether it is densely or sparsely filled.
</p>
<p>
<strong>Fill in the Blanks</strong></p>
<p>
In the block below replace the #FINISH# tags with code so that after execution, we see the total number of rows deleted from the employees table, and the number of rows deleted in departments 10 and 100.
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.department_id%TYPE;
   l_ids ids_t := ids_t (10, 100);
BEGIN
   #FINISH#
      DELETE FROM employees
            WHERE department_id = l_ids (l_index);

   #FINISH#
   ROLLBACK;
END;
</code>

<p><strong> Exercise 8:</strong></p>
<p>
Write the rest of the procedure whose signature is shown below. The procedure uses FORALL to update the salaries of all employees in each department to the corresponding value in the salaries array. Afterwards, display the total number of rows modified. Raise the PROGRAM_ERROR exception if any of the update statements (for a specific department ID) change less than 2 rows.
</p>
<pre>
PROCEDURE update_salaries (
   department_ids_in IN DBMS_SQL.NUMBER_TABLE,
   salaries_in IN DBMS_SQL.NUMBER_TABLE)
</pre>",17-MAY-19 08.02.01.652782000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-19 02.35.33.028808000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
151905700198081898751582520693324818494,151905700197614044459391659202713528382,"Generating Subtotals",80,"<p>You can also use group by to generate subtotals for each value in the grouping columns. You can do this with rollup or cube.</p>

<h3>Rollup</h3>

<p>Rollup generates the subtotals for the columns within it, working from right to left. So a rollup of:</p>

<pre>rollup ( colour, shape )</pre>

<p>calculates:</p>

<ul><li>Totals for each ( colour, shape ) pair</li>
<li>Totals for each colour</li>
<li>The grand total</li></ul>

<p>The groups using every column in the rollup are regular rows. Those based on a subset are supperaggregate rows. For these superaggreagtes, the database returns null for the grouped columns. So the colour totals show null for shape. And the grand total null for colour and shape:</p>

<code>select colour, shape, count (*)
from   bricks
group  by rollup ( colour, shape );</code>

<p>You can combine a rollup with non-rolledup columns. In this case the ""grand total"" is for the columns outside the rollup. The following calculates the total for each ( colour, shape ) pair and the number of rows for each colour:</p>

<code>select colour, shape, count (*)
from   bricks
group  by colour, rollup ( shape );</code>

<p>Rollup calculates N+1 groupings, where N is the number of expressions in the rollup. So a rollup with three columns returns 3+1 = 4 groupings.</p>

<h3>Cube</h3>

<p>Cube calculates the subtotals for every combination of columns within it. So if you use:</p>

<pre>cube ( colour, shape )</pre>

<p>You get groupings for:</p>

<ul><li>Each ( colour, shape ) pair</li>
<li>Each colour</li>
<li>Each shape</li>
<li>All the rows in the table</li></ul>

<code>select colour, shape, count (*)
from   bricks
group  by cube ( colour, shape );</code>

<p>Cube calculates 2^N groupings, where N is the number of expressions in the cube. So a cube with three columns returns 2^3 = 8 groupings.</p>",31-JUL-18 08.55.02.234371000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.44.49.618047000 AM,"CHRIS.SAXON@ORACLE.COM"
151905700197643058679062410302906476606,151905700197614044459391659202713528382,"Aggregate Functions",20,"<p>Aggregate functions combine many rows into one. The query returns one row for each group. If you use these without group by, you have one group. So the query will return one row.</p>

<p>For example, count() returns the number of rows the query processed. So this query gets one row, showing you how many rows there are in the bricks table:</p>

<code>select count (*) from bricks;</code>

<p>Count is unusual for accepting *. This means return the total number of rows. You can also pass an expression (column) to it. This returns the number of non-null rows for the expression. For example, the following shows you how many rows where colour is not null:</p>

<code>select count ( colour ) from bricks;</code>

<p>All other (non-count) aggregates use an expression. Oracle Database includes many statistical aggregate functions. Common ones include:</p>

<ul><li>Sum: the result of adding up all the values for the expression in the group</li>
<li>Min: the smallest value in the group</li>
<li>Max: the largest value in the group</li>
<li>Avg: the arithmetic mean of all the values in the group</li>
<li>Stddev: the standard deviation</li>
<li>Median: the middle value in the data set</li>
<li>Variance: the statistical variance of the values</li>
<li>Stats_mode: the most common value in the group</li></ul>

<p>The following query shows these in action:</p>

<code>select sum ( weight ), min ( weight ), max ( weight ), 
       avg ( weight ), stddev ( weight ),
       median ( weight ), variance ( weight ),
       stats_mode ( weight ) 
from   bricks;</code>

<p>There are also many other, more specialized aggregates. You can find a <a href=""http://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-62BE676B-AF18-4E63-BD14-25206FEA0848"">complete list of aggregate functions</a> in the docs.</p>",31-JUL-18 08.49.29.421809000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.16.39.486885000 AM,"CHRIS.SAXON@ORACLE.COM"
192581019425029625822404320746380362244,192497261674369450442611783127247019514,"Setup - 1",1,"<code>

REM Create demo package specification
CREATE OR REPLACE PACKAGE oow_soda
AUTHID CURRENT_USER
AS
    -- Constants
    SHOW_ALL               CONSTANT NUMBER(2) := 1;
    SHOW_KEY_ONLY          CONSTANT NUMBER(2) := 2;
    SHOW_NO_CONTENT        CONSTANT NUMBER(2) := 3;
    SHOW_KEY_VERSION_ONLY  CONSTANT NUMBER(2) := 4;

    SHOW_COLL_NAME_ONLY    CONSTANT NUMBER(2) := 2;

    -- Show collection components
    PROCEDURE show(collection  IN  SODA_Collection_T,
                   setting   IN  NUMBER DEFAULT oow_soda.SHOW_ALL,
                   comments    IN  VARCHAR2 DEFAULT NULL);

    -- Show document components
    PROCEDURE show(document  IN  SODA_Document_T,
                   setting   IN  NUMBER DEFAULT oow_soda.SHOW_ALL,
                   comments  IN  VARCHAR2 DEFAULT NULL);

    -- Generate JSON content given a number
    PROCEDURE get_doc(docNum      IN   NUMBER, 
                      docContent  OUT  VARCHAR2);

    -- Load nDocs into collection with a given sql type for doc content.
    -- Assumes collection key is server-assigned.
    -- Inserts all nDocs and commits autonomously on success, rollsback
    -- on failure, returning 0 nDocs.
    PROCEDURE load_docs(collection  IN      SODA_Collection_T,
                        nDocs       IN OUT  NUMBER,
                        dType       IN      NUMBER DEFAULT DBMS_SODA.DOC_BLOB);

    -- Convert VARCHAR2 to BLOB
    FUNCTION to_blob (v in varchar2) return BLOB;

    -- Get Metadata for given collection type
    FUNCTION get_Metadata(dType IN  NUMBER DEFAULT DBMS_SODA.DOC_BLOB)
    return VARCHAR2;

END oow_soda;

</code>",24-AUG-19 06.24.47.190928000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 06.25.49.977264000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192581019425372960755174875431996916228,192497261674369450442611783127247019514,"QBE: Using Geo Spatial Query using $near",110,"The following example fetches the cursor to a list of employees whose title is Director and whose location is within 100 miles from a given point. It orders the results in ascending order of salary.
<code>

REM Geo Spatial Query - $near

DECLARE                        
    collection SODA_Collection_T;
    operation  SODA_Operation_T; 
    document   SODA_Document_T;  
    cur        SODA_Cursor_T;                           
    qbe        VARCHAR2(800);
    pos        NUMBER  := 0;
    hasNext    BOOLEAN := TRUE;
    status     BOOLEAN;
BEGIN     
                                              
    collection := dbms_soda.open_collection('Employees');
    oow_soda.show(collection,
                   setting => oow_soda.SHOW_COLL_NAME_ONLY);

    qbe := '{
	       ""$query"": {
		  ""title"": { ""$eq"": ""Director"" },
		  ""location"": {
			""$near"": {
				""$geometry"": {
					""type"": ""Point"",
					""coordinates"": [122, 47]
				 },
				 ""$unit"": ""mile"",
				 ""$distance"": 100
			     }
		         }
	        },
	        ""$orderby"": [{
		   ""path"": ""salary"",
		    ""order"": ""asc""
	        }]
            }';
    operation := collection.find().filter(qbe);  
    cur := operation.get_Cursor;

    -- Loop over the cursor
    WHILE cur.has_Next()
    LOOP
        pos := pos + 1;
        document := cur.next;
        oow_soda.show(document,
                       comments => 'Doc ('|| pos ||')');
    END LOOP;
    dbms_output.put_line('No. of docs read: ' || pos);
    status := cur.close;
END;

</code>",24-AUG-19 06.35.45.746238000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.24.36.663837000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
184950473716922141912275023587833474512,182241645422959190146194127811898911783,"Oracle Dev Gym Workouts",170,"<p>
The Oracle Dev Gym offers multiple choices quizzes, workouts and classes on a wide variety of Oracle Database topics. Find below a set of four workouts (three featuring content by Tim Hall) on FORALL and BULK COLLECT. They are a great follow-up and reinforcement for this tutorial.

<p><strong>BULK COLLECT by Tim Hall</strong></p>
<p>
Tim Hall of oracle-base.com fame explores the BULK COLLECT feature of PL/SQL, which allows you to retrieve multiple rows with a single fetch. Note that Tim's article also covers FORALL, which is for multi-row, non-query DML (inserts, updates, deletes) and will be explored in a separate workout. After you read his article and check out the documentation, it's time to take four quizzes written by Steven Feuerstein to test your knowledge of this feature.
</p>
<pre>
https://devgym.oracle.com/pls/apex/dg/workout/bulk-collect.html
</pre>

<p><strong>FORALL - Basic Concepts by Tim Hall</strong></p>
<p>
Tim Hall offers a comprehensive review of bulk processing in PL/SQL; this workout focuses in on FORALL, covering the basic concepts behind this powerful performance enhancer. We complement Tim's article with a link to documentation and FORALL quizzes from the Dev Gym library.
</p>
<pre>
https://devgym.oracle.com/pls/apex/dg/workout/forall-basic-concepts.html
</pre>

<p><strong>FORALL and SAVE EXCEPTIONS by Tim Hall</strong></p>
<p>
Tim Hall of Oracle-BASE.com offers a comprehensive review of bulk processing in PL/SQL in this workout's leading exercise. Drill down to the SAVE EXCEPTIONS section of Tim's article to explore how to handle exceptions that may be raised when FORALL executes. Check out the documentation for more details. Then finish up with quizzes from Steven Feuerstein on SAVE EXCEPTIONS. Go beyond FORALL basics with this workout!
</p>
<pre>
https://devgym.oracle.com/pls/apex/dg/workout/forall-and-save-exceptions.html
</pre>

</p>
<p><strong>An Hour of Bulk Processing Quizzes</strong></p>
<p>
Ten quizzes on FORALL and BULK COLLECT, ranging in difficulty from beginner to intermediate.</p>
<pre>
https://devgym.oracle.com/pls/apex/dg/workout/an-hour-of-bulk-processing-quizzes.html
</pre>",12-JUN-19 05.22.33.142594000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",17-JUN-19 07.58.42.375401000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
154984359428418967282075541398778282117,153496376830076594111863106943541076337,"Handling NULL Data",50,"<p>In the SALES_FACT table PERCENT_MARGIN is dense relative to UNITS.  That is, for every row where there is UNITS data there is also PERCENT_MARGIN data. Many times it will be the case that one of the measure will be NULL.  These cases require addition handling.</p>

<p>Consider the case where a row exists with a value for UNITS and there are NULL values for PERCENT_MARGIN.  In this case the sum of UNITS cannot be used at the denominator to the weighted average calculation because weight of those rows is unknown.</p>
 
<p>Run the following query and note that for every value of UNITS there is a value for PERCENT_MARGIN.</p>

<code>
SELECT *
FROM
  sales_fact
WHERE
   state_province_id like '%_MX'
   AND month_id = 'Dec-15' 
ORDER BY
   state_province_id,
   category_id;
</code> 
 
<p>Run the following query to select UNITS, PERCENT_MARGIN_WEIGHT and WEIGHTED_AVERAGE_PERCENT_MARGIN from the tables.</p>

<code>
SELECT
    f.month_id,
    g.country_id,
    SUM(f.units),
    SUM(f.percent_margin_weight) AS percent_margin_weight,
    ROUND(SUM(f.percent_margin_weight) / SUM(f.units),3) AS weighted_average_percent_margin
  FROM
    av.geography_dim g,
    sales_fact f
  WHERE
    g.state_province_id = f.state_province_id
    AND g.country_id = 'MX'
    AND f.month_id = 'Dec-15'
  GROUP BY 
    f.month_id,
    g.country_id;
</code>
 
<p>The weighted average percent margin is 12316.52 percent_margin_weight / 95778 units = .129. </p>

<p>Next, set PERCENT_MARGIN = NULL for Baja California.</p>

<code>
UPDATE sales_fact
SET percent_margin = null
WHERE
  month_id = 'Dec-15'
  AND state_province_id = 'BAJA_CALIFORNIA_MX';

COMMIT;
</code>

<p>View the updated data.</p>

<code>
SELECT * 
FROM sales_fact
WHERE
  state_province_id LIKE '%_MX'
  AND month_id = 'Dec-15'
ORDER BY
  state_province_id,
  category_id;
</code>

<p>Note that PERCENT_MARGIN_WEIGHT is also NULL.</p>

<p>Run the following query to select UNITS, PERCENT_MARGIN_WEIGHT and WEIGHTED_AVERAGE_PERCENT_MARGIN again.</p>

<code>
SELECT
    f.month_id,
    g.country_id,
    SUM(f.units),
    SUM(f.percent_margin_weight) AS percent_margin_weight,
    ROUND(SUM(f.percent_margin_weight) / SUM(f.units),3) AS weighted_average_percent_margin
  FROM
    av.geography_dim g,
    sales_fact f
  WHERE
    g.state_province_id = f.state_province_id
    AND g.country_id = 'MX'
    AND f.month_id = 'Dec-15'
  GROUP BY 
    f.month_id,
    g.country_id;
</code>

<p>Note that the WEIGHTED_AVERAGE_PERCENT_MARGIN is .118 rather than .129.  This is because the sum of PERCENT_MARGIN weight has decreased while the sum of UNITS has remained the same.</p>

<p>Because the weight of rows with NULL values is unknown those rows should not be considered in the weighted average calculation.  To account for this, create a new column in the fact table that returns NULL when PERCENT_MARGIN is NULL.</p>

<code>
ALTER TABLE sales_fact
ADD percent_margin_units
  AS (CASE
        WHEN percent_margin IS NULL THEN NULL
        ELSE units
      END);</code>

<p>View the data.</p>

<code>SELECT * FROM sales_fact WHERE state_province_id like '%_MX' and month_id = 'Dec-15' ORDER BY state_province_id, category_id;</code>

<p>Run the following query to view UNITS, PERCENT_MARGIN_UNITS, PERCENT_MARGIN_WEIGHT and WEIGHTED_AVERAGE_PERCENT_MARGIN again.</p>

<code>
SELECT
    f.month_id,
    g.country_id,
    SUM(f.units),
    SUM(f.percent_margin_units),
    SUM(f.percent_margin_weight) AS percent_margin_weight,
    ROUND(SUM(f.percent_margin_weight) / SUM(f.percent_margin_units),3) AS weighted_average_percent_margin
  FROM
    av.geography_dim g,
    sales_fact f
  WHERE
    g.state_province_id = f.state_province_id
    AND g.country_id = 'MX'
    AND f.month_id = 'Dec-15'
  GROUP BY 
    f.month_id,
    g.country_id;
</code>

<p>The rows where PERCENT_MARGIN are NULL are not longer considered in the calculation and the weighted average percent margin is .129.</p>

<p>To apply this the analytic view add the PERCENT_MARGIN_UNITS measure and use it in the denominator of the WEIGHTED_AVERAGE_PERCENT_MARGIN measure.</p>

<code>
CREATE OR REPLACE ANALYTIC VIEW sales_av
  CLASSIFICATION caption VALUE 'Sales AV'
  CLASSIFICATION description VALUE 'Sales Analytic View'
  CLASSIFICATION created_by VALUE 'George Jones'
USING sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_id
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales AGGREGATE BY SUM,
  units FACT units AGGREGATE BY SUM,
  -- Weighted Average Units.
  percent_margin_units FACT percent_margin_units AGGREGATE BY SUM,
  -- Simple average.
  avg_percent_margin FACT percent_margin AGGREGATE BY AVG,
  -- Weighting factor.
  percent_margin_weight FACT percent_margin_weight AGGREGATE BY SUM,
  -- Weighted average.
  weighted_average_percent_margin AS (percent_margin_weight / percent_margin_units)
  )
DEFAULT MEASURE SALES;
</code>

<p>Run the following query to select UNITS, PERCENT_MARGIN_UNITS, PERCENT_MARGIN_WEIGHT and WEIGHTED_AVERAGE_PERCENT_MARGIN from the analytic view.</p>

<code>
SELECT
  time_hier.member_name AS time,
  geography_hier.member_name AS geography,
  units,
  percent_margin_units,
  percent_margin_weight,
  ROUND(weighted_average_percent_margin,3) AS weighted_average_percent_margin
FROM sales_av
  HIERARCHIES (
    time_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'MONTH'
  AND geography_hier.level_name = 'COUNTRY'
  AND time_hier.member_name = 'Dec-15'
  AND geography_hier.member_name = 'Mexico'
ORDER BY
  time_hier.member_name,
  geography_hier.member_name;
</code>",29-AUG-18 07.32.06.407524000 PM,"WILLIAM.ENDRESS@ORACLE.COM",30-AUG-18 04.02.48.076131000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
214087169950754776095695534388824157336,210640540520278905448644863263597314383,"Impact of Data Changes",40,"<p>To do query rewrite, by default there must be an <strong>exact</strong> match between the rows in the table and the data in the MV. If there are <strong>any</strong> changes to the base table, there is a mismatch. And the optimizer will no longer do a rewrite.</p>

<p>This adds one row to BRICKS:</p>
<code>insert into bricks values ( 0, 'red', 'cube', 100, sysdate, default );
commit;</code>

<p>The MV is now <em>stale</em>. This stops the optimizer from using BRICK_COLOURS_MV for query rewrite:</p>

<code>select /*+ gather_plan_statistics */colour, count(*) num_rows
from   bricks
group  by colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'BASIC LAST'));
</code>

<p>You can check if an MV is stale by querying *_MVIEWS:</p>

<code>select mview_name, staleness 
from   user_mviews;</code>

<p>There may be cases where you still want to use a stale MV. For example, when querying old data you know is correct in the MV. Or it may be better to get an out-of-date answer quickly than wait a long time for a result.</p>

<p>By tweaking session settings, you can allow the optimizer to use stale MVs in these cases.</p>",17-MAR-20 04.21.02.005251000 PM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.49.53.950243000 PM,"CHRIS.SAXON@ORACLE.COM"
192501726247356673613401329610152097443,192497261674369450442611783127247019514,"Setup - 2",2,"<code>
REM Creating a test package body for SODA

CREATE OR REPLACE PACKAGE BODY oow_soda AS

    PROCEDURE show(collection  IN  SODA_Collection_T,
                   setting   IN  NUMBER DEFAULT oow_soda.SHOW_ALL,
                   comments  IN  VARCHAR2 DEFAULT NULL)
    IS
    BEGIN
        IF comments IS NOT NULL
        THEN
            dbms_output.put_line(comments || ':');
        END IF;

        IF collection IS NULL
        THEN
            dbms_output.put_line('Collection is null');
            RETURN;
        END IF;
        dbms_output.put_line('Collection name: ' || collection.get_Name);
        IF setting = oow_soda.SHOW_COLL_NAME_ONLY
        THEN
          RETURN;
        END IF;
        dbms_output.put_line('Collection descriptor: ' ||
                             JSON_QUERY(collection.get_Metadata, '$' pretty));
    END;

    PROCEDURE show(document  IN  SODA_Document_T,
                   setting   IN  NUMBER   DEFAULT oow_soda.SHOW_ALL,
                   comments  IN  VARCHAR2 DEFAULT NULL)
    IS
    BEGIN
        IF comments IS NOT NULL
        THEN
            dbms_output.put_line(comments || ':');
        END IF;
        IF document IS NULL
        THEN
            dbms_output.put_line('Document is null');
            RETURN;
        END IF;
        dbms_output.put_line('Document key: ' || document.get_Key);
        IF setting = oow_soda.SHOW_KEY_ONLY
        THEN
          RETURN;
        END IF;
        dbms_output.put_line('Document version: ' || document.get_Version);
        IF setting = oow_soda.SHOW_KEY_VERSION_ONLY
        THEN
          RETURN;
        END IF;
        IF setting <> oow_soda.SHOW_NO_CONTENT
        THEN
        dbms_output.put_line('Document content: ' || 
           CASE(document.get_Data_Type)
               WHEN DBMS_SODA.DOC_VARCHAR2 THEN JSON_QUERY(document.get_Varchar2, '$' pretty)
               WHEN DBMS_SODA.DOC_BLOB THEN JSON_QUERY(document.get_Blob, '$' pretty)
               WHEN DBMS_SODA.DOC_CLOB THEN JSON_QUERY(document.get_Clob, '$' pretty)
           END);
        END IF;        
        dbms_output.put_line('Document data type: ' || 
                               CASE(document.get_Data_Type)
                                 WHEN DBMS_SODA.DOC_VARCHAR2 THEN 'VARCHAR2'
                                 WHEN DBMS_SODA.DOC_BLOB THEN 'BLOB'
                                 WHEN DBMS_SODA.DOC_CLOB THEN 'CLOB'
                               END);
        dbms_output.put_line('Document created on: ' || 
                              document.get_Created_On);
        dbms_output.put_line('Document last modified: ' ||
                              document.get_Last_Modified);
    END;
                   
    PROCEDURE get_doc(docNum      IN   NUMBER, 
                      docContent  OUT  VARCHAR2)
    IS
        TYPE oow_list IS TABLE OF VARCHAR2(30);
        i           NUMBER := docnum;
        str         VARCHAR2(1000);
        first_names oow_list := oow_list
            ('Joseph','Robert','William','Edward','Sally',
             'John','Linda', 'Samuel','Melissa','Joan','Ann');
        last_names  oow_list := oow_list
            ('Smith','Jones','White','Black','Robinson');
        companies   oow_list := oow_list
            ('Oracle','Microsoft','Google','IBM','SAP',
             'Salesforce','Facebook');
        titles      oow_list := oow_list
            ('Manager','Group Manager','Director','Vice President');
        orgs        oow_list := oow_list
            ('Development','Consulting','Sales');
        mails       oow_list := oow_list
            ('yahoo.com','gmail.com');
        cities      oow_list := oow_list
            ('New York','Los Angeles','Boston','San Francisco',
             'Washington','Chicago','Atlanta','Dallas','Seattle');
        locations   oow_list := oow_list
            ('[74.0059,40.7127]','[118.2500,34.0500]','[71.0589,42.3601]',
             '[122.4167,37.7833]','[77.0164,38.9047]','[87.6847,41.8369]',
             '[84.3900,33.7550]','[96.7970,32.7767]','[122.3331,47.6097]');
        states      oow_list := oow_list
            ('NY','CA','MA','CA','DC','IL','GA','TX','WA');    
        streets     oow_list := oow_list
            ('1313 Mockingbird Lane','123 Main Street',
             '1600 Pennsylvania Ave');
        first_name  VARCHAR2(100);
        last_name   VARCHAR2(100);
        salary      NUMBER;
    BEGIN
        first_name := first_names(mod(i, first_names.COUNT)+1);
        last_name := last_names(mod(i, last_names.COUNT)+1);
        salary := 100000 + i; 
        salary := salary + (mod(i, titles.COUNT) * 20000);
        salary := salary + (mod(i, companies.COUNT) * 5000);
        salary := salary + (mod(i, orgs.COUNT) * 50000);

        str := str || '{ ""empno"" : ' || (i+10000);
        str := str || ', ""name"" : ""';
        str := str || first_name || ' ' || last_name || '""';
        str := str || ', ""email"" : [""';
        str := str || first_name || '_' || last_name || '@';
        str := str || mails(mod(i, mails.COUNT)+1) || '""';
        str := str || ', ""';
        str := str || first_name || '_' || last_name || '@icloud.com""]';
        str := str || ', ""location"" : {""type"": ""Point"", ""coordinates"": ';
        str := str || locations(mod(i, locations.COUNT)+1);
        str := str || '}';
        str := str || ', ""address"" : {';
        str := str || ' ""street"" : ""';
        str := str || streets(mod(i, streets.COUNT)+1);
        str := str || '"", ""city"" : ""';
        str := str || cities(mod(i, cities.COUNT)+1);
        str := str || '"", ""state"" : ""';
        str := str || states(mod(i, states.COUNT)+1);
        str := str || '""},';
        str := str || '""title"" : ""';
        str := str || titles(mod(i, titles.COUNT)+1);
        str := str || '"", ""department"" : ""';
        str := str || orgs(mod(i, orgs.COUNT)+1);
        str := str || '"", ""company"" : ""';
        str := str || companies(mod(i, companies.COUNT)+1);
        str := str || '"", ""spouse"" : null ';
        str := str || ', ""salary"" : ' || salary;
        str := str || ' }';
        docContent := str;
    END;

    PROCEDURE load_docs(collection  IN      SODA_Collection_T,
                        nDocs       IN OUT  NUMBER,
                        dType       IN      NUMBER  DEFAULT DBMS_SODA.DOC_BLOB)
    IS
    PRAGMA AUTONOMOUS_TRANSACTION;
        docContent  VARCHAR2(1000);
        i           NUMBER;
        tot         NUMBER := nDocs;
        doc         SODA_Document_T;
        status      NUMBER;
    BEGIN
        IF collection IS NULL
        THEN
            raise_application_error(-20000,
                                    'Collection cannot be null');
        END IF;

        FOR i in 1..tot
        LOOP
            oow_soda.get_doc(docnum     => i, 
                              docContent => docContent);
            CASE(dType)
                WHEN DBMS_SODA.DOC_VARCHAR2 
                THEN
                    doc := SODA_Document_T(v_Content => docContent);
                WHEN DBMS_SODA.DOC_BLOB
                THEN
                    doc := SODA_Document_T(b_Content => 
                                               utl_raw.cast_to_raw(docContent));
                WHEN DBMS_SODA.DOC_CLOB
                THEN
                    doc := SODA_Document_T(c_Content => docContent);
            END CASE;

            status := collection.insert_One(doc);

            IF status = 1
            THEN
                nDocs := i;
            ELSE
                raise_application_error (-20000,
                                         'Error during insert:' || i);
            END IF;
        END LOOP;
        COMMIT;
    EXCEPTION
        WHEN OTHERS
        THEN
            dbms_output.put_line (SQLERRM);
            ROLLBACK;
            nDocs := 0;
    END;

    FUNCTION to_blob (v in varchar2) return BLOB
    IS
    BEGIN
        return UTL_RAW.CAST_TO_RAW(v);
    END;

    -- Get Metadata for given collection type
    FUNCTION get_Metadata(dType IN  NUMBER DEFAULT DBMS_SODA.DOC_BLOB)
    return VARCHAR2
    IS
      metadata VARCHAR2(4000);
    BEGIN
      CASE(dType)
          WHEN DBMS_SODA.DOC_VARCHAR2 
          THEN
              metadata :='{""contentColumn"":{""name"":""JSON_DOCUMENT"",""sqlType"":""VARCHAR2""},""lastModifiedColumn"":{""name"":""LAST_MODIFIED""},""versionColumn"":{""name"":""VERSION"",""method"" : ""SHA256""},""creationTimeColumn"":{""name"":""CREATED_ON""}}';
          WHEN DBMS_SODA.DOC_BLOB
          THEN
              metadata := NULL;
          WHEN DBMS_SODA.DOC_CLOB
          THEN
              metadata :='{""contentColumn"":{""name"":""JSON_DOCUMENT"",""sqlType"":""CLOB""},""lastModifiedColumn"":{""name"":""LAST_MODIFIED""},""versionColumn"":{""name"":""VERSION"",""method"" : ""SHA256""},""creationTimeColumn"":{""name"":""CREATED_ON""}}';
      END CASE;
      return metadata;
    END;

END oow_soda;

</code>",24-AUG-19 12.17.25.932150000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 06.25.57.553895000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192501726247429209162578207360634468003,192497261674369450442611783127247019514,"Dropping a collection using SQL",16,"This example drops a collection using a SQL statement. The PL/SQL function dbms_soda.drop_collection returns a NUMBER type, 0 of the collection was not dropped, 1 if it was dropped.
<code>
SELECT dbms_soda.drop_Collection('myCollectionName') ""Status"" from dual;
</code>",24-AUG-19 12.22.12.084692000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 03.39.46.957972000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192501726247874093864196390896926340771,192497261674369450442611783127247019514,"Creating a Document with JSON Content",26,"This example uses SODA_DOCUMENT_T constructors to create three documents, one of each content type. The example provides only the document content (which is the same for each).

The content parameter is different in each case; it specifies the SQL data type to use to store the content. The first document creation here uses content parameter v_content, which specifies VARCHAR2 content; the second uses parameter c_content, which specifies CLOB content; the third uses parameter b_content, which specifies BLOB content.

After creating each document, the example uses getter methods to get the document content. Note that the getter method that is appropriate for each content storage type is used: get_blob() for BLOB content, and so on.

The document with content type BLOB would be appropriate for writing to the collection created earlier, because that collection (which has the default metadata) accepts documents with (only) BLOB content. The other two documents would not be appropriate for that collection; trying to insert them would raise an error.
<code>
REM Creating a Document with JSON Content
DECLARE
    v_Doc  SODA_Document_T;
    b_Doc  SODA_Document_T;
    c_Doc  SODA_Document_T;
BEGIN
    -- Create VARCHAR2 document
    v_Doc := SODA_Document_T(v_Content => '{""name"" : ""Alexander""}');
    dbms_output.put_Line('Varchar2 Doc content: ' || v_Doc.get_Varchar2);
    
    -- Create BLOB document
    b_Doc := SODA_Document_T(b_Content => utl_raw.cast_to_raw('{""name"" : ""Alexander""}'));
    dbms_output.put_Line('Blob Doc content: ' || utl_raw.cast_to_varchar2(b_Doc.get_Blob));

    -- Create CLOB document
    c_Doc := SODA_Document_T(c_Content => '{""name"" : ""Alexander""}');
    dbms_output.put_Line('Clob Doc content: ' || c_Doc.get_Clob);
END;
</code>",24-AUG-19 12.24.13.373838000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 11.55.59.634848000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
201023902951084519090310328650362945416,92046253613515805458457481846201153366,"Comparing Table and Analytic View Queries",80,"<p>One of the benefits of using an analytic view is simplified SQL generation and avoidance of data errors.  The following examples compare ratio to parent queries using tables and an analytic view.</p>

<p>Create an analytic view with share to parent calculations.</p>

<code>
CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_share_of_time_parent AS (SHARE_OF(sales HIERARCHY time_hier PARENT)),
  sales_share_of_product_parent AS (SHARE_OF(sales HIERARCHY product_hier PARENT)),
  sales_share_of_geog_parent AS (SHARE_OF(sales HIERARCHY geography_hier PARENT))
  )
DEFAULT MEASURE SALES;
</code>

<p>The first example looks for the share of quarters to years.</p>

<p>The table example uses RATIO_TO_REPORT to calculate the ratio of a sales values at the quarter level to its' parent (years).  The RATIO_TO_REPORT function doesn't understand the relationship between years and quarters, so it is up to the application to understand this relationship as well as relationships to any other columns in the select list.  This is seen in PARTITION BY of RATIO_TO_REPORT.</p>

<code>
SELECT
  p.category_name,
  t.year_name,
  t.quarter_name,
  SUM(f.sales),
  ROUND(RATIO_TO_REPORT(SUM(f.sales)) OVER (PARTITION BY t.year_name, p.category_name),2) AS sales_ratio_to_report
FROM
  av.time_dim t,
  av.product_dim p,
  av.sales_fact f
WHERE
  t.month_id = f.month_id
  AND p.category_id = f.category_id
GROUP BY
  p.category_name,
  t.year_name,
  t.quarter_name;
</code>

<p>With the analytic view the application just selects from the SALES_SHARE_OF_TIME_PARENT column.  The application doesn't need to understand the relationship between year and quarter or any other column in the select list.  Those relationships are embedded in the analytic view (as well as joins and aggregation).</p>

<code>
SELECT
  product_hier.member_name    AS product_member,
  time_hier.year_name         AS year_name,
  time_hier.member_name       AS time_member,
  sales,
  ROUND(sales_share_of_time_parent,2)    AS sales_share_of_time_parent
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  time_hier.level_name = 'QUARTER'
  AND product_hier.level_name = 'CATEGORY'
ORDER BY
  product_hier.member_name,
  time_hier.hier_order;
</code>

<p>If the table query changes to month level data and DEPARTMENT_NAME is added to the SELECT list, QUARTER_NAME must also be added to the SELECT list the RATIO_TO_REPORT calculation must be updated to PARTITION BY those columns.  Failure to do so might result in unexpected results.</p>

<code>
SELECT
  p.department_name,
  p.category_name,
  t.year_name,
  t.quarter_name,
  t.month_name,
  SUM(f.sales),
  ROUND(RATIO_TO_REPORT(SUM(f.sales)) OVER (PARTITION BY t.year_name, t.quarter_name, p.department_name, p.category_name),2) AS sales_share_of_quarter
FROM
  av.time_dim t,
  av.product_dim p,
  av.sales_fact f
WHERE
  t.month_id = f.month_id
  AND p.category_id = f.category_id
GROUP BY
  p.department_name,
  p.category_name,
  t.year_name,
  t.quarter_name,
  t.month_name;
</code>

<p>If we 'forget' to PARTITION BY quarter_name, the query returned incorrect values for time share to parent.</p>

<code>
SELECT
  p.department_name,
  p.category_name,
  t.year_name,
  t.quarter_name,
  t.month_name,
  SUM(f.sales),
  ROUND(RATIO_TO_REPORT(SUM(f.sales)) OVER (PARTITION BY t.year_name, 
 p.department_name, p.category_name),2) AS sales_share_of_quarter
FROM
  av.time_dim t,
  av.product_dim p,
  av.sales_fact f
WHERE
  t.month_id = f.month_id
  AND p.category_id = f.category_id
GROUP BY
  p.department_name,
  p.category_name,
  t.year_name,
  t.quarter_name,
  t.month_name;
</code>

<p>When querying the analytic view, all that is needed is changing the TIME_HIER.LEVEL_NAME filter to 'MONTH'.  DEPARTMENT_NAME and CATEGORY_NAME are added to be consistent with the table query.</p>

<code>
SELECT
  product_hier.department_name AS department_name,
  product_hier.category_name   AS category_name,
  product_hier.member_name     AS product_member,
  time_hier.year_name          AS year_name,
  time_hier.quarter_name       AS quarter_name,
  time_hier.member_name        AS time_member,
  sales,
  ROUND(sales_share_of_time_parent,2)    AS sales_share_of_time_parent
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  time_hier.level_name = 'MONTH'
  AND product_hier.level_name = 'CATEGORY'
ORDER BY
  product_hier.hier_order,
  time_hier.hier_order;
</code>

<p>Omitting DEPARTMENT_NAME, YEAR_NAME AND QUARTER_NAME from the analytic_view query does not change the SALES_SHARE_OF_TIME_PARENT calculation because the analytic view knows that QUARTER is the parent of MONTH and the relationships between TIME and PRODUCT hierarchies.</p>

<code>
SELECT
  --product_hier.department_name  AS department_name,
  product_hier.member_name        AS product_member,
  -- time_hier.year_name          AS year_name,
  -- time_hier.quarter_name       AS quarter_name,
  time_hier.member_name           AS time_member,
  sales,
  ROUND(sales_share_of_time_parent,2)    AS sales_share_of_time_parent
FROM
  sales_av HIERARCHIES (product_hier, time_hier)
WHERE
  time_hier.level_name = 'MONTH'
  AND product_hier.level_name = 'CATEGORY'
ORDER BY
  product_hier.hier_order,
  time_hier.hier_order;
</code>",13-NOV-19 04.37.36.173850000 PM,"WILLIAM.ENDRESS@ORACLE.COM",13-NOV-19 05.24.10.861963000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
152139165451382292494324426351234027619,152139165451368994310308665430312259683,"Introduction",10,"<p>Insert is a form of DML (Data Manipulation Language) statement. You use it to store values in a table.</p>

<p>This tutorial shows you how insert works using these two tables:</p>

<code>select * from toys;

select * from bricks;

select table_name, column_name, data_type
from   user_tab_columns
where  table_name in ( 'TOYS', 'BRICKS' )
order  by table_name, column_id</code>",02-AUG-18 01.47.45.217911000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 01.59.28.630306000 PM,"CHRIS.SAXON@ORACLE.COM"
151905700197690206786027380840720017470,151905700197614044459391659202713528382,"Distinct vs. All",30,"<p>By default aggregate functions use every input value. But most allow you to work on the unique values in the input. You do this with the keyword distinct.</p>

<p>For example, you can find the number of different values in the colour column by passing ""distinct colour"" to count. There are three colours (red, green, and blue). So doing this returns three:</p>

<code>select count ( distinct colour ) number_of_different_colours
from   bricks;</code>

<p>You can explicitly tell the function to process every row with the keyword all. You can also use unique as a synonym for distinct:</p>

<code>select count ( all colour ) total_number_of_rows, 
       count ( distinct colour ) number_of_different_colours, 
       count ( unique colour ) number_of_unique_colours
from   bricks;</code>

<p>You can also use distinct in most stats functions, like sum and avg. The brick table stores the weights 1, 1, 1, 2, 2, and 3. Which has the distinct values 1, 2, and 3. So the overall weight and mean are 10 and 1.66... respectively. But the distinct sum and weight are 6 and 2:</p>

<code>select sum ( weight ) total_weight, sum ( distinct weight ) sum_of_unique_weights, 
       avg ( weight ) overall_mean, avg ( distinct weight ) mean_of_unique_weights
from   bricks;</code>

<p>Not all aggregates support distinct. Refer to the documentation for full details.</p>",31-JUL-18 08.50.20.506551000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.36.00.161577000 AM,"CHRIS.SAXON@ORACLE.COM"
117674707139436321061714962490896036022,117675390209390608523249818520886328918,"Filtering Data",20,"<p>It's rare you want to return all the rows from a table. Usually you only want those matching some search criteria.</p>

<p>You do this filtering in the where clause. The query only returns rows where the whole clause is true. For example, this gets the row that stores ""Sir Stripypants"" in the column toy_name:</p>

<code>select * from toys
where  toy_name = 'Sir Stripypants';</code>

<p>A condition can match many rows. The database will return all of them. The rows for Sir Stripypants & Mr Bunnykins both have the colour red. So this query fetches them both:</p>

<code>select * from toys
where  colour = 'red';</code>",06-SEP-17 03.33.21.785037000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.09.29.428585000 AM,"CHRIS.SAXON@ORACLE.COM"
117674707139510065536711454870553112758,117675390209390608523249818520886328918,"Combining Criteria",30,"<p>You can combine many filters with AND & OR.</p>

<h3>AND</h3>

<p>This returns the rows where both conditions are true.</p>

<p>The following searches for rows where the toy_name is Sir Stripypants. And the colour is green. Sir Stripypants is red, so this query returns nothing:</p>

<code>select * from toys
where  toy_name = 'Sir Stripypants'
and    colour = 'green';</code>

<h3>OR</h3>

<p>OR fetches all the rows where either of the criteria are true. Baby Turtle has the colour green. So this query gets the row for them and Sir Stripypants:</p>

<code>select * from toys
where  toy_name = 'Sir Stripypants' or
       colour = 'green';</code>

<h3>Order of Precedence</h3>

<p>AND has higher priority than OR. So if you include both in a where clause, the order you place them affects the results.</p> 

<p>For example, the following two queries search for the same values from each column. But they return different rows:</p>

<code>select * from toys
where  toy_name = 'Mr Bunnykins' or toy_name = 'Baby Turtle'
and    colour = 'green';

select * from toys
where  colour = 'green'
and    toy_name = 'Mr Bunnykins' or toy_name = 'Baby Turtle';</code>

<p>Why? Well the database processes the filters in different orders.</p>

<p>The first query searches for rows where:</p>

<ul><li>The colour is green and the toy_name is ""Baby Turtle""</li>
<li>Or the toy_name is ""Mr Bunnykins""</li></ul>

<p>But the second looks for rows where:</p>

<ul><li>The colour is green and the toy_name is ""Mr Bunnykins""</li>
<li>Or the toy_name is ""Baby Turtle""</li></ul>

<p>So the first will always return Mr Bunnykins. The second will always return Baby Turtle. Mr Bunnykins is red. So you get this row in the first query, but not the second.</p>

<p>To avoid confusion in queries combining AND with OR, use parentheses. The database processes conditions inside the brackets first. So the following two queries both search for rows where:</p>

<ul>
<li>The toy_name is ""Mr Bunnykins"" or ""Baby Turtle""</li>
<li>And the colour is green</li>
</ul>

<code>select * from toys
where  ( toy_name = 'Mr Bunnykins' or toy_name = 'Baby Turtle' )
and    colour = 'green';

select * from toys
where  colour = 'green'
and    ( toy_name = 'Mr Bunnykins' or toy_name = 'Baby Turtle' );</code>",06-SEP-17 03.37.15.941643000 PM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.25.27.180464000 PM,"CHRIS.SAXON@ORACLE.COM"
192501726247916406267882902918041056931,192497261674369450442611783127247019514,"Specifying Pagination Queries with Methods skip() and limit()",103,"This example uses SODA_COLLECTION_T methods filter(), skip() and limit() in a pagination query.

<code>
REM Specifying Pagination Queries with Methods skip() and limit()

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    cur         SODA_Cursor_T;
    status      BOOLEAN;
    pos         number := 0;
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('Employees');

    -- Find a document using a key
    cur := collection.find().skip(50).limit(100).get_cursor;

    -- Loop through the cursor
    WHILE cur.has_Next
    LOOP
      document := cur.next;
      pos := pos + 1;
      IF document IS NOT NULL THEN
          DBMS_OUTPUT.put_line('Document components:');
          DBMS_OUTPUT.put_line('Key: ' || document.get_key);
          DBMS_OUTPUT.put_line('Content: ' ||
                               JSON_QUERY(document.get_blob, '$' PRETTY));
          DBMS_OUTPUT.put_line('Creation timestamp: ' || 
                               document.get_created_on);
          DBMS_OUTPUT.put_line('Last modified timestamp: ' ||
                               document.get_last_modified);
          DBMS_OUTPUT.put_line('Version: ' || document.get_version);
      END IF;
    END LOOP;
    dbms_output.put_line('Docs. processed: ' || pos);
    status := cur.close;
END;
</code>",24-AUG-19 12.28.55.165564000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.29.37.165288000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
149111946068204143127221237955200581995,149075222712493902096666428584818910723,"Serializable",60,"<p>In serializable, you have transaction-level consistency. This acts as if you are the only user of the database. Changes made by other transactions are hidden from you. Some other databases refers to Oracle Database's implementation of serializable as snapshot isolation.</p>

<p>The following code starts a serializable transaction. The nested transaction updates the prices and adds a row for Purple Ninja.</p>

<p>But this happens after the start of the parent transaction. So the parent can't see the new values. The query after the PL/SQL block returns the same rows as the query at the start. You can only see the updated and inserted rows after the parent transaction ends with a commit.</p>

<code>set transaction isolation level serializable;

select * from toys;

declare 
  pragma autonomous_transaction;
begin
  update toys set price = 2.71;
  insert into toys values ( 'Purple Ninja', 7.95 );  
  commit;
end;
/

select * from toys;
commit;
select * from toys;</code>

<p>Serializable also stops you changing rows modified by other transactions. The nested transaction in the code below set the price of all the rows to 3.14. The parent transaction then tries to update and delete rows. But, because these have changed since it started, they both trigger an ORA-08177 error:</p>

<code>set transaction isolation level serializable;

select * from toys;

declare 
  pragma autonomous_transaction;
begin
  update toys set price = 3.14;
  commit;
end;
/

update toys 
set    price = price * 10
where  toy_name = 'Miss Snuggles';

delete toys 
where  toy_name = 'Baby Turtle';

commit;
select * from toys;</code>

<p>You should consider using serializable when a transaction accesses the same rows many times. And you will have many people running the transaction at the same time. </p>

<p>If you do use serializable, you'll need to consider how to handle ORA-08177 errors. The easiest way is to tell the user. Then get them to try the transaction again. But this is a bad user experience. So you could get your code to try again automatically. But this could lead to further errors!</p>

<p>To choose the best method you need to understand what your data requirements are. Work closely with your end users to figure out the way forward for your application.</p>

<p>But whatever you do, you'll reduce how many people can use your application at the same time. So you may need to make some trade-offs between data consistency and performance.</p>",04-JUL-18 04.33.15.833863000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 12.40.32.448629000 PM,"CHRIS.SAXON@ORACLE.COM"
192350069292338073064157640572623552675,192344988227908944268158345689617286517,"Introduction",5,"Faceted navigation uses the XML-based Result Set Interface (RSI) to run Oracle Text queries, rather than the typical SQL ""select ... from ... where ..."" query API.
<p>
Why would we want to do that? Consider what you might want to return as the result of a text query:
<ul>
<li>A page full of hits - say the first 10 hits, ordered by score. Each hit would consist of some summary information (name, title, date maybe) and perhaps a snippet - a small portion of text with the search terms highlighted.
<li>The total count of hits, so you can say ""Hits 1-10 of 625"" or similar
<li>Summary information about categories, or ""facets"". For example ""100 hits from Southern region, 525 hits from Northern region"". These facets could then later be used to drill-down into the results - hence ""faceted navigation"".
</ul>
<p>
This type of query is difficult or impossible from a single SQL query. Just getting a count of hits requires you to fetch the entire result set back, or do a separate ""select count(*)"" query as well.
<p>
Hence the Result Set Interface. You provide a Result Set Descriptor (RSD), which is a block of XML specifying what you want to get back (but not the search term - that's provided separately), and the RSI returns to you a Result Set, which is a block of XML describing the top-N hits and all the summary information.
<p>
This tutorial walks you through using the Result Set Interface on some short, sample data.",22-AUG-19 01.27.42.617104000 PM,"ROGER.FORD@ORACLE.COM",22-AUG-19 01.33.13.131860000 PM,"ROGER.FORD@ORACLE.COM"
213559237301235514260273666378043103366,210639064323015416218443333736849885877,"Try It!",55,"<p>Replace /*TODO*/ section in the call to DBMS_XPlan to see how much memory this query used:</p>
<code>select /*+ gather_plan_statistics */* from bricks
order by brick_id
fetch first 1 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => '/*TODO*/'));</code>",12-MAR-20 02.53.23.719914000 PM,"CHRIS.SAXON@ORACLE.COM",25-MAR-20 09.51.07.733676000 AM,"CHRIS.SAXON@ORACLE.COM"
213640854269761215092286503858400306771,210639799513782729864465643940593969908,"Try It!",70,"<p>Replace /* TODO */ to complete the index definition, so the query only reads the index, not the table:</p>

<code>create index brick_junk_colour_i
  on bricks ( /* TODO */ );

select /*+ gather_plan_statistics */colour, count(*) 
from   bricks
where  junk = rpad ( 'A', 1000, 'x' )
group  by colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>When you have a covering index, BRICKS should <strong>not</strong> appear in the Name column of the plan.</p>",13-MAR-20 09.53.28.202564000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 09.07.18.934210000 AM,"CHRIS.SAXON@ORACLE.COM"
214087312489837799290970200267517736984,210640540520278905448644863263597314383,"Keeping MVs In Sync with the Table",60,"<p>You can update the data in an MV by calling exec dbms_mview.refresh. Pass this the name of the MV and the refresh type. C does a complete refresh:</p>
<code>exec dbms_mview.refresh ( 'brick_colours_mv', 'C' );</code>

<p>A complete refresh re-runs the query in the MV, bringing it up-to-date with the underlying data. The MV is now fresh, so the optimizer can use query rewrite again using enforced integrity:</p>

<code>select mview_name, staleness 
from   user_mviews;

select colour, count(*) rws
from   bricks
group  by colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'BASIC LAST'));
</code>

<p>The query in an MV may take minutes or hours to complete. During this time it's likely rows in the tables it uses will change. So the MV is already out-of-date when the refresh finishes!</p>

<p>This is a problem if you need to get current data from an MV. To combat this, you can change the refresh so it starts with the stale MV. Then applies any changes to the tables' data to it.</p>",17-MAR-20 04.22.51.006956000 PM,"CHRIS.SAXON@ORACLE.COM",20-MAY-20 02.55.43.596444000 PM,"CHRIS.SAXON@ORACLE.COM"
213642649139085519215691807631688228340,210639799513782729864465643940593969908,"Creating an Index",20,"<p>A database index stores the values from the indexed columns and points to the rows with these values in the table. Standard indexes in Oracle Database are B-Trees. Using this the database can search the index for entries matching the WHERE clause. This can lead to a big reduction in work.</p>

<p>Use CREATE INDEX to make an index on a table. There are three key parts to this:</p>
<ul>
<li>The index name</li>
<li>The name of the table it's on</li>
<li>A comma separated list of the columns in the index</li>
</ul>
<p>This defines an index on the COLOUR column of BRICKS:</p>
<code>create index brick_colour_i 
  on bricks ( colour );</code>

<p>With the index in place, the optimizer can use it when searching for rows of a given colour. For example, this query searches the index for all ""red"" entries:</p> 

<code>select /*+ gather_plan_statistics red bricks */count(*) from bricks
where  colour = 'red';

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Full scanning the table did nearly 200 consistent reads. With the index in place this query did just 2 - a huge saving!</p>",13-MAR-20 09.36.00.656976000 AM,"CHRIS.SAXON@ORACLE.COM",27-APR-20 08.38.35.464533000 AM,"CHRIS.SAXON@ORACLE.COM"
213553644091946694598980877174203672151,210639064323015416218443333736849885877,"How Much Memory Did an Operation Use?",50,"<p>Sorting and joining operations need to read and write data too. Often these are able to process the rows in memory. You can see how much memory an operation needed by getting the plan with the MEMSTATS format:</p>

<code>select /*+ gather_plan_statistics */* from complex_query;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'MEMSTATS LAST'));</code>

<p>This adds OMem, 1Mem, and Used-Mem columns to the plan:</p>

<pre>-------------------------------------------------------------
| Id  | Operation                |  OMem |  1Mem | Used-Mem |
-------------------------------------------------------------
|   0 | SELECT STATEMENT         |       |       |          |
|   1 |  VIEW                    |       |       |          |
|   2 |   SORT AGGREGATE         |       |       |          |
|   3 |    VIEW                  |       |       |          |
|   4 |     SORT ORDER BY        |    17M|  1546K|   15M (0)|
|   5 |      COUNT               |       |       |          |
|*  6 |       HASH JOIN          |  1856K|  1856K| 1115K (0)|
|   7 |        TABLE ACCESS FULL |       |       |          |
|*  8 |        HASH JOIN         |  2171K|  2171K| 1241K (0)|
|   9 |         TABLE ACCESS FULL|       |       |          |
|  10 |         TABLE ACCESS FULL|       |       |          |
-------------------------------------------------------------</pre>

<p>Unlike the I/O figures, these values are specific to each operation in the plan. The OMem and 1Mem figures are estimates. Used-Mem reports how much memory it actually used.</p>

<p>So the database estimated that the HASH JOIN at operation 8 would need 2,171Kb of memory. But it only used about half that; 1,241Kb.</p>

<p>If memory is limited or when processing huge data sets all the rows may not fit in memory. In this case the database needs to write the data to disk to complete the operation. Oracle Database uses the temporary tablespace to do this.</p>",12-MAR-20 01.50.43.143456000 PM,"CHRIS.SAXON@ORACLE.COM",16-JUN-20 04.35.18.828370000 PM,"CHRIS.SAXON@ORACLE.COM"
213641718555415662043055494729076215801,210639799513782729864465643940593969908,"Multi-column Indexes",60,"<p>This query searches BRICKS for rows with the colour red and a weight of 1:</p>

<code>select /*+ gather_plan_statistics */*
from   bricks
where  colour = 'red'
and    weight = 1;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>This uses the colour index to locate the rows with the colour red. The database reads all these rows in the table. Then filters out the rows with weights other than one.</p>

<p>You can see this from the ""Predicate Information"" section of the plan, which looks like:</p>

<pre>   1 - filter(""WEIGHT""=1)
   2 - access(""COLOUR""='red')</pre>

<p>There are two different operations:</p>

<ul>
<li>Access - only read rows that match the search criteria</li>
<li>Filter - read all the input rows, discarding those that don't match the search</li>
</ul>

<p>So the query first goes to the index and only reads entries where the colour is red (line 2 of the plan). For all 250 of these entries the query gets the corresponding row from the table. It then searches all these for those where the weight is one. This drops the number of rows returned down from 250 to 14 (line 1 in the plan).</p>

<p>It would be more efficient to only read the rows that meet both criteria. To do this, create a multi-column index on COLOUR and WEIGHT. This enables the database locate the 14 matching rows using the index:</p>

<code>create index brick_colour_weight_i 
  on bricks ( colour, weight ) ;
  
select /*+ gather_plan_statistics indexed */*
from   bricks
where  colour = 'red'
and    weight = 1;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>The database now only reads rows that match the search. You can see this by the fact every line in the plan returns 14 rows. The total buffers for the query is also lower (30 vs 69) and there is only one predicate, an ACCESS that reads both columns:</p>

<pre>2 - access(""COLOUR""='red' AND ""WEIGHT""=1)</pre>

<p>So having an index including all the columns in a WHERE clause can make queries faster. You can also include columns in the SELECT list to improve performance further</p>",13-MAR-20 09.52.00.590391000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 01.35.34.398994000 PM,"CHRIS.SAXON@ORACLE.COM"
224394209164286047826390682328697016847,210639799513881861781674043532919876340,"Create Procedures to Trace",8,"<p>Create this procedure to add student attendance entries for schools:</p>

<code>create or replace procedure drop_off_students (
  school integer
) as
  att_count pls_integer;
begin
  for studs in ( 
    select student_id
    from   students
    where  school_id = school
  ) loop
   
    select count (*) into att_count
    from   school_attendance
    where  student_id = studs.student_id
    and    school_id = school;
  
    if att_count = 0 then 
      insert into school_attendance ( student_id, school_id )
        values ( studs.student_id, school );
    end if;
      
  end loop;
end drop_off_students;
/

create or replace procedure start_school as
begin
  for s in (
    select school_id from schools
  ) loop
    drop_off_students ( s.school_id );
  end loop;
end start_school;
/</code>",24-JUN-20 08.35.51.797126000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.59.51.403327000 PM,"CHRIS.SAXON@ORACLE.COM"
151047051108182249648502457499422313152,117123875509271941312470520788803021948,"Try It!",12,"<p>Complete the following statement to create a table to store the following details about bricks:</p>

<ul><li>Colour</li>
<li>Shape</li></ul>

<p>Use the data type varchar2(10) for both columns.</p>

<code>create table bricks (
  /*TODO*/
);

select table_name
from   user_tables
where  table_name = 'BRICKS';</code>

<p>When you create the table, the query should return the value ""BRICKS"".</p>",23-JUL-18 08.25.58.928101000 AM,"CHRIS.SAXON@ORACLE.COM",06-AUG-18 08.50.32.818524000 AM,"CHRIS.SAXON@ORACLE.COM"
214162357929332099147780927449012564542,210639064323069817880325992049711663797,"Nested Loops",30,"<p>The process for nested loops is:</p>
<ul>
<li>Read rows from the first (outer) data set</li>
<li>For each row in this, query the second (inner) data set for matching rows</li>
<li>Repeat until all the rows are read from the outer table or the client stops fetching rows</li>
</ul>
<p>This query to find all rows with different values in each table uses nested loops:</p>

<code>select /*+ gather_plan_statistics */count(*)
from   card_deck d1
join   card_deck d2
on     d1.suit <> d2.suit
and    d1.val <> d2.val;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 
<p>Note that this reads the inner table once for each row from the outer table. This means it does 52 full table scans of the second table!</p>

<p>The optimizer may choose nested loops when:</p>
<ul>
<li>Joining small data sets</li>
<li>Fetching a small fraction of large data sets</li>
<li>The query of the inner table is efficient (e.g. there is an index on its join columns)</li>
<li>All the join criteria are inequalities (not equals)</li>
</ul>",18-MAR-20 09.41.16.397495000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 04.10.46.625042000 PM,"CHRIS.SAXON@ORACLE.COM"
151905700197950125837244526113281845310,151905700197614044459391659202713528382,"Filtering Aggregates",60,"<p>The database processes group by after the where clause. So the following excludes rows with a weight <= 1 from the count:</p>

<code>select colour, count (*) 
from   bricks
where  weight > 1
group  by colour;</code>

<p>You can only filter unaggregated values in the where clause. If you include aggregate functions here, you'll get an error.</p>

<p>For example, the following tries to return the colours which have more than one row:</p>

<code>select colour, count (*)
from   bricks
where  count (*) > 1
group  by colour;</code>

<p>But it throws an ORA-00934 error.</p>

<p>to filter aggregate functions, use the having clause. This can go before or after group by. So the following both return the colours which have more than one row in bricks:</p>

<code>select colour, count (*)
from   bricks
group  by colour
having count (*) > 1;

select colour, count (*) 
from   bricks
having count (*) > 1
group  by colour;</code>

<p>You can use different functions in your select and having clauses. For example, the following finds the colours which have a total weight greater than 1. And returns how many rows there are for each of these colours:</p>

<code>select colour, count (*) 
from   bricks
having sum ( weight ) > 1
group  by colour;</code>",31-JUL-18 08.53.24.798976000 AM,"CHRIS.SAXON@ORACLE.COM",31-AUG-18 08.50.43.245863000 AM,"CHRIS.SAXON@ORACLE.COM"
153500689852627152282510020086671919399,153496376830076594111863106943541076337,"About Weighted Averages",10,"<p>A simple average calculates the center of a data set.  For example, the simple average of 1,2,3,4,5 is 2.5.  A simple average is meaningful when each data point counts equally in the average.  In many cases each data point does not county equally, for example when certain data points have a higher population or a greater number of units sold. In that case, a weighted average is more appropriate.</p>

<p>Consider a case where there are a certain number of students in a school between the ages of 5 and 10.</p>
<ul>
<li>Age 5, 100 students.</p>
<li>Age 6, 200 students.</p>
<li>Age 7, 300 students.</p>
<li>Age 8, 500 students.</p>
<li>Age 9, 800 students.</p>
<li>Age 10, 1200 students.</p>
</ul>

<p>A simple average of ages would be calculated as:</<p>
<p>(5 + 6 + 7+ 8 + 9 + 10) / 6 = 7.5</p>

<p>Because there are more older students than younger students, a simple average might be misleading.  An average of age weighted by the number of students might be more appropriate.  The weighted average would be calculated as the sum of the products of age and number of students, divided by the number of students.  In this example:</p>

<p>((5 * 100) + (6 * 200) + (7 * 300) + (8 * 500) + (9 * 800) + (10 * 1200)) / (100 + 200 + 300 + 500 + 800 + 1200)
<p>(500 + 1200 + 2100 + 4000 + 7200 + 12000 ) / 3100</p>
<p>27000 / 3100</p>
<p>Weighted Average = 8.7 years.</p>",15-AUG-18 02.31.30.451966000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-MAR-21 03.32.14.907725000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
153500689855319430082791799258742573351,153496376830076594111863106943541076337,"Sample Data Used In This Tutorial",20,"<p>This tutorial uses data in the AV schema, which contains a star schema with sales data varying by time, product and geography.  A copy of the fact table was created in the current schema by the setup script of this tutorial.  This copy of the SALES_FACT table includes a new column PERCENT_MARGIN, which is the profit margin on for that particular sale.</p>

<code>
SELECT * FROM sales_fact WHERE rownum <= 10;
</code>

<p>The setup script also created three attribute dimensions and three hierarchies.</p>

<code>
SELECT * FROM user_hier_levels ORDER BY hier_name, order_num;
</code>",15-AUG-18 02.56.32.425328000 PM,"WILLIAM.ENDRESS@ORACLE.COM",29-AUG-18 04.26.51.771877000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
152639219300740505308247930654033761477,152639219300562793212764580165351953605,"Optimistic Locking",110,"<p>You could avoid the issue above by having separate forms for managing weights and quantities. Saving changes in each of these runs an update which only changes values in the relevant column.</p>

<p>But separation like this isn't always possible. And it's more work to develop. In general to avoid lost updates you can use pessimistic or optimistic locking.</p>

<p>Pessimistic locking locks rows from the time the user first queries them with select for update. The lock remains until they save their changes. To use it, you need a stateful application. This is rare in the web, where most applications are stateless. Most applications you work on will use optimistic locking.</p>

<p>Optimistic locking works by checking values the user read are still the same when they update the row. If they aren't the update does nothing.</p>

<p>There are many ways you can do this, including:</p>

<ul><li>Adding a version number to each row</li>
<li>Adding a last updated timestamp to each row</li>
<li>Checking that the current values match those queried when you run your update</li>
</ul>

<p>For example, to avoid the lost update described in the previous module, you could change the updates to:</p>

<code>update bricks
set    quantity = 1001,
       unit_weight = 13
where  colour = 'red'
and    shape  = 'cylinder'
and    quantity = 60    -- original quantity
and    unit_weight = 13 -- original weight 
;

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>And:</p>

<code>update bricks
set    quantity = 60,
       unit_weight = 8
where  colour = 'red'
and    shape  = 'cylinder'
and    quantity = 60    -- original quantity
and    unit_weight = 13 -- original weight
;

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>This method gets cumbersome when the update changes many columns. You need a large where clause to check all the original values!</p>

<p>An easier way is to add a version or timestamp column to the table. When you run an update, check it has the same value as read at query time. And change it during the update.</p>

<p>For example, this adds the column version_number to bricks:</p>

<code>alter table bricks add ( version_number integer default 1 );

select * from bricks;</code>

<p>The updates check this value in their where clause. And increment it in the set clause.</p>

<p>When they query the row, both users get it with a version number of 1. So this is the value the application passes back to the database during update.</p>

<p>The first update succeeds, increasing the version number to 2:</p>

<code>update bricks
set    quantity = 60,
       unit_weight = 8,
       version_number = version_number + 1
where  colour = 'red'
and    shape  = 'cylinder'
and    version_number = 1;

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>So when the second runs, the condition</p>

<pre>version_number = 1</pre>

<p>is false. So the update does nothing:</p>

<code>update bricks
set    quantity = 1001,
       unit_weight = 13,
       version_number = version_number + 1
where  colour = 'red'
and    shape  = 'cylinder'
and    version_number = 1;

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>When using optimistic locking, your code needs to check for ""no change"" updates. Then inform the user the data have changed since they last read it. And ask them to resubmit their changes.</p>",07-AUG-18 08.49.02.263663000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.26.42.603333000 PM,"CHRIS.SAXON@ORACLE.COM"
92046253613526685790834013508773508950,92046253613524267939194784250424096598,"Setting up a staging table",20,"<p>For this tutorial letâ€™s build some simple staging and target tables in our schema:</p>
<code>CREATE TABLE STAGING_EMP
( ""EMPNO"" VARCHAR2(6),
  ""ENAME"" VARCHAR2(10),
  ""JOB"" VARCHAR2(9), 
  ""MGR"" VARCHAR2(4), 
  ""HIREDATE"" VARCHAR2(10),
  ""SAL"" VARCHAR2(7),
  ""COMM"" VARCHAR2(9),
  ""DEPTNO"" VARCHAR2(6));
</code>
<p>Now let's insert some data into the staging table</p>
<code>-- INSERTING DATA INTO STAGING_EMP
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('GB9369','SMITH','CLERK','7902','17-DEC-80','800',null,'20');
-- INVALID DATE
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('9499','ALLEN','SALESMAN','7698','31-FEB-81','1600','300','30');
-- INVALID NUMBER FOR DEPTNO
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('9521','WARD','SALESMAN','7698','22-FEB-81','1250','500','SALES');
-- INVALID NUMBER FOR EMPNO KEY
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('US9566','JONES','MANAGER','7839','02-APR-81','2975',null,'20');
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('9782','CLARK','MANAGER','7839','09-JUN-81','2450',null,'10');
-- INVALID NUMBER FOR EMPNO KEY
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('FR9788','SCOTT','ANALYST','7566','19-APR-87','3000',null,'20');
-- INVALID NUMBER FOR MGR KEY
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('9839','KING','PRESIDENT',null,'17-NOV-81','5000',null,'10');
-- INVALID NUMBER FOR EMPNO KEY
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('DE9844','TURNER','SALESMAN','7698','08-SEP-81','1500',0,'30');
Insert into STAGING_EMP (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO) values ('9876','ADAMS','CLERK','7788','23-MAY-87','1100',null,'20');
</code>
<p>Now we have a data set that we can try to load into our target table</p>",04-JAN-17 05.58.02.114835000 AM,"SYS",18-DEC-18 03.21.19.964597000 PM,"SHARON.KENNEDY@ORACLE.COM"
192586115685802255415326802076162990090,192497261674369450442611783127247019514,"Using SQL against SODA Collections",130,"You can also use native SQL statements to select documents from a SODA collection. The following SQL query selects Employee's ID, name, salary and company from the employee document matching a filter criteria.

<code>
REM Using SQL/JSON operators to filter documents

  SELECT emp.ID ""ID"",
         JSON_Value(emp.""JSON_DOCUMENT"",
                    '$.name') ""Name"",
         JSON_Value(emp.""JSON_DOCUMENT"",
                    '$.salary') ""Salary"",
         JSON_Value(emp.""JSON_DOCUMENT"",
                    '$.company') ""Company"",
         JSON_Query(emp.""JSON_DOCUMENT"",
                    '$.address' returning VARCHAR2(500)) ""Address""
    FROM ""Employees"" emp
   WHERE JSON_Exists(emp.""JSON_DOCUMENT"",
                     '$?(@.empno <= $B0 && @.salary > $B1 && @.name starts with $B2)'
                     PASSING 10041 AS ""B0"", 200000 AS ""B1"", 'Melissa' AS ""B2"")
     AND JSON_TextContains(emp.""JSON_DOCUMENT"",
                           '$.company',
                           'Salesforce')              
ORDER BY JSON_Value(emp.""JSON_DOCUMENT"",
                    '$.salary') ASC;
</code>",24-AUG-19 07.30.21.694116000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",26-AUG-19 07.02.51.476480000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
153500689855399219186886364784273180967,153496376830076594111863106943541076337,"Calculating Averages Using SQL From Tables",30,"<p>By looking at a small slice of the fact table it can be seen that a simple average of PERCENT_MARGIN is not very meaningful.  Category -532 has a high margin at 36% but relatively few sales at 675 units.  Category -529 has a low margin at 7% and the most sales at 7,336 units.  Category -532 should have a greater weighting in the average margin calculation.</p>

<code> 
SELECT * FROM sales_fact WHERE rownum <= 50 AND month_id = 'Apr-15' AND state_province_id = 'TEXAS_US'; 
</code>

<p>The following query returns simple average of PERCENT_MARGIN.  This might be very misleading.</p>

<code>SELECT AVG(percent_margin) FROM sales_fact WHERE rownum <= 50 AND month_id = 'Apr-15' AND state_province_id = 'TEXAS_US';</code>

It is probably more meaningful to weight the average of PERCENT_MARGIN using the number of units sold.  The weighted average is calculated as the sum of (UNITS * PERCENT_MARGIN) divided by the sum of UNITS.</p>

<code>
SELECT
   SUM(units * percent_margin) / SUM(units)
FROM
  sales_fact
WHERE
  rownum <= 50
  AND month_id = 'Apr-15'
  AND state_province_id = 'TEXAS_US';
</code>

<p>Using this formula, categories that have a higher number of units sold are given a higher weight than categories with a lower number of units sold.</p>

<p>The following query shows that there are varying numbers of units sold for each product category in South America.</>

<code>
SELECT
  t.year_name,
  p.category_name,
  g.region_name,
  SUM(f.units) AS units
FROM
  av.time_dim t,
  av.product_dim p,
  av.geography_dim g,
  sales_fact f
WHERE
  t.month_id = f.month_id
  AND p.category_id = f.category_id
  AND g.state_province_id = f.state_province_id
  AND t.year_name = 'CY2015'
  AND g.region_name = 'South America'
GROUP BY
  t.year_name,
  p.category_name,
  g.region_name;
</code>

<p>The next query calculates the simple and weighted average of percent margin.  Note that in this query, PERCENT_MARGIN * UNITS is first calculated for each row and then summed.</p>

<code>
SELECT
  t.year_name,
  g.region_name,
  SUM(f.units) AS units,
  SUM(f.sales) AS sales,
  ROUND(AVG(f.percent_margin),3) AS simple_average_percent_margin,
  ROUND(SUM(f.percent_margin * f.units) / SUM(f.units),3) AS units_weighted_average_percent_margin
FROM
  av.time_dim t,
  av.geography_dim g,
  sales_fact f
WHERE
  t.month_id = f.month_id
  AND g.state_province_id = f.state_province_id
  AND t.year_name = 'CY2015'
  AND g.region_name = 'South America'
GROUP BY
  t.year_name,
  g.region_name; 
</code>",15-AUG-18 02.59.27.632699000 PM,"WILLIAM.ENDRESS@ORACLE.COM",30-AUG-18 03.49.59.766176000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
192501726247886183122392537188673402531,192497261674369450442611783127247019514,"Inserting a Document into a Collection and Getting the Result Document",46,"This example creates a document and inserts it into a collection using method insert_one_and_get(). It then gets (and prints) each of the generated components from the result document (which contains them). To obtain the components it uses SODA_DOCUMENT_T methods get_key(), get_created_on(), get_last_modified(), and get_version().
<code>
REM Inserting a Document into a Collection and Getting the Result Document
DECLARE
    collection  SODA_Collection_T;
    document    SODA_Document_T;
    ins_Doc     SODA_Document_T;
BEGIN
    -- Open the collection
    collection := dbms_soda.open_Collection('myCollectionName');
    document := SODA_Document_T(b_Content => utl_raw.cast_to_raw('{""name"" : ""Ballimer""}'));
    ins_Doc := collection.insert_One_And_Get(document);

    -- Insert the document and get its components
    IF ins_Doc IS NOT NULL THEN
        dbms_output.put_line('Inserted document components:');
        dbms_output.put_line('Key: ' || ins_Doc.get_Key);
        dbms_output.put_line('Creation timestamp: ' || ins_Doc.get_Created_On);
        dbms_output.put_line('Last modified timestamp: ' || ins_Doc.get_Last_Modified);
        dbms_output.put_line('Version: ' || ins_Doc.get_Version);
    END IF;
END;
</code>",24-AUG-19 12.26.15.598326000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.06.24.579060000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
152139165451686941800867312903259983971,152139165451368994310308665430312259683,"Single Row Insert",20,"<p>You can add one row at a time to a table with the insert clause. This takes the form:</p>

<pre>insert into &lt;table_name&gt; ( col1, col2, ... )
  values ( 'value1', 'value2', ... )</pre>

<p>The column list is optional. If you omit this, you must provide a value for every visible column in the table.</p>

<p>For example, the following adds a row for a toy named Miss Snuggles coloured pink with the toy_id 1 into the toy table:</p>

<code>insert into toys values ( 1, 'Miss Snuggles', 'pink' );

select * from toys;</code>

<p>If the number of values doesn't match the number and data types of the target columns, you'll get an error. The insert below only provides two values. But the table has three columns. So it throws an exception:</p>

<code>insert into toys values ( 2, 'Baby Turtle' );</code>

<p>This makes inserts without a column list brittle. If you change the columns in the table, the insert is likely to fail. It's much better to list the columns you're providing values for. For example:</p>

<code>insert into toys ( toy_id, toy_name ) values ( 2, 'Baby Turtle' );

select * from toys;</code>

<p>This will still work if someone adds another column or removes the colour column from toys. Any table columns you don't provide a value for will store null.</p>",02-AUG-18 01.52.00.581382000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.15.06.717217000 PM,"CHRIS.SAXON@ORACLE.COM"
210636710534374068149664599419250147823,210639064319457547531317480075689609909,"Get a Basic Plan with DBMS_XPlan",10,"<p>You can get the execution plan for a query using DBMS_XPlan. The table function DISPLAY_CURSOR fetches the plan from memory for the requested SQL statement.</p>

<p>Run this to get the execution plan for the join:</p>

<code>select *
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID));</code>

<p>The first argument of the DISPLAY_CURSOR is the SQL ID for the statement. LIVESQL_LAST_SQL_ID is a bind variable specific to Live SQL. Only use this in Live SQL. Normally you'll want to pass NULL or a SQL ID instead.</p>

<p>Passing NULL gets the plan for the last SQL statement run in this session. Using a SQL ID searches for plans in the cursor cache for that statement.</p>",13-FEB-20 03.33.20.933358000 PM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.23.40.129518000 AM,"CHRIS.SAXON@ORACLE.COM"
214688466351416420251971998559253442223,210639064319457547531317480075689609909,"Reading an Execution Plan",17,"<p>An execution plan is a tree. The database uses depth-first search to traverse it.</p>

<p>This starts from the SELECT operation at the top of the plan. Then travels down to the top-most leaf. After reading the data from this source, it goes back up to its parent.</p>

<p>At this point the process repeats. Travel down the plan to find the next unvisited leaf. After reading, this, pass the rows back up to its parent.</p>

<code>select *
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID));</code>

<ol>
	<li>
	<p>Start from the top (<strong>SELECT STATEMENT</strong>) and go down the tree to the first leaf. This is the <strong>TABLE ACCESS FULL</strong> of the <strong>COLOURS</strong> table.</p>
	</li>
	<li>
	<p>Pass the rows from this table up to the first leafâ€™s parent, the <strong>HASH JOIN</strong>.</p>
	</li>
	<li>
	<p>Look for the next unvisited child of step 1. This is the <strong>TABLE ACCESS FULL</strong> of the <strong>BRICKS</strong> table.</p>
	</li>
	<li>
	<p>Pass the rows from this table up to its parent, the <strong>HASH JOIN</strong>.</p>
	</li>
	<li>
	<p>All the children of step 1 have been accessed, so pass the rows that survive the join to the <strong>SELECT STATEMENT</strong> and back to the client.</p>
	</li>
</ol>",23-MAR-20 09.58.48.674452000 AM,"CHRIS.SAXON@ORACLE.COM",23-MAR-20 10.13.13.041485000 AM,"CHRIS.SAXON@ORACLE.COM"
117123570088844762665360811515429148136,117123875509271941312470520788803021948,"Temporary Tables",40,"<p>Temporary tables store session specific data. Only the session that adds the rows can see them. This can be handy to store working data.</p>

<p>There are two types of temporary table in Oracle Database: global and private.</p>

<h3>Global Temporary Tables</h3>

<p>To create a global temporary table add the clause ""global temporary"" between create and table. For example:</p>

<code>create global temporary table toys_gtt (
  toy_name varchar2(100)
);</code>

<p>The definition of the temporary table is permanent. All users of the database can access it. But only your session can view rows you insert.</p>

<h3>Private Temporary Tables</h3>

<p>Starting in Oracle Database 18c, you can create private temporary tables. These tables are only visible in your session. Other sessions can't see the table!</p>

<p>To create one use ""private temporary"" between create and table. You must also prefix the table name with ora$ptt_:</p>

<code>create private temporary table ora$ptt_toys (
  toy_name varchar2(100)
);</code>

<p>For both temporary table types, by default the rows disappear when you end your transaction. You can change this to when your session ends with the ""on commit"" clause. </p>

<p>But either way, no one else can view the rows. Ensure you copy data you need to permanent tables before your session ends!</p>

<h3>Viewing Temporary Table Details</h3>

<p>The column temporary in the *_tables views tell you which tables are temporary:</p>

<code>select table_name, temporary
from   user_tables
where  table_name in ( 'TOYS_GTT', 'ORA$PTT_TOYS' );</code>

<p>Note that you can only see a row for the global temporary table. The database doesn't write private temporary tables to the data dictionary!</p>",01-SEP-17 08.35.35.526240000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 07.44.53.341241000 AM,"CHRIS.SAXON@ORACLE.COM"
184084400232780500934938049623303562676,184084400229208125137976820412046812596,"Part 1 - Basic Index Creation and Queries",10,"<h3>Creating an index and running a query against it</h3>
<p/>
If you're running this tutorial outside of Live SQL, you'll need to make sure the user who runs it has CTXAPP role. CREATE JOB privilege is strongly recommended as well.
<p/>
Let's create a simple table:
<code>create table quickstart (
  id         number primary key,
  country    varchar2(2),
  full_name  varchar2(40) 
);
</code>
Now insert some test data into that table:
<code>insert into quickstart values (1, 'US', 'John Doe');
insert into quickstart values (2, 'GB', 'John Smith');
insert into quickstart values (3, 'NZ', 'Peter Smith-Smith');
</code>
Now a simple query against that table might look like
<code>select * from quickstart where country = 'GB';
</code>
Now let's say we want to search for anyone with ""Smith"" in their name.  We could do this by simply extending the query to:

<code>select * from quickstart where country = 'GB' and upper(full_name) like '%SMITH%';
</code>

but there are a few problems with this:

<ul>
<li>There is no index. The kernel must read every full_name field and scan it. Not a problem here, but certainly a problem if there are millions of records.</li>
<li>The query would match SMITHSON and BLACKSMITH. Not a problem if that's what we want, but tricky otherwise.</li>
</ul>
<p/>We can create a word-based index on the FULL_NAME column using the following syntax:
<code>create index full_name_index on quickstart( full_name ) indextype is ctxsys.context;
</code>
Note this is like any other ""create index"" statement, with the addition of the phrase ""indextype is ctxsys.context"", telling the kernel to create a specialized CONTEXT index and allow the use of query operators associated with that indextype.
<p/>
For example we can do:
<code>select * from quickstart where country = 'GB' and contains ( full_name, 'smith') > 0;
</code>
A few things to note:
<ul>
<li>The CONTAINS operator can only be used if an index is present on the column specified in the first argument.</li>
<li>The search string doesn't need any wildcards either side.  We are looking for the whole word 'smith' (if we wanted to match 'smithson' as well, then we could use a wildcard: 'smith%').</li>
<li>SQL doesn't have boolean functions, so CONTAINS returns zero for a non-match, and greater-than-zero for a match.  The actual value returned is the 'score' (see later) but there are very rarely any circumstances where you do anything other than testing where the return value is greater than zero.</li>
</ul>
<p/>Scoring: Most SQL queries just select between rows that match a criterion, and those that don't. There is rarely the concept of a good or better match. But if we're searching a lot of technical papers for the word ""Exadata"", then a document which has many occurrences of this term is likely to be a better match for our search than one where there is only a single match.
<p/>Therefore CONTEXT indexes return a score for each search.  As mentioned, the score is returned by the CONTAINS function, but is rarely useful there.  It's usually better to use it in the SELECT clause, and perhaps in an ORDER BY clause as well.
<p/>A score is associated with a particular CONTAINS clause, of which a query may have several, so we use a number as the third argument in the CONTAINS function, and SCORE operator takes the same number.  It doesn't matter what that number is so long as it's the same in both cases.  We'll use 99 in this example:
<code>select score(99), id, full_name from quickstart where contains ( full_name, 'smith', 99) > 0 order by score(99) desc;
</code>
This produces the output:
<pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         7          3 Peter Smith-Smith
         4          2 John Smith
</pre>
<p/>Note that the first item here scores higher than the second, because the search term smith appears twice.  Note there is no ""absolute"" meaning to those scores - you can't say  the first is a ""7% match"" or anything like that - all you can say is that a record with a higher score is more relevant than one with a lower score.
<p/>
Scoring is quite a complex topic, which we'll cover in more detail later.",04-JUN-19 10.05.06.277791000 AM,"ROGER.FORD@ORACLE.COM",04-JUN-19 02.52.40.388040000 PM,"ROGER.FORD@ORACLE.COM"
214687396398955443727257917179292185235,210639064319457547531317480075689609909,"Find the SQL ID for a Statement",11,"<p>To get the SQL ID for a statement, search for it in v$sql:</p>

<code>select sql_id, sql_text
from   v$sql
where  sql_text like 'select *%bricks b%'
/* exclude this query */
and    sql_text not like '%not this%';</code>

<p>The SQL ID is a hash of the statement's text. This means that changes to the formatting of a SQL statement will give it a new SQL ID, even though its meaning is identical. For example the only difference between these queries is the case of select. But they have different SQL IDs!</p>

<code>select *
from   bricks b
join   colours c
on     b.colour = c.colour;

SELECT *
from   bricks b
join   colours c
on     b.colour = c.colour;

select sql_id, sql_text
from   v$sql
where  sql_text like '%bricks%join%colours%'
/* exclude this query */
and    sql_text not like '%not this%';</code>",23-MAR-20 10.04.31.084136000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.26.43.265936000 AM,"CHRIS.SAXON@ORACLE.COM"
214688466353482474477693399818826297007,210639064319457547531317480075689609909,"Try It!",15,"<p>Replace /* TODO */ to find the SQL ID for the query fetching rows from BRICKS:</p>

<code>select * from bricks;

select sql_id from v$sql
where sql_text = '/* TODO */';
</code>",23-MAR-20 10.15.08.935918000 AM,"CHRIS.SAXON@ORACLE.COM",15-JUL-20 03.21.51.652475000 PM,"CHRIS.SAXON@ORACLE.COM"
117123570088913671437078845378387400168,117123875509271941312470520788803021948,"Partitioning Tables",50,"<p>Partitioning logically splits up a table into smaller tables according to the partition column(s). So rows with the same partition key are stored in the same physical location.</p>

<p>There are three types of partitioning available:</p>

<ul><li>Range</li><li>List</li><li>Hash</li></ul>

<p>To create a partitioned table, you need to:</p>

<ul>
<li>Choose a partition method</li>
<li>State the partition columns</li>
<li>Define the initial partitions</li></ul>

<p>The following statements create one table for each partitioning type:</p>

<code>create table toys_range (
  toy_name varchar2(100)
) partition by range ( toy_name ) (
  partition p0 values less than ('b'),
  partition p1 values less than ('c')
);

create table toys_list (
  toy_name varchar2(100)
) partition by list ( toy_name ) (
  partition p0 values ('Sir Stripypants'),
  partition p1 values ('Miss Snuggles')
);

create table toys_hash (
  toy_name varchar2(100)
) partition by hash ( toy_name ) partitions 4;</code>

<p>By default a partitioned table is heap-organized. But you can combine partitioning with some other properties. For example, you can have a partitioned IOT:</p>

<code>create table toys_part_iot (
  toy_id   integer primary key,
  toy_name varchar2(100)
) organization index 
  partition by hash ( toy_id ) partitions 4;</code>

<p>The database sets the partitioned column of *_tables to YES if the table is partitioned. You can view details about the partitions in the *_tab_partitions tables:</p>

<code>select table_name, partitioned 
from   user_tables
where  table_name in ( 'TOYS_HASH', 'TOYS_LIST', 'TOYS_RANGE', 'TOYS_PART_IOT' );

select table_name, partition_name
from   user_tab_partitions;</code>

<p>Note that partitioning is a separately licensable option of Oracle Database. Ensure you have this option before using it!</p>",01-SEP-17 08.39.50.155747000 AM,"CHRIS.SAXON@ORACLE.COM",10-AUG-18 08.00.57.685236000 AM,"CHRIS.SAXON@ORACLE.COM"
117123180204898209725218572605663640136,117123875509271941312470520788803021948,"Dropping Tables",80,"<p>You can remove existing tables with the drop table command. Just add the name of the table you want to destroy:</p>

<code>select table_name
from   user_tables
where  table_name = 'TOYS_HEAP';

drop table toys_heap;

select table_name
from   user_tables
where  table_name = 'TOYS_HEAP';</code>

<p>Once you've dropped a table you can't access it. So take care with this command!</p>",01-SEP-17 09.08.32.021915000 AM,"CHRIS.SAXON@ORACLE.COM",06-AUG-18 09.22.00.324065000 AM,"CHRIS.SAXON@ORACLE.COM"
152639219300578509248419570344623133893,152639219300562793212764580165351953605,"Introduction",10,"<p>This tutorial teaches you how to change values using the update statement. Like select and insert, this is also a DML statement. The examples use the following table:</p>

<code>select * from bricks;</code>",07-AUG-18 08.45.15.808190000 AM,"CHRIS.SAXON@ORACLE.COM",06-SEP-18 09.11.00.417535000 AM,"CHRIS.SAXON@ORACLE.COM"
152639219300602687764811862928117257413,152639219300562793212764580165351953605,"Try It!",30,"<p>Complete the following update to set the unit_weight of all bricks to 21:</p>

<code>update bricks
set    /* TODO */;

select unit_weight from bricks;</code>

<p>The query should return the following rows:</p>

<pre><b>UNIT_WEIGHT</b>
21
21
21</pre>",07-AUG-18 08.46.11.618436000 AM,"CHRIS.SAXON@ORACLE.COM",06-SEP-18 04.35.45.920552000 PM,"CHRIS.SAXON@ORACLE.COM"
152639219300637746613580687174183736517,152639219300562793212764580165351953605,"Filtering Updates",40,"<p>It's rare you want to update all the rows in a table. Usually you only want to change a few.</p>

<p>You can do this by providing a where clause. This works in the same way as in select statements. Only rows where the conditions are true will change.</p>

<p>This update changes the quantity to 1,422 for all rows with colour of red:</p>

<code>update bricks
set    quantity = 1422
where  colour = 'red';

select * from bricks;</code>",07-AUG-18 08.46.40.730909000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.17.35.232961000 PM,"CHRIS.SAXON@ORACLE.COM"
192579982972734845492619806211796806137,192497261674369450442611783127247019514,"Load documents",3,"<code>
REM Load docs to a collection
DECLARE
    coll        SODA_Collection_T;
    tot         NUMBER := 1000;
BEGIN
    coll := dbms_soda.create_Collection('Employees');
    oow_soda.load_docs(collection  => coll,
                        nDocs       => tot);
    dbms_output.put_line('Docs loaded: '|| tot);
EXCEPTION
    WHEN OTHERS THEN
    dbms_output.put_line (SQLERRM);
    ROLLBACK;
END;

</code>",24-AUG-19 06.20.24.379272000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 07.25.45.422219000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
211136333804975792720632860936441072185,210639064319457547531317480075689609909,"View Rows Processed in the Plan",32,"<p>A basic plan only gives you its shape. To assess whether the plan is <em>good</em>, you need to see the number of rows flowing out of each step of the plan.</p>

<p>You do this with the format parameter of DBMS_XPlan. Using ROWSTATS adds the estimated and actual number of rows to each step.</p>

<p>You should also specify which execution details you want. Control this with LAST or ALL:</p>
<ul >
	<li>LAST - Only display the stats for the last execution. </li>
	<li>ALL - Cumulative stats for every execution. This is the default.</li>
</ul>

<p>This gets the row details for the previous execution of the query:</p>

<code>select *
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>
<p>But it only shows the E(stimated)-rows column. The A(ctual)-rows are missing!</p>
<p>As the note indicates, to capture this information either you need to:</p>

<ul >
	<li>Hint the query - add the hint /*+ gather_plan_statistics */ to your query</li>
	<li>Alter the session - Run alter session set statistics_level = all before the query</li>
</ul>
<p>This captures row stats by adding the hint to the query: </p>
<code>select /*+ gather_plan_statistics */*
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>",18-FEB-20 09.46.25.950779000 AM,"CHRIS.SAXON@ORACLE.COM",23-MAR-20 10.59.42.340104000 AM,"CHRIS.SAXON@ORACLE.COM"
212176360604154247251078389001263694647,210639799513641285543570732327153347316,"Try It!",120,"<p>The optimizer also struggles to get correct row estimates when you apply functions to columns. This query fetches 150 rows from the table, but the optimizer thinks it'll get just far less:</p>
<code>select /*+ gather_plan_statistics */ count (*) from bricks
where  weight + 2 = 3;

select *
from table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>You can also overcome this with extended stats. Complete this code to create extended stats on (weight + 2) for the bricks table:</p>

<code>select dbms_stats.create_extended_stats ( null, 'bricks', '(TODO)' )
from dual;

exec dbms_stats.gather_table_stats ( null, 'bricks', method_opt => 'for all columns size skewonly', no_invalidate => false ) ;</code>

<p>Re-run the query to verify that the optimizer now has better row estimates:</p>

<code>select /*+ gather_plan_statistics */count (*) from bricks
where  weight + 2 = 3;

select *
from table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>

<p><em>It's even better to remove functions from columns in queries to avoid issues like this. For example, changing this query to weight = ( 3-2 ). But it may not be possible to do this with more complex formulas.</em></p>",28-FEB-20 09.04.41.019125000 AM,"CHRIS.SAXON@ORACLE.COM",12-APR-21 11.10.07.035785000 AM,"CHRIS.SAXON@ORACLE.COM"
152639219300704237533659491778792576197,152639219300562793212764580165351953605,"Transactions",50,"<p>A transaction is the smallest unit of work which leaves the database in a consistent state. When a user saves changes in your application, this may generate many inserts, updates or deletes in your code. All these changes form one transaction.</p>

<p>A transaction ends when you commit or rollback (to the previous commit). You should only commit after running all the changes.</p>

<p>For example, say you paint ten of the green cubes blue. So you need to increase the quantity of blue cubes by 10. And decrease the green bricks by the same amount. If one update completes but not the other the total quantity will be out by 10.</p>

<p>This is can happen if you place a commit between the updates, as in this code:</p>

<code>update bricks
set    quantity = quantity - 10
where  colour = 'green'
and    shape = 'cube';

commit;
  
update bricks
set    quantity = quantity + 10
where  colour = 'blue'
and    shape = 'cube';
  
commit;

select * from bricks;</code>

<p>If the update of the blue rows fails for some reason, you've removed ten green cubes, but not added ten blue. So these bricks are ""missing"". Resolving this in code is hard. To fix the error, it's likely you'll need to run a one-off update.</p>

<p>To avoid this problem, move the commit to the end:</p>

<code>update bricks
set    quantity = quantity - 10
where  colour = 'green'
and    shape = 'cube';
  
update bricks
set    quantity = quantity + 10
where  colour = 'blue'
and    shape = 'cube';
  
commit;

select * from bricks;</code>

<p>Now both updates are in the same transaction. If either fails, you can rollback the whole transaction and try again.</p>

<p>Note that you should leave it up to the client to commit or rollback. Placing commits at the end of your transaction makes it hard to reuse code. If you have two transactions that both include a commit, you can't combine them into one, larger transaction.</p>",07-AUG-18 08.48.19.927827000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.19.46.135681000 PM,"CHRIS.SAXON@ORACLE.COM"
152639219300879531777503613009124971717,152639219300562793212764580165351953605,"Try It!",45,"<p>Complete the update below to set the unit_weight of all the rows storing the shape cube to 5:</p>

<code>update bricks
set    /* TODO */
where  /* TODO */;

select shape, unit_weight
from   bricks;</code>

<p>The query should return the following rows:</p>

<pre><b>SHAPE	UNIT_WEIGHT</b>
cylinder         21
cube	          5
cube	          5</pre>",07-AUG-18 08.55.55.904966000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.18.04.569537000 PM,"CHRIS.SAXON@ORACLE.COM"
152639219300669178684890667532726097093,152639219300562793212764580165351953605,"Deadlocks",60,"<p>When a transaction runs many updates, you need to take care. If two or more people try to change the same rows at the same time, you can get into deadlock situations. This is where sessions form a circle, each waiting on the other.</p>

<p>In the example below, both transactions start by updating different rows. So the first session has locked the rows with the colour red. And the second locks rows with the colour blue.</p>

<p>The first session then tries to update rows storing blue. But transaction two has these locked! So it's blocked, waiting for transaction 2 to complete.</p>

<p>But transaction two, instead of committing or rolling back, tries to update rows storing red. But transaction one has these rows locked! So it's stuck. At this point both sessions are waiting for the other to release a lock.</p>

<p>Deadlock!</p>

<table>
  <tr>
    <th>Transaction 1</th>
    <th>Transaction 2</th>
  </tr>
  <tr>
    <td>
<pre>
update bricks
set    quantity = 1001
where  colour = 'red';
</pre></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>
<pre>
update bricks
set    unit_weight = 8
where  colour = 'blue';</pre></td>
  </tr>
  <tr>
    <td>
<pre>update bricks
set    quantity = 1722
where  colour = 'blue';

-- this session is now blocked, 
-- waiting for transaction 2</pre></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>

<pre>update bricks
set     unit_weight = 13
where   colour = 'red'
-- at this point both sessions 
-- are waiting on each other
-- the database will stop one
-- of them, raising an ORA-60</pre></td>
  </tr>
</table>

<p>When deadlock happens, Oracle Database will detect it. The database will then stop one of the statements, raising an ORA-00060.</p>

<p>You can simulate a deadlock in one session with autonomous transactions. These start a transaction within a transaction.</p>

<p>So the following code contains two transactions. The parent transaction updates all the rows in bricks, blocking other changes to these rows. The autonomous transaction after it also tries to update all the rows.</p>

<p>This is impossible. For the parent transaction to continue, the child  autonomous transaction must complete. But the update in the parent blocks the update in the child child!</p>

<p>So the database identifies this as a deadlock. It stops the second update, raising ORA-00060. And allows you to commit or rollback the first update:</p>

<code>update bricks
set    quantity = 60;

declare
  pragma autonomous_transaction;
begin
  update bricks
  set    unit_weight = 55;
  commit;
end;
/

select * from bricks;

rollback;</code>

<p>Note that an update locks the whole row. Even though the two updates set different columns, the second is still blocked. Leading to deadlock.</p>

<p>Deadlocks are caused by errors in your code. To avoid them, you must change how you write your transactions.</p>",07-AUG-18 08.47.22.613288000 AM,"CHRIS.SAXON@ORACLE.COM",17-MAY-22 10.43.18.864622000 AM,"CHRIS.SAXON@ORACLE.COM"
192504621639844272055612418528272615597,192497261674369450442611783127247019514,"QBE: Using $orderby clause and numbers",106,"The following examples finds all the employee documents whose employee number is less than or equal to 20040, whose salary is greater than 200000 and whose name starts with ""Melissa"". It also orders the result in ascending order of salary.
<code>
REM Using order by clause and numbers in QBE


DECLARE
    collection SODA_Collection_T;
    operation  SODA_Operation_T;
    document   SODA_Document_T;
    cur        SODA_Cursor_T;
    qbe        VARCHAR2(400);
    pos        NUMBER  := 0;
    status     BOOLEAN;
BEGIN
    collection := dbms_soda.open_collection('Employees');
    oow_soda.show(collection,
                  setting => oow_soda.SHOW_COLL_NAME_ONLY);

    qbe := '{""$query"" :{
                ""empno"" : { ""$lte"": 20040 },
                ""name"" : { ""$startsWith"" : ""Melissa"" },
                ""salary"" : { ""$gt"" : 200000 }
              },
             ""$orderby"" : [
                  {""path"" : ""salary"", ""datatype"" : ""number"", ""order"" : ""asc""}
                ]
            }';
    operation := collection.find().filter(qbe);
    cur := operation.get_Cursor;
    -- Loop over the cursor
    WHILE cur.has_Next()
    LOOP
        pos := pos + 1;
        document := cur.next;
        oow_soda.show(document,
                      comments => 'Doc ('|| pos ||')');
    END LOOP;
    dbms_output.put_line('No. of docs read: ' || pos);
    status := cur.close;
END;

</code>",24-AUG-19 12.31.43.838768000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.22.54.726356000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
210639799513363232605059367616970926836,210639064319457547531317480075689609909,"Reading a Four Table Join Plan",20,"<p>Run this four table join to get its plan:</p>

<code>select c.*, pen_type, shape, toy_name 
from   colours c
join   pens p
on     c.colour = p.colour
join   cuddly_toys t
on     c.colour = t.colour
join   bricks b
on     c.colour = b.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID));

/*
--------------------------------------------                              
| Id  | Operation            | Name        |                              
--------------------------------------------                              
|   0 | SELECT STATEMENT     |             |                              
|   1 |  HASH JOIN           |             |                              
|   2 |   HASH JOIN          |             |                              
|   3 |    HASH JOIN         |             |                              
|   4 |     TABLE ACCESS FULL| COLOURS     |                              
|   5 |     TABLE ACCESS FULL| CUDDLY_TOYS |                              
|   6 |    TABLE ACCESS FULL | PENS        |                              
|   7 |   TABLE ACCESS FULL  | BRICKS      |                              
--------------------------------------------
*/
</code>

<p>The order of operations in this plan is:</p>
<ol>
	<li>
	<p>Start from the top of the plan (<strong>SELECT STATEMENT</strong>) and go down to the first leaf. This is the <strong>TABLE ACCESS FULL</strong> of the <strong>COLOURS</strong> table in execution plan step 4.</p>
	</li>
	<li>
	<p>Pass the rows from this table up to the first leaf&rsquo;s parent, which is the <strong>HASH JOIN</strong> in step 3.</p>
	</li>
	<li>
	<p>Find the next unvisited child, which is the <strong>TABLE ACCESS FULL</strong> of the <strong>CUDDLY_TOYS</strong> table in step 5.</p>
	</li>
	<li>
	<p>Pass the rows to the <strong>HASH JOIN</strong> in step 3. Step 3 has no more children, so return the rows that survive the <strong>HASH JOIN</strong> in step 3 to the <strong>HASH JOIN</strong> in step 2.</p>
	</li>
	<li>
	<p>Search for the next child of step 2. This is the <strong>TABLE ACCESS FULL</strong> of the <strong>PENS</strong> table in step 6.</p>
	</li>
	<li>
	<p>Pass these rows to the <strong>HASH JOIN</strong> in step 2. Step 2 has no more children, so return the rows that survive the <strong>HASH JOIN</strong> in step 2 to the <strong>HASH JOIN</strong> in step 1.</p>
	</li>
	<li>
	<p>Repeat the process until you&rsquo;ve run all the operations. So the complete order for accessing the execution plan step IDs is: 4, 3, 5, 3, 2, 6, 2, 1, 7, 1, and 0.</p>
	</li>
</ol>",13-FEB-20 03.36.43.074548000 PM,"CHRIS.SAXON@ORACLE.COM",15-JUL-20 03.26.20.321617000 PM,"CHRIS.SAXON@ORACLE.COM"
211970144622190635130226204062069173728,210639799513641285543570732327153347316,"Gathering Histograms",60,"<p>Currently there are no histograms on BRICK_ID and WEIGHT in BRICKS. You can verify this by checking the HISTOGRAM and NUM_BUCKETS columns in *_TAB_COL_STATISTICS:</p>

<code>select utcs.column_name, utcs.histogram, utcs.num_buckets
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'BRICKS'
and    utcs.column_name in ( 'BRICK_ID', 'WEIGHT' );
</code>

<p>To overcome these skew problems, you can gather histograms on the relevant columns. The optimizer will do this <strong>automatically</strong> based on column usage and data skew. The rules for when this happens are:</p>

<ol>
	<li>The column has value skew <strong>and</strong> statements use the column in range (<, >=, etc.), LIKE, or equality conditions</li>
	<li>The column has range skew <strong>and</strong> the column is used in range or LIKE conditions.</li>
	<li>The column has a small number of distinct values (with some repeated values) <strong>and</strong> the column is used in range (<, >=, etc.), LIKE, or equality conditions</li>
	<li>It may also capture histograms when using  incremental statistics are used, even if there is no skew. These are ignored by optimizer stats and are out-of-scope for this tutorial.</li>
</ol>

<p>The queries in the previous module set the necessary column usage details. So gathering stats will create histograms on the WEIGHT and BRICK_ID columns:</p>

<code>exec dbms_stats.gather_table_stats ( null, 'bricks', no_invalidate => false ) ;

select utcs.column_name, utcs.histogram, utcs.num_buckets
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'BRICKS'
and    utcs.column_name in ( 'BRICK_ID', 'WEIGHT' );
</code>

<p>With the histogram in place, the optimizer gets much better row estimates for the previous queries on skewed values:</p>

<code>select /*+ gather_plan_statistics */count (*) c
from bricks
where  weight = 1;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) c
from bricks
where  weight = 200;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) c
from bricks
where  brick_id between 0 and 100;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) c
from bricks
where  brick_id between 400 and 500;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>",26-FEB-20 09.28.58.885755000 AM,"CHRIS.SAXON@ORACLE.COM",15-JUL-20 03.32.01.655071000 PM,"CHRIS.SAXON@ORACLE.COM"
213640895348968690629296058119540578417,210639799513782729864465643940593969908,"Viewing Indexes",40,"<p>The *_INDEXES views return details of the indexes in a database. This query returns the indexes on the BRICKS table:</p>

<code>select * from user_indexes
where  table_name = 'BRICKS';
</code>

<p>It's possible to list up to 32 columns in an index. This query returns the indexes on BRICKS and a list of their columns:</p>

<code>select index_name, column_name, column_position 
from   user_ind_columns
where  table_name = 'BRICKS'
order  by index_name, column_position;
</code>",13-MAR-20 09.39.13.631388000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 09.19.28.393587000 AM,"CHRIS.SAXON@ORACLE.COM"
213643325217329570216370652671275346799,210639799513782729864465643940593969908,"Function-based indexes",80,"<p>If you apply a function to a column in the where clause, the database is unable to range scan regular indexes on that column. This query uppercases the colours, searching for rows with the value RED. Although there is an index on COLOUR, the optimizer uses a full-table scan:</p>

<code>select /*+ gather_plan_statistics */ shape, count ( distinct insert_datetime )
from   bricks
where  upper ( colour ) = 'RED'
group  by shape;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>This is because the values you're searching for no longer match the values in the index. To overcome this, you can create a function-based index. Do this by calling the function in the column list of the index:</p>

<code>create index brick_shape_upper_i 
  on bricks ( upper ( colour ) );

select /*+ gather_plan_statistics */shape, count ( distinct insert_datetime ) times#
from   bricks
where  upper ( colour ) = 'RED'
group  by shape;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>To ensure that the optimizer can use a function-based index, call the function in exactly the same way in the index and WHERE clause.</p>

<p><em>Oracle Database has optimizations which allow it to use functions-based indexes with similar functions to the where clause, but it's best not to rely on these!</em></p>",13-MAR-20 10.01.51.940170000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 01.48.00.204875000 PM,"CHRIS.SAXON@ORACLE.COM"
151386174394914023010617197153055698931,151382938526402044313447081869310725033,"Introduction",10,"<p>You can combine rows from two tables with a join. This tutorial explains the join methods using these two tables:</p>

<code>select * from toys;

select * from bricks;
</code>",26-JUL-18 08.46.31.910955000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUL-18 08.46.31.910996000 AM,"CHRIS.SAXON@ORACLE.COM"
151386174395010737076186367487032193011,151382938526402044313447081869310725033,"Cross Joins",20,"<p>A cross join returns every row from the first table matched to every row in the second. This will always return the Cartesian product the two table's rows. I.e. the number of rows in the first table times the number in the second.</p>

<p>Toys and bricks both store three rows. So cross joining them returns 3 * 3 = 9 rows.</p>

<p>To cross join tables using Oracle syntax, simply list the tables in the from clause:</p>

<code>select * 
from   toys, bricks;</code>

<p>Using ANSI style, type cross join between the tables you want to combine:</p>

<code>select * 
from   toys
cross join bricks;</code>

<p>It's rare you'll use cross joins in your code. If you need to use cross joins often it's a sign something is wrong with your data model! If you do need a cross join, ANSI syntax makes it clear this is your intent. And not that you forgot your join clause! </p>",26-JUL-18 08.49.01.978430000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.41.56.639879000 AM,"CHRIS.SAXON@ORACLE.COM"
151386174397977441037520667481761148915,151382938526402044313447081869310725033,"Try It!",35,"<p>Complete the query below to:</p>

<ul><li>Inner join toys and bricks</li>
<li>Where the toy_id is greater than the brick_id</li></ul>

<code>select * 
from   toys
join   bricks
/* TODO */</code>

<p>The query should return the following row:</p>
<pre><b>TOY_ID   TOY_NAME      TOY_COLOUR   BRICK_ID   BRICK_COLOUR   BRICK_SHAPE   </b>
       3 Baby Turtle   green                 2 blue           cube</pre>",26-JUL-18 08.57.09.040809000 AM,"CHRIS.SAXON@ORACLE.COM",26-JUL-18 09.33.54.999033000 AM,"CHRIS.SAXON@ORACLE.COM"
151386174398321984896110836796552409075,151382938526402044313447081869310725033,"Join Syntax: Oracle vs. ANSI",15,"<p>Oracle Database has two syntaxes for joining tables. The proprietary Oracle method. And the ANSI standard way.</p>

<p>Oracle syntax joins tables in the where clause. ANSI style has a separate join clause. This tutorial will show both methods.</p>

<p>We recommend you use ANSI syntax. This clearly separates the join and filter clauses. This can make your query easier to read, particularly with outer joins. But which approach you use is largely a matter of personal preference.</p>

<p>Whatever you do, choose one style and stick with it!</p>",26-JUL-18 09.02.42.674389000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.40.07.712124000 AM,"CHRIS.SAXON@ORACLE.COM"
211962976259104281722757656700221316037,210639799513641285543570732327153347316,"Creating Extended Statistics",110,"<p>Define extended stats using dbms_stats as shown:</p>

<code>select dbms_stats.create_extended_stats ( null, 'bricks', '(colour_rgb_value, shape)' )
from   dual;

exec dbms_stats.gather_table_stats ( null, 'bricks', method_opt => 'for columns (colour_rgb_value, shape)', no_invalidate => false ) ;
</code>

<p>The argument (colour_rgb_value, shape) tells the optimizer to gather stats on the combinations of these columns. You can create extended stats on any set of columns in a table. Or even on expressions such as upper ( shape ).</p>

<p>With extended stats in place, the plans should now show the correct row estimates:</p>

<code>select /*+ gather_plan_statistics */count (*) c
from   bricks
where  colour_rgb_value = 'FF0000'
and    shape = 'cylinder';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>As with histograms, the optimizer can detect column groups automatically based on column usage. To do this, you need to set the preference AUTO_STAT_EXTENSIONS to ON; by default this is OFF.</p>",26-FEB-20 09.31.17.418777000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.54.35.819143000 PM,"CHRIS.SAXON@ORACLE.COM"
152639219300680059017267199195298452677,152639219300562793212764580165351953605,"Select For Update",70,"<p>You're at risk of deadlock if a transaction runs two or more update statements on the same table. And two people running the transaction may update the same rows, but in a different order.</p>

<p>In these cases you should lock the rows before running any updates. You can do this with select for update. Like update itself, this locks rows matching the where clause. To use is, place the ""for update"" clause after your query.</p>

<p>So the following locks all the rows with the colour red:</p>

<code>select * from bricks
where  colour = 'red'
for    update;</code>

<p>No one else can update, delete or select for update these rows until you commit or rollback.</p>

<p>You can use this to avoid deadlock by running a select for update at the start of your transaction. For example, to avoid the deadlock described in the previous module, a run select for update at the start. This selects rows for the colours red and blue. Then run the updates:</p>

<code>select * from bricks
where  colour in ( 'red', 'blue' )
for    update;

update bricks
set    quantity = 1001
where  colour = 'red';

update bricks
set    quantity = 1722
where  colour = 'blue';</code>

<p>Changing the transactions to do this gives the following outcome:</p>

<table>
  <tr>
    <th>Transaction 1</th>
    <th>Transaction 2</th>
  </tr>
  <tr>
    <td>
<pre>select * from bricks
where  colour in ( 'red', 'blue' )
for    update;
</pre></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>
<pre>select * from bricks
where  colour in ( 'red', 'blue' )
for    update;
-- this session is now blocked, 
-- waiting for transaction 1</pre></td>
  </tr>
  <tr>
    <td>
<pre>update bricks
set    quantity = 1001
where  colour = 'red';

update bricks
set    quantity = 1722
where  colour = 'blue';
commit;
-- transaction 2 can now continue
</pre></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>
<pre>update bricks
set    unit_weight = 8
where  colour = 'blue';

update bricks
set    unit_weight = 13
where  colour = 'red';

commit;</pre></td>
  </tr>
</table>

<p>Deadlock can also happen when a transaction updates two or more different tables. If two separate transactions update two tables in a different order, you'll have deadlock.</p>

<p>This is harder to avoid. You need to ensure all transactions lock tables in the same order. You need define an order for acquiring locks. And ensure all developers on the application using the same method. This requires thorough documentation, code reviews, and testing to ensure deadlock is impossible.</p>",07-AUG-18 08.47.40.619881000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.23.31.749573000 PM,"CHRIS.SAXON@ORACLE.COM"
152787299263578848359874214993138048183,152769908351014758555746151630340122318,"Analytic View Aggregation and Filtering ",30,"<p>In the analytic view, measures are aggregated using hierarchies and the aggregation operator of the measure.  Filters in the SELECT statement (that is, the WHERE clause) filter the rows returned by the analytic view, but do not change aggregated values returned by the analytic views.</p>

<p>In other words, the analytic view aggregates first and the SELECT statement filters the results.   If queries over tables ""filter, then aggregate"" you can think of queries after analytic views as ""aggregate, then filter"".</p>

<p>Each approach as advantages.  Filtering before aggregation provides flexibility.  Aggregating before filtering provides consistency in that the aggregate values are always the same regardless of how the query is written.</p>

<p>The following query returns the same rows with the same values as the first query that selected from tables.</p>

<code>
SELECT time_hier.member_name,
  sales
FROM sales_av HIERARCHIES(time_hier)
WHERE time_hier.level_name = 'YEAR'
ORDER BY time_hier.hier_order;
</code>

<p>Let's see what happens when a filter for the first two quarters of the year are added to the query of the analytic view</p>

<code>
SELECT time_hier.member_name,
  sales
FROM sales_av HIERARCHIES(time_hier)
WHERE time_hier.level_name = 'YEAR'
AND TO_CHAR(month_end_date,'Q') IN (1,2)
ORDER BY time_hier.hier_order;
</code>

<p>No rows are returned.  This is because values for MONTH_END_DATE are NULL for year level member in the TIME_HIER hierarchy.</p>

<code>
SELECT * FROM time_hier WHERE level_name = 'YEAR';
</code>

<p>Keep in mind that in the hierarchy view there are rows for all hierarchy members at all rows.  When another attribute (MONTH_END_DATE in this case) is not determined by the member that attribute will be NULL.</p>

<p>The FILTER FACT keyword can be used in the USING clause to filter data prior to processing by the analytic view.  This could be thought of as an inner select to the analytic view query.</p>

<p>In the following query FILTER FACT is used to filter to QUARTER level hierarchy members that begin with 'Q1' and Q2'.  Note that FILTER FACT references elements in hierarchy objects rather than columns in a table.  The analytic view SQL generator will automatically access underlying objects as needed to resolve the query.</p>

<code>
SELECT time_hier.member_name, sales
FROM ANALYTIC VIEW (
  USING sales_av HIERARCHIES(time_hier)
  FILTER FACT (
    time_hier TO level_name = 'QUARTER' AND (quarter_name like 'Q1%' OR quarter_name like 'Q2%') 
  )
) 
WHERE time_hier.level_name = 'YEAR'
ORDER BY time_hier.hier_order;
</code>

<p>Note that the previous query returned the same rows and values for sales as the table query returning sales for the first half of each year.</p>

<p>FILTER FACT can be used to filter multiple hierarchies.  The next example returns YEAR and REGION level members where years aggregate only the first two quarters the countries Mexico and Canada.</p>

<code>
SELECT time_hier.member_name AS time,
  geography_hier.member_name AS geography,
  sales
FROM ANALYTIC VIEW (
  USING sales_av HIERARCHIES(time_hier, geography_hier)
  FILTER FACT (
    time_hier TO level_name = 'QUARTER' AND (quarter_name like 'Q1%' OR quarter_name like 'Q2%'),
    geography_hier TO level_name = 'COUNTRY' AND country_name in ('Mexico','Canada')
  )
) 
WHERE time_hier.level_name = 'YEAR'
  AND geography_hier.level_name = 'REGION'
ORDER BY time_hier.hier_order;
</code>",08-AUG-18 06.53.09.199980000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-AUG-18 08.08.04.611881000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
152639219300722371420953711216413168837,152639219300562793212764580165351953605,"Lost Updates",100,"<p>A lost update happens when two people change the same row and one overwrites another's changes. This is a common problem if you set values for all column not in the where clause. Regardless of whether the user changed the value. 

<p>For example, say currently there are 60 red cylinders in stock, with a unit weight of 13. Run this update to set these values:</p>

<code>update bricks
set    quantity = 60,
       unit_weight = 13
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>You have two people managing brick stock. One handles quantities, the other weights. They both load the current details for red cylinders to the edit form using this query:</p>

<code>select *
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>At this point they both see a quantity of 60 and weight of 13.</p>

<p>Then one person sets the weight to 8. And the other the quantity to 1,001. But they leave the value for the other attribute the same. They then both save their changes. The application updates both columns each time.</p>

<p>For each person, the update runs with the original value for the unchanged column. So the update to change the weight runs with these values:</p>

<code>update bricks
set    quantity = 60,  -- original quantity
       unit_weight = 8 -- new weight 
where  colour = 'red'
and    shape  = 'cylinder';

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>And the quantity change runs with these values:</p>

<code>update bricks
set    quantity = 1001, -- new quantity
       unit_weight = 13 -- original weight
where  colour = 'red'
and    shape  = 'cylinder';

select * 
from   bricks
where  colour = 'red'
and    shape  = 'cylinder';</code>

<p>As you can see, the second update sets the unit_weight from the new value of 8 back to the original, 13. So the update setting the weight to 8 is lost.</p>

<p>This problem can be hard to spot. Immediately after the person setting the weight saves their changes, everything looks fine. So they go on to do something else. Same for the stock level manager. So it may take some time for staff to realise there's a data error.</p>",07-AUG-18 08.48.44.606021000 AM,"CHRIS.SAXON@ORACLE.COM",07-SEP-18 03.26.07.983899000 PM,"CHRIS.SAXON@ORACLE.COM"
160467922254742178644966626287436862450,160204041387679825048250629187623908505,"Indexing XML Documents",7,"<p>Create a complete XML Index on the PURCHASEORDER table</p>

<code>
CREATE INDEX PURCHASEORDER_IDX
    on PURCHASEORDER (OBJECT_VALUE)
       indexType is xdb.xmlIndex
/
</code>

<p>A set of simple queries to demonstrate how indexing can optimize XQuery operations.</p>

<p>Q1. </p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder[Reference/text()=$REFERENCE] return $r' passing object_value, 'ACABRIO-1PDT' AS
       ""REFERENCE"") t

/ 
</code>

<p>Q2. </p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder[User/text()=$USER] return $r' passing object_value, 'ACABRIO-1' AS
       ""USER"") t

/  
</code>

<p>Q3. </p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder[LineItems/LineItem[Part/text()=$UPC]] return $r' passing object_value, '1' AS ""UPC"") t

/  
</code>

<p>Q4. </p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder[LineItems/LineItem[Part/text() = $UPC or Quantity > $QUANTITY]] return $r' passing object_value, '1' AS ""UPC"", 0
       AS
       ""QUANTITY"") t
WHERE ROWNUM <= 5
/  
</code>

<p><strong>Path-Subsetted XML Index</strong></p>
<code>
DROP INDEX purchaseorder_idx
/
CREATE INDEX PURCHASEORDER_IDX
    on PURCHASEORDER (OBJECT_VALUE)
       indextype is XDB.XMLINDEX
       parameters (
        'paths (
           include (
              /PurchaseOrder/Reference
              /PurchaseOrder/LineItems/LineItem/Part/* ))'
       )
/
</code>

<p><strong>Structured XML Index</strong></p>
<code>
DROP INDEX PURCHASEORDER_IDX
/
 BEGIN
    dbms_xmlindex.Dropparameter('PO_SXI_PARAMETERS');
END;

/  
begin 
  DBMS_XMLINDEX.registerParameter(
                 'PO_SXI_PARAMETERS',
                 'GROUP   PO_LINEITEM
                    xmlTable PO_INDEX_MASTER ''/PurchaseOrder''
                       COLUMNS
                         REFERENCE 	     varchar2(30) 	PATH ''Reference/text()'',
                         LINEITEM            xmlType   	PATH ''LineItems/LineItem''
                    VIRTUAL xmlTable PO_INDEX_LINEITEM ''/LineItem''
                       PASSING lineitem
                       COLUMNS
                         ITEMNO         number(38)  	 PATH ''@ItemNumber'',
                         UPC            varchar2(14)   PATH ''Part/text()'', 	
                         DESCRIPTION    varchar2(256)  PATH ''Part/@Description''
                 ');
end;                
/
CREATE INDEX PURCHASEORDER_IDX
          on PURCHASEORDER (OBJECT_VALUE)
             indextype is XDB.XMLINDEX
             parameters ('PARAM PO_SXI_PARAMETERS')
/
CREATE UNIQUE INDEX REFERENCE_IDX 
       on PO_INDEX_MASTER (REFERENCE)
/ 
CREATE INDEX UPC_IDX 
       on PO_INDEX_LINEITEM (UPC)
/
</code>",21-OCT-18 07.23.07.182593000 AM,"HARICHANDAN.ROY@ORACLE.COM",21-OCT-18 07.57.58.613981000 AM,"HARICHANDAN.ROY@ORACLE.COM"
192348356942810485596170364010361444647,192344988227908944268158345689617286517,"Querying facets using the Result Set Interface",20,"As mentioned earlier, the Result Set Interface is an alternative to normal SQL queries in Oracle Text.
<p>
What we commonly want as the result of a text query is:
<ul>
  <li>A ""page full"" of results - say the top 10 hits ordered by score
  <li>A count of the total number of hits that would result from the query
  <li>Summary information, such as how many results (from the full set, not just our page full) fall into certain categories, or facets.
</ul>
<p>
These types of queries are almost impossible to do as a single query in SQL without fetching every single hit to get the count and build the summary info. <p>
So instead we provide the XML-based Result Set Interface. The developer provides us with Result Set Descriptor (RSD) which specifies all the information they want returned. We process the RSD and a query string, and return a Result Set (RS). Both the RSD and the RS are written in XML.
<p>
The application can then process the XML in order to display the results to the end-user. They can do that using native XML capabilities of the application's language, or by using SQL-XML capabilities built into Oracle Database, which extract strings or relational tables (for repeated info) from the XML text.
<p>
For this demo, we're going to use a temporary table to store the RSD and RS. In most situations, you would use a suitable variables in your programming language to store them, but in LiveSQL it's easier to use a temporary table.
<p>
So let's create a suitable table:
<code>create table temp_rs (rsd clob, rs_text xmltype);
</code>

Rather than using xmltype, we could use a CLOB for the RSD as well, but then we'd need to cast it to xmltype using xmltype(clobname) each time we invoked an XML-specific operation on it.
<p>
Now let's construct our Result Set Descriptor:
<br>(Note: only do this once. If you need to change or rerun it, delete from temp_rs first)
<code>begin
  insert into temp_rs (rsd) values ('
&lt;ctx_result_set_descriptor&gt;
  &lt;count /&gt;
  &lt;hitlist start_hit_num=""1"" end_hit_num=""2"" order=""score desc""&gt;
    &lt;score /&gt;
    &lt;rowid /&gt;
    &lt;sdata name=""source"" /&gt;
  &lt;/hitlist&gt;
  &lt;group sdata=""source""&gt; &lt;count exact=""true""/&gt; &lt;/group&gt;
  &lt;group sdata=""recordtype""&gt; &lt;count exact=""true""/&gt; &lt;/group&gt;
  &lt;group sdata=""region""&gt; &lt;count exact=""true""/&gt; &lt;/group&gt;
&lt;/ctx_result_set_descriptor&gt;
');
end;
/
</code>
Some explanation is needed there.
<ul>
   <li>&lt;count&gt;  tells us we want the total hit count returned.
   <li>&lt;hitlist&gt;  specifies how many results we want, how they should be sorted, and what information (rowids and/or SDATA columns) should be returned for each result.
   <li>&lt;group&gt;  specified a facet and what should be returned for that facet.
   <br> The count exact=""true"" part tells us that the facet count should be exact - otherwise we get an estimated value which does not account for any deleted records, nor for any SDATA clauses used in the query. That's important for the next part.
</ul>
<p>
For this tutorial, we're only considering single-valued facets. For example REGION might contain NORTH, SOUTH, EAST and WEST. In 19c and later it's possible to use range buckets for numeric or date values (for example price less than $10, price $10 - $20, etc) but we're not going to cover that here.
<p>
Before we go any further, let's list our RSD.
<code>select rsd from temp_rs;
</code>
(In SQL*Plus you would want to SET LONG 50000 and do select xmltype(rsd)..."" in order to format the XML nicely).
<p>
Make sure there's one and only one row returned. If you have more than one, future parts of the tutorial will fail. If that's the case, drop the table, and go back to the start of this section (section 2).

<p>
Now having specified our Result Set Descriptor, we can run a query using it. We do this using the PL/SQL procedure CTX_QUER.RESULT_SET. We need to specify the index to search, the search term or query string, the Resul Set Descritor as an input, and a variable for the Result Set output. Our query is for the word ""RECORD"" which should return six of the seven rows in the table.

<p>
So this is the actual query procedure:
<code>declare
   ResSetDesc clob;
   ResultSet  clob;
begin

   -- fetch the RSD from our temporary table (assumes only we are using the temp table and it was cleared before running the module)

   select rsd into ResSetDesc from temp_rs;

   -- and run our query using the RSD
   
   ctx_query.result_set(
      index_name => 'docsindex',
      query      => 'record',
      result_set_descriptor => ResSetDesc,
      result_set => ResultSet );

   -- ResSetOut now contains XML result set: write it to temporary table

   update temp_rs set rs_text = xmltype(ResultSet);

end;
/
</code>

After running that, you'll only see ""Statement processed"". But the Result Set output has been written to our temporary table. Let's check it. We'll use getClobVal to make sure it's formatted nicely:

<code>select xmltype.getclobval(rs_text) from temp_rs;
</code>

Take a few minutes to look through that, and compare it to what we requested in our Result Set Descriptor earlier. Remember we only asked for the top 2 results in our hitlist section.
<p>
In particular note the group counts towards the end. We can see that from our entire result set (not just the top 2 hits) there were 4 records with SOURCE=External and 2 records with SOURCE=Internal.
<br>These are our <b>facet counts</b>.",22-AUG-19 12.48.22.624963000 PM,"ROGER.FORD@ORACLE.COM",29-AUG-19 11.19.56.679072000 AM,"ROGER.FORD@ORACLE.COM"
192353363371972144936992520355676029250,192344988227908944268158345689617286517,"Processing the Result Set XML",30,"<p>
This section shows how you can use the SQL-XML capabilities of Oracle Database to process that block of XML back into SQL-like constructs such as VARCHAR2 values and relational tables.

<p>
First of all, we'll use the XMLTable table function to get the hitlist rows.
<p>
Let's take a look at the hitlist section of the Result Set output:

<pre>  &lt;hitlist&gt;
    &lt;hit&gt;
      &lt;score&gt;6&lt;/score&gt;
      &lt;rowid&gt;ABLKUlAAkAAAAQcAAB&lt;/rowid&gt;
      &lt;sdata name=""SOURCE""&gt;Internal&lt;/sdata&gt;
    &lt;/hit&gt;
    &lt;hit&gt;
      &lt;score&gt;3&lt;/score&gt;
      &lt;rowid&gt;ABLKUlAAkAAAAQcAAA&lt;/rowid&gt;
      &lt;sdata name=""SOURCE""&gt;Internal&lt;/sdata&gt;
    &lt;/hit&gt;
  &lt;/hitlist&gt;
</pre>

<p>
We want to fetch that info back as a relational table with two rows in it. We can do that using the XMLTable table function and passing it our Result Set from the temporary table.
<p>
XMLTable requires us to specify the XML path to each data item, and how that should appear as column in the table function (its name and datatype). Then since we've got ROWIDs from the original table DOCS, we might as well join our table function with the DOCS table so we can fetch back the original text, or any metadata columns in the original table which are not returned by SDATA values:

<code>select score, d.text, rso.source from 
  xmltable ( '/ctx_result_set/hitlist/hit' 
    PASSING ( select rs_text from temp_rs )
    COLUMNS
      xrowid varchar2(18)  PATH 'rowid',
      score number         PATH 'score',
      source varchar2(128) PATH 'sdata[@name=""SOURCE""]' ) rso,
  docs d
  where d.rowid = rso.xrowid
  order by score desc;
</code>

<p>
There you go - our XML hitlist has now become a SQL table.
<p>
<b>Getting the total hit count</b>
<p>
Getting the total hit count is easier - there's only a single value so we don't need XMLTable, we can use ExtractValue, passing it the path of the hitcount:

<code>select extractvalue( (select rs_text from temp_rs ), '/ctx_result_set/count' ) as ""Total Hits"" from dual;
</code>

And finally we can get the facets and facet counts. We have three facet types, SOURCE, RECORDTYPE and REGION. Each of those have a section in the XML. For SOURCE, it looks like:

<pre>  &lt;groups sdata=""SOURCE""&gt;
    &lt;group value=""External""&gt;
      &lt;count&gt;4&lt;/count&gt;
    &lt;/group&gt;
    &lt;group value=""Internal""&gt;
      &lt;count&gt;2&lt;/count&gt;
    &lt;/group&gt;
  &lt;/groups&gt;
</pre>

<p>
It's another repeated field, so we need another XMLTable. And we need the same for each facet type:

<code>select * from
  xmltable( '/ctx_result_set/groups[@sdata=""SOURCE""]/group' 
    PASSING ( ( select rs_text from temp_rs ) )
    COLUMNS 
      source      VARCHAR2(30) PATH '@value',
      facet_count NUMBER       PATH 'count/text()') as rso;

select * from
  xmltable( '/ctx_result_set/groups[@sdata=""RECORDTYPE""]/group' 
    PASSING ( ( select rs_text from temp_rs ) )
    COLUMNS 
      recordtype      VARCHAR2(30) PATH '@value',
      facet_count NUMBER       PATH 'count/text()') as rso;

select * from
  xmltable( '/ctx_result_set/groups[@sdata=""REGION""]/group' 
    PASSING ( ( select rs_text from temp_rs ) )
    COLUMNS 
      region      VARCHAR2(30) PATH '@value',
      facet_count NUMBER       PATH 'count/text()') as rso;
</code>

And that's it.
<p>

We've shown how we can use the XML-based Result Set Interface to fetch back a hitlist and summary information in a single operation, and how to manipulate that XML back into relational form.",22-AUG-19 01.46.14.721426000 PM,"ROGER.FORD@ORACLE.COM",29-AUG-19 12.16.58.599857000 PM,"ROGER.FORD@ORACLE.COM"
192600456659718804360937593806346947759,192497261674369450442611783127247019514,"Finding the top matching document",70,"This example uses SODA_COLLECTION_T methods find(), key(), and get_one() to find the unique document whose key is ""key1"".
<code>
REM Finding the top matching document

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    key         VARCHAR2(255);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    SELECT ID into key from ""myCollectionName"" where rownum < 2;

    -- Find a document using a key
    document := collection.find().key(key).get_one;

    IF document IS NOT NULL THEN
        DBMS_OUTPUT.put_line('Document components:');
        DBMS_OUTPUT.put_line('Key: ' || document.get_key);
        DBMS_OUTPUT.put_line('Content: ' || JSON_QUERY(document.get_blob, '$' PRETTY));
        DBMS_OUTPUT.put_line('Creation timestamp: ' || document.get_created_on);
        DBMS_OUTPUT.put_line('Last modified timestamp: ' || document.get_last_modified);
        DBMS_OUTPUT.put_line('Version: ' || document.get_version);
    END IF;
END;
</code>",25-AUG-19 12.07.39.933770000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.17.51.452961000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192600456659740565025690657131491658927,192497261674369450442611783127247019514,"Specifying Document Version to search",90,"This example uses SODA_COLLECTION_T method version() to specify the document version. This is useful for implementing optimistic locking, when used with the terminal methods for write operations.

You typically use version() together with method key(), which specifies the document. You can also use version() with methods keyLike() and filter(), provided they identify at most one document.

<code>
REM Specifying Document Version to search

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    k           varchar2(255);
    v           varchar2(255);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    select ID, VERSION into k, v from ""myCollectionName"" where rownum < 2;

    -- Find a particular version of the document that has a given key
    document := collection.find().key(k).version(v).get_one;

    IF document IS NOT NULL THEN
        DBMS_OUTPUT.put_line('Document components:');
        DBMS_OUTPUT.put_line('Key: ' || document.get_key);
        DBMS_OUTPUT.put_line('Content: ' ||
                             JSON_QUERY(document.get_blob, '$' PRETTY));
        DBMS_OUTPUT.put_line('Creation timestamp: ' || document.get_created_on);
        DBMS_OUTPUT.put_line('Last modified timestamp: ' ||
                             document.get_last_modified);
        DBMS_OUTPUT.put_line('Version: ' || document.get_version);
    END IF;
END;
</code>",25-AUG-19 12.12.20.787568000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.39.26.394942000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
213950881288288645516805909643759623038,210639064323027505476639480028596947637,"Improving Stats for Mostly Clustered Rows",50,"<p>The TABLE_CACHED_BLOCKS preference tells the optimizer to keep track of how recently it saw a value. By default this is one:</p>

<code>select dbms_stats.get_prefs ( 'table_cached_blocks', null, 'bricks' ) 
from   dual;</code>

<p>This means when consecutive index entries point to different blocks, the clustering factor increases. Upping the preference slightly, e.g. to 16 enables the database to spot these mostly clustered values. You do this by setting the table preference:</p>

<code>begin 
  dbms_stats.set_table_prefs ( 
    null, 'bricks', 'table_cached_blocks', 16 
  );
end;
/
</code>

<p>You need to regather stats for this change to take effect; doing so brings the clustering factor for BRICK_INSERT_DATE_I down to the same value as the primary key index:</p>

<code>select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';

exec dbms_stats.gather_table_stats ( null, 'bricks' ) ;

select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';
</code>

<p>This makes the optimizer much more likely to use the index when searching by date. In fact, the optimizer will now use this index when fetching more than 5% of the rows in the table:</p>

<code>select /*+ gather_plan_statistics cached_blocks */ count ( distinct junk ), count (*)
from   bricks
where  insert_date >= date'2020-02-01'
and    insert_date < date'2020-03-01';

select * 
from   table(dbms_xplan.display_cursor ( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST' ) );
</code>",16-MAR-20 09.53.43.354291000 AM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.11.18.335759000 PM,"CHRIS.SAXON@ORACLE.COM"
186922817553604517163874718298493574764,186921076608050787168935825902406386005,"Adding stopwords",10,"First we'll create an index using the provided EMPTY_STOPLIST
<code>create table foo(text varchar2(80));
insert into foo values ('the cat sat on the mat');
create index fooindex on foo(text) indextype is ctxsys.context
parameters('stoplist CTXSYS.EMPTY_STOPLIST sync(on commit)');
</code>
Check the indexed words:
<code>select distinct token_text from dr$fooindex$i;
</code>
Create a new stoplist. Note that LiveSQL doesn't properly clear stoplists when you clear a session, so you may need to drop an existing stoplist first if you've run the tutorial before
<code>exec ctx_ddl.drop_stoplist('new_stoplist')
</code>
Now create the new one
<code>
exec ctx_ddl.create_stoplist('new_stoplist', 'BASIC_STOPLIST')
exec ctx_ddl.add_stopword('new_stoplist', 'the')
exec ctx_ddl.add_stopword('new_stoplist', 'on')
exec ctx_ddl.add_stopword('new_stoplist', 'a')
</code>
Alter the index to use the new stoplist
<code>
alter index fooindex rebuild parameters('REPLACE METADATA stoplist new_stoplist');
</code>
That won't affect previously-indexed words in the index
Add a new doc which uses the word 'a' just defined as a stopword
<code>insert into foo values ('two dogs sat on a cat');
commit;
</code>
Now look again at the words indexed - no 'A' in there!
<code>select distinct token_text from dr$fooindex$i;
</code>
Check that stopwords are working properly. If they are, then 'the' in a query will match any word:
<code>REM 'cat the' in the query will match 'cat sat' in the text. See the documentation if that doesn't make sense!
select * from foo where contains (text, 'cat the on the mat') > 0;
</code>
Next, we might want to clean up the wordlist to remove our stopwords. This isn't completely necessary (and, arguably, is not fully supported) but it will free up space in the index. Be careful doing this if you use the SUBSTRING_INDEX or WILDCARD_INDEX options.
<code>delete from dr$fooindex$i where token_text in ('THE', 'ON', 'A');
select distinct token_text from dr$fooindex$i;
</code>
Finally, we'll show a way of adding individual stopwords to an index without replacing the whole stoplist
<code>alter index fooindex rebuild parameters('add stopword ONE');
REM the word ONE is now a stopword, so when we add this string ONE will not be indexed:
insert into foo values ('the cat sat on one mat');
commit;
select distinct token_text from dr$fooindex$i;
</code>",01-JUL-19 02.08.09.496549000 PM,"ROGER.FORD@ORACLE.COM",01-JUL-19 02.58.56.708131000 PM,"ROGER.FORD@ORACLE.COM"
192580726292366282787898580480417230999,192497261674369450442611783127247019514,"QBE: Filter documents that contain a text",120,"The following example uses $contains QBE operator to filter employee documents that contains the text 'President' in the title field and the text 'Salesforce' in company field.

<code>
REM Test text search of the documents
REM Get employees whose salary > 240000 whose title has the word 'President' and company has the word 'Salesforce'

DECLARE
    collection SODA_Collection_T;
    operation  SODA_Operation_T;
    document   SODA_Document_T;
    cur        SODA_Cursor_T;
    qbe        VARCHAR2(400);
    pos        NUMBER  := 0;
BEGIN
    collection := dbms_soda.open_collection('Employees');

    qbe := '{""$query"" :{
                ""salary"" : { ""$gt"" : 240000 },
                ""title"" : { ""$contains"" : ""President"" },
                ""company"" : { ""$contains"" : ""Salesforce"" }
              },
             ""$orderby"" : [
                  {""path"" : ""salary"", ""order"" : ""asc""}
                ]
            }';
    operation := collection.find().filter(qbe);
    document := operation.get_One;
    oow_soda.show(document);
END;
/

</code>",24-AUG-19 07.12.21.437713000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.10.10.815634000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
211969877226114539446090810768377962513,210639799513641285543570732327153347316,"Value Skew",50,"<p>There is an uneven distribution of the weight values in the bricks table:</p>

<code>select weight, count (*)
from   bricks
group  by weight
order  by weight;
</code> 

<p>There are 300 rows in the bricks table. And 27 unique values. So these optimizer estimates ( 300 / 27 ) ~ 11 rows for both of these queries. But the first returns 150 rows and the second just one!</p>

<code>select /*+ gather_plan_statistics */count (*) from bricks
where  weight = 1;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) from bricks
where  weight = 200;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>",26-FEB-20 09.31.48.712296000 AM,"CHRIS.SAXON@ORACLE.COM",24-APR-20 04.14.59.136634000 PM,"CHRIS.SAXON@ORACLE.COM"
212060087831833891252488738383585785542,210639799513641285543570732327153347316,"Try it!",80,"<p>Re-enable histograms for bricks by setting the size to auto in using table preferences:</p>
<code>begin 
  dbms_stats.set_table_prefs ( 
    null, 'bricks', 
    'method_opt', 'for TODO' 
  ); 
  dbms_stats.gather_table_stats ( null, 'bricks' ) ;
end;
/

select utcs.column_name, utcs.histogram
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'BRICKS';
</code> 

<p>The query should show a HYBRID histogram for BRICK_ID and FREQUENCY histograms for WEIGHT and COLOUR_RGB_VALUE</p>",27-FEB-20 10.13.27.071253000 AM,"CHRIS.SAXON@ORACLE.COM",28-FEB-20 09.41.22.031328000 AM,"CHRIS.SAXON@ORACLE.COM"
216253673361037885368064048639003759442,210639064323069817880325992049711663797,"Try It Challenge!",100,"<p>There are no indexes on the join columns for this query, so the optimizer uses a hash join:</p>

<code>select /*+ gather_plan_statistics */*
from   card_deck d1
join   card_deck d2
on     d1.damaged = d2.damaged
where  d1.notes = 'SQL is awesome!';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>Complete the index definition, so the optimizer chooses nested loops instead:</p>

<code>drop  index card_join_i;

create index card_join_i
  on card_deck ( /* TODO */ );

select /*+ gather_plan_statistics */ *
from   card_deck d1
join   card_deck d2
on     d1.damaged = d2.damaged
where  d1.notes = 'SQL is awesome!';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Experiment with different indexes. Which enables the query to do the least consistent gets/buffers? Does creating two indexes make any difference?</p>",07-APR-20 09.47.59.968160000 AM,"CHRIS.SAXON@ORACLE.COM",26-MAY-20 03.16.45.663651000 PM,"CHRIS.SAXON@ORACLE.COM"
214165560201492845273520158240102053958,210639064323069817880325992049711663797,"Merge Joins",20,"<p>Also known as a sort merge join, the algorithm for this is:</p>
<ul>
<li>Sort the rows in the first data set</li>
<li>Sort the rows in the second data set</li>
<li>For each row in the first data set, find a starting row in the second data set</li>
<li>Read the second data set until you find a row with a value greater than the current row in the first data set</li>
<li>Read the next row in the first data set and repeat</li>
</ul>

<p>This query shows a merge join in action:</p>

<code>select /*+ gather_plan_statistics */count(*)
from   card_deck d1
join   card_deck d2
on     d1.suit < d2.suit
and    d1.val < d2.val;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Note that the database reads the second table once (starts = 1 at line 7). But runs the BUFFER SORT once for each row from the first table (starts = 52 for line 6).</p>

<p>The optimizer may choose a merge join when:</p>
<ul>
<li>The join uses range comparisons (<, >=, between, etc.)</li>
<li>The join has an equality (=) and one of the data sets is already ordered, enabling the database to avoid a sort</li>
</ul>
<p>Sorting is slow, so it's rare for the optimizer to use a merge join.</p> 

<p>Indexes are ordered data structures. So if there is an index on the join columns for one table, the optimizer can use this to avoid a sort. Oracle Database will always sort the second data set, even if it there are indexes on the second table's join columns.</p>",18-MAR-20 09.40.26.237367000 AM,"CHRIS.SAXON@ORACLE.COM",26-MAY-20 03.04.13.299763000 PM,"CHRIS.SAXON@ORACLE.COM"
201040610291115875688933990285911398929,92046253613429971725264843174797014870,"Comparing Table and Analytic View Queries",120,"<p>As with any other calculated measure, applications merely need to include the calculated measure in the select list of a query selecting from an analytic view.  The analytic view will generate SQL needed to execute the query.</p>

<p>In the case of a query selecting from tables, it is up to the application to generate SQL necessary to resolve calculations.  A query selecting time series calculations such as sales year ago and sales percent change year ago will require:</p>

<ol>
<li>Expanding time filters to include the previous year(s).</li>
<li>Partitioned outer joins to account for sparse data.</li>
<li>Aggregating data.</li>
<li>Calculating sales year-to-date and sales year-to-date percent change year ago.</li>
<li>Finally filtering for the desired time periods (that is, eliminating years added to filters for prior years.</li>
</ol>

<p>Select year, department, region, sales, sales_year_ago, sales_chg_year_ago and sales_pct_chg_year_ago from tables.</p>

<code>
WITH
  fact_dense AS
      --
      -- Step 1:  Get the aggregated value of sales for year 2015 and 2014.
      --
      (
        SELECT
          b.year_name,
          b.year_end_date,
          a.department_name,
          a.region_name,
          a.sales
        FROM
          (
            SELECT
              d1.year_name,
              d1.year_end_date,
              d2.department_name,
              d3.region_name,
              SUM(f.sales) sales
            FROM
              av.time_dim d1,
              av.product_dim d2,
              av.geography_dim d3,
              av.sales_fact f
            WHERE
              d1.month_id            = f.month_id
            AND d2.category_id       = f.category_id
            AND d3.state_province_id = f.state_province_id
            --
            -- Filter to years 2015 and 2014.  Find the prior year by subtracting
            -- 12 months from the last day of the 2015.
            --
            AND year_end_date       IN (add_months('31-DEC-15', - 12),'31-DEC-15')
            GROUP BY
              d1.year_name,
              d1.year_end_date,
              d2.department_name,
              d3.region_name
          )
          a
          --
          -- Join to the dimension table using a right outer join to cover the
          -- case where there are where no sales for a particular department and
          -- region in the previous year (else the query is at risk for returning
          -- wrong results).
          --
          PARTITION BY (a.department_name, a.region_name)
        RIGHT OUTER JOIN
          (
            SELECT DISTINCT
              year_name,
              year_end_date
            FROM
              av.time_dim
            WHERE
            --
            -- Filter to years 2015 and 2014.  Find the prior year by subtracting
            -- 12 months from the last day of the 2015.
            --
              year_end_date IN (add_months('31-DEC-15', - 12), '31-DEC-15')
          )
          b
        ON
          (
            a.year_name = b.year_name
          )
      )
--
-- Step 2:  Calculate sales change year ago and sales percentage change year ago.
--
SELECT
  c.region_name,
  c.department_name,
  c.year_name,
  c.sales,
  -- Last year.
  d.sales sales_year_ago,
  -- This year minus last year.
  (c.sales        - d.sales) sales_chg_year_ago,
  -- Pecent change this year vs last year.
  ROUND(((c.sales - d.sales) / d.sales), 3) AS sales_pct_chg_year_ago
FROM
  fact_dense c
  --
  -- Cover the sparse data case with an outer join.
  --
LEFT OUTER JOIN fact_dense d
ON
  (
    add_months(c.year_end_date, - 12)    = d.year_end_date
  AND c.department_name                  = d.department_name
  AND c.region_name                      = d.region_name
  )
WHERE
  -- Filter out CY2014.
  c.year_name      = 'CY2015'
ORDER BY
  c.region_name,
  c.department_name,
  c.year_name;
</code>

<p>Create an analytic view with sales, sales_year_ago, sales_chg_year_ago and sales_pct_chg_year_ago measures</p>

<code>
CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY  ( 
   time_attr_dim KEY month_id REFERENCES month_id HIERARCHIES (time_hier DEFAULT),
   product_attr_dim  KEY category_id REFERENCES category_id HIERARCHIES (product_hier DEFAULT),
   geography_attr_dim KEY state_province_id REFERENCES state_province_id HIERARCHIES (geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_chg_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year)),
  sales_pct_chg_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
  )
DEFAULT MEASURE SALES;
</code>

<p>Select year, department, region, sales, sales_year_ago, sales_chg_year_ago and sales_pct_chg_year_ago from tables.</p>

<code>
SELECT
  geography_hier.member_name AS geography,
  product_hier.member_name   AS product,
  time_hier.year_name        AS year,
  sales,
  sales_year_ago,
  sales_chg_year_ago,
  ROUND(sales_pct_chg_year_ago,3) AS sales_pct_chg_year_ago
FROM
  sales_av HIERARCHIES (
    time_hier,
    product_hier,
    geography_hier)
WHERE
  time_hier.level_name = 'YEAR'
  AND time_hier.year_name = 'CY2015'  
  AND product_hier.level_name = 'DEPARTMENT'
  AND geography_hier.level_name = 'REGION'
ORDER BY
  geography_hier.member_name,
  product_hier.member_name,
  geography_hier.member_name;
</code>",13-NOV-19 06.12.07.992565000 PM,"WILLIAM.ENDRESS@ORACLE.COM",13-NOV-19 07.11.37.761751000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
213955002612900056347503983544398132549,210639064323027505476639480028596947637,"Fetching Partly Clustered Rows",40,"<p>The INSERT_DATE index has an ""average"" clustering factor. This means the database is unlikely to use it, even when fetching relatively few rows. For example, this query fetches less than 5% of the rows, but still opts for a full table scan:</p>

<code>select /*+ gather_plan_statistics */ count ( distinct junk ), count (*)
from  bricks
where  insert_date >= date'2020-02-01'
and    insert_date < date'2020-02-21';

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>But zooming out months, it seems the values are almost as well clustered as BRICK_ID:</p>

<code>with rws as ( 
  select trunc ( insert_date, 'mm' ) dt, 
         ceil ( 
           dense_rank () over (
             order by dbms_rowid.rowid_block_number ( rowid )
           ) / 10 
         ) rid
  from   bricks
)
  select * from rws
  pivot (
    count (*) for rid in (
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
    )
  )
  order by dt;</code>

<p>So why is the clustering factor for its index so high?</p>

<p>This is because there are several rows and the start and end of each day that hop back-and-forth between two blocks:</p>

<code>with rws as (
  select brick_id,
         to_char ( insert_date, 'DD MON HH24:MI' ) dt, 
         dbms_rowid.rowid_block_number ( rowid ) current_block,
         lag ( dbms_rowid.rowid_block_number ( rowid ) ) over (
           order by insert_date
         ) prev_block
  from   bricks
  where  insert_date >= date '2020-01-01'
  and    insert_date <  date '2020-02-01'
)
  select * from rws
  where  current_block <> prev_block
  order  by dt;</code>

<p>This gives the optimizer the impression that the rows are much poorly clustered than they really are. This is a common problem for always increasing values such as insert dates which are ""mostly"" clustered. Luckily there's a trick to help in these scenarios:</p>

<p>Set the TABLE_CACHED_BLOCKS preference!</p>",16-MAR-20 09.52.06.294394000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.03.54.583476000 PM,"CHRIS.SAXON@ORACLE.COM"
216143811565068377670998872582347449616,210640540520278905448644863263597314383,"Try It!",35,"<p>Replace /* TODO */ to enable query rewrite for the BRICK_SHAPES_MV you created in module 3:</p>

<code>alter materialized view brick_shapes_mv 
  /* TODO */;

select /*+ gather_plan_statistics */shape, count(*) 
from   bricks
group  by shape;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>When successful, you should see the plan use the MV with a rewrite operation.</p>",06-APR-20 08.41.54.568543000 AM,"CHRIS.SAXON@ORACLE.COM",20-MAY-20 02.47.31.439669000 PM,"CHRIS.SAXON@ORACLE.COM"
216141597552982707668010296089164557426,210640540520278905448644863263597314383,"Try It!",75,"<p>Replace /* TODO */ to make brick_shapes_mv fast refreshable:</p>

<code>exec dbms_mview.refresh ( 'brick_shapes_mv', 'C' );

alter materialized view brick_shapes_mv 
  /* TODO */;

select refresh_mode, refresh_method
from   user_mviews
where  mview_name = 'BRICK_SHAPES_MV';</code>

<p>The REFRESH_MODE should be COMMIT and the REFRESH_METHOD be FAST.</p>",06-APR-20 09.20.06.827799000 AM,"CHRIS.SAXON@ORACLE.COM",06-APR-20 04.28.34.703085000 PM,"CHRIS.SAXON@ORACLE.COM"
214087830146641573186496461625897912727,210640540520278905448644863263597314383,"Fast Refresh Limitations",80,"<p>Sadly there are many restrictions on the types of query you can use in a fast refresh on commit MV. For example, you can't use COUNT ( DISTINCT ):</p>

<code>create materialized view bricks_not_fast_mv 
as
  select colour, count(*), count ( distinct shape )
  from   bricks
  group  by colour;

alter materialized view bricks_not_fast_mv 
  refresh fast on commit;
</code>

<p>This is because if you insert a row with the shape PRISM, there's no way to know if this is already included in the distinct count.</p>

<p>There are many other limitations for fast refreshable MVs. The documentation contains a <a href=""https://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-505C24CF-5D56-4820-88AA-2221410950E7"">complete list of these restrictions</a>. You can see which refresh options are allowed on an MV <a href=""https://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-651B08EB-4D32-4A93-A260-A965C40AE136"">using dbms_mview.explain_mview</a>.</p>

<p>A FAST REFRESH ON COMMIT MV also adds some overhead to every transaction using the tables in the MV. In write heavy applications this can put too much stress on the database.</p>

<p>It would be nice if queries could apply changes in an MV log to a stale MV <em>while running a query.</em> Oracle Database 12.2 makes this possible with <strong>Real-Time Materialized Views</strong>.</p>",17-MAR-20 04.25.44.607592000 PM,"CHRIS.SAXON@ORACLE.COM",20-MAY-20 02.48.19.616731000 PM,"CHRIS.SAXON@ORACLE.COM"
192602763308992519071361464306986827171,192497261674369450442611783127247019514,"Finding Multiple Documents with Specified Document Keys",80,"This example defines key list myKeys, with (string) keys ""key1"", ""key2"", and ""key3"". It then finds the documents that have those keys, and it prints their components. SODA_COLLECTION_T method keys()  specifies the documents with the given keys.

<code>

REM  Finding Multiple Documents with Specified Document Keys

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    cur         SODA_CURSOR_T;
    status      BOOLEAN;
    myKeys      SODA_KEY_LIST_T;
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    -- Set the keys list
    SELECT ID bulk collect into myKeys from ""myCollectionName"";

    -- Find documents using keys
    cur := collection.find().keys(myKeys).get_cursor;

    -- Loop through the cursor
    WHILE cur.has_next
    LOOP
      document := cur.next;
      IF document IS NOT NULL THEN
          DBMS_OUTPUT.put_line('Document components:');
          DBMS_OUTPUT.put_line('Key: ' || document.get_key);
          DBMS_OUTPUT.put_line('Content: ' || json_query(document.get_blob, '$' PRETTY));
          DBMS_OUTPUT.put_line('Creation timestamp: ' || document.get_created_on);
          DBMS_OUTPUT.put_line('Last modified timestamp: ' || document.get_last_modified);
          DBMS_OUTPUT.put_line('Version: ' || document.get_version);
      END IF;
    END LOOP;
    status := cur.close;
END;

</code>",25-AUG-19 12.08.40.355025000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.20.46.733787000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192602763309501476841419223189538127267,192497261674369450442611783127247019514,"Finding Documents with a Filter Specification",82,"SODA_COLLECTION_T method filter() provides a powerful way to filter JSON documents in a collection. Its parameter is a JSON query-by-example (QBE, also called a filter specification).

The syntax of filter specifications is an expressive pattern-matching language for JSON documents. This example uses only a very simple QBE, just to indicate how you make use of one in SODA for PL/SQL.

This example does the following:
<ol>
<li>Creates a filter specification that looks for all JSON documents whose name field has value ""Alexander"".</li>
<li>Uses the filter specification to find the matching documents.</li>
<li>Prints the components of each document.</li>
</ol>
<code>
REM Finding Documents with a Filter Specification

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    cur         SODA_CURSOR_T;
    status      BOOLEAN;
    qbe         VARCHAR2(128);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    -- Define the filter specification (QBE)
    qbe := '{""name"" : ""Alexander""}';

    -- Open a cursor for the filtered documents
    cur := collection.find().filter(qbe).get_cursor;

    -- Loop through the cursor
    WHILE cur.has_next
    LOOP
      document := cur.next;
      IF document IS NOT NULL THEN
          DBMS_OUTPUT.put_line('Document components:');
          DBMS_OUTPUT.put_line('Key: ' || document.get_key);
          DBMS_OUTPUT.put_line('Content: ' || JSON_QUERY(document.get_blob, '$' PRETTY));
          DBMS_OUTPUT.put_line('Creation timestamp: ' || document.get_created_on);
          DBMS_OUTPUT.put_line('Last modified timestamp: ' || document.get_last_modified);
          DBMS_OUTPUT.put_line('Version: ' || document.get_version);
      END IF;
    END LOOP;
    status := cur.close;
END;
</code>",25-AUG-19 12.10.36.546109000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.24.33.810746000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
213553181838656143107650897883614038711,210639064323015416218443333736849885877,"Reducing I/O: Creating New Data Structures",100,"<p>A full table scan reads all the rows in a table. If the table stores millions of rows, this leads to lots of wasted work if your query only reads a few rows. You can create or change data structures to enable the database to read rows more efficiently.</p>

<p>The most common way to do this is by creating indexes! These can help the database find a few rows quickly.</p>

<p>These two queries show the power an index can have. The BRICKS and BRICKS_INDEXED tables store the same rows. But the second table has an index on COLOUR. Because the queries only select COLOUR, the database is able to answer the second query by reading the index instead of the table:</p>

<code>select /*+ gather_plan_statistics */
       colour, count(*) 
from   bricks
group  by colour
order  by colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

select /*+ gather_plan_statistics */
       colour, count(*) 
from   bricks_indexed
group  by colour
order  by colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>The second query does around a quarter of the number of gets as the first, a considerable saving! This is because the index is a much smaller data structure than the table. You can verify this by querying *_SEGMENTS, which reports the size of each data structure in bytes:</p>

<code>select segment_name, segment_type, bytes
from   user_segments
where  segment_name in ( 
  'BRICKS', 'BRICKS_INDEXED', 'BRICK_COLOUR_I' 
);</code>",12-MAR-20 01.57.11.805316000 PM,"CHRIS.SAXON@ORACLE.COM",03-JUN-20 07.33.51.230394000 AM,"CHRIS.SAXON@ORACLE.COM"
152850831184112255786842567510089049498,117675390209390608523249818520886328918,"Try It!",55,"<p>Complete the following query to find the rows that include uppercase B in toy_name:</p>

<code>select toy_name
from   toys
where  /* TODO */</code>

<p>This should return the following rows:</p>

<pre><b>TOY_NAME       </b>
Mr Bunnykins   
Baby Turtle  </pre>",09-AUG-18 09.15.27.884444000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.22.35.599979000 AM,"CHRIS.SAXON@ORACLE.COM"
214087830146038319202508761667719530903,210640540520278905448644863263597314383,"Using Materialized Views Automatically: Query Rewrite",30,"<p>Querying an MV can be orders of magnitude faster than running the query you use to build it. But changing existing queries to use an MV could be a huge task.</p>

<p>Fortunately the optimizer has a trick up its sleeve. If you run the query used in an MV, the optimizer can detect this. And change your query to use the MV instead of the base table!</p>

<p>To do this, you need to enable query rewrite for the MV:</p>

<code>alter materialized view brick_colours_mv
  enable query rewrite;</code>

<p>Now this is enabled, the optimizer can redirect the queries matching the one in an MV to the saved output, instead of the bricks table. You can check this is happening by the ""MAT_VIEW REWRITE ACCESS FULL"" operation in the execution plan:</p>

<code>select /*+ gather_plan_statistics */colour, count(*) c
from   bricks
group  by colour;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>A rewrite is also possible when the query includes a where clause - provided the filter columns are in the MV. So this query to get the count of red rows uses the MV:</p>

<code>select /*+ gather_plan_statistics */count(*) row#
from   bricks
where  colour = 'red'
group  by colour;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>The optimizer can also use query rewrite for other queries it can derive the result of using an MV. For example, the MV gets the count by colour. So you can get the total number of rows in the table by adding up these totals.</p>

<p>This means the optimizer can use BRICK_COLOURS_MV to get a count of the total number rows in the table:</p>

<code>select /*+ gather_plan_statistics */count(*) 
from   bricks;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>",17-MAR-20 04.18.48.033867000 PM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.36.56.625967000 PM,"CHRIS.SAXON@ORACLE.COM"
190604157232622512345265250678698840445,188704173216096418598025228629326881042,"Tutorial flow",15,"<p>Each time you log into LiveSQL you are provided a fresh schema, and all objects are dropped when you log out. This tutorial assumes you are in a fresh LiveSQL session with no existing objects.. If you need to start over just log out of LiveSQL and log back in and you'll have a clean slate.",05-AUG-19 08.29.28.989831000 PM,"DAVID.LAPP@ORACLE.COM",05-AUG-19 08.30.21.548095000 PM,"DAVID.LAPP@ORACLE.COM"
149111946068195680646483935550977638763,149075222712493902096666428584818910723,"Read Phenomena",30,"<p>The SQL standard defines three read phenomena; issues that can occur when many people read and write to the same rows. These are:</p>

<ul><li>Dirty reads</li>
<li>Non-repeatable (or fuzzy) reads</li>
<li>Phantom reads</li></ul>

<p>These cause the following issues:</p>

<h3>Dirty Reads</h3>

<p>A dirty read is when you see uncommitted rows in another transaction. There is no guarantee the other transaction will commit. So when these are possible, you could return data that was never saved to the database!</p>

<p>Dirty reads are impossible in Oracle Database. You can only view uncommitted rows in your own transaction.</p>

<h3>Non-repeatable (Fuzzy) Reads</h3>

<p>A non-repeatable read is when selecting the same row twice returns different results. This happens when someone else updates the row between queries. For example, after the first query in transaction 1, transaction 2 changes the shape of the row. So the second query sees the new value:</p>

<table align=""center"" cellpadding=""1"" cellspacing=""1"">
	<thead>
		<tr>
			<th scope=""col"">Transaction 1</th>
			<th scope=""col"">Transaction 2</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<pre>
insert into bricks ( 
  colour, shape 
) values ( 
  &#39;red&#39;, &#39;cube&#39; 
);
commit;

select shape from bricks 
where  colour = &#39;red&#39;;

<strong>SHAPE</strong>
cube
</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<td>&nbsp;</td>
			<td>
			<pre>
update bricks 
set    shape = &#39;pyramid&#39;; 
commit;</pre>
			</td>
		</tr>
		<tr>
			<td>
			<pre>
select shape from bricks 
where  colour = &#39;red&#39;;

<strong>SHAPE</strong>
pyramid
</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
	</tbody>
</table>

<p>The SQL standard also allows for fuzzy reads in a single query. This can cause problems when swapping values for two rows. If the other session updates these part way through your query, you can double count values. In this example, the bricks table has 20,000 rows, with a 50:50 split between red and blue.</p>

<p>The first transaction counts how many rows there are of each colour. Halfway through the second query it has counted 5,000 rows of each. At this point, another transaction updates the colour of all the rows to red.</p>

<p>In databases without statement-level consistency, the query could see these values immediately. This leads it to count the remaining 10,000 rows as red. So the query returns 5,000 blue rows and 15,000 red. A total that never existed in the table!</p>

<table align=""center"" cellpadding=""1"" cellspacing=""1"">
	<thead>
		<tr>
			<th scope=""col"">Transaction 1</th>
			<th scope=""col"">Transaction 2</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<pre>
select colour, count(*)
from   bricks
group by colour;

<strong>COLOUR COUNT(*)</strong>
blue     10,000
red      10,000

select colour, count(*)
from   bricks
group by colour;
</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<td>... query still running ...</td>
			<td>
			<pre>update bricks 
set    colour = &#39;red&#39;; 
commit;</pre>
			</td>
		</tr>
		<tr>
			<td>
			<pre>
<strong>COLOUR COUNT(*)
</strong>blue &nbsp; &nbsp;  5,000
red &nbsp; &nbsp; &nbsp;15,000
</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
	</tbody>
</table>

<p>Luckily Oracle Database always has statement-level consistency. So fuzzy reads are impossible in one query.</p>

<h3>Phantom Reads</h3>

<p>A phantom read is a special case of fuzzy reads. This happens when another session inserts or deletes rows that match the where clause of your query. So repeated queries can return different rows:</p>

<table align=""center"" cellpadding=""1"" cellspacing=""1"">
	<thead>
		<tr>
			<th scope=""col"">Transaction 1</th>
			<th scope=""col"">Transaction 2</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<pre>
insert into bricks ( 
  colour, shape 
) values ( 
  &#39;red&#39;, &#39;cube&#39; 
);
commit;

select shape from bricks 
where  colour = &#39;red&#39;;

<strong>SHAPE</strong>
cube</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<td>&nbsp;</td>
			<td>
			<pre>
insert into bricks ( colour, shape )&nbsp;
values ( &#39;red&#39;, &#39;pyramid&#39; );
commit;</pre>
			</td>
		</tr>
		<tr>
			<td>
			<pre>
select shape from bricks 
where  colour = &#39;red&#39;;

<strong>SHAPE</strong>
cube
pyramid</pre>
			</td>
			<td>&nbsp;</td>
		</tr>
	</tbody>
</table>

<p>As with fuzzy reads, these are impossible in a single statement in Oracle Database.</p>",04-JUL-18 04.32.33.520090000 PM,"CHRIS.SAXON@ORACLE.COM",19-FEB-21 05.48.08.655932000 PM,"CHRIS.SAXON@ORACLE.COM"
149117521792652039106032969324558076122,149075222712493902096666428584818910723,"Read Only",70,"<p>Besides read committed and serializable, Oracle Database offers another transaction mode: read-only.</p>

<p>For queries this works in the same way as serializable. You have transaction-level consistency. Queries return data as it existed at the time the transaction began.</p>

<p>It has the added restriction that you can only run selects. Any attempts to change rows will throw an exception. For example, the first update below throws an ORA-01456:</p>

<code>set transaction read only;

select * from toys;

update toys
set    price = price + 1;

declare 
  pragma autonomous_transaction;
begin
  update toys set price = 99.00;
  commit;
end;
/

select * from toys;
commit;
select * from toys;</code>

<p>This mode can be useful in reporting environments. A set of reports may need to query the same tables many times. But to ensure the results are correct, you must ignore any changes made by other users while the reports run. Users must also only be able to read data. You need to stop all non-select DML.</p>

<p>Setting the transaction mode to read only addresses both of these needs.</p>

<p>As a complement to read-only, you can set a transaction to read-write. You do this with the following statement:</p>

<code>set transaction read write;</code>

<p>This works in the same way as the read committed isolation level. It's the default mode in Oracle Database.</p>",04-JUL-18 04.33.30.004601000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 08.24.22.642736000 AM,"CHRIS.SAXON@ORACLE.COM"
160209225971702507619918179227551845243,160204041387679825048250629187623908505,"Querying XML Content",2,"<p>There are many ways to query XML content in Oracle XML DB. We will see some basic examples here.</p>

<p>Q1. Getting the number of XML documents. There are many ways, following is one of them:</p>
<code>
SELECT Count(*)
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder return $r' passing object_value) t;  
</code>

<p>Q2. Retrieving the content of an XML document using pseudocolumn OBJECT_VALUE:</p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder t
WHERE  rownum = 1;  
</code>

<p>Q3. Accessing fragments or nodes of an XML document:</p>
<code> 
SELECT Xmlquery('/PurchaseOrder/Reference' passing object_value returning
       content)
FROM   purchaseorder
WHERE  ROWNUM <= 5
/  
</code>

<p>Q4. Accessing text node value:</p>
<code>
SELECT xmlcast(xmlquery('$p/PurchaseOrder/Reference/text()' passing object_value AS ""p"" returning content) AS varchar2(30))
FROM   purchaseorder
WHERE  ROWNUM <= 5
/ 
</code>

<p>Q5. Searching an xml document:</p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder t
WHERE  xmlexists('/PurchaseOrder[Reference/text()=$REFERENCE]' passing object_value, 'ACABRIO-1PDT' AS ""REFERENCE"" ); 
</code>

<p>Q6. Using XMLTABLE:</p>
<p>You can use XMLTable to perform SQL operations on a set of nodes that match an XQuery expression. XMLTable breaks up an XML fragment contained in an XMLType instance, inserts the collection-element data into a new, virtual table, which you can then query using SQLâ€‰â€”â€‰in a join expression, for example. In particular, converting an XML fragment into a virtual table makes it easier to process the result of evaluating an XMLQuery expression that returns multiple nodes.</p>
<code>
SELECT   reference,
         Count(*)
FROM     purchaseorder,
         xmltable('/PurchaseOrder' passing object_value columns reference varchar2(32) path 'Reference', lineitem xmltype path 'LineItems/LineItem'),
         xmltable('LineItem' passing lineitem)
WHERE    xmlexists('$p/PurchaseOrder' passing object_value AS ""p"")
         AND ROWNUM <= 5
GROUP BY reference
ORDER BY reference;
</code>

<p>XMLTABLE and XQuery (single predicate):</p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE('for $r in /PurchaseOrder[Reference/text()=$REFERENCE] return $r' passing object_value, 'ACABRIO-1PDT' AS
       ""REFERENCE"") t;  
</code>

<p>XMLTABLE and XQuery (multiple predicates):</p>
<code>
SELECT t.object_value.getclobval()
FROM   purchaseorder p,
       XMLTABLE(
'for $r in /PurchaseOrder[CostCenter=$CC or Requestor=$REQUESTOR or count(LineItems/LineItem) > $QUANTITY]/Reference return $r'
passing object_value, 'A1' AS ""CC"", 'A. Cabrio 10' AS ""REQUESTOR"", 0 AS
""QUANTITY"") t
WHERE ROWNUM <= 5
/ 
</code>

<p>Q7. Constructing a new summary document from the documents that match the specified predicates:</p>
<code>
SELECT t.object_value.getclobval() 
FROM   Purchaseorder p, 
       XMLTable(
         '<Summary>
          {
           for $r in /PurchaseOrder
           return $r/Reference/text()
          }
          </Summary>' 
          passing object_value
       ) t
WHERE  ROWNUM <= 5
/
</code>

<p>Q8. Using XMLSerialize to format the XMLType and serialize it as a CLOB. Allows result to be viewed in products that do not support XMLType. XMLSerialize allows control over the layout of the serialized XML:</p>

<code>
SELECT XMLSERIALIZE(CONTENT COLUMN_VALUE AS CLOB INDENT SIZE=2) 
FROM   Purchaseorder p, 
       XMLTable(
         '<Summary>
          {
           for $r in /PurchaseOrder
           return $r/Reference
          }
          </Summary>' 
          passing object_value
       )
WHERE  ROWNUM <= 5
/
</code>

<p>Q9. Using XMLTable to create an in-line relational view from the documents that match the XQuery expression:</p>

<code>
SELECT * 
FROM   Purchaseorder p, 
       XMLTable( 
        'for $r in /PurchaseOrder
          for $l in $r/LineItems/LineItem
          return 
            <Result ItemNumber=""{fn:data($l/@ItemNumber)}""> 
                { 
                  $r/Reference, 
                  $r/Requestor, 
                  $r/User, 
                  $r/CostCenter, 
                  $l/Quantity 
                } 
                <Description>{fn:data($l/Part/@Description)}</Description> 
                <UnitPrice>{fn:data($l/Part/@UnitPrice)}</UnitPrice> 
                <PartNumber>{$l/Part/text()}</PartNumber> 
             </Result>' 
             passing object_value
             columns 
             SEQUENCE      for ordinality, 
             ITEM_NUMBER       NUMBER(3) path '@ItemNumber', 
             REFERENCE     VARCHAR2( 30) path 'Reference', 
             REQUESTOR     VARCHAR2(128) path 'Requestor', 
             USERID        VARCHAR2( 10) path 'User', 
             COSTCENTER    VARCHAR2(  4) path 'CostCenter', 
             DESCRIPTION   VARCHAR2(256) path 'Description',  
             PARTNO        VARCHAR2( 14) path 'PartNumber',  
             QUANTITY       NUMBER(12,4) path 'Quantity',  
             UNITPRICE      NUMBER(14,2) path 'UnitPrice'
       )
WHERE  ROWNUM <= 5 
/
</code>

<p>Q10. Joining relational and XML tables using XQuery:</p>
<code>
SELECT requestor,
       department_name
FROM   hr.employees e,
       hr.departments d,
       purchaseorder p,
       XMLTABLE( 'for $r in /PurchaseOrder where $r/Reference=$REFERENCE or $r/User=$EMAIL return $r' passing object_value, 'ACABRIO-1PDT' AS
""REFERENCE"", e.email AS ""EMAIL"" COLUMNS requestor path 'Requestor/text()' )
WHERE  e.department_id = d.department_id
       AND  ROWNUM <= 5
/  
</code>",18-OCT-18 08.07.42.590286000 PM,"HARICHANDAN.ROY@ORACLE.COM",11-SEP-19 08.41.49.365872000 PM,"HARICHANDAN.ROY@ORACLE.COM"
216488589389025593537673349886353478016,210638425667855918502469137992560342521,"Tuning Delete as DDL: Removing Most of the Rows Performance Comparison",45,"<p>This compares the relative performance of these methods and a regular DELETE to remove 90% of the rows from a table:</p>

<code>drop table bricks_keep purge;

declare
  num_rows pls_integer := 100000;
begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  delete bricks 
  where  brick_id <= num_rows * 0.9; 
   
  timing_pkg.calc_runtime ( 'Delete-where' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  execute immediate ' 
  create table bricks_keep ( 
    brick_id primary key, 
    colour, shape, weight 
  ) as 
    select * from bricks 
    where  brick_id > ' || ( num_rows * 0.9 ); 

  execute immediate 'truncate table bricks'; 

  execute immediate 'insert into bricks
    select * from bricks_keep';
   
  timing_pkg.calc_runtime ( 'Delete-ctas-swap-rows' ); 
  execute immediate 'drop table bricks_keep purge';
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  execute immediate ' 
  create table bricks_keep ( 
    brick_id primary key, 
    colour, shape, weight 
  ) as 
    select * from bricks 
    where  brick_id > ' || ( num_rows * 0.9 ); 

  execute immediate 'drop table bricks purge';
  execute immediate 'rename bricks_keep to bricks'; 
   
  timing_pkg.calc_runtime ( 'Delete-ctas-swap-table' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  execute immediate ' 
  alter table bricks 
    move  including rows 
    where brick_id > ' || ( num_rows * 0.9 ); 
   
  timing_pkg.calc_runtime ( 'Delete-move' ); 

end;
/</code>

<p>All are notably faster than a regular DELETE. This data set is small, so there's little performance difference between them.</p>

<p>These methods are most effective when removing a large fraction (50%+) of the rows from a table. But can be slower than a plain delete when removing a large absolute number of rows (millions or more) that are only a small fraction of a table. This kind of operation is common when archiving data. Such as removing the oldest month of data.</p>

<p>When doing this, partitioning is a great way to remove large numbers of rows fast.</p>",09-APR-20 03.57.10.683342000 PM,"CHRIS.SAXON@ORACLE.COM",09-APR-20 03.57.42.083511000 PM,"CHRIS.SAXON@ORACLE.COM"
216491594665575265995726418057701800338,210638425667855918502469137992560342521,"Insert Performance Comparison",22,"<p>This compares methods for copying 100,000 rows from one table to another</p>
<ol>
<li>A cursor-for loop with 100,000 iterations, each adding one row</li>
<li>One INSERT-SELECT statement, inserting all 100,000 rows</li>
<li>One BULK COLLECTING the rows into an array, followed by a FORALL INSERT, adding all 100k rows</li>
</ol>

<code>declare
  num_rows pls_integer := 100000;
  type bricks_rec is record (
    brick_id integer, colour varchar2(10),
    shape    varchar2(10), weight integer
  );
  type bricks_array is table of bricks_rec
    index by pls_integer;
    
  brick_values bricks_array;
begin 
  delete bricks;
  commit;
  timing_pkg.set_start_time; 
  for rw in (
    with rws as ( 
      select level x from dual 
      connect by level <= num_rows 
    ) 
      select rownum id, 
             case mod ( rownum, 3 )  
               when 0 then 'red' 
               when 1 then 'blue' 
               when 2 then 'green' 
             end, 
             case mod ( rownum, 2 )  
               when 0 then 'cube' 
               when 1 then 'cylinder' 
             end, 
             round ( dbms_random.value ( 2, 10 ) ) 
      from   rws
  ) loop 
    insert into bricks  
      values (  
        rw.id,  
        case mod ( rw.id, 3 )  
          when 0 then 'red' 
          when 1 then 'blue' 
          when 2 then 'green' 
        end, 
        case mod ( rw.id, 2 )  
          when 0 then 'cube' 
          when 1 then 'cylinder' 
        end, 
        round ( dbms_random.value ( 2, 10 ) ) 
       ); 
  end loop; 
  timing_pkg.calc_runtime ( 'Insert-loop' ); 
   
  rollback; 
   
  timing_pkg.set_start_time; 
  insert into bricks  
    with rws as ( 
      select level x from dual 
      connect by level <= num_rows 
    ) 
      select rownum, 
             case mod ( rownum, 3 )  
               when 0 then 'red' 
               when 1 then 'blue' 
               when 2 then 'green' 
             end, 
             case mod ( rownum, 2 )  
               when 0 then 'cube' 
               when 1 then 'cylinder' 
             end, 
             round ( dbms_random.value ( 2, 10 ) ) 
      from   rws; 
  timing_pkg.calc_runtime ( 'Insert-select' ); 
  
  rollback;
 
  timing_pkg.set_start_time; 
  
  with rws as ( 
    select level x from dual 
    connect by level <= num_rows 
  ) 
    select rownum, 
           case mod ( rownum, 3 )  
             when 0 then 'red' 
             when 1 then 'blue' 
             when 2 then 'green' 
           end, 
           case mod ( rownum, 2 )  
             when 0 then 'cube' 
             when 1 then 'cylinder' 
           end, 
           round ( dbms_random.value ( 2, 10 ) ) 
    bulk collect
    into   brick_values
    from   rws; 

  forall rws in 1 .. brick_values.count
    insert into bricks  
    values brick_values ( rws );

  timing_pkg.calc_runtime ( 'Insert-forall' ); 

end; 
/</code>

<p>The single statement can be up to 10x faster than the looped approach; a huge saving! Copying the data using BULK COLLECT then INSERT INTO is also much faster than the loop, though typically marginally slower than plain SQL.</p>",09-APR-20 04.27.59.742669000 PM,"CHRIS.SAXON@ORACLE.COM",27-MAY-20 02.28.43.367386000 PM,"CHRIS.SAXON@ORACLE.COM"
217013143900011826941970175597692504393,210638425667855918502469137992560342521,"Update Performance Comparison",25,"<p>This compares running:</p>
<ul>
<li>100k UPDATE statements changing one row</li>
<li>One UPDATE changing 100k rows</li>
<li>One FORALL UPDATE changing 100k rows</li>
</ul>

<code>declare
  num_rows pls_integer := 100000;
  rws dbms_sql.number_table;

begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 

  for i in 1 .. num_rows loop 
   
    update bricks 
    set    colour = 'pink',  
           shape = 'cone' 
    where  brick_id = i; 
     
  end loop; 

  timing_pkg.calc_runtime ( 'Update-loop' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  update bricks 
  set    colour = 'pink',  
         shape = 'cone'; 
     
  timing_pkg.calc_runtime ( 'Update-all' ); 
  ins_rows ( num_rows );
  for i in 1 .. num_rows loop
    rws (i) := i;
  end loop;
  timing_pkg.set_start_time; 
  
  forall i in 1 .. rws.count 
    update bricks 
    set    colour = 'pink',  
           shape = 'cone'
    where  brick_id = rws (i); 
     
  timing_pkg.calc_runtime ( 'Update-forall' ); 
end;
/</code>

<p>In this example the single update statement is typically 3-4x faster than looping through 100k rows and updating each one. Bulk processing gives similar performance to plain SQL, though is usually slightly slower.</p>",14-APR-20 04.14.03.367898000 PM,"CHRIS.SAXON@ORACLE.COM",28-MAY-20 04.52.35.118023000 PM,"CHRIS.SAXON@ORACLE.COM"
214291664218983644047129388579367321592,210638425667855918502469137992560342521,"Getting the Execution Plan for DML",10,"<p>As with selects, you can get execution plans for inserts, updates, and deletes. The process is the same as for queries. Run the statement then call DBMS_XPlan as shown:</p>

<code>insert /*+ gather_plan_statistics */ into bricks 
  values ( 0, 'red', 'cylinder', 1 ); 
  
select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

insert /*+ gather_plan_statistics */ into bricks 
  select level, 'red', 'cylinder', 1 
  from   dual
  connect by level <= 100;
  
select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

update /*+ gather_plan_statistics */ bricks 
set    shape = 'cube'
where  brick_id = 1;
  
select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

delete /*+ gather_plan_statistics */ bricks 
where  brick_id <= 10;
  
select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>For INSERT-AS-SELECT, UPDATE, and DELETE review the plan to see if you make changes to help the optimizer find the rows faster.</p>

<p>But even if you have an optimal execution plan for a statement, rewriting DML processes can often give order-of-magnitude performance gains.</p>",19-MAR-20 04.41.21.737257000 PM,"CHRIS.SAXON@ORACLE.COM",21-MAY-20 03.17.43.639186000 PM,"CHRIS.SAXON@ORACLE.COM"
211138708409707056401290711903337378965,210639064319457547531317480075689609909,"Try It Challenge!",50,"<p>Can you rewrite the scalar subquery in ""Starts Statistic"" module so it only accesses BRICKS once?

Replace the /* TODO */ sections to do this:</p>

<code>select /*+ gather_plan_statistics */c.rgb_hex_value,
       /* TODO */ 
from   colours c
/* TODO */;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>

<p><em>HINT: Can you change the query to use a join instead of a subquery? If you are struggling with this, take these free SQL courses:
<ul>
<li><a href=""https://devgym.oracle.com/pls/apex/dg/class/databases-for-developers-foundations.html"">SQL for beginners Databases for Developers: Foundations</a></li>
<li><a href=""https://devgym.oracle.com/pls/apex/dg/class/databases-for-developers-next-level.html"">Intermediate SQL Databases for Developers: Next Level</a></li>
</ul>
</em></p>",18-FEB-20 10.19.22.699616000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.25.18.929012000 AM,"CHRIS.SAXON@ORACLE.COM"
160204041388376166320348655592254665881,160204041387679825048250629187623908505,"Setup",1,"<p>Let's first reset the tutorial by running following statement:<p>

<code>
DECLARE
    CURSOR gettable IS
      SELECT table_name
      FROM   user_xml_tables
      WHERE  table_name IN ( 'PURCHASEORDER' );
BEGIN
    FOR t IN gettable() LOOP
        EXECUTE IMMEDIATE 'DROP TABLE ""'|| t.table_name|| '"" PURGE';
    END LOOP;
END;

/  
</code>

<p> Now, please run the following statements to create an xmltype table and insert some xml documents.</p>

<code>
DROP TABLE purchaseorder;

CREATE TABLE purchaseorder OF xmltype;

BEGIN
    FOR i IN 1..100 LOOP
        INSERT INTO purchaseorder
        VALUES      ('<PurchaseOrder><Reference>ACABRIO-'
                     ||i
                     ||'PDT</Reference><Actions><Action><User>ACABRIO-'
                     ||i
                     ||
        '</User></Action></Actions><Rejection/><Requestor>A. Cabrio '
                     || i
                     ||'</Requestor><User>ACABRIO-'
                     ||i
                     ||'</User><CostCenter>A'
                     ||i
                     ||'</CostCenter><ShippingInstructions><name>A. Cabrio '
                     ||i
                     ||'</name><Address><street>'
                     ||i
                     ||' Sporting Green Centre, Science Club, building '
                     ||i
                     ||', Magdalen</street><city>SFO-'
                     ||i
                     ||
'</city><state>CA</state><zipCode>99236</zipCode><country>United States of America</country></Address><telephone>269-'
||i
||
'-4036</telephone></ShippingInstructions><SpecialInstructions>Priority Overnight</SpecialInstructions><LineItems><LineItem ItemNumber=""1""><Part Description=""Face to Face: First Seven Years"" UnitPrice=""19.95"">'
||i
||'</Part><Quantity>'
||i
||
'</Quantity></LineItem><LineItem ItemNumber=""2""><Part Description=""Runaway"" UnitPrice=""27.95"">'
||i
||'</Part><Quantity>'
||i
||
'</Quantity></LineItem><LineItem ItemNumber=""3""><Part Description=""Founding Fathers: Men Who Shaped"" UnitPrice=""19.95"">'
||i
||'</Part><Quantity>'
||i
||'</Quantity></LineItem></LineItems></PurchaseOrder>');
END LOOP;

COMMIT;
END;

/  
</code>",18-OCT-18 06.53.31.617374000 PM,"HARICHANDAN.ROY@ORACLE.COM",19-OCT-18 10.13.57.256334000 PM,"HARICHANDAN.ROY@ORACLE.COM"
214297899743445656640501439995283425078,210638425667855918502469137992560342521,"Tuning Updates Using DDL",30,"<p>An UPDATE with a where clause is much faster than looping through rows and running an UPDATE for each row. But the single update can still take a long time to finish. And it locks all the affected rows from the time the UPDATE starts until the transaction finishes.</p>

<p>This could take unacceptably long on large tables. If you're changing most or all of the rows in a table, you can make the process faster doing an ""update"" in DDL with this process:</p>

<ul>
<li>Create a holding table using CREATE-TABLE-AS-SELECT (CTAS). Provide the new column values in this select statement</li>
<li>Switch the real and temporary table over</li>
</ul>

<p><em>It's rare you need to use this trick. Reserve this for one-off migrations or other cases where speed is of the essence.</em></p>

<p>For example, this creates BRICKS_UPDATE, ""updating"" the colour and shape of every row to yellow prism by selecting these values instead of the columns in the table:</p>

<code>exec ins_rows ( 100000 );

create table bricks_update ( 
  brick_id primary key, 
  colour, shape, weight 
) as  
  select brick_id,  
         cast ( 'yellow'as varchar2(10) ) colour, 
         cast ( 'prism' as varchar2(10) ) shape,  
         weight 
  from   bricks;</code>

<p>Note that when selecting literal values, you need to CAST them to ensure the new table has the same data types as the original. </p>

<p>To complete the ""update"", you need to:</p>
<ul>
<li>Copy any indexes, grants, triggers, etc. from the old table to the new</li>
<li>Switch the tables over</li>
</ul>
<p>Adding the dependent objects to the new table - particularly indexes - can take a while. Be sure to measure the total time to complete the process!</p>

<p>This completes the process by dropping the original table, then renaming the new one:</p>

<code>drop table bricks purge; 
 
rename bricks_update to bricks; </code>

<p><em>This <strong>destroys</strong> your rollback position!</em> If there's an error in the process, you'll have to restore from backup. It's safer to rename the original table (e.g. BRICKS_OLD), then rename the new table.</p>

<p>Whichever method you use to swap the tables, there will be a brief period where there is no BRICKS table! You <strong>need to take the application(s) offline to complete this process safely</strong>.</p>",19-MAR-20 04.45.04.029742000 PM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.41.21.745548000 PM,"CHRIS.SAXON@ORACLE.COM"
214295509800794438808182053888074615995,210638425667855918502469137992560342521,"Tuning Delete as DDL: Removing Most of the Rows",40,"<p>As with UPDATE, there are DDL tricks you can use to ""delete"" data. These are:</p>
<ul>
<li>CTAS to save rows, then switch the rows</li>
<li>CTAS to save rows, then switch the tables</li>
<li>A filtered table move</li>
</ul>

<p>With create-table-as-select, write a query fetching the rows you want to keep. Then wrap this in a create-table statement:</p>

<code>exec ins_rows ( 10000 );

select count (*) from bricks;

create table bricks_keep ( 
    brick_id primary key, 
    colour, shape, weight 
  ) as 
    select * from bricks 
    where  brick_id > 9500;

select count (*) from bricks_keep;</code>

<p>From here, you can either switch just the rows over by truncating the original table and re-inserting the rows:</p>

<code>truncate table bricks; 

insert into bricks
  select * from bricks_keep;</code>

<p>Or you could switch the tables themselves over by dropping or renaming the original table. Then renaming the new table to the old.</p>

<p>As with UPDATE, if you switch the tables over, you also need to copy any index, constraints, etc. from the old table to the new. Ensure you test the runtime of the complete process!</p>

<p>Both of these methods need an outage complete safely. Making them unusable except in extreme cases. Fortunately, Oracle Database 12.2 added an online DDL method to remove lots of data: a filtered table move.</p>

<p>This added the ""including rows"" clause to ALTER TABLE MOVE. You place a where clause after this to state the rows you want to keep. The database discards the non-matching rows in the process.</p>

<code>exec ins_rows ( 10000 );

select count(*) from bricks;

alter table bricks 
  move  including rows 
  where brick_id > 9500;

select count(*) from bricks;</code>",19-MAR-20 04.45.48.240703000 PM,"CHRIS.SAXON@ORACLE.COM",15-APR-20 03.23.10.151749000 PM,"CHRIS.SAXON@ORACLE.COM"
217012992320940569346584356996145408517,210638425667855918502469137992560342521,"Tuning Delete Using DDL: Removing Many Rows Performance Comparison",60,"<p>This compares removing 20,000 rows with DELETE to truncating the first two partitions:</p>

<code>declare
  num_rows pls_integer := 100000;
begin
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
  
  delete bricks
  where brick_id <= 20000;
   
  timing_pkg.calc_runtime ( 'Delete-where' ); 
  timing_pkg.set_start_time; 
   
  execute immediate ' 
  alter table bricks
    truncate partition p0, p1
  '; 
   
  timing_pkg.calc_runtime ( 'Truncate-partition' ); 
end; 
/</code>

<p>Note: 20,000 rows is tiny in modern database terms. You may notice little or no performance difference between the DELETE and truncating the partitions in the above process.</p>",14-APR-20 04.25.55.778489000 PM,"CHRIS.SAXON@ORACLE.COM",14-APR-20 04.26.26.646507000 PM,"CHRIS.SAXON@ORACLE.COM"
210634392498033431909841210931036220678,210639064319457547531317480075689609909,"Reading a Complex Query Plan",30,"<p>Run this query and get its plan: </p>
<code>select c.colour, count(*)
from   colours c 
join   (
  select colour, shape from bricks
  union all
  select colour, toy_name from cuddly_toys
  union all
  select colour, pen_type from pens
) t
on     t.colour = c.colour
group  by c.colour;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID));

/*
---------------------------------------------                              
| Id  | Operation             | Name        |                              
---------------------------------------------                              
|   0 | SELECT STATEMENT      |             |                              
|   1 |  HASH GROUP BY        |             |                              
|   2 |   HASH JOIN           |             |                              
|   3 |    TABLE ACCESS FULL  | COLOURS     |                              
|   4 |    VIEW               |             |                              
|   5 |     UNION-ALL         |             |                              
|   6 |      TABLE ACCESS FULL| BRICKS      |                              
|   7 |      TABLE ACCESS FULL| CUDDLY_TOYS |                              
|   8 |      TABLE ACCESS FULL| PENS        |                              
---------------------------------------------
*/</code>

<p>This uses the same four tables as the previous query. But accesses them in a different way. Here the order of operations is:</p>

<ol>
	<li>
	<p>Travel down the plan to the first leaf. This is the <strong>TABLE ACCESS FULL</strong> of the <strong>COLOURS</strong> table in step 3.</p>
	</li>
	<li>
	<p>Pass the rows from this table up to the first leaf&rsquo;s parent, the <strong>HASH JOIN</strong> in step 2.</p>
	</li>
	<li>
	<p>Find the next leaf, which is the <strong>TABLE ACCESS FULL</strong> of the <strong>BRICKS</strong> table in step 6.</p>
	</li>
	<li>
	<p>Its parent is a multichild operation&mdash;<strong>UNION-ALL</strong>&mdash;so the database will next execute steps 7 and 8. (There is an optimization&mdash;concurrent execution of <strong>UNION-ALL</strong>&mdash;that means that the database can run all of these table scans at the same time in parallel queries.)</p>
	</li>
	<li>
	<p>Pass the rows from the tables at steps 6, 7, and 8 up to the <strong>UNION-ALL</strong> in step 5. This step combines the rows into one dataset.</p>
	</li>
	<li>
	<p>Work back up the tree to the <strong>HASH JOIN</strong> in step 2.</p>
	</li>
	<li>
	<p>Join the rows from steps 3 and 5, passing the surviving rows up to the <strong>HASH GROUP BY</strong> in step 1.</p>
	</li>
	<li>
	<p>Finally, return the data to the client.</p>
	</li>
</ol>
<p>Note that UNION-ALL can combine many tables in one operation. This is different to joins, which always combine exactly two data sources. Joins and unions are separate operations - the optimizer can't swap one for another.</p>",13-FEB-20 03.37.38.255623000 PM,"CHRIS.SAXON@ORACLE.COM",10-SEP-20 03.02.53.253590000 PM,"CHRIS.SAXON@ORACLE.COM"
213955392064752696760217259922627911941,210639064323027505476639480028596947637,"Changing the Physical Data Model",70,"<p>Deleting then re-inserting all the rows in a table is an unworkable solution for most tables. It's better to place rows where you want them on insert.</p>

<p>Oracle Database has several data structures that do this to a greater or lesser extent. These include:</p>

<ul>
<li>Index-organized tables</li>
<li>Partitioning</li>
<li>Table clusters</li>
</ul>

<p>These all have various pros and cons. And require a rebuild to add to existing tables. Making it impractical to change most existing tables. There is another, more lightweight technique available from 12.2:</p>

<p>Attribute Clustering</p>",16-MAR-20 09.56.53.941854000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.09.38.382269000 PM,"CHRIS.SAXON@ORACLE.COM"
213955392065087569212250512204021522693,210639064323027505476639480028596947637,"Interleaved Attribute Clustering",90,"<p>Interleaved clustering sorts similar values together. This can improve the clustering for two or more loosely correlated columns at the same time. This interleaves the values for BRICK_ID and SHAPE:</p>

<code>select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';

alter table bricks
  drop clustering ;
  
alter table bricks
  add clustering 
  by interleaved order ( brick_id, shape );
  
alter table bricks
  move online;
  
exec dbms_stats.gather_table_stats ( null, 'bricks' ) ;

select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';</code>

<p>This gives near-perfect clustering for these two columns. The effect of this is to trace a Z-shape in a matrix of these values:</p>

<code>with rws as ( 
  select ceil ( brick_id / 100 ) id, 
         shape,
         dense_rank () over ( order by dbms_rowid.rowid_block_number ( rowid ) ) rid
  from   bricks
)
  select * from rws
  pivot (
    max ( rid ) 
    for id in (
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10
    )
  )
  order by shape;</code>

<p>Notice that some RID values appear in two rows. If you trace these you'll follow a Z-like pattern through the matrix.</p>",16-MAR-20 09.58.56.770104000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.12.59.998725000 PM,"CHRIS.SAXON@ORACLE.COM"
214813673946860448487363241573310137459,210639064323015416218443333736849885877,"Reducing I/O: Only Select Needed Values",90,"<p>When writing a query, it's good practice to only select the columns you need. This reduces the amount of data sent over the network from the database to the client. And it can make operations like sorting and grouping more efficient.</p>

<p>These two queries get the first 25 rows in the table, sorted by colour. But the first fetches all the columns in the table. The second only gets the colour values:</p>

<code>select /*+ gather_plan_statistics */* 
from   bricks
order by colour
fetch first 25 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'MEMSTATS LAST'));

select /*+ gather_plan_statistics */ colour
from   bricks
order by colour
fetch first 25 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'MEMSTATS LAST'));
</code> 

<p>This leads to a big reduction in the memory needed to do the sort. When working with huge data sets, this reduces the chance the database will have to use temporary disk to order the data.</p>

<p>Selecting only the values you need also makes it more likely that the optimizer will use an index.</p>",24-MAR-20 03.57.56.472803000 PM,"CHRIS.SAXON@ORACLE.COM",25-MAR-20 09.14.07.343805000 AM,"CHRIS.SAXON@ORACLE.COM"
192502045604127456480705670904323641674,192497261674369450442611783127247019514,"Get Dataguide using Search Index",126,"You use SODA_COLLECTION_T method get_Data_Guide() to get a data guide for a collection. A data guide is a JSON document that summarizes the structural and type information of the JSON documents in the collection. It records metadata about the fields used in those documents.

Note: Before you can obtain a data guide for your collection you must create a data guide-enabled JSON search index on it.

This example gets a data guide for collection MyCollectionName using SODA_COLLECTION_T method get_Data_Guide(). It then uses SQL/JSON function json_query to pretty-print the content of the data-guide document. Finally, it frees the temporary LOB used for the data-guide document.

<code>
REM Fetch the data guide now that index is created

DECLARE
    coll  SODA_Collection_T;
    dg    CLOB;
BEGIN
    coll := dbms_soda.open_Collection('Employees');
    dg := coll.get_Data_Guide;
    dbms_output.put_line(JSON_QUERY(dg, '$' pretty));
    if dbms_lob.isTemporary(dg) = 1
    then
      dbms_output.put_Line('Temporary lob needs to be freed');
      dbms_lob.freeTemporary(dg);
    end if;   
END;

</code>",24-AUG-19 12.33.54.875900000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.27.06.748783000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
214086959373211765064290622437449195711,210640540520278905448644863263597314383,"Create Summaries with Materialized Views",20,"<p>Unlike a regular view, an MV stores the result of its query. When you access the MV it reads these saved data.</p>

<p>This creates an MV counting the number of rows for each colour:</p>

<code>create materialized view brick_colours_mv
as
  select colour, count(*) 
  from   bricks
  group  by colour;

select /*+ gather_plan_statistics */* from brick_colours_mv;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>There are 10,000 rows in the bricks table. But only four different colours. So the MV only stores four rows! So querying the MV does a lot less work compared to running the query against the base table.</p>",17-MAR-20 04.18.22.566190000 PM,"CHRIS.SAXON@ORACLE.COM",06-APR-20 04.10.50.875106000 PM,"CHRIS.SAXON@ORACLE.COM"
193067730042865611652694764065393395857,192344988227908944268158345689617286517,"""Drilling Down"" with Facets - Facet Filtering",40,"Once we've presented the results to our users, complete with facets, we want them to be able to pick facets of interest, and add them to the query.
<p>
The actual mechanism for displaying and allowing the selection of facets is beyond this tutorial, but let's assume there's some sort of checkbox system, and the user has indicated that they want to see only documents where the SOURCE facet has value ""Internal"".
<p>
We then want to re-run the query, but with SOURCE=""Internal"".  We do that by adding an SDATA clause to the query parameter of ctx_query.result_set. We don't have to modify the Result Set Descriptor atat all. So our query procedure is now:

<code>declare
   ResSetDesc clob;
   ResultSet  clob;
begin

   -- fetch the RSD from our temporary table (assumes only we are using the temp table and it was cleared before running the module)

   select rsd into ResSetDesc from temp_rs;

   -- and run our query using the RSD
   
   ctx_query.result_set(
      index_name => 'docsindex',
      query      => 'record AND SDATA(source=""Internal"")',
      result_set_descriptor => ResSetDesc,
      result_set => ResultSet );

   -- ResSetOut now contains XML result set: write it to temporary table

   update temp_rs set rs_text = xmltype(ResultSet);

end;
/
</code>
Notice that the 'query' parameter now includes an SDATA clause to restrict the result set. Also note that case is important in SDATA - it's an exact match, not the equivalent of a match in the CONTAINS operator.
<p>
Once again we can list the output from our query with:
<code>select xmltype.getclobval(rs_text) from temp_rs;
</code>

We can see that both the records selected have ""Internal"" for the source, and that the facets for source now show only

<pre>  &lt;groups sdata=""SOURCE""&gt;
    &lt;group value=""Internal""&gt;
      &lt;count&gt;2&lt;/count&gt;
    &lt;/group&gt;
  &lt;/groups&gt;
</pre>
<p>
What if we want to select multiple facets? Easy enough, we can use AND and OR with the SDATA operators (though be careful to use parentheses properly!)

<code>declare
   ResSetDesc clob;
   ResultSet  clob;
begin

   -- fetch the RSD from our temporary table (assumes only we are using the temp table and it was cleared before running the module)

   select rsd into ResSetDesc from temp_rs;

   -- and run our query using the RSD
   
   ctx_query.result_set(
      index_name => 'docsindex',
      query      => 'record AND ( ( SDATA(source=""Internal"") OR SDATA(source=""External"") ) AND SDATA(region=""South-East"") )',
      result_set_descriptor => ResSetDesc,
      result_set => ResultSet );

   -- ResSetOut now contains XML result set: write it to temporary table

   update temp_rs set rs_text = xmltype(ResultSet);

end;
/
select xmltype.getclobval(rs_text) from temp_rs;
</code>

So that query looked for External or Internal sources, but only from South-East region.

<p>
That's all for now - please try these techniques out with your own data.

<p>
Feedback on, or corrections to these tutorials are welcomed. Hit the ""Feedback"" button at the top of the page, or contact the author, <a href=""mailto:roger.ford@oracle.com?subject=LiveSQL%20Faceted%20Navigation%20Tutorial"">Roger Ford</a>",29-AUG-19 10.43.57.231060000 AM,"ROGER.FORD@ORACLE.COM",29-AUG-19 12.20.01.859911000 PM,"ROGER.FORD@ORACLE.COM"
215414926508135222981674768625895659798,210639799513782729864465643940593969908,"Try It Challenge!",110,"<p>Which expressions can you replace /* TODO */ with to make an index that will improve the performance of this query?</p>

<code>create index brick_challenge_i
  on bricks ( /* TODO */ );

select /*+ gather_plan_statistics */ shape, count (*) 
from   bricks
where  upper ( colour_mixed_case ) = 'RED'
and    substr ( junk, 1, 1 ) = 'A'
group  by shape;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>Try creating several different indexes. Which enables the query to use the fewest buffers?</p>",30-MAR-20 09.35.03.514706000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 09.12.36.467324000 AM,"CHRIS.SAXON@ORACLE.COM"
214367928810978734813677506269165524455,210639799513881861781674043532919876340,"Formatting SQL Trace Files",40,"<p>A raw trace file is hard to read. To get it in a better format, the command line tool TKPROF parses the file.</p>

<p>The basic syntax for TKPROF is:</p>

<pre>tkprof trace_file_name.trc output_filename.txt
</pre>

<p><em>You run TKPROF from the operating system command line, it's NOT a SQL statement!</em></p>

<p>In the file you'll see each SQL command, its run time stats and execution plan. For example, the formatted trace file captured by the process in Tracing SQL Execution will include these statements:</p>

<pre>SQL ID: akvmgkngc3u8t Plan Hash: 1674119367

SELECT COUNT (*) 
FROM
 SCHOOL_ATTENDANCE WHERE STUDENT_ID = :B2 AND SCHOOL_ID = :B1


call     count       cpu    elapsed       disk      query    current        rows
------- ------  -------- ---------- ---------- ---------- ----------  ----------
Parse        1      0.00       0.00          0          0          0           0
Execute  10000      1.84       2.23          0          0          1           0
Fetch    10000      0.13       0.13          0      19559          0       10000
------- ------  -------- ---------- ---------- ---------- ----------  ----------
total    20001      1.97       2.37          0      19559          1       10000

Misses in library cache during parse: 1
Misses in library cache during execute: 2
Optimizer mode: ALL_ROWS
Parsing user id: 107     (recursive depth: 1)
Number of plan statistics captured: 1

Rows (1st) Rows (avg) Rows (max)  Row Source Operation
---------- ---------- ----------  ---------------------------------------------------
         1          1          1  SORT AGGREGATE (cr=0 pr=0 pw=0 time=11 us starts=1)
         0          0          0   INDEX UNIQUE SCAN SYS_C0013583 (cr=0 pr=0 pw=0 time=3 us starts=1 cost=1 size=26 card=1)(object id 82226)

********************************************************************************

SQL ID: 3zdbqjbhkhbjn Plan Hash: 0

INSERT INTO SCHOOL_ATTENDANCE ( STUDENT_ID, SCHOOL_ID ) 
VALUES
 ( :B2 , :B1 )


call     count       cpu    elapsed       disk      query    current        rows
------- ------  -------- ---------- ---------- ---------- ----------  ----------
Parse        1      0.00       0.00          0          0          0           0
Execute  10000      3.79       4.07          0        422      61525       10000
Fetch        0      0.00       0.00          0          0          0           0
------- ------  -------- ---------- ---------- ---------- ----------  ----------
total    10001      3.79       4.07          0        422      61525       10000

Misses in library cache during parse: 1
Misses in library cache during execute: 1
Optimizer mode: ALL_ROWS
Parsing user id: 107     (recursive depth: 1)
Number of plan statistics captured: 1

Rows (1st) Rows (avg) Rows (max)  Row Source Operation
---------- ---------- ----------  ---------------------------------------------------
         0          0          0  LOAD TABLE CONVENTIONAL  SCHOOL_ATTENDANCE (cr=5 pr=0 pw=0 time=759 us starts=1)</pre>

<p>This shows you that:</p>
<ul>
<li>Both the SELECT and INSERT were executed 10,000 times (count execute)</li>
<li>The SELECT ran for 2.37 seconds (total elapsed)</li>
<li>The INSERT ran for 4.07 seconds (total elapsed)</li>
<li>Both statements processed an average of one row per execution or fetch (10,000 in the rows column / 10,000 in fetch or execute)</li>
</ul>

<p>You can also see the execution plan for the statements. The query did an INDEX UNIQUE SCAN. The stats reported are for one execution.</p>

<ul>
<li>cr = consistent reads</li>
<li>pr = physical reads</li>
<li>pw = physical writes</li>
<li>time = duration in microseconds (cumulative)</li>
<li>starts = number of times this step was executed</li>
</ul>

<p>Given this analysis, what would you suggest to make this process faster?</p>

<p>TKPROF is included in the client tools for Oracle Database. You can also open a trace file in Oracle SQL Developer to get it in a readable format.</p>",20-MAR-20 08.46.16.311205000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 04.02.49.029957000 PM,"CHRIS.SAXON@ORACLE.COM"
214690855309429266190351197511392837305,210639064319457547531317480075689609909,"Using SQL ID in DBMS_XPlan",13,"<p>If you know the SQL ID for a statement, you can pass it directly to DBMS_XPlan:</p>

<code>select *
from   bricks b
join   colours c
on     b.colour = c.colour;

select * 
from   table(dbms_xplan.display_cursor('1jqgaskqzc3tq'));
</code>

<p>This is handy if you want to get the plan for a statement running in a different session.</p>

<p>If you don't know the SQL ID, but have the statement's text, you can lookup its SQL ID and pass it to DBMS_XPlan in one statement with this query:</p>

<code>select /* colours query */* from colours;

select p.*  
from   v$sql s, table (  
  dbms_xplan.display_cursor (  
    s.sql_id, s.child_number, 'BASIC'  
  )  
) p  
where s.sql_text like '%colours query%'  /* enter text from the target statement here */
and   s.sql_text not like '%not this%';
</code>",23-MAR-20 10.50.51.370743000 AM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.27.20.324839000 AM,"CHRIS.SAXON@ORACLE.COM"
192478723709790147168536369122222148944,192497261674369450442611783127247019514,"Creating a SODA collection",4,"You can use PL/SQL function DBMS_SODA.create_collection to create a document collection with the default metadata.

The default collection metadata has the following characteristics.

<ul>
<li>Each document in the collection has these document components:
<ul>
<li>Key</li>
<li>Content</li>
<li>Creation timestamp</li>
<li>Last-modified timestamp</li>
<li>Version</li>
</ul>
</li>
<li>The collection can store only JSON documents.</li>

<li>Document keys are automatically generated for documents that you add to the collection.</li>
</ul>
<code>
REM Create collection with default metadata
DECLARE
    collection  SODA_Collection_T;
BEGIN
    collection := dbms_soda.create_Collection('myCollectionName');   
    dbms_output.put_Line('Name: ' || collection.get_Name);
    dbms_output.put_Line('Metadata: ' || 
                     JSON_QUERY(collection.get_Metadata, '$' pretty));
END;
</code>
The default collection configuration is recommended in most cases, but collections are highly configurable. When you create a collection you can specify things such as the following:
<ul>
<li>Storage details, such as the name of the table that stores the collection and the names and data types of its columns.</li>
<li>The presence or absence of columns for creation timestamp, last-modified timestamp, and version.</li>
<li>Whether the collection can store only JSON documents.</li>
<li>Methods of document key generation, and whether document keys are client-assigned or generated automatically.</li>
<li>Methods of version generation.</li>
</ul>",24-AUG-19 12.19.18.748982000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",13-SEP-19 12.45.57.331855000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192585953765294256279096138188385299523,192497261674369450442611783127247019514,"Handling Transactions with SODA",140,"As usual in PL/SQL and SQL, you can treat individual SODA read and write operations, or groups of them, as a transaction. To commit a transaction, use a SQL COMMIT statement. If you want to roll back changes, use a SQL ROLLBACK statement.

SODA operations DBMS_SODA.create_collection and DBMS_SODA.drop_collection do not automatically commit before or after they perform their action. This differs from the behavior of SQL DDL statements, which commit both before and after performing their action.

One consequence of this is that, before a SODA collection can be dropped, any outstanding write operations to it must be explicitly committed or rolled back â€” you must explicitly use SQL COMMIT or ROLLBACK. This is because DBMS_SODA.drop_collection does not itself issue commit before it performs its action. In this, the behavior of DBMS_SODA.drop_collection differs from that of a SQL DROP TABLE statement.

This example shows the use of SQL COMMIT and ROLLBACK statements in an anonymous PL/SQL block. It opens a SODA collection, inserts a document, and then replaces its content. The combination of the document insertion and document content replacement operations is atomic: a single transaction.

<code>
REM Handling Transactions with SODA

DECLARE
    collection SODA_COLLECTION_T;
    status NUMBER;
    doc SODA_Document_T;
BEGIN
    collection := dbms_soda.open_collection('myCollectionName');
    doc := SODA_Document_T(b_Content => utl_raw.cast_to_raw('{""a"":""aval"",""b"":""bval""}'));
    status := collection.insert_one(doc);
    status := collection.replace_one('ABCDEF', SODA_DOCUMENT_T(b_Content => utl_raw.cast_to_raw('{""x"":""xval"",""y"":""yval""}')));
    -- Commit the transaction
    COMMIT;
    DBMS_OUTPUT.put_line('Transaction is committed');
-- Catch exceptions and roll back if an error is raised
EXCEPTION
  WHEN OTHERS THEN
    DBMS_OUTPUT.put_line (SQLERRM);
    ROLLBACK;
    DBMS_OUTPUT.put_line('Transaction has been rolled back');
END;

</code>",24-AUG-19 07.54.47.051967000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.27.35.425300000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
215620450002015300056846426730698292734,210639799513782729864465643940593969908,"Index Only Scans",62,"<p>A composite index can also enable the database to only read the index and bypass the table altogether. To do this, all the columns listed in the query must be present in an index.</p>

<p>For example, this query counts the number of red bricks for each weight. Both of the columns in the query are in the BRICK_COLOUR_WEIGHT_I index. So the query only scans the index:</p>

<code>select /*+ gather_plan_statistics */weight, count(*) 
from   bricks
where  colour = 'red'
group  by weight;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Notice that BRICKS <strong>doesn't</strong> appear in the execution plan! This is also called a covering index</p>

<p>The optimizer can also do an index only scan when you select all the columns in an index without a WHERE clause:</p>

<code>select /*+ gather_plan_statistics */colour, weight
from   bricks
fetch  first 10 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>For the optimizer to do this, at least one column must be NOT NULL. This is because Oracle Database excludes rows where all columns are NULL from (BTree) indexes. If all the columns allow nulls, the query could give incorrect results using an index as it would not return wholly NULL rows!</p>",01-APR-20 08.36.43.592941000 AM,"CHRIS.SAXON@ORACLE.COM",03-JUN-20 04.39.35.907303000 PM,"CHRIS.SAXON@ORACLE.COM"
215609772429667511648910352830236901976,210639799513782729864465643940593969908,"Primary and Unique Key Indexes",78,"<p>When you create a primary key or unique constraint, Oracle Database will create a unique index to police it. Unless there's an existing index with the constraint's columns at the start!</p>

<p>For example, the previous module created a unique index on BRICK_ID. So making this column the primary key uses the existing unique index on this column:</p>

<code>select index_name
from   user_ind_columns
where  table_name = 'BRICKS'
and    column_name = 'BRICK_ID';

alter table bricks
  add primary key ( brick_id );
  
select constraint_name, index_name
from   user_constraints
where  constraint_type = 'P';</code>

<p>If you remove the primary key and index, then re-create the primary key, the database will create a new index with a name matching the constraint:</p>
  
<code>alter table bricks
  drop primary key;
drop index brick_brick_id_u;

alter table bricks
  add constraint brick_pk
  primary key ( brick_id );
  
select constraint_name, index_name
from   user_constraints
where  constraint_type = 'P';

select index_name
from   user_ind_columns
where  table_name = 'BRICKS'
and    column_name = 'BRICK_ID';</code>

<p>The database can use existing non-unique indexes with the primary key columns at the start to police the constraint. To ensure it uses the index you want when adding a primary key, you can specify it in the INDEX clause.</p>",01-APR-20 09.07.14.816344000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 01.45.37.956143000 PM,"CHRIS.SAXON@ORACLE.COM"
192343480696705734109936640081542179077,192344988227908944268158345689617286517,"Creating a table and index for use with faceted navigation",10,"<p>
We're going to create a table which has some structured information (our ""facets"") and a text field. All but one row will contain the word 'record' which we'll later use as a full-text search term.

<code>create table docs (
   source     varchar2(20), 
   recordtype varchar2(20),
   region     varchar2(20),
   text       clob );
</code>

<p>
And insert some data into that table:

<code>insert into docs values (
   'Internal', 'Customer Info', 'South',      'This is record 1');
insert into docs values (
   'Internal', 'Customer Info', 'North',      'This record is record 2.');
insert into docs values (
   'External', 'Customer Info', 'South-East', 'This is record 3');
insert into docs values (
   'External', 'Customer Info', 'South-East', 'This is record 4');
insert into docs values (
   'External', 'Office Doc',    'North-West', 'This is record 5');
insert into docs values (
   'External', 'Office Doc',    'North-East', 'This is record 6');
insert into docs values (
   'External', 'Office Doc',    'North-East', 'No r-word in this one');
</code>

<p>
Now the setup necessary for the index. Facets must be defined as SDATA (<bold>S</bold>tructured <bold>DATA</bold>) fields. They can either be embedded in the text, or they can be separate FILTER BY columns in the table as we're going to do here.

<code>-- only needed if re-running: exec ctx_ddl.drop_section_group  ('mysg')
exec ctx_ddl.create_section_group('mysg', 'HTML_SECTION_GROUP')
exec ctx_ddl.add_sdata_column    ('mysg', 'source', 'source')
exec ctx_ddl.add_sdata_column    ('mysg', 'recordtype', 'recordtype')
exec ctx_ddl.add_sdata_column    ('mysg', 'region', 'region')
</code>

Now we'll create the actual index. Notice that the SDATA columns specified above MUST be specified as FILTER BY columns for SDATA fields to be created from them. The section group created above is specified in the PARAMETERS clause.

<code>create index docsindex on docs(text) indextype is ctxsys.context
filter by source, recordtype, region
parameters ('section group mysg');
</code>

We now have an index with SDATA sections defined for our three facets, and we're ready to move onto the next module - using the Result Set Interface to query the table and fetch back facets.",22-AUG-19 12.09.56.338305000 PM,"ROGER.FORD@ORACLE.COM",22-AUG-19 06.49.37.015997000 PM,"ROGER.FORD@ORACLE.COM"
212075289882379011279286045714632339207,210639799513641285543570732327153347316,"Histogram Types",70,"<p>There are four types of histograms in Oracle Database:</p>

<ul>
<li>Frequency</li>
<li>Height-balanced</li>
<li>Hybrid</li>
<li>Top-frequency</li>
</ul>

<p>When there are few different values in a column, the database will create a frequency histogram. These store the exact number of rows for each value in the column. These give the best row estimates for queries.</p>

<p>But there's a limit to how many different values the optimizer can collect exact stats for. So when there are a large number of values for a histogram the optimizer will switch to a different type.</p>

<p>From Oracle Database 12c, the optimizer will choose a hybrid or top-frequency histogram if there are many values in the column. Before this it would create height-balanced balanced histograms. These were deprecated in 12c. You'll only see height-balanced histograms in 12c and later if you upgrade from an earlier release.</p>

<p>For more details on the algorithms for these, read about the <a href=""https://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-BE10EBFC-FEFC-4530-90DF-1443D9AD9B64"">histogram types in the docs</a>.</p>",27-FEB-20 09.53.00.099994000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.51.56.164365000 PM,"CHRIS.SAXON@ORACLE.COM"
213644673999021454209855294248942446203,210639799513782729864465643940593969908,"Try It!",90,"<p>Replace /* TODO */ in this index, so the query will use it to search for bricks inserted on 1 Jan 2020:</p>

<code>create index bricks_insert_date_fbi 
  on bricks ( /* TODO */ );
  
select /*+ gather_plan_statistics  1st Jan weights */weight, count ( * ) 
from   bricks
where  trunc ( insert_datetime ) = date'2020-01-01'
group  by weight;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));</code>

<p><em>Remember: the function in the WHERE clause must appear in the index definition!</em></p>",13-MAR-20 10.03.45.312113000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 02.02.22.029304000 PM,"CHRIS.SAXON@ORACLE.COM"
211969973559213742359192146361230979981,210639799513641285543570732327153347316,"Try It!",22,"<p>Replace /* TODO */ in this code to gather stats for the bricks table:</p>

<code>select ut.num_rows
from   user_tables ut
where  ut.table_name = 'BRICKS';

exec dbms_stats.gather_table_stats ( null, ' /* TODO */' ) ;

select count (*) from bricks;

select ut.num_rows
from   user_tables ut
where  ut.table_name = 'BRICKS';</code> 

<p>Check this has updated the stats by verifying that the number of rows in the table matches the number reported in USER_TABLES.</p>

<p>After gathering stats on BRICKS and COLOURS, the join in the first module should now have the correct plan. This reads COLOURS first and the row estimates matching the actual rows:</p>

<code>select /*+ gather_plan_statistics */c.colour_name, count (*) c
from   bricks b
join   colours c
on     c.colour_rgb_value = b.colour_rgb_value
group  by c.colour_name;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>For simple queries having up-to-date stats is often good enough for the optimizer to choose the best plan. But there are a couple of common problems which can still lead to incorrect estimates:</p>
<ul>
<li>Data skew</li>
<li>Column correlation</li>
</ul>",26-FEB-20 09.33.14.597513000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.47.52.420414000 PM,"CHRIS.SAXON@ORACLE.COM"
214895684886175545953636721835054362791,210639064323015416218443333736849885877,"Challenge!",110,"<p>Run this query and get its execution stats:<p>

<code>select /*+ gather_plan_statistics */ *
from   colours c
join   bricks b
on     c.colour = b.colour
order  by b.colour
fetch first 10 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ALLSTATS LAST'));</code>

<p>This only needs to return the COLOUR of the first ten rows stored in BRICKS, sorted by COLOUR. Other columns are unnecessary. What changes can you make to this query to reduce the resources it uses?</p>

<p><em>Hint: remember BRICKS_INDEXED stores the same data as BRICKS.</em></p>

<p>Experiment with selecting different values. What's the smallest amount of memory and buffers you can get a query returning the top 10 colours to use?</p>

<p><em>DBMS_XPlan only returns metrics which have non-zero values for at least one operation in the plan. If a column is missing (e.g. Used-Mem) this means the database needed no extra memory to process the query!</em></p>",25-MAR-20 09.57.36.265418000 AM,"CHRIS.SAXON@ORACLE.COM",03-JUN-20 07.51.08.788257000 AM,"CHRIS.SAXON@ORACLE.COM"
192500308251507503596494350204533352761,192497261674369450442611783127247019514,"Finding All Documents in a Collection",76,"This example uses SODA_COLLECTION_T methods find() and getCursor() to obtain a cursor for a query result list that contains each document in a collection. It then uses the cursor in a WHILE statement to get and print the content of each document in the result list, as a string. Finally, it closes the cursor.

It uses SODA_DOCUMENT_T methods get_key(), get_blob(), get_created_on(), get_last_modified(), and get_version(), to get the document components, which it prints. It passes the document content to SQL/JSON function json_query to pretty-print (using keyword PRETTY).
<code>
REM Finding All Documents in a Collection

DECLARE
    collection    SODA_Collection_T;
    document      SODA_Document_T;
    cur           SODA_Cursor_T;
    status        BOOLEAN;
BEGIN
    -- Open the collection to be queried
    collection := DBMS_SODA.open_collection('myCollectionName');

    -- Open the cursor to fetch the documents
    cur := collection.find().get_cursor();

    -- Loop through the cursor
    WHILE cur.has_Next
    LOOP
      document := cur.next;
      IF document IS NOT NULL THEN
          DBMS_OUTPUT.put_line('Document components:');
          DBMS_OUTPUT.put_line('Key: ' || document.get_key);
          DBMS_OUTPUT.put_line('Content: ' ||
                               JSON_QUERY(document.get_blob, '$' PRETTY));
          DBMS_OUTPUT.put_line('Creation timestamp: ' || 
                               document.get_created_on);
          DBMS_OUTPUT.put_line('Last modified timestamp: ' ||
                               document.get_last_modified);
          DBMS_OUTPUT.put_line('Version: ' || document.get_version);
      END IF;
    END LOOP;

    -- IMPORTANT: You must close the cursor to release resources!
    status := cur.close;
END;
</code>",24-AUG-19 12.28.19.795095000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.04.43.279125000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192605740821473023329646977733572041808,192497261674369450442611783127247019514,"Counting the Number of Documents Found",92,"This example uses SODA_COLLECTION_T method count() to get a count of all of the documents in the collection. It then gets a count of all of the documents that are returned by a filter specification (QBE).

<code>
REM Counting the Number of Documents Found

DECLARE
    collection  SODA_COLLECTION_T;
    num_docs    NUMBER;
    qbe         VARCHAR2(128);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    -- Get a count of all documents in the collection
    num_docs := collection.find().count;
    DBMS_OUTPUT.put_line('Count: ' || num_docs);
    
    -- Set the filter
    qbe := '{""name"" : ""Alexander""}';

    -- Get a count of all documents in the collection that match a filter spec
    num_docs := collection.find().filter(qbe).count;
    DBMS_OUTPUT.put_line('Count: ' || num_docs);
END;
</code>",25-AUG-19 12.13.28.930183000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.13.28.930199000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
214267387552683864460570381290878698697,210639064323069817880325992049711663797,"Try It!",90,"<p>Find how many rows in the table have DAMAGED = 'Y'</p>

<code>select count (*)
from   card_deck d1
where  d1.damaged = 'Y';
</code>

<p>Replace /* TODO */ in the get the adaptive execution details for this query. But before you do - predict whether the query will use NESTED LOOPS or a HASH JOIN:</p>

<code>select /*+ gather_plan_statistics */* 
from   card_deck d1
join   card_deck d2
on     d1.suit = d2.suit
and    d1.val = d2.val
where  d1.damaged = 'Y';

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => '/* TODO */'));
</code>

<p>Which join method did the query use?</p>",19-MAR-20 09.36.59.863765000 AM,"CHRIS.SAXON@ORACLE.COM",19-MAR-20 09.41.58.997218000 AM,"CHRIS.SAXON@ORACLE.COM"
214267387555372515483393316575425234121,210639064323069817880325992049711663797,"Try It!",60,"<p>Replace /*TODO*/ in this query to experiment with fetching different numbers of rows:</p>

<code>select /*+ gather_plan_statistics */*
from   card_deck d1
join   card_deck d2
on     d1.suit = d2.suit
and    d1.val = d2.val
order  by d1.val
fetch first /*TODO*/ rows only;

select * 
from   table(dbms_xplan.display_cursor ( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>How many rows do you have to fetch before the optimizer changes from nested loops to a hash join?</p>",19-MAR-20 09.46.10.737917000 AM,"CHRIS.SAXON@ORACLE.COM",19-MAR-20 09.46.10.737933000 AM,"CHRIS.SAXON@ORACLE.COM"
213636360925565065352909992401083984054,210639799513782729864465643940593969908,"Creating Unique Indexes",75,"<p>The BRICK_ID is a unique identifier for rows in BRICKS. But there are no constraints on this column, so it's possible to have two or more rows with the same value for this:</p>

<code>insert into bricks 
  values ( 1, 'red', 'cylinder', 1, 'RED', sysdate, 'stuff' );

select * from bricks
where  brick_id = 1;

rollback;
</code>

<p>To avoid this you can create a unique index. This is a special type of index that is guaranteed to have at most one entry for each set of values. To do this, add the UNIQUE keyword between CREATE and INDEX:</p>

<code>create unique index brick_brick_id_u 
  on bricks ( brick_id );</code>

<p>There can now be at most one row in the table for each BRICK_ID. If you try and insert an existing value, the database will throw an ORA-00001 exception:</p>

<code>insert into bricks 
  values ( 1, 'red', 'cylinder', 1, 'RED', sysdate, 'stuff' );
</code>

<p>The optimizer can also use a unique index to do an INDEX UNIQUE SCAN when a query has equality conditions on all its columns:</p>
  
<code>select /*+ gather_plan_statistics brick_id */* from bricks
where  brick_id = 1;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>Because there can be at most one entry in the index for each BRICK_ID, this is a highly efficient search method.</p>

<p>But the main value of unique indexes is improving data quality by preventing duplicate rows. A common way to do this is by creating a primary key or unique constraint.</p>",13-MAR-20 09.45.31.867548000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 02.00.26.413204000 PM,"CHRIS.SAXON@ORACLE.COM"
151386982902114832866741592997405400282,151382938526402044313447081869310725033,"Inner Joins",30,"<p>An inner join (or just join) links two tables. It compares values in one or more columns from each. It only returns rows which match the join conditions in both tables.</p>

<p>The simplest join checks if the values in a column from one table equal the values in a column from the other.</p>

<p>For example, you can join toys and bricks where the toy_id equals the brick_id. Only the ids 2 & 3 are in both tables. And there is only one row for each value in each table. So this join returns two rows:</p>

<code>select * 
from   toys, bricks
where  toy_id = brick_id;</code>

<p>Using the ANSI way, the join criteria go in the on clause.</p>

<code>select * 
from   toys
inner  join bricks
on     toy_id = brick_id;</code>

<p>An inner join can return any number of rows from zero to the Cartesian product of the two tables.</p>

<p>If you join on a column that has repeated values, you'll get many copies of the matching rows in the joined table. For example, there are two blue bricks. The toy Cuteasaurus is blue. So joining the tables on colour returns two copies of the row for Cuteasaurus:<p>

<code>select * 
from   toys
join   bricks
on     toy_colour = brick_colour;</code>

<p>Note that the keyword inner is optional.</p>

<p>You can also join on inequalities. For example, to return all the rows that have different colours, write:</p>

<code>select * 
from   toys
join   bricks
on     toy_colour <> brick_colour;</code>

<p>Note there are no pink bricks. So this returns a copy of the pink toy (Miss Snuggles) for each row in bricks. So Miss Snuggles appears three times.</p>",26-JUL-18 08.49.25.836177000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.34.04.862412000 PM,"CHRIS.SAXON@ORACLE.COM"
210619333242461077693374311447225087507,210639064319457547531317480075689609909,"The Starts Statistic",40,"<p>In the queries so far, the database reads each table once during execution. In some queries the database will read the same table many times.</p>

<p>To check this, look at the Starts column included in the row stats. This states how many times the operation began while the query was running.</p>

<p>This query uses a scalar subquery. The database may run this once for each row from the colours table. Verify this by looking at the starts column:</p>

<code>select /*+ gather_plan_statistics */c.rgb_hex_value,
       ( select count (*)
         from   bricks b
         where  b.colour = c.colour 
       ) brick#
from   colours c;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>
<p>Note that this plan also breaks the <em>read the top-most leaf first</em> rule! BRICKS appears above COLOURS in the plan. But it receives rows from the colours table. So the database must read COLOURS before BRICKS.</p>

<p>Be aware that there are many special cases when it comes to execution plans. This series discusses general principles. But you will be able to find cases where the rules don't apply.</p>",13-FEB-20 03.38.03.663867000 PM,"CHRIS.SAXON@ORACLE.COM",23-APR-20 09.30.12.180887000 AM,"CHRIS.SAXON@ORACLE.COM"
213641816556803479959999932945304039914,210639799513782729864465643940593969908,"Composite Index Column Order",65,"<p>The order of columns in an index can have a big impact on how effective it is. And whether the optimizer is able to use it!</p>

<p>The database searches columns in an index left-to-right. To be most effective, use the leading columns of the index the WHERE clause.</p>

<p>For example, COLOUR is the first column in BRICK_COLOUR_WEIGHT_I. Queries looking for colour values in the WHERE clause can restrict their search of the index to only these colours. This will often appear as an INDEX RANGE SCAN in the plan.</p>

<p>But queries with WEIGHT in the WHERE clause and COLOUR in the SELECT list can't restrict their search to only those entries matching the weight value. Instead they check every entry in the index with a (FAST) FULL INDEX SCAN:</p>

<code>select /*+ gather_plan_statistics */colour, count(*) 
from   bricks
where  weight = 1
group  by colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>So the query reads all the entries in the index, discarding those where the WEIGHT is not one. The filter predicate for the plan confirms this:</p>

<pre>2 - filter(""WEIGHT""=1) </pre>

<p>A full index scan is usually faster than a full table scan. But slower than an index range scan. To enable a range scan for the previous query, you need to create an index with WEIGHT as the first column:</p>

<code>create index bricks_weight_colour_i
  on bricks ( weight, colour );

select /*+ gather_plan_statistics */colour, count(*) 
from   bricks
where  weight = 1
group  by colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>While covering indexes can be the fastest way to read data, creating one for every query leads to an explosion in the number of indexes you have. This increases your storage requirements and makes it harder for the optimizer to choose the best index for each query.</p>

<p>Keep the number of indexes you create to a minimum. Reserve covering indexes for critical queries!</p>",13-MAR-20 10.17.17.501672000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 01.42.07.576127000 PM,"CHRIS.SAXON@ORACLE.COM"
213955002613296584016337581913701758277,210639064323027505476639480028596947637,"Improving Clustering for a Column",60,"<p>Setting TABLE_CACHED_BLOCKS helps the optimizer spot <em>mostly</em> clustered values.</p>

<p>But setting this to a huge value merely tricks the optimizer. This can fool it to thinking that values such as WEIGHT that are spread throughout the whole table are clustered in a few blocks. The database still needs to do the same number of I/Os to fetch the rows.</p>

<p>To make indexes on WEIGHT more effective, you need to physically re-order rows in the table. This re-ordering must move rows with the same value for WEIGHT to the same blocks.</p>

<p>It's possible to do this by copying all the rows to a temporary table. Then deleting all the rows from BRICKS and re-inserting them sorted by WEIGHT:</p>

<code>insert into bricks_temp
  select * from bricks;
  
delete bricks;

insert into bricks
  select * from bricks_temp
  order  by weight;
  
commit;

exec dbms_stats.gather_table_stats ( null, 'bricks' );

select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';
</code>

<p>This leads to a big improvement for the clustering of the weight index. But comes at the cost of the other columns. The indexes on BRICK_ID and INSERT_DATE are now nearly imperfectly clustered!</p>

<p>Re-sorting all the rows in a table like this is impractical in most applications. Luckily Oracle Database has several data structures you can use to force physical order on data.</p>",16-MAR-20 09.56.31.136982000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.07.58.674149000 PM,"CHRIS.SAXON@ORACLE.COM"
192605740821540723175545396967355587664,192497261674369450442611783127247019514,"Replacing a Document in a Collection, Given Its Key, and Getting the Result Document",94,"This example replaces a document in a collection, given its key. It then gets (and prints) the key and the generated components from the result document. To obtain the components it uses SODA_DOCUMENT_T methods get_key(), get_created_on(), get_last_modified(), and get_version().

<code>
REM Replacing a Document in a Collection, Given Its Key, and Getting the Result Document

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    new_doc     SODA_DOCUMENT_T;
    k           VARCHAR2(255);
BEGIN
    collection := DBMS_SODA.open_collection('myCollectionName');
    SELECT ID into k from ""myCollectionName"" where rownum < 2;
    document := SODA_DOCUMENT_T(
                  b_content => utl_raw.cast_to_raw('{""name"" : ""Sriky""}'));
    new_doc := collection.find().key(k).replace_one_and_get(document);

    IF new_doc IS NOT NULL THEN
        DBMS_OUTPUT.put_line('Document components:');
        DBMS_OUTPUT.put_line('Key: ' || new_doc.get_key);
        DBMS_OUTPUT.put_line('Creation timestamp: ' || new_doc.get_created_on);
        DBMS_OUTPUT.put_line('Last modified timestamp: ' ||
                             new_doc.get_last_modified);
        DBMS_OUTPUT.put_line('Version: ' || new_doc.get_version);
    END IF;
END;
</code>",25-AUG-19 12.15.54.948413000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.41.37.205138000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192605740821551603507921928629927943248,192497261674369450442611783127247019514,"Replacing a Particular Version of a Document",96,"To implement optimistic locking when replacing a document, you can chain together methods key() and version(), as in this example.
<code>
REM Specifying Document Version to search

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    k           varchar2(255);
    v           varchar2(255);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    select ID, VERSION into k, v from ""myCollectionName"" where rownum < 2;

    -- Find a particular version of the document that has a given key
    document := collection.find().key(k).version(v).get_one;

    IF document IS NOT NULL THEN
        DBMS_OUTPUT.put_line('Document components:');
        DBMS_OUTPUT.put_line('Key: ' || document.get_key);
        DBMS_OUTPUT.put_line('Content: ' ||
                             JSON_QUERY(document.get_blob, '$' PRETTY));
        DBMS_OUTPUT.put_line('Creation timestamp: ' || document.get_created_on);
        DBMS_OUTPUT.put_line('Last modified timestamp: ' ||
                             document.get_last_modified);
        DBMS_OUTPUT.put_line('Version: ' || document.get_version);
    END IF;
END;
</code>",25-AUG-19 12.17.11.818788000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.37.54.289867000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
213935460979640081389250040253886640022,210639064323027505476639480028596947637,"The Clustering Factor",10,"<p>The clustering factor is a measure of how closely the logical index order matches the physical table order for rows. The database calculates this when gathering stats. It does this by asking:</p>

<p><em>Is the row for the current index entry in the same block as the previous entry or a different one?</em></p>

<p>Each time consecutive index entries are in different blocks, the optimizer increments a counter. When stats gathering is complete this gives a value ranging from the number of blocks in the table to the number of rows. The lower this value, the better clustered the rows are and the more likely the database is to use the index.</p>

<p>You can view the clustering factor by querying the *_INDEXES views:</p>

<code>select index_name, clustering_factor, ut.num_rows, ut.blocks
from   user_indexes ui
join   user_tables ut
on     ui.table_name = ut.table_name
where  ui.table_name = 'BRICKS';
</code>

<p>The index on BRICK_ID (BRICK_PK) has perfect clustering (clustering factor <= blocks). Whereas the clustering for the weight index is almost as bad as it can be (clustering factor ~ number of rows). The other indexes all vary between these extremes.</p>",16-MAR-20 09.46.46.781974000 AM,"CHRIS.SAXON@ORACLE.COM",03-APR-20 09.33.12.946523000 AM,"CHRIS.SAXON@ORACLE.COM"
211970272182806065191138869330116775092,210639799513641285543570732327153347316,"Gathering Statistics",20,"<p>To update the statistics on a table, call the GATHER_TABLE_STATS routine in DBMS_stats:</p>
<code>exec dbms_stats.gather_table_stats ( null, 'colours' ) ;

select ut.table_name, ut.num_rows, utcs.column_name, utcs.num_distinct
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'COLOURS';
</code>

<p>Oracle Database has an automatic job that gathers table stats for you. In most cases you won't need to gather stats manually. But there are a couple of cases where you may want to do this.</p>

<p><h3>After large batch processes</h3></p>

<p>If you insert, update or delete more than ten percent of the rows in a table it's likely there's a big mismatch between the stats and the real data. You know the process made a significant change to the data. So it can be worth gathering stats immediately, instead of waiting for the background job.</p>

<p><h3>The default gather process is too rare</h3></p>

<p>By default the optimizer gathers stats on tables where more than ten percent of the rows have changed. On tables with billions of rows this means 100 million rows need to change before the optimizer will regather stats!</p>

<p>You can make the database start gathering sooner by lowering the gather threshold. Do this by setting table preferences. You view and set these using DBMS_stats.</p>

<p>This sets the change threshold (stale percent) to one percent for the colours table:</p>

<code>select dbms_stats.get_prefs ( 'STALE_PERCENT', null, 'colours' ) from dual;
exec dbms_stats.set_table_prefs ( null, 'colours', 'STALE_PERCENT', 1 );
select dbms_stats.get_prefs ( 'STALE_PERCENT', null, 'colours' ) from dual;</code>

<p>Table preferences apply to both manual and automatic stats gathering routines. When gathering stats manually you can override the preference settings.</p>",26-FEB-20 09.28.47.213750000 AM,"CHRIS.SAXON@ORACLE.COM",02-JUN-20 07.36.40.329480000 AM,"CHRIS.SAXON@ORACLE.COM"
192605740821557648137020001775801474128,192497261674369450442611783127247019514,"Removing a Particular Version of a Document",99,"To implement optimistic locking when removing a document, you can chain together methods key() and version(), as in this example.

<code>
REM Removing a Particular Version of a Document
DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    status      NUMBER;
    k           VARCHAR2(255);
    v           VARCHAR2(255);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    select ID, VERSION into k, v from ""myCollectionName"";

    -- Remove version 'version1' of the document that has key 'key1'.
    status := collection.find().key(k).version(v).remove;

    -- Count is 1, if specified version of document with key 'key1' is found
    IF status = 1 THEN
        DBMS_OUTPUT.put_line('Document was removed!');
    END IF;
END;
</code>",25-AUG-19 12.18.44.418161000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.43.06.519001000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
213556468759598482268827437550378089703,210639064323015416218443333736849885877,"How Many Logical I/Os Did an Operation Do?",20,"<p>The IOSTATS format reports the logical I/O in the buffers column for each operation in the plan. These figures are cumulative. So to find the I/Os for an operation, you need to subtract the values for its direct children.</p>

<code>select /*+ gather_plan_statistics */
       count (*)
from   colours c
join   bricks b
on     c.colour = b.colour;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>The Buffers reported at the top of the plan (step 0) is the total I/O for the query. So this query did ~44 gets. The vast majority of these were reading the BRICKS table, with a couple for reading the COLOURS table.</p>

<p>The HASH JOIN does little or no gets. Verify this by subtracting the total gets for reading COLOURS and BRICKS from the Buffers at operation 2.</p>

<p>So does this mean the join does no work?! No! The database will attempt to process the join in memory. But if there's not enough memory available it will have to write to temporary disk.</p>",12-MAR-20 01.59.21.209447000 PM,"CHRIS.SAXON@ORACLE.COM",24-APR-20 09.20.23.327405000 AM,"CHRIS.SAXON@ORACLE.COM"
190586887836265669710664698753878495883,188704173216096418598025228629326881042,"Create spatial indexes",45,"<p>
You are now ready to create spatial indexes for CUSTOMERS and WAREHOUSES:
<code>CREATE INDEX customers_sidx ON customers(CUST_GEO_LOCATION)
           indextype is mdsys.spatial_index;
           
CREATE INDEX warehouses_sidx ON warehouses(WH_GEO_LOCATION)
           indextype is mdsys.spatial_index;   
</code>
<p>",05-AUG-19 06.24.20.624941000 PM,"DAVID.LAPP@ORACLE.COM",05-AUG-19 07.50.44.549019000 PM,"DAVID.LAPP@ORACLE.COM"
212105071401268230678975467542245208108,210639799513641285543570732327153347316,"Data Skew",30,"<p>The row estimates for this query are out by a factor of ten:</p>

<code>select /*+ gather_plan_statistics */count (*) 
from   bricks
where  weight = 1;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>This is because the optimizer assumes there is an even distribution of the values. So it estimates the number of rows matching a particular value is:</p>

<pre># non-null rows in the table / number of distinct values in the column</pre>

<p>In the real world many columns display skew - unevenness in the spread of values. Assuming a uniform distribution for these leads to wildly incorrect row estimates and slow plans.</p>

<p>Skew comes in two forms: range and value.</p>",27-FEB-20 04.10.20.810298000 PM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.48.12.737008000 PM,"CHRIS.SAXON@ORACLE.COM"
213958447287211808745790022617053939263,210639064323027505476639480028596947637,"Try It!",55,"<p>Replace /* TODO */ to set the table_cached_blocks to 128 for the bricks table:</p>

<code>begin 
  dbms_stats.set_table_prefs ( 
    null, 'bricks', 'table_cached_blocks', /* TODO */
  );  
end;
/

select dbms_stats.get_prefs ( 'table_cached_blocks', null, 'bricks' ) 
from   dual;</code>

<p>What effect do you think this will have on the clustering factor for all the indexes? Regather the stats to find out:</p>

<code>select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';

exec dbms_stats.gather_table_stats ( null, 'bricks' ) ;

select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';

select dbms_stats.get_prefs ( 'table_cached_blocks', null, 'bricks' ) 
from   dual;</code>

<p>So why has this happened?</p> 

<p>All the rows fit within 128 blocks. So the cached blocks parameter is higher than the number of blocks in the table. This makes all indexes <strong>too</strong> attractive. Avoid setting this value too high!</p>

<p>Set the TABLE_CACHED_BLOCKS to 8 before continuing:</p>

<code>begin 
  dbms_stats.set_table_prefs ( 
    null, 'bricks', 'table_cached_blocks',  8
  );  
end;
/</code>",16-MAR-20 10.21.42.786921000 AM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.11.53.231920000 PM,"CHRIS.SAXON@ORACLE.COM"
192605740820763383873533190408019516496,192497261674369450442611783127247019514,"Inserting a Document into a Collection",40,"To insert a document into a collection, you invoke SODA_COLLECTION_T method (member function) insert_one() or insert_one_and_get(). These methods create document keys automatically, unless the collection is configured with client-assigned keys and the input document provides the key.

Both method insert_one() and method insert_one_and_get() insert a document into a collection and automatically set the values of the creation time stamp, last-modified time stamp, and version (if the collection is configured to include these components and to generate the version automatically, as is the case by default).

In addition to inserting the document, insert_one_and_get returns a result document, which contains the generated document components, such as the key, and which does not contain the content of the inserted document.

This example creates a document and inserts it into a collection using SODA_COLLECTION_T method insert_one() .

REM Inserting a Document into a Collection
<code>
DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    status      NUMBER;
BEGIN
    -- Open the collection
    collection := DBMS_SODA.create_collection('myCollectionName');
    document := SODA_DOCUMENT_T(
                  b_content => utl_raw.cast_to_raw('{""name"" : ""Alexander""}'));

    -- Insert a document
    status := collection.insert_one(document);
    dbms_output.put_line('Status: ' || status);
END;
</code>",24-AUG-19 11.59.10.866778000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 03.34.17.629803000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
211969973558760395176836660420716163981,210639799513641285543570732327153347316,"Incorrect Row Estimates",10,"<p>When choosing a plan for a query, the optimizer uses the statistics to estimate how many rows it will get from each table.</p>

<p>The bricks and colours tables have incorrect stats. So the optimizer chooses the ""wrong"" plan for this query:</p>

<code>select /*+ gather_plan_statistics */c.colour_name, count (*)
from   bricks b
join   colours c
on     c.colour_rgb_value = b.colour_rgb_value
group  by c.colour_name;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>The optimizer prefers to start a join with the table <em>returning</em> the fewest rows. For example, say your query joins a billion-row table to a hundred-row table. Before joining, it filters the billion-row table down to just ten rows. But it gets all one hundred rows from the other table. So the optimizer still prefers to read the billion-row table first because the returns the fewest rows.</p>

<p>The previous query reads BRICKS first, which returns 300 rows. Whereas the second table, COLOURS, returns just three. The tables should be the other way around in the join!</p>

<p>This is because there's a large difference between the estimated and actual number of rows for each table. The closer the number of estimated and actual numbers of rows are at each step in the plan, the more likely it is the optimizer has found the best plan.</p>

<p>When doing this assessment, you need to multiply the estimated rows by the number of starts for the operation:</p>

<pre>estimated rows * starts ~ actual rows</pre>

<p>When these are more than an order of magnitude different, it's likely there's a better plan available. To correct this, you should refresh the statistics!</p>",26-FEB-20 09.28.35.065127000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.42.13.945114000 PM,"CHRIS.SAXON@ORACLE.COM"
211990926582979726526631861792478499279,210639799513641285543570732327153347316,"Range Skew",55,"<p>You can also have skew in the distribution of the values. For example, there is a gap in the BRICK_IDs from 101 to 999:</p>
<code>with rws as (
  select level r from dual
  connect by level <= 15
)
  select r, count ( brick_id ) 
  from   rws
  left   join bricks
  on     ceil ( brick_id / 100 ) = r
  group  by r
  order  by r;</code>

<p>Again, initially the database assumes the values are evenly distributed between the min and max values for BRICK_ID. So it estimates the same number of rows for these queries. Even though the first and last return 100 rows and the middle zero rows:</p>

<code>select /*+ gather_plan_statistics */count (*) from bricks
where  brick_id between 0 and 100;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) from bricks
where  brick_id between 400 and 500;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */count (*) from bricks
where  brick_id between 1000 and 1100;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>To improve the row estimates, you can gather histograms on these columns.</p>",26-FEB-20 02.15.22.349696000 PM,"CHRIS.SAXON@ORACLE.COM",15-JUL-20 03.30.22.665851000 PM,"CHRIS.SAXON@ORACLE.COM"
214163751114040331666103867806415720647,210639064323069817880325992049711663797,"Hash Joins",10,"<p>The steps to do a hash join are:</p>
<ol>
<li>Return all the rows from the smaller data set</li>
<li>Build a hash table using the join columns for these rows</li>
<li>Read the rows in the larger table</li>
<li>Probe the hash table built at step 2 by applying the same hash function to the join columns of the second table</li>
<li>If this finds a matching entry in the hash table, the database passes the joined rows to the next step of the plan</li>
</ol>
<p>This query uses a hash join:</p>
<code>select /*+ gather_plan_statistics */count(*)
from   card_deck d1
join   card_deck d2
on     d1.suit = d2.suit
and    d1.val = d2.val;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 
<p>There must be equality ""="" conditions in the join for the optimizer to choose a hash join. When there is, it may choose this join method when joining either:</p>
<ul>
<li>Large data sets OR </li>
<li>Most of the rows in small tables OR</li>
<li>There are no indexes on the join columns for either table</li>
</ul>",18-MAR-20 09.40.00.200468000 AM,"CHRIS.SAXON@ORACLE.COM",21-MAY-20 02.09.27.025031000 PM,"CHRIS.SAXON@ORACLE.COM"
14823693069972545935549919470460910513,14810007204900148430867509211759172961,"Sample Hierarchical Queries",70,"<p>Every hierarchy and analytic view has a hierarchical representation that includes attributes, hierarchical attributes, and measures. The data sources (tables and views), joins, aggregation rules, and measure calculations are embedded in the analytic view and thus do not need to be expressed within the query. You can think of the HIERARCHIES clause as replacing joins and GROUP BY.</p>

<p>The following query SELECTs from the analytic view using hierarchical columns. SELECT .. FROM ... HIERARCHIES ... WHERE is the basic template used to query the hierarchical representation of the analytic view. Note that aggregation operators are not required and that the query selects a calculated measure.</p>

<code>SELECT
    time_hier.member_name      AS time
  , product_hier.member_name   AS product
  , geography_hier.member_name AS geography
  , sales
  , sales_year_ago
  , sales_chg_year_ago
  , ROUND(sales_pct_chg_year_ago,2) as sales_pct_chg_year_ago
FROM
    sales_av HIERARCHIES (
        time_hier
    , product_hier
    , geography_hier
    )
WHERE
        time_hier.level_name = 'YEAR'
    AND product_hier.level_name = 'DEPARTMENT'
    AND geography_hier.level_name = 'REGION'
    AND time_hier.year_name = 'CY2015'
ORDER BY
    time_hier.hier_order
  , product_hier.hier_order
  , geography_hier.hier_order;</code>

<p>The next query selects sales and the share of sales by product. Note that the query follows the same pattern as the previous query.</p>

<code>
SELECT
    time_hier.member_name      AS time
  , product_hier.member_name   AS product
  , sales
  , ROUND(sales_shr_of_product,2) AS sales_shr_of_product
FROM
    sales_av HIERARCHIES (
        time_hier
    , product_hier
    )
WHERE
        time_hier.level_name = 'YEAR'
    AND product_hier.level_name = 'DEPARTMENT'
    AND time_hier.year_name = 'CY2015'
ORDER BY
    time_hier.hier_order
  , product_hier.hier_order;
</code>

<p>These are typical examples of the type of queries that an ""AV-aware"" application might generate.</p>",27-NOV-23 05.59.51.291475000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.29.08.026211000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14827839979296878037271266985374708485,14810007204900148430867509211759172961,"Create a Relational Representation of the Analytic View",80,"<p>Unless your tool or application generates AV-aware queries, it most likely generates queries using the SELECT .. SUM .. FROM .. GROUP BY pattern. Analytic views can also be queried with this pattern, allowing just about any tool to query an analytic view.</p>

<p>Because each tool might work best with different variations of the relational representation, you create the view using a CREATE VIEW statement. This selects columns from the analytic view and includes the FACT ROWS keywords to indicate that queries will be processed by the analytic view (rather than the analytic view simply supplying rows that will be processed afterward in SQL).</p>

<p>The following CREATE VIEW statement creates a relational representation of the analytic view that includes columns representing attributes (rather than keys) and the measure of the analytic view.</p>

<p>Note that attributes and measures are qualified by hierarchy name. For the purpose of creating this view, measures are members of a MEASURES hierarchy.</p>

<code>
CREATE OR REPLACE VIEW sales_av_view AS
    SELECT
        year_name
      , quarter_name
      , month_name
      , month_end_date
      , department_name
      , category_name
      , region_name
      , country_name
      , state_province_name
      , measures.sales
      , measures.sales_prior_period
      , measures.sales_chg_prior_period
      , measures.sales_pct_chg_prior_period
      , measures.sales_year_ago
      , measures.sales_chg_year_ago
      , measures.sales_pct_chg_year_ago
      , measures.sales_shr_of_product
    FROM
        sales_av FACT ROWS;</code>

<p>The CREATE VIEW statement ends with FACT ROWS. This tells the database that this is a relational representation of the analytic view, enabling GROUP BY transformation into the analytic view.</p>

<p>Like other views, the relational representation of the analytic view is represented in the USER_VIEWS and USER_TAB_COLUMNS dictionary views.</p>

<code>SELECT * FROM user_views WHERE view_name = 'SALES_AV_VIEW';</code>

<code>SELECT
    table_name
  , column_name
  , data_type
  , data_length
FROM
    user_tab_columns
WHERE
    table_name = 'SALES_AV_VIEW'
ORDER BY
    column_id;</code>

<p>Note that column names do not include the hierarchy qualifier.</p>

<p>Also like other views, columns may be aliased. For example:</p>

<code>CREATE OR REPLACE VIEW sales_av_view AS
    SELECT
        time_hier.year_name                 AS year
      , time_hier.quarter_name              AS quarter
      , time_hier.month_name                AS month
      , time_hier.month_end_date            AS month_end_date
      , product_hier.department_name        AS department
      , product_hier.category_name          AS category
      , geography_hier.region_name          AS region
      , geography_hier.country_name         AS country
      , geography_hier.state_province_name  AS state_province
      , measures.sales                      AS sales
      , measures.sales_prior_period         AS sales_prior_period
      , measures.sales_chg_prior_period     AS sales_change_prior_period
      , measures.sales_pct_chg_prior_period AS sales_percent_change_prior_period
      , measures.sales_year_ago             AS sales_year_ago
      , measures.sales_chg_year_ago         AS sales_change_year_ago
      , measures.sales_pct_chg_year_ago     AS sales_percent_change_year_ago
      , measures.sales_shr_of_product       AS sales_share_of_product
    FROM
        sales_av fact ROWS;</code>

<code>SELECT
    table_name
  , column_name
  , data_type
  , data_length
FROM
    user_tab_columns
WHERE
    table_name = 'SALES_AV_VIEW'
ORDER BY
    column_id;</code>

<p>Most BI tools can use quoted column names. If this is the case with your tool, a view such as this will present nicely in the tool.</p>

<code>CREATE OR REPLACE VIEW sales_av_view AS
    SELECT
        time_hier.year_name                 AS ""Year""
      , time_hier.quarter_name              AS ""Quarter""
      , time_hier.month_name                AS ""Month""
      , time_hier.month_end_date            AS ""Month End Date""
      , product_hier.department_name        AS ""Department""
      , product_hier.category_name          AS ""Category""
      , geography_hier.region_name          AS ""Region""
      , geography_hier.country_name         AS ""Country""
      , geography_hier.state_province_name  AS ""State Province""
      , measures.sales                      AS ""Sales""
      , measures.sales_prior_period         AS ""Sales Prior Period""
      , measures.sales_chg_prior_period     AS ""Sales Change Prior Period""
      , measures.sales_pct_chg_prior_period AS ""Sales Percent Change Prior Period""
      , measures.sales_year_ago             AS ""Sales Year Ago""
      , measures.sales_chg_year_ago         AS ""Sales Change Year Ago""
      , measures.sales_pct_chg_year_ago     AS ""Sales Percent Change Year Ago""
      , measures.sales_shr_of_product       AS ""Sales Share of Product""
    FROM
        sales_av FACT ROWS;</code>

<code>SELECT
    table_name
  , column_name
  , data_type
  , data_length
FROM
    user_tab_columns
WHERE
    table_name = 'SALES_AV_VIEW'
ORDER BY
    column_id;</code>

<p>If column names are unique across hierarchies, the hierarchy qualifier may be omitted. For example:</p>

<code>CREATE OR REPLACE VIEW sales_av_view AS
    SELECT
        year_name                   AS year
      , quarter_name                AS quarter
      , month_name                  AS month
      , month_end_date
      , department_name             AS department
      , category_name               AS category
      , region_name                 AS region
      , country_name                AS country
      , state_province_name         AS state_province
      , sales
      , sales_prior_period
      , sales_chg_prior_period
      , sales_pct_chg_prior_period
      , sales_year_ago
      , sales_chg_year_ago
      , sales_pct_chg_year_ago
      , sales_shr_of_product
    FROM
        sales_av FACT ROWS;</code>",27-NOV-23 07.08.17.141495000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 01.37.11.927124000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
190590339676554169701083872798444276990,188704173216096418598025228629326881042,"Summary",60,"<p>
In this tutorial, you learned how to:
<ul>
<li>Create tables with a geometry columns</li>
<li>Insert spatial metadata</li>
<li>Create Spatial Index</li>
<li>Load data with geometries</li>
<li>Perform location-based queries</l>
</ul>
<p>
For more information, please see the Spatial Developer's Guide at  <a href=""https://docs.oracle.com/en/database/oracle/oracle-database/19/spatl/index.html"">https://docs.oracle.com/en/database/oracle/oracle-database/19/spatl/index.html</a>",05-AUG-19 04.40.32.360963000 PM,"DAVID.LAPP@ORACLE.COM",06-AUG-19 02.50.21.558780000 PM,"DAVID.LAPP@ORACLE.COM"
192681485232501863228147213554340790026,192497261674369450442611783127247019514,"Creating a B-Tree Functional Index for a JSON Field",105,"You index the documents in a SODA collection with SODA_COLLECTION_T method create_index(). Its input parameter is a textual JSON index specification. This can specify support for B-tree, spatial, full-text, and ad hoc indexing, and it can specify support for a JSON data guide.

This example creates a B-tree non-unique index for numeric field salary of the JSON documents in collection Employees.
<code>
REM Creating a B-Tree functional index

DECLARE
  spec    VARCHAR2(700);
  coll    SODA_Collection_T;
  status  NUMBER;
BEGIN
  coll := dbms_soda.open_Collection('Employees');
  spec := '{""name""   : ""Salary_IDX"",
            ""unique"" : false,
            ""fields"" : [{""path""     : ""salary"",
                         ""datatype"" : ""NUMBER""}]
           }';
  status := coll.drop_Index('Salary_IDX');
  status := coll.create_Index(spec);
  dbms_output.put_Line('Status: ' || status);
END;
</code>",25-AUG-19 05.18.45.517037000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 06.04.00.309730000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
215630416546291829377949846742850231849,210639064323027505476639480028596947637,"When Is an Index Useful?",5,"<p>An index is said to be useful when it locates few rows in the table. But what exactly does ""few"" mean?</p>

<p>The BRICKS table stores 10,000 rows:</p>

<code>select count(*) from bricks;</code>

<p>It has indexes on the BRICK_ID, COLOUR, WEIGHT, SHAPE, and INSERT_DATE columns:</p>

<code>select ui.index_name, 
       listagg ( uic.column_name, ',' )
         within group ( order by column_position ) cols
from   user_indexes ui
join   user_ind_columns uic
on     ui.index_name = uic.index_name
where  ui.table_name = 'BRICKS'
group  by ui.index_name</code>

<p>There are 88 rows with weights between 1 and 10, a small fraction of the total in the table (0.88%). But the database opts for a full table scan:</p>

<code>select /*+ gather_plan_statistics */ count ( distinct junk ), count (*)
from  bricks
where  weight between 1 and 10;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>But this query searches for the first thousand BRICK_IDs returns 10% of the table's rows. Yet uses an index!</p>

<code>select /*+ gather_plan_statistics */ count ( distinct junk ), count (*)
from  bricks
where  brick_id between 1 and 1000;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>So why does the optimizer use an index for one query but not the other? It's all to do with how the rows are stored in the table.</p>",01-APR-20 10.12.09.403555000 AM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.05.04.836994000 PM,"CHRIS.SAXON@ORACLE.COM"
192606955056645004965780496855167840661,192497261674369450442611783127247019514,"Removing Documents from a Collection with SODA for PL/SQL",98,"This example removes the document whose document key is ""key1"". The removal status (1 if the document was removed; 0 if not) is returned and printed.

<code>

REM Removing Documents from a Collection with SODA for PL/SQL

DECLARE
    collection  SODA_COLLECTION_T;
    document    SODA_DOCUMENT_T;
    status      NUMBER;
    k           varchar2(255);
BEGIN
    -- Open the collection
    collection := DBMS_SODA.open_collection('myCollectionName');

    SELECT ID into k from ""myCollectionName"" where rownum < 2;

    -- Remove document that has key
    status := collection.find().key(k).remove;

    -- Count is 1 if  document was found
    IF status = 1 THEN
        DBMS_OUTPUT.put_line('Document was removed!');
    END IF;
END;
</code>",25-AUG-19 12.18.05.691878000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.45.07.008012000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
151280682522955198057799959673616213249,117675390209390608523249818520886328918,"Ranges of Values",45,"<p>You can also find all the rows matching a range of values with inequalities such as <, >=, etc.</p>

<p>For example, to find all the toys that cost less than 10, use:</p>

<code>select * from toys
where  price < 10;</code>

<p>Or those with a price greater than or equal to 6 with:</p>

<code>select * from toys
where  price >= 6;</code>

<p>You can also use the condition between. This returns rows with values from a lower to an upper bound. This is inclusive, so it returns rows with values matching either limit. So the following gets all the data with a price equal to 6, 20, or any value between these two:

<code>select * from toys
where  price between 6 and 20;</code>

<p>It is the same as the following query:</p>

<code>select * from toys
where  price >= 6
and    price <= 20;</code>

<p>If you want to exclude rows at either boundary, you need to write the separate tests. For example, to get all the rows where the price is greater than 6 and less than or equal to 20, use:</p>

<code>select * from toys
where  price > 6 
and    price <= 20;</code>",25-JUL-18 09.10.16.724018000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 08.19.15.215102000 AM,"CHRIS.SAXON@ORACLE.COM"
214165560201819255244816108117272721478,210639064323069817880325992049711663797,"Adaptive Plans",80,"<p>It can be tough to choose between nested loops and hash joins. Provided there is an index on the join columns, nested loops are fast when you fetch few rows from the first table.</p>

<p>But nested loops query the inner table once for each row from the outer table. As you fetch more rows from the first table, you quickly reach a point where it's faster to use a hash join. This fetches all the rows from the second table once.</p>

<p>There is only one row in the table where the NOTES store ""SQL is awesome!"". So this query uses nested loops:</p>

<code>select /*+ gather_plan_statistics */*
from   card_deck d1
join   card_deck d2
on     d1.val = d2.val
where  d1.notes = 'SQL is awesome!';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>Picking the best join method relies on the optimizer knowing how many rows have the value ""SQL is awesome!"". If the table stats are even slightly out-of-date, the optimizer may choose the ""wrong"" join.</p> 

<p>To handle this, Oracle Database 12c added adaptive plans. This is one plan which considers two different join methods. You can see this query uses an adaptive plan by looking at the Note section: </p>

<pre>Note
-----
   - this is an adaptive plan</pre>

<p>The database can then choose the best join method at runtime. It does this by looking at the actual number of rows the query processes.</p>

<p>To see which joins the optimizer considered, get the plan with the +ADAPTIVE format:</p>

<code>select /*+ gather_plan_statistics */*
from   card_deck d1
join   card_deck d2
on     d1.val = d2.val
where  d1.notes = 'SQL is awesome!';

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST +ADAPTIVE'));
</code>

<p>This has some key differences to a standard plan:</p>
<ul>
<li>The plan includes both NESTED LOOPS and HASH JOIN operations</li>
<li>There is a STATISTICS COLLECTOR operation above the first table in the plan</li>
<li>A minus ('-') appears before some of the steps in the plan</li>
</ul>

<p>The new information shows how the optimizer managed the plan.</p>

<h3>Statistics Collector</h3>

<p>This operation watches the number of rows flowing out of the table below it. Provided this number stays under a threshold, the optimizer will use nested loops. But if the number of rows exceeds this threshold, it'll switch to a hash join.</p>

<h3>Inactive Operations</h3>

<p>The minus before an operation indicates that the database did not execute this step. You can verify this further by noting that the Starts column reports zero for the full scan in step 8.</p>",18-MAR-20 09.47.01.786192000 AM,"CHRIS.SAXON@ORACLE.COM",26-MAY-20 03.15.10.290650000 PM,"CHRIS.SAXON@ORACLE.COM"
192500308251460355489529379666719811897,192497261674369450442611783127247019514,"Creating a Document with key and JSON Content",36,"This example is similar to the previous example, but it provides the document key (myKey) as well as the document content.

<code>
REM Creating a Document with key and JSON Content

DECLARE
    v_Doc  SODA_Document_T;
    b_Doc  SODA_Document_T;
    c_Doc  SODA_Document_T;
BEGIN
    -- Create VARCHAR2 document
    v_Doc := SODA_Document_T('myKey' , v_Content => '{""name"" : ""Alexander""}');
    dbms_output.put_Line('Varchar2 Doc key: ' || v_Doc.get_Key);
    dbms_output.put_Line('Varchar2 Doc content: ' || v_Doc.get_Varchar2);
        
    -- Create BLOB document
    b_Doc := SODA_Document_T('myKey' , b_Content => utl_raw.cast_to_raw('{""name"" : ""Alexander""}'));
    dbms_output.put_Line('Blob Doc key: ' || b_Doc.get_Key);
    dbms_output.put_Line('Blob Doc content: ' || utl_raw.cast_to_varchar2(b_Doc.get_Blob));
    
    -- Create CLOB document
    c_Doc := SODA_Document_T('myKey' , c_Content => '{""name"" : ""Alexander""}');
    dbms_output.put_Line('Clob Doc key: ' || c_Doc.get_Key);
    dbms_output.put_Line('Clob Doc content: ' || c_Doc.get_Clob);
END;
</code>",24-AUG-19 12.24.47.671780000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",24-AUG-19 11.56.33.976752000 PM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
192500308251474862599364755216816286009,192497261674369450442611783127247019514,"Finding a unique Document that has a given key",66,"The following example finds a unique document using a key
<code>

REM Finding a unique Document that has a given key

DECLARE
    collection  SODA_Collection_T;
    document    SODA_Document_T;
    key         VARCHAR2(255);
BEGIN
    -- Open the collection
    collection := dbms_soda.open_Collection('myCollectionName');

    SELECT ID into key from ""myCollectionName"" where rownum < 2;

    -- Find a document using a key
    document := collection.find_One(key);

    IF document IS NOT NULL THEN
        dbms_output.put_line('Document components:');
        dbms_output.put_line('Key: ' || document.get_Key);
        dbms_output.put_line('Content: ' || utl_raw.cast_to_varchar2(document.get_Blob));
        dbms_output.put_line('Creation timestamp: ' || document.get_Created_On);
        dbms_output.put_line('Last modified timestamp: ' || document.get_Last_Modified);
        dbms_output.put_line('Version: ' || document.get_Version);
    END IF;
END;

</code>",24-AUG-19 12.26.43.252334000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 05.15.25.920386000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
214896305693854365375075094766344083341,210639064323015416218443333736849885877,"Try It!",30,"<p>Replace /*TODO*/ section in the call to DBMS_XPlan to see how many buffers were needed to run this query:</p>
<code>select /*+ gather_plan_statistics */ *
from   colours c
join   bricks b
on     c.colour = b.colour
order  by b.brick_id
fetch first 1 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => '/*TODO*/'));</code>",25-MAR-20 09.50.25.487368000 AM,"CHRIS.SAXON@ORACLE.COM",24-APR-20 09.21.18.618094000 AM,"CHRIS.SAXON@ORACLE.COM"
215717035372209043382524514696006647309,210639064323027505476639480028596947637,"Physical Row Location",7,"<p>Oracle Database stores rows in blocks. You can find the block number for a row using DBMS_rowid:</p>

<code>select brick_id, 
       dbms_rowid.rowid_block_number ( rowid ) blk# 
from   bricks
where  mod ( brick_id, 100 ) = 0;</code>

<p>By default, tables are heap organized in Oracle Database. This means that the database is free to place rows wherever it wants.</p>

<p>But indexes are ordered data structures. New entries must go in the correct location. For example, if you insert 42 in a number column, this entry must go between the last entry of 41 or lower and the first entry of 43 or higher in any indexes on that column.</p>

<p>The closer the physical order of rows matches the logical order of an index, the more effective that index will be.</p>

<p>The smallest unit of I/O in Oracle Database is a block. So the more consecutive index entries there are that point to the same block, the more rows you can fetch in one I/O. Thus the more effective the index will be.</p>

<p>This query build a matrix of BRICK_IDs and blocks, counting the number of rows in the block and brick ranges:</p>

<code>with rws as ( 
  select ceil ( brick_id / 1000 ) id, 
         ceil ( 
           dense_rank () over (
             order by dbms_rowid.rowid_block_number ( rowid )
           ) / 10 
         ) rid
  from   bricks
)
  select * from rws
  pivot (
    count (*) for rid in (
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
    )
  )
  order by id;</code>

<p>You can see the values for BRICK_ID are well clustered. All the rows for a given ID range are all in the same block range. There are many block ranges with zero rows for an ID range.</p>

<p>Contrast this with the weights. Plugging these values into the matrix shows weight values are spread throughout the whole table:</p>
  
<code>with rws as ( 
  select ceil ( weight / 100 ) wt, 
         ceil ( 
           dense_rank () over (
             order by dbms_rowid.rowid_block_number ( rowid )
           ) / 10 
         ) rid
  from   bricks
)
  select * from rws
  pivot (
    count (*) for rid in (
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
    )
  )
  order by wt;</code> 

<p>The rows for the first weight range are stored in every block in the table. Whereas the rows for the first ID range are in 3-4 blocks.</p>

<p>This means the database will have to do lots more I/O to fetch the same number of rows for a range of weights compared to a range of identifiers. Thus an index on WEIGHT will be less effective than on BRICK_ID.</p>

<p><strong>When determining how effective an index is, it's <em>the number of I/O operations that matter</em>. Not how many rows it accesses!</strong></p>

<p>So how does the optimizer know how closely the logical and physical orders match?</p>

<p>It estimates using the <strong>clustering factor</strong>.</p>",02-APR-20 08.35.33.006508000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.02.14.187085000 PM,"CHRIS.SAXON@ORACLE.COM"
213935825083645220557972407525812398076,210639064323027505476639480028596947637,"Attribute Clustering",80,"<p>Attribute clustering defines a physical order for rows. Using a linear order sorts the values in the same way as ORDER BY; it works through the columns left-to-right, only using the next column to resolve ties.</p>

<p>This adds linear clustering by INSERT_DATE. So the database will sort the rows by this column:</p>

<code>alter table bricks
  add clustering 
  by linear order ( insert_date );</code>

<p>But this has no effect on existing data. Only rows added using direct-path inserts use this ordering. To apply it to current rows, move the table:</p>
  
<code>select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';

alter table bricks
  move online;
  
exec dbms_stats.gather_table_stats ( null, 'bricks' ) ;

select index_name, clustering_factor 
from   user_indexes
where  table_name = 'BRICKS';
</code>

<p>Linear sorting gives perfect clustering for the leading column of the cluster. Along with any other columns highly correlated with these values.</p>

<p>But it's rare all queries on a table use the same column in the where clause. So sorting on one column improves the performance of these queries, at the cost of queries on other columns.</p>

<p>Usually there are several key queries, each filtering on different columns. You want to ensure all these columns can use an index if possible. To help with this it's possible to <em>interleave</em> the values from two or more columns.</p>",16-MAR-20 09.58.07.211835000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.11.43.883798000 PM,"CHRIS.SAXON@ORACLE.COM"
216143811563909017809988443203804226832,210640540520278905448644863263597314383,"Try It!",25,"<p>Replace /* TODO */ to create an MV that stores the count of rows for each shape:</p>

<code>create materialized view brick_shapes_mv 
as
  select /* TODO */
  from   bricks
  group  by shape;

select * from brick_shapes_mv;</code>",06-APR-20 08.30.10.216411000 AM,"CHRIS.SAXON@ORACLE.COM",06-APR-20 04.11.19.174545000 PM,"CHRIS.SAXON@ORACLE.COM"
214087312489809993997119063796499494936,210640540520278905448644863263597314383,"Using Stale Data",50,"<p>It's possible to let the optimizer use stale materialized views for query rewrite. Do this by changing the QUERY_REWRITE_INTEGRITY parameter. This supports these values:</p>
<ul>
<li>ENFORCED - there must be an exact match between table and MV data for a rewrite to happen. This is the default.</li>
<li>TRUSTED - the database can do rewrites based on declared constraints that are disabled. This setting rarely affects MV rewrites.</li>
<li>STALE_TOLERATED - the optimizer can do a query rewrite using stale MVs</li>
</ul>

<p>Changing this parameter enables the optimizer to use an MV:</p>

<code>alter session set query_rewrite_integrity = stale_tolerated;

select /* stale */count(*) c
from   bricks;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'BASIC LAST'));

alter session set query_rewrite_integrity = enforced;

select /* enforced */count(*)
from   bricks;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'BASIC LAST'));</code>

<p>Note that the queries now return different results!</p>

<p>While this trick is handy in some cases, usually you want fresh data in an MV. So you need to bring the MV up-to-date.</p>",17-MAR-20 04.22.23.772838000 PM,"CHRIS.SAXON@ORACLE.COM",20-MAY-20 02.53.29.275997000 PM,"CHRIS.SAXON@ORACLE.COM"
216249454625471084580308005227430817095,210639064323069817880325992049711663797,"Introduction",5,"<p>When you join tables, the optimizer has to decide how to combine the data sets. There are three key algorithms to do this:</p>

<ul>
<li>Hash Joins</li>
<li>(Sort) Merge Joins</li>
<li>Nested Loops</li>
</ul>

<p>This tutorial gives examples of these by joining a table of playing cards to itself in various ways:</p>

<code>select * from card_deck;</code>",07-APR-20 09.11.25.522037000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 04.05.48.724019000 PM,"CHRIS.SAXON@ORACLE.COM"
214270963991032817097905283439400727946,210639064323069817880325992049711663797,"Merge Join Cartesian",35,"<p>This query does a cross join - linking every row in the first table to every row in the second table. This uses a Cartesian merge join:</p>

<code>select /*+ gather_plan_statistics */count(*)
from   card_deck d1
cross join card_deck d2;

select * 
from   table(dbms_xplan.display_cursor ( :LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 
<p>The optimizer considers a merge join Cartesian if:</p>
<ul>
<li>It's joining every row in one table to every row in another (a cross join)</li>
<li>It expects to get exactly one row from the first table and join it to every row in the second table</li>
<li>It may generate the Cartesian product of two tiny tables before joining these to a huge table</li>
</ul>
<p>It's rare to see this join method. And it's often a sign of a problem. For example missing join criteria or out-of-date statistics.</p>

<p>If you see this double-check your join criteria and table stats. If a join is missing, add it!</p>

<p>If you get this because the optimizer estimates one row for the first table, check this is correct. Look at the actual number of rows fetched from this table. If this is greater than one - even just two or three rows, Cartesian joins can take hours to complete! When this happens see if you can tweak the stats to improve row estimates.</p>",19-MAR-20 10.01.58.343651000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.29.29.953585000 PM,"CHRIS.SAXON@ORACLE.COM"
192500308251536517816165101304726300985,192497261674369450442611783127247019514,"Creating Dataguide enabled Search Index",116,"This example indexes the documents in Employees collection for ad hoc queries and full-text search (queries using QBE operator $contains), and it automatically accumulates and updates data-guide information about your JSON documents (aggregate structural and type information). The index specification has only field name (no field fields).

<code>
REM Creating Dataguide enabled Search Index

-- Now, create an index
DECLARE
  spec VARCHAR2(100);
  coll SODA_Collection_T;
  n number;
BEGIN
  spec := '{""name"" : ""SODAPLSDG_txt"",
            ""dataguide"" : ""on"",
            ""search_on"" : ""text_value""}';
  coll := dbms_soda.open_Collection('Employees');

  -- Drop the index
  n := coll.drop_Index('SODAPLSDG_txt');
  dbms_output.put_Line('Drop status: ' || n);

  -- Create the index
  n := coll.create_Index(spec);
  dbms_output.put_line('Create status: ' || n);
END;
/

</code>",24-AUG-19 12.33.25.952632000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM",25-AUG-19 12.25.47.549014000 AM,"SRIKRISHNAN.S.SURESH@ORACLE.COM"
214792989997933581911151270488852578484,210639799513641285543570732327153347316,"Plan Invalidation",21,"<p>Gathering stats on a table may change the optimizer's row estimates for queries against it. If you have lots of queries accessing a table, updating stats may cause the database to have to create a new plan for them all. This adds significant workload to your database!</p>

<p>So optimizer decides when to invalidate the cursors. This means you may not see plan changes immediately after gathering stats on a table.</p>

<p>To force the optimizer to check queries for new plans immediately, set NO_INVALIDATE to FALSE when calling gather stats:</p>

<code>exec dbms_stats.gather_table_stats ( null, 'colours', no_invalidate => false ) ;
</code> 

<p>When getting plans in these tutorials you may find DBMS_XPlan reporting the old, ""wrong"" plan. In these cases making small changes to the query (e.g. adding column aliases) forces the optimizer to reparse the query.</p>

<p>Remember: invalidating cursors when gathering stats this will cause the optimizer to reparse all queries on this table. <strong>Use with caution in production environments!</strong></p>",24-MAR-20 10.11.22.877761000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.46.01.105671000 PM,"CHRIS.SAXON@ORACLE.COM"
155038910858198418617615411346003492762,152848903946926018441986284250590889613,"Supertypes and Subtypes",75,"<p>At this point you may note that we've stored people's names in both the consultant and patient tables. And a person could be both a consultant and a patient! This can lead to recording different names for the same person.</p>

<p>To avoid this, scrap the patient and consultant tables. And create a single table to store people's details instead. For example:</p>

<code>create table people (
  person_id integer,
  full_name varchar2(100)
);</code>

<p>Now we have a single place to record all information about people. But there may be details specific to either consultants or patients. For example, consultants have a salary, speciality, and so on. And patients may have a hospital number, etc.</p>

<p>If so, you can create these tables.</p>

<code>create table consultants (
  consultant_id  integer,
  salary         number(10,2),
  speciality     varchar2(30)
);

create table patients (
  patient_id      integer,
  hopsital_number integer
);</code>

<p>So we have a supertype/subtype relationship. A supertype is a generalization. It stores attributes common to all the subtype tables below it. A subtype is a specialization. It stores attributes specific to this instance of the parent table above it.</p>

<p>So person is a supertype of consultant and patient. It stores details common to everyone, such as their name and birth date. The consultant and patient tables are subtypes of people. These only store details specific to people who are a consultant or patient.</p>

<p>In the current requirements, there is no need for the subtype tables. So you need to review your needs to determine when you need to combine entities into a supertype. Or split them into subtypes.</p>

<p>Here are some guidelines for determining whether to combine or split tables:</p>

<ul>
<li>If two or more tables have the same columns which store the same information, you're likely missing a supertype. Consider merging them into a single table</li>
<li>If one table has columns which are only apply if the row is of a certain type, consider splitting these out into subtypes</li>
</ul>

<p>Again, identifying supertypes & subtypes is an iterative process. As you build the database, you may find you need to split a table into subtypes. Or merge two tables into one. Knowing what your application will do and store is key to choosing the correct design.</p>",30-AUG-18 08.10.37.974265000 AM,"CHRIS.SAXON@ORACLE.COM",14-SEP-18 08.20.33.877369000 AM,"CHRIS.SAXON@ORACLE.COM"
155038910858205672172533099121051729818,152848903946926018441986284250590889613,"Normalization",60,"<p>As described above, normalization is the process of removing redundancy in your design. So you store each fact once. This stops data errors appearing.</p>

<p>Normal forms are numbered, starting with first normal form. Followed by second, third, etc. up to fifth. These are usually referred to by their abbreviation, xNF where x is its number. So first normal form is 1NF and so on.</p>

<p>There are a few other normal forms. The most common is Boyce-Codd normal form. This is a refinement of 3NF. So it is sometimes called 3.5NF.</p>

<p>To be in a normal form, you must meet its requirements and those of the forms lower than it. So to reach 3NF, your tables must also be in 1NF & 2NF.</p>

<p>A full discussion of normal forms is outside the scope of this tutorial. For more details on these, read this <a href=""http://www.bkent.net/Doc/simple5.htm"">simple guide to five normal forms</a>.</p>",30-AUG-18 08.10.52.244559000 AM,"CHRIS.SAXON@ORACLE.COM",14-SEP-18 03.00.44.571721000 PM,"CHRIS.SAXON@ORACLE.COM"
234739286354384358581350052980350140055,234738183173691734382399251082742733633,"Running your first SQL Macro",40,"<p>Now let's run the macro we just created...</p>
<code>
SELECT * 
FROM total_sales(zip_code => '60332');
</code>
<br>
<br>
<p>This should return a table showing a single row for zip code 60332 with total sales revenue of 181143.93.
</p>",01-OCT-20 08.52.55.263977000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 08.52.55.263999000 AM,"KEITH.LAKER@ORACLE.COM"
182324141533866830453306492113193166767,182241645422959190146194127811898911783,"Dynamic SQL and FORALL",140,"<p>
Have you got a dynamic update, insert or delete sitting inside a loop? Don't worry....FORALL will <i>still</i> come to the rescue! Yes, that's right, you can put an EXECUTE IMMEDIATE statement inside FORALL, as well as static SQL. Let's take a look.
</p>
<code>CREATE OR REPLACE TYPE numlist IS TABLE OF NUMBER
/

CREATE OR REPLACE TYPE namelist IS TABLE OF VARCHAR2 (15)
/

CREATE OR REPLACE PROCEDURE update_emps (col_in      IN VARCHAR2
                                       , empnos_in   IN numlist)
IS
   enames   namelist;
BEGIN
   FORALL indx IN empnos_in.FIRST .. empnos_in.LAST
      EXECUTE IMMEDIATE
            'UPDATE employees SET '
         || col_in
         || ' = '
         || col_in
         || ' * 1.1 WHERE employee_id = :1
         RETURNING last_name INTO :2'
         USING empnos_in (indx)
         RETURNING BULK COLLECT INTO enames;

   FOR indx IN 1 .. enames.COUNT
   LOOP
      DBMS_OUTPUT.put_line ('10% raise to ' || enames (indx));
   END LOOP;
END;
/

DECLARE
   l_ids   numlist := numlist (138, 147);
BEGIN
   update_emps ('salary', l_ids);
END;
/
</code>
<p>
Notice that I also used BULK COLLECT inside my FORALL statement, since I wanted get back from FORALL the last names of employees who got a raise. You need to include the RETURNING clause <i>inside</i> the dynamic DML statement <i>and</i> in the EXECUTE IMMEDIATE statement itself.
</p>
<p>
The bottom line: if you can execute the DML statement statically, you can do it dynamically, too. Same FORALL syntax, just a different way of expressing the SQL statement inside it.
</p>",18-MAY-19 01.37.20.457178000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",30-MAY-19 06.24.14.299165000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247472530045544141869522262338231017,182241645422959190146194127811898911783,"Cursor FOR Loops and BULK COLLECT",50,"<p>
When should you convert a non-bulk query to one using BULK COLLECT? More specifically, should you convert a cursor FOR loop to an explicit cursor and FETCH BULK COLLECT with limit? Here are some things to keep in mind:
</P>
<ul><li>
As long as your PL/SQL optimization level is set to 2 (the default) or higher, the compiler will automatically optimize cursor FOR loops to retrieve 100 rows with each fetch. You cannot modify this number.
</li><li>
If your cursor FOR loop is ""read only"" (it does not execute non-query DML), then you can probably leave it as is. That is, fetching 100 rows with each fetch will usually give you sufficient improvements in performance over row-by-row fetching. 
</li><li>
Only cursor FOR loops are optimized this way, so if you have a simple or WHILE loop that fetches individual rows, you should convert to BULK COLLECT - with LIMIT!
</li><li>
If you are fetching a very large number of rows, such as might happen with data warehouse processing or a nightly batch process, then you should experiment with larger LIMIT values to see what kind of ""bang for the buck"" you will get.
</li><li>
If your cursor FOR loop (or any other kind of loop for that matter) contains one or more non-query DML statements (insert, update, delete, merge), you should convert to BULK COLLECT and FORALL.
</li></ul>
<p>
Run the following code to see how optimization affects cursor FOR loop performance.
</p>
<code>CREATE OR REPLACE PROCEDURE test_cursor_performance (approach IN VARCHAR2)
IS
   CURSOR cur IS
      SELECT * FROM all_source WHERE ROWNUM < 100001;

   one_row cur%ROWTYPE;

   TYPE t IS TABLE OF cur%ROWTYPE INDEX BY PLS_INTEGER;

   many_rows     t;
   last_timing   NUMBER;
   cntr number := 0;

   PROCEDURE start_timer
   IS
   BEGIN
      last_timing := DBMS_UTILITY.get_cpu_time;
   END;

   PROCEDURE show_elapsed_time (message_in IN VARCHAR2 := NULL)
   IS
   BEGIN
      DBMS_OUTPUT.put_line (
            '""'
         || message_in
         || '"" completed in: '
         || TO_CHAR (
               ROUND ( (DBMS_UTILITY.get_cpu_time - last_timing) / 100, 2)));
   END;
BEGIN
   start_timer;

   CASE approach
      WHEN 'implicit cursor for loop'
      THEN
         FOR j IN cur
         LOOP
            cntr := cntr + 1;
         END LOOP;

         DBMS_OUTPUT.put_line (cntr);

      WHEN 'explicit open, fetch, close'
      THEN
         OPEN cur;

         LOOP
            FETCH cur INTO one_row;
            EXIT WHEN cur%NOTFOUND;
            cntr := cntr + 1;
         END LOOP;

         DBMS_OUTPUT.put_line (cntr);

         CLOSE cur;
      WHEN 'bulk fetch'
      THEN
         OPEN cur;

         LOOP
            FETCH cur BULK COLLECT INTO many_rows LIMIT 100;
            EXIT WHEN many_rows.COUNT () = 0;

            FOR indx IN 1 .. many_rows.COUNT
            loop
               cntr := cntr + 1;
            end loop;
         END LOOP;

         DBMS_OUTPUT.put_line (cntr);

         CLOSE cur;
   END CASE;

   show_elapsed_time (approach);
END test_cursor_performance;
/

/* Try different approaches with optimization disabled. */

ALTER PROCEDURE test_cursor_performance
COMPILE plsql_optimize_level=0
/

BEGIN
   dbms_output.put_line ('No optimization...');
   
   test_cursor_performance ('implicit cursor for loop');

   test_cursor_performance ('explicit open, fetch, close');

   test_cursor_performance ('bulk fetch');
END;
/

/* Try different approaches with default optimization. */

ALTER PROCEDURE test_cursor_performance
COMPILE plsql_optimize_level=2
/

BEGIN
   DBMS_OUTPUT.put_line ('Default optimization...');

   test_cursor_performance ('implicit cursor for loop');

   test_cursor_performance ('explicit open, fetch, close');

   test_cursor_performance ('bulk fetch');
END;
/
</code>
<p><strong>Exercise 3</strong></p>
<p>
This exercise has two parts (and for this exercise assume that the employees table has 1M rows with data distributed equally amongst departments: (1) Write an anonymous block that contains a cursor FOR loop that does <i>not</i> need to be converted to using BULK COLLECT. (2) Write an anonymous block that contains a cursor FOR loop that <i>does</i> need to use BULK COLLECT (assume it cannot be rewritten in ""pure"" SQL).
</p>",17-MAY-19 08.00.28.300014000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",10-JUN-19 04.31.45.323084000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
188697542692611054641893933528916874655,188704173216096418598025228629326881042,"Scenario",20,"<p>
MyCompany has several major warehouses. It needs to locate its customers who are near a given warehouse, to inform them of new advertising promotions. To locate its customers and perform location-based analysis, MyCompany must store location data for both its customers and warehouses.
<p>
This tutorial uses CUSTOMERS and WAREHOUSES tables. WAREHOUSES are created from scratch. CUSTOMERS are copied from the OE schema that is available in LiveSQL. 
<p>
Each table stores location using Oracle's native spatial data type, SDO_GEOMETRY. A location can be stored as a point in an SDO_GEOMETRY column of a table. The customer's location is associated with longitude and latitude values on the Earth's surfaceâ€”for example, -63.13631, 52.485426.
<p>",18-JUL-19 03.39.26.531121000 PM,"DAVID.LAPP@ORACLE.COM",05-AUG-19 07.12.52.614290000 PM,"DAVID.LAPP@ORACLE.COM"
151072222469028697438775880437507830934,117123875509271941312470520788803021948,"Try It!",25,"<p>Complete the following statement to create the index-organized table bricks_iot:</p>

<code>create table bricks_iot (
  bricks_id integer primary key
) /*TODO*/;

select table_name, iot_type
from   user_tables
where  table_name = 'BRICKS_IOT';</code>

<p>The query afterwards should return the following row:</p>

<pre><b>TABLE_NAME   IOT_TYPE   </b>
BRICKS_IOT   IOT    </pre>",23-JUL-18 08.49.55.461303000 AM,"CHRIS.SAXON@ORACLE.COM",06-AUG-18 08.55.31.265295000 AM,"CHRIS.SAXON@ORACLE.COM"
151072222469353898484252215685503792278,117123875509271941312470520788803021948,"Try It!",55,"<p>Complete the following statement to create a hash-partitioned table. This should be partitioned on brick_id and have 8 partitions:</p>

<code>create table bricks_hash (
  brick_id integer
) partition by /*TODO*/;

select table_name, partitioned 
from   user_tables
where  table_name = 'BRICKS_HASH';</code>

<p>The query should return the following row:</p>

<pre><b>TABLE_NAME    PARTITIONED   </b>
BRICKS_HASH   YES   </pre>",23-JUL-18 08.52.44.471448000 AM,"CHRIS.SAXON@ORACLE.COM",27-DEC-18 07.25.14.488241000 PM,"SHARON.KENNEDY@ORACLE.COM"
160219058982673567295653553960286574928,160204041387679825048250629187623908505,"Generating XML Content from Relational Data",6,"<p>You can use standard SQL/XML functions to generate one or more XML documents.</p>
<p><strong>Using XQUERY</strong></p>
<p>Q1. An XQuery showing how fn:collection can be used to generate 
a simple XML document from each row in a relational table (i.e. DEPARTMENTS here).</p>

<code>
SELECT xmlserialize(document column_value AS clob indent size=2)
FROM   xmltable ( 'fn:collection(""oradb:/HR/DEPARTMENTS"")' )
WHERE  rownum < 3 
/ 
</code>

<p>Q2. Using XQuery and fn:collection to create XML documents 
from more than one relational table.</p>

<code>
SELECT D.object_value.getclobval()
FROM   XMLTable
       (
         'for $d in fn:collection(""oradb:/HR/DEPARTMENTS"")/ROW, 
		          $l in fn:collection(""oradb:/HR/LOCATIONS"")/ROW,
		          $c in fn:collection(""oradb:/HR/COUNTRIES"")/ROW
	   					where $d/LOCATION_ID = $l/LOCATION_ID
								and $l/COUNTRY_ID = $c/COUNTRY_ID 
	   					return
	   						<Department DepartmentId= ""{$d/DEPARTMENT_ID/text()}"" >
									<Name>{$d/DEPARTMENT_NAME/text()}</Name>
									<Location>
		  							<Address xsi:type=""address_DE"">{$l/STREET_ADDRESS/text()}</Address>
		  							<City>{$l/CITY/text()}</City>
		  							<State>{$l/STATE_PROVINCE/text()}</State>
		  							<Zip>{$l/POSTAL_CODE/text()}</Zip>
		  							<Country>{$c/COUNTRY_NAME/text()}</Country>
									</Location>
									<EmployeeList>
									{
		   							for $e in fn:collection(""oradb:/HR/EMPLOYEES"")/ROW,
		  	 								$m in fn:collection(""oradb:/HR/EMPLOYEES"")/ROW,
			 									$j in fn:collection(""oradb:/HR/JOBS"")/ROW
		   									where $e/DEPARTMENT_ID = $d/DEPARTMENT_ID
		      								and $j/JOB_ID = $e/JOB_ID
													and $m/EMPLOYEE_ID = $e/MANAGER_ID
		   										return
		   											<Employee employeeNumber=""{$e/EMPLOYEE_ID/text()}"" >
															<FirstName>{$e/FIRST_NAME/text()}</FirstName>
															<LastName>{$e/LAST_NAME/text()}</LastName>
															<EmailAddress>{$e/EMAIL/text()}</EmailAddress>
															<Telephone>{$e/PHONE_NUMBER/text()}</Telephone>
															<StartDate>{$e/HIRE_DATE/text()}</StartDate>
															<JobTitle>{$j/JOB_TITLE/text()}</JobTitle>
															<Salary>{$e/SALARY/text()}</Salary>
															<Manager>{$m/LAST_NAME/text(), "", "", $m/FIRST_NAME/text()}</Manager>
		      										<Commission>{$e/COMMISSION_PCT/text()}</Commission>
		    										</Employee>
		 							}
		 							</EmployeeList>
	    					</Department>'
	    	) D
WHERE rownum < 3
/
</code>

<p><strong>Using SQLXML</strong></p>
<p>Generating XML data from DEPARTMENTS, HR.COUNTRIES c, HR.LOCATIONS relational tables: </p>
<code>
SELECT xmlelement ( ""Department"", xmlattributes( d.department_id as ""DepartmentId""), xmlelement(""Name"", d.department_name), xmlelement ( ""Location"", xmlforest ( street_address AS ""Address"", city AS ""City"", state_province AS ""State"", postal_code AS ""Zip"",country_name AS ""Country"" ) ), xmlelement ( ""EmployeeList"",
       (
              SELECT xmlagg ( xmlelement ( ""Employee"", xmlattributes ( e.employee_id AS ""employeeNumber"" ), xmlforest ( e.first_name AS ""FirstName"", e.last_name AS ""LastName"", e.email AS ""EmailAddress"", e.phone_number AS ""Telephone"", e.hire_date AS ""StartDate"", j.job_title AS ""JobTitle"", e.salary AS ""Salary"", m.first_name
                            || ' '
                            || m.last_name AS ""Manager"" ), xmlelement ( ""Commission"", e.commission_pct ) ) )
              FROM   hr.employees e,
                     hr.employees m,
                     hr.jobs j
              WHERE  e.department_id = d.department_id
              AND    j.job_id = e.job_id
              AND    m.employee_id = e.manager_id ) ) ).getclobval() AS xml
FROM   hr.departments d,
       hr.countries c,
       hr.locations l
WHERE  d.location_id = l.location_id
AND    l.country_id = c.country_id
AND    rownum < 3 
/ 
</code>",18-OCT-18 10.11.38.588337000 PM,"HARICHANDAN.ROY@ORACLE.COM",19-OCT-18 10.29.05.443575000 PM,"HARICHANDAN.ROY@ORACLE.COM"
230371332073620509540520686611755317774,92046253613416673541249082253875246934,"Joining Hierarchies, Analytic Views and Tables",120,"<p>Hierarchies and analytic views can be joined to tables just like any other object that can be queried in the database.</p>.

<p>Hierarchies are generally referenced in the FROM clause just like any other table or views.  Note that in this example values in the TIME_HIER hierarchy are filtered to the Month level because the hierarchy joins the TIME_DIM table at the month level</p>

<code>
SELECT
    t.year_name,
    p.department_name,
    g.region_name,
    SUM(f.sales) AS sales
FROM
    time_hier          t,              -- Hierarchy
    av.product_dim     p,              -- Table
    av.geography_dim   g,              -- Table
    av.sales_fact      f               -- Table
WHERE
    t.month_id = f.month_id            -- Table joins to hierarchy
    AND t.level_name = 'MONTH'         -- Months only from hierarchy
    AND p.category_id = f.category_id
    AND g.state_province_id = f.state_province_id
GROUP BY
    t.year_name,
    p.department_name,
    g.region_name;
</code>

<p> When joining tables and analytic views it is usually best to use a WITH clause.  This example aggregates from the table and then joins to the analytic view at the aggregate levels.  The is probably the most common use case.</p>

<code>
-- SELECT from the table.
WITH my_table_query AS (
    SELECT
        t.year_name,
        p.department_name,
        g.region_name,
        SUM(f.sales) AS sales
    FROM
        av.time_dim        t,
        av.product_dim     p,
        av.geography_dim   g,
        av.sales_fact      f
    WHERE
        t.month_id = f.month_id
        AND p.category_id = f.category_id
        AND g.state_province_id = f.state_province_id
    GROUP BY
        t.year_name,
        p.department_name,
        g.region_name
),
-- SELECT from the analytic view.
  my_av_query AS (
    SELECT
        time_hier.member_name        AS time,
        product_hier.member_name     AS product,
        geography_hier.member_name   AS geography,
        sales_prior_period
    FROM
        sales_av HIERARCHIES (
            time_hier,
            product_hier,
            geography_hier
        )
    WHERE
        time_hier.level_name = 'YEAR'
        AND product_hier.level_name = 'DEPARTMENT'
        AND geography_hier.level_name = 'REGION'
)
-- Final select
SELECT
    a.year_name,
    a.department_name,
    a.region_name,
    a.sales,
    b.sales_prior_period
FROM
    my_table_query   a,
    my_av_query      b
WHERE
    a.year_name = b.time
    AND a.department_name = b.product
    AND a.region_name = b.geography;
</code>

<p>In the next example, the SEASON column is joined into the analytic view query from the TIME_DIM table using the WITH clause.  (Note that this sample data does not include all months. 
 The query works as expected.)</p>

<code>
WITH my_av_query AS (
    SELECT
        time_hier.member_name        AS time,
        product_hier.member_name     AS product,
        geography_hier.member_name   AS geography,
        sales_prior_period
    FROM
        sales_av HIERARCHIES (
            time_hier,
            product_hier,
            geography_hier
        )
    WHERE
        time_hier.level_name = 'MONTH'
        AND month_name = month_name
        AND time_hier.year_name = 'CY2014'
        AND product_hier.level_name = 'DEPARTMENT'
        AND geography_hier.level_name = 'REGION'
)
SELECT
    a.time,
    b.season,
    a.product,
    a.geography,
    a.sales_prior_period
FROM
    my_av_query a,
    av.time_dim b
WHERE a.time = b.month_name;
</code>",20-AUG-20 01.50.48.438282000 PM,"WILLIAM.ENDRESS@ORACLE.COM",20-AUG-20 05.28.42.933513000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
234736916666784748103754388621503404538,234738183173691734382399251082742733633,"Why Not Use PL/SQL?",20,"<p><span>In the past, if you wanted to extend the&nbsp;capabilities of&nbsp;SQL, then PL/SQL was the usually the best&nbsp;way to do add additional, more complex&nbsp;processing logic. However, by using PL/SQL this created some underlying complexities at execution time since we needed to keep swapping between the SQL context and the PL/SQL context. This tended to have an impact on performance and most people tried to avoid doing it whenever possible.&nbsp;</span></p>

<h3>Why Is A SQL Macro Better Than A PL/SQL Function?</h3>

<p>SQL Macros have an important advantage over&nbsp;ordinary PL/SQL functions in that they&nbsp;make the&nbsp;reusable SQL code completely&nbsp;transparent to the Optimizer &ndash; and that brings big benefits! It makes it&nbsp;possible for the optimizer to transform the original&nbsp;code for efficient execution because the underlying&nbsp;query inside the&nbsp;macro function can be merged into outer query. That means there is&nbsp;no context switching between PL/SQL and SQL and the query inside the&nbsp;macro function is now executed under same snapshot as outer query. So we get both simplicity and faster execution.<br />
</p>",01-OCT-20 08.44.38.283554000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.51.32.673945000 AM,"KEITH.LAKER@ORACLE.COM"
160212874467271505630764995079043706531,160204041387679825048250629187623908505,"Creating and Querying Relational Views of XML Content",4,"<p><strong>Creating Relational Views</strong></p>
<p>V1. Creating a Master View, from elements that occur at most once per document:</p>

<code>
CREATE OR replace VIEW purchaseorder_master_view
AS
  SELECT m.*
  FROM   purchaseorder p,
         XMLTABLE ( '$p/PurchaseOrder' passing p.object_value AS ""p"" COLUMNS
         reference
         path 'Reference/text()', requestor path 'Requestor/text()', userid path
         'User/text()', costcenter path 'CostCenter/text()', ship_to_name path
         'ShippingInstructions/name/text()', ship_to_street path
         'ShippingInstructions/Address/street/text()', ship_to_city path
         'ShippingInstructions/Address/city/text()', ship_to_county path
         'ShippingInstructions/Address/county/text()', ship_to_postcode path
         'ShippingInstructions/Address/postcode/text()', ship_to_state path
         'ShippingInstructions/Address/state/text()', ship_to_province path
         'ShippingInstructions/Address/province/text()', ship_to_zip path
         'ShippingInstructions/Address/zipCode/text()', ship_to_country path
         'ShippingInstructions/Address/country/text()', ship_to_phone path
         'ShippingInstructions/telephone/text()', instructions path
         'SpecialInstructions/text()' ) m

/  
</code>

<p>V2. Creating a Detail View, from the contents of the LineItem collection. LineItem can occur more than once is a document. The rows in this view can be joined with the rows in the previous view using REFERENCE, which is common to both views.</p>

<code>
CREATE OR replace VIEW purchaseorder_detail_view
AS
  SELECT m.reference,
         l.*
  FROM   purchaseorder p,
         XMLTABLE ( '$p/PurchaseOrder' passing p.object_value AS ""p"" COLUMNS
         reference
         path 'Reference/text()', lineitems xmltype path 'LineItems/LineItem' )
         m,
         XMLTABLE ( '$l/LineItem' passing m.lineitems AS ""l"" COLUMNS itemno path
         '@ItemNumber', description path 'Part/@Description', partno path
         'Part/text()',
         quantity path 'Quantity', unitprice path 'Part/@UnitPrice' ) l

/  
</code>

<p><strong>Querying Over Relational Views</strong></p>
<p>Q1. Execute a simple SQL query over the relational view of XML content showing the use of SQL Group By. Note- XQuery 1.0 does not support the concept of group by.</p>

<code>
SELECT costcenter,
       Count(*)
FROM   purchaseorder_master_view
WHERE  ROWNUM <= 5
GROUP  BY costcenter

/  
</code>

<p>Q2. A simple Query showing a join between the master and detail views with relational predicates on both views.</p>

<code>
SELECT m.reference,
       instructions,
       itemno,
       partno,
       description,
       quantity,
       unitprice
FROM   purchaseorder_master_view m,
       purchaseorder_detail_view d
WHERE  m.reference = d.reference
       AND m.requestor = 'A. Cabrio 1'
       AND d.quantity > 0
       AND D.unitprice > 17.00
       AND ROWNUM <= 5
/  
</code>

<p>Q3. A simple Query showing a join between the master and detail views with relational predicate on detail view. </p>

<code>
SELECT M.reference,
       L.itemno,
       L.partno,
       L.description
FROM   purchaseorder_master_view m,
       purchaseorder_detail_view l
WHERE  M.reference = L.reference
       AND l.partno = '1'
       AND ROWNUM <= 5

/  
</code>

<p>Q4. A SQL Query on detail view making use of SQL Analytics 
functionality not provided by XQuery. The Group by extension ROLLUP function enables a SELECT statement to calculate multiple levels of subtotals across a specified group of dimensions, as well as a grand total.</p>

<code>
SELECT partno,
       Count(*) ""Orders"",
       quantity ""Copies""
FROM   purchaseorder_detail_view
WHERE  partno = '1'
GROUP  BY rollup( partno, quantity )

/  
</code>

<p>Q5. A SQL Query on detail view making use of SQL Analytics 
functionality not provided by XQuery. The analytic function LAG provides access to more than one row of a table at the same time without a self join. Given a series of rows returned from a query and a position of the cursor, LAG provides access to a row at a given physical offset prior to that position.</p>

<code>
SELECT partno,
       reference,
       quantity,
       quantity - Lag(quantity, 1, quantity)
                    over (
                      ORDER BY Substr(reference, Instr(reference, '-') + 1)) AS
       DIFFERENCE
FROM   purchaseorder_detail_view
WHERE  partno = '1'
ORDER  BY Substr(reference, Instr(reference, '-') + 1) DESC

/  
</code>",18-OCT-18 09.32.27.912337000 PM,"HARICHANDAN.ROY@ORACLE.COM",11-SEP-19 08.52.06.000242000 PM,"HARICHANDAN.ROY@ORACLE.COM"
160212874471847289858006366505306582691,160204041387679825048250629187623908505,"Creating and Querying XMLType Views of Relational Content",5,"<p><strong>Creating XMLType Views</strong></p>

<p>Creating a persistant XML view of relational content:</p>

<code>
CREATE OR replace VIEW DEPARTMENT_XML of xmltype
with object id
(
  XMLCAST(XMLQUERY('/Department/Name' passing OBJECT_VALUE returning CONTENT) as VARCHAR2(30))
) 
as
SELECT xmlElement
       (
         ""Department"",
         xmlAttributes( d.DEPARTMENT_ID as ""DepartmentId""),
         xmlElement(""Name"", d.DEPARTMENT_NAME),
         xmlElement
         (
           ""Location"",
           xmlForest
           (
              STREET_ADDRESS as ""Address"", CITY as ""City"", STATE_PROVINCE as ""State"",
              POSTAL_CODE as ""Zip"",COUNTRY_NAME as ""Country""
           )
         ),
         xmlElement
         (
           ""EmployeeList"",
           (
             select xmlAgg
                    (
                      xmlElement
                      (
                        ""Employee"",
                        xmlAttributes ( e.EMPLOYEE_ID as ""employeeNumber"" ),
                        xmlForest
                        (
                          e.FIRST_NAME as ""FirstName"", e.LAST_NAME as ""LastName"", e.EMAIL as ""EmailAddress"",
                          e.PHONE_NUMBER as ""Telephone"", e.HIRE_DATE as ""StartDate"", j.JOB_TITLE as ""JobTitle"",
                          e.SALARY as ""Salary"", m.FIRST_NAME || ' ' || m.LAST_NAME as ""Manager""                
                        ),
                        xmlElement ( ""Commission"", e.COMMISSION_PCT )
                      )
                    )
               from HR.EMPLOYEES e, HR.EMPLOYEES m, HR.JOBS j
              where e.DEPARTMENT_ID = d.DEPARTMENT_ID
                and j.JOB_ID = e.JOB_ID
                and m.EMPLOYEE_ID = e.MANAGER_ID
           )
         )
       ) as XML
FROM HR.DEPARTMENTS d, HR.COUNTRIES c, HR.LOCATIONS l
WHERE d.LOCATION_ID = l.LOCATION_ID and 
      l.COUNTRY_ID  = c.COUNTRY_ID
/
</code>

<p><strong>Querying Over XMLType Views</strong></p>
<p>Now let's execute XQueries over XMLType views created above.</p>
<p>Q1. Returning department named ""Executive"":</p>
<code>
SELECT T.object_value.getclobval() 
FROM   department_xml D,
       XMLTABLE ( 'for $r in /Department[Name=""Executive""]           return $r' passing object_value ) T

/  
</code>

<p>Q2. Returning departments having employee with last name ""Grant"":</p>

<code>
SELECT T.object_value.getclobval() 
FROM   department_xml D,
       XMLTABLE ( 'for $r in /Department[EmployeeList/Employee/LastName=""Grant""]/Name           return $r' passing object_value ) T

/  
</code>",18-OCT-18 10.21.39.974282000 PM,"HARICHANDAN.ROY@ORACLE.COM",21-OCT-18 08.16.26.372282000 AM,"HARICHANDAN.ROY@ORACLE.COM"
234737976893391607451829294627168103292,234738183173691734382399251082742733633,"Extending Your Macro",50,"<p>Let's extend the above macro to take two input parameters and also allow the user to not provide any parameters....
</p>
<p>
Note the references in the WHERE clause that specify the input parameters via the function name which is total_sales:
<br>
<br>
<code>
Total_Sales.country
Total_Sales.region
</code>
</p>
<code>
CREATE OR REPLACE FUNCTION Total_Sales(country VARCHAR2 default null,
                                                                region  VARCHAR2 default null) 
                   RETURN clob SQL_MACRO is
BEGIN
  RETURN q'{
     SELECT 
            r.country_name            name, 
            r.country_region          region, 
            ROUND(SUM(s.amount_sold)) total_sales
     FROM sh. countries r, sh.customers c, sh.sales s
     WHERE r.country_id = c.country_id
     AND c.cust_id = s.cust_id
     AND r.country_name   = NVL(INITCAP(Total_Sales.country), r.country_name)
     AND r.country_region = NVL(INITCAP(Total_Sales.region),  r.country_region)
     GROUP BY r.country_id, r.country_name, r.country_region
  }';
END;
/
</code>",01-OCT-20 08.58.10.696697000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.38.12.874816000 AM,"KEITH.LAKER@ORACLE.COM"
182247472530026201328755688195542932201,182241645422959190146194127811898911783,"Possible Solutions",20,"<p>
Generally, the way to improve performance over row-by-row context switching is to not perform row-by-row DML operations. This can be accomplished in one of two ways:
</p>
<ol>
<li>
Implement the functionality in ""pure"" SQL - no PL/SQL loop.
</li>
<li>
Use the bulk processing features of PL/SQL.
</li>
</ol>
<p>
If you can change your implementation to avoid a loop and instead simply execute a single DML statement, that should be done. For example, we can do this with the increase_salary procedure:
</p>
<code>CREATE OR REPLACE PROCEDURE increase_salary (
   department_id_in   IN employees.department_id%TYPE,
   increase_pct_in    IN NUMBER)
IS
BEGIN
   UPDATE employees emp
      SET emp.salary =
               emp.salary
             + emp.salary * increase_salary.increase_pct_in
    WHERE emp.department_id = 
             increase_salary.department_id_in;
END increase_salary;
</code>
<p>
Of course, it is not always this easy. You might be doing some very complex processing of each row before doing the insert, update or delete that would be hard to do in SQL. You may need to do more nuanced error management than ""all or nothing"" SQL will allow. Or you might simply not have sufficient knowledge of SQL to do what's needed.
</p>
<p>
In an ideal world, you would stop programming and take an advanced SQL class (the Oracle Dev Gym offers a free one on SQL analytic functions). In the real world, you need to get the program up and running ASAP. 
</p>
<p>
Whatever your situation, the bulk processing features of PL/SQL offer a straightforward solution - though there will be a lot to consider as you implement your conversion to BULK COLLECT and FORALL.
</p>
<p>
Let's first take a look at BULK COLLECT, which improves the performance of multi-row querying and is relatively simple. Then we'll move on to FORALL, which is used to execute the same non-query DML statement repeatedly, with different bind variables. That feature has a lot more moving parts and issues you need to take into account (which should come as no surprise, as you are <i>changing</i> data, not simply querying it. 
</p>",17-MAY-19 07.58.52.143934000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",30-MAY-19 06.29.29.692843000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247472530060051251704897812434705129,182241645422959190146194127811898911783,"Introduction to FORALL",80,"<p>
Whenever you execute a non-query DML statement inside of a loop, you should convert that code to use FORALL - and that's if you cannot get rid of the loop entirely and handle the requirement with ""pure SQL."" 
</p><p>
The FORALL statement is not a loop; it is a declarative statement to the PL/SQL engine: â€œGenerate all the DML statements that would have been executed one row at a time, and send them all across to the SQL engine with one context switch.â€
</p>
<p>We will start with a very simple example, showing the basic conversion from a loop to FORALL. Then we will explore the many nuances of working with this powerful feature.
</p><p>
In the block below, I have a classic example of the row-by-row ""anti-pattern:"" delete each employee in department 50 and 100.
</p>
<code>BEGIN
   FOR emp_rec IN (SELECT employee_id
                     FROM employees
                    WHERE department_id IN (50, 100))
   LOOP
      DELETE FROM employees
            WHERE employee_id = emp_rec.employee_id;
   END LOOP;

   ROLLBACK;
END;
</code>
<p>
Now, first of all I am sure you have noticed that this entire loop could be replaced by a single UPDATE statement, as in:
</p>
<pre>
DELETE FROM employees
 WHERE department_id IN (50, 100)
</pre>
<p>
For the purposes of demonstration (and for the remainder of this tutorial), please assume that there is more going on that justifies the use of FORALL. In which case, here is the rewrite of the original block:
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.department_id%TYPE;
   l_ids ids_t := ids_t (50, 100);
BEGIN
   FORALL l_index IN 1 .. l_ids.COUNT
      DELETE FROM employees
            WHERE department_id = l_ids (l_index);

   DBMS_OUTPUT.put_line (SQL%ROWCOUNT);
   ROLLBACK;
END;
</code>
<p>
Here's what I modified to convert to bulk processing:
</p><ol>
<li>
Declare a nested table type of department IDs, and declare a variable based on that type. Initialize the nested table with two departments.
</li><li>
Replace the FOR loop with a FORALL header. In this simplest of all cases, it looks just like a numeric FOR loop header, iterating from 1 to the number of elements in the nested table. Note the implicitly declared l_index iterator.
</li><li>
Place the DELETE statement ""under"" the FORALL; replace the IN clause with an equality check and reference an element in the l_ids bind array using the l_index iterator; and terminate the whole statement with a semi-colon.
</li><li>
Display the number of rows modified by the entire FORALL statement, and then rollback changes so that the table is left intact for the next example.
</li></ol>
<p>
That's about as simple an example of FORALL as one could imagine. The real world is usually a little bit more interesting. For example, you will often have an array of values that you bind into the WHERE clause <i>and</i> also one or more arrays of values that you use in the SET clause of an update statement (or to provide values for columns of an INSERT statement in the VALUES clause). 
</p><p>
FORALL takes care of all that exactly as you'd expect - but it is up to you to make sure that the data is in synch across all your collections. In the block below, I update rows by employee ID with a new salary and first name specific to each ID.
</p>
<code>DECLARE
   /* I use pre-defined collection types to reduce code volume. */
   l_ids        DBMS_SQL.number_table;
   l_names      DBMS_SQL.varchar2a;
   l_salaries   DBMS_SQL.number_table;
BEGIN
   l_ids (1) := 101;
   l_ids (2) := 112;
   l_ids (3) := 120;
   
   l_names (1) := 'Sneezy';
   l_names (2) := 'Bashful';
   l_names (3) := 'Happy';
   
   l_salaries (1) := 1000;
   l_salaries (2) := 1500;
   l_salaries (3) := 2000;

   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET first_name = l_names (indx), salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line (SQL%ROWCOUNT);

   FOR emp_rec IN (  SELECT first_name, salary
                       FROM employees
                      WHERE employee_id IN (101, 112, 120)
                   ORDER BY employee_id)
   LOOP
      DBMS_OUTPUT.put_line (emp_rec.first_name || ' - ' || emp_rec.salary);
   END LOOP;

   ROLLBACK;
END;
</code>
<p>
If I do not keep the indexes in synch across all columns, I will get errors, as you will see when you run this block:
</p>
<code>DECLARE
   l_ids        DBMS_SQL.number_table;
   l_names      DBMS_SQL.varchar2a;
   l_salaries   DBMS_SQL.number_table;
BEGIN
   l_ids (1) := 101;
   l_ids (2) := 112;
   l_ids (3) := 120;
   
   l_names (1) := 'Sneezy';
   l_names (100) := 'Happy';
   
   l_salaries (-1) := 1000;
   l_salaries (200) := 1500;
   l_salaries (3) := 2000;

   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET first_name = l_names (indx), salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);
   
   ROLLBACK;
END;
</code>
<pre>
ORA-22160: element at index [N] does not exist
</pre>
<p>So: by all means, you can reference multiple bind arrays, but make sure they are consistent across index values (often this is not an issue, as the collections are loaded from the same BULK COLLECT query).
</p>

<p>
Here are some important things to remember about FORALL:
</p><ul>
<li>
    Each FORALL statement may contain just a single DML statement. If your loop contains two updates and a delete, then you will need to write three FORALL statements.
</li><li>
    PL/SQL declares the FORALL iterator as an integer, just as it does with a FOR loop. You do not need to (and should not) declare a variable with this same name.
</li><li>
    In at least one place in the DML statement, you need to reference a collection and use the FORALL iterator as the index value in that collection.
</li><li>
    When using the IN low_value . . . high_value syntax in the FORALL header, the collections referenced inside the FORALL statement must be densely filled. That is, every index value between the low_value and high_value must be defined.
</li><li>
    If your collection is not densely filled, you should use the INDICES OF or VALUES OF syntax in your FORALL header.
</li></ol>

<p>
<strong>Fill in the Blanks</strong></p>
<p>
In the block below replace the #FINISH# tags with code so that the last names of employees with IDs 111, 121 and 131 are all upper-cased.
</p>
<code>DECLARE
   l_ids        DBMS_SQL.number_table;
   l_names      DBMS_SQL.varchar2a;
BEGIN
   #FINISH#

   FORALL indx IN 0 .. #FINISH#
      UPDATE employees
         SET first_name = UPPER (l_names (indx))
       WHERE employee_id = l_ids (indx);
   
   ROLLBACK;
END;
</code>

<p><strong> Exercise 6:</strong></p>
<p>
Write an anonymous block that uses BULK COLLECT to populate two collections, one with the employee Id and the other with the employee's last name. Use those collections in a FORALL statement to update the employee's last name to the UPPER of the name in the name collection, for each employee ID. Note: clearly, you do not need BULK COLLECT and FORALL to do this. Please pretend. :-)
</p>
</p>",17-MAY-19 08.01.48.068995000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",03-JUL-19 08.08.29.518853000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
234738140154008412171020211832941577601,234738183173691734382399251082742733633,"Building Your First SQL Macro",30,"<p>Suppose I want to have a simple way for my developers and users to view total sales revenue from the fact table SALES for a specific zip code in the CUSTOMERS table.  In effect, we need to create a paramterized view using a table macro.  
</p>
<p>
This requires a join for the two tables SALES and CUSTOMERS, then we need to find the matching rows for the zip code and finally sum the result. The table macro will look like this:
</p>
<code>
CREATE OR REPLACE FUNCTION total_sales(zip_code varchar2) return varchar2 SQL_MACRO is
BEGIN
  RETURN q'{
   SELECT cust.cust_postal_code as zip_code,
             SUM(amount_sold) as revenue
   FROM sh.customers cust, sh.sales s
   WHERE cust.cust_postal_code = total_sales.zip_code
   AND s.cust_id = cust.cust_id
   GROUP BY cust.cust_postal_code
   ORDER BY cust.cust_postal_code
  }';
END;
/
 </code>",01-OCT-20 08.48.05.377052000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 10.03.00.530980000 AM,"KEITH.LAKER@ORACLE.COM"
234738161442730286474949512275249773699,234738183173691734382399251082742733633,"What is a SQL Macro",10,"<p>It&#39;s a new, simpler way to encapsulate complex processing logic directly within&nbsp;SQL. SQL Macros allow&nbsp;developers to encapsulate complex processing within a new structure called a &quot;macro&quot; which can then be used within SQL statement. Essentially there&nbsp;two types of SQL Macros:&nbsp;<strong>SCALAR</strong> and <strong>TABLE</strong>. What&#39;s the difference:</p>

<ul>
	<li>SCALAR expressions can be used in SELECT list, WHERE/HAVING, GROUP BY/ORDER BY clauses &nbsp;</li>
	<li>TABLE expressions used in a FROM-clause</li>
</ul>
<p>Right now (Oct 2020) only table macros are available in LiveSQL but don't panic because scalar macros will be arriving shortly.
</p>
<p<Note the examples in this tutorial use the built-in sales history schema. There is a link below to the schema diagram.
</p>",01-OCT-20 08.43.29.434312000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.59.02.916946000 AM,"KEITH.LAKER@ORACLE.COM"
155036228497465093078796657745279586468,155044276401612634730144596161551172634,"Try It!",30,"<p>Complete the following statement to remove the row for the toy_name Purple Ninja:</p>

<code>select * from toys;

delete toys /* TODO */;

select * from toys;
rollback;</code>
<p>The query afterwards should return the following rows:</p>
<pre><b>TOY_NAME            PRICE   </b>
Baby Turtle          0.01 
Miss Snuggles        0.51 
Cuteasaurus         10.01 
Sir Stripypants     14.03</pre>",30-AUG-18 09.51.52.485252000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 07.57.33.424834000 AM,"CHRIS.SAXON@ORACLE.COM"
182322179208360421338433705772298468068,182241645422959190146194127811898911783,"Binding Sparse Arrays with VALUES OF ",130,"<p>
The VALUES OF clause, like its ""sister"" INDICES OF, makes it easier to use FORALL with bind arrays that are sparse - or with bind arrays from which you need to select out specific elements for binding. With VALUES OF, you specify a collection whose element values (not the index values) specify the index values in the bind arrays that will be used to generate DML statements in the FORALL. Sound complicated? It is, sort of. It is another level of ""indirectness"" from INDICES OF, and not as commonly used. 
</p>
<p>
Notice in the block below that the three element values are -77, 13067 and 1070. These in turn are index values in the l_employees array. And those elements in turn have the values 124, 123 and 129. You should therefore see in the output of the last step in the script that the employees with these 3 IDs now earn a $10000 salary.
</p>
<code>DECLARE
   TYPE employee_aat IS TABLE OF employees.employee_id%TYPE
      INDEX BY PLS_INTEGER;

   l_employees         employee_aat;

   TYPE values_aat IS TABLE OF PLS_INTEGER
      INDEX BY PLS_INTEGER;

   l_employee_values   values_aat;
BEGIN
   l_employees (-77) := 134;
   l_employees (13067) := 123;
   l_employees (99999999) := 147;
   l_employees (1070) := 129;
   --
   l_employee_values (100) := -77;
   l_employee_values (200) := 13067;
   l_employee_values (300) := 1070;

   --
   FORALL l_index IN VALUES OF l_employee_values
      UPDATE employees
         SET salary = 10000
       WHERE employee_id = l_employees (l_index);

   DBMS_OUTPUT.put_line (SQL%ROWCOUNT);
END;
</code>
<p>
The datatype of elements in a collection used in the VALUES OF clause must be 
PLS_INTEGER. It also must be indexed by PLS_INTEGER.
</p>
<p>
You are much less likely to use VALUES OF than INDICES OF, but it's good to know that it's there.
</p>",18-MAY-19 01.37.05.596420000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-19 01.23.58.370395000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
234738598777163126655787770139967680300,234738183173691734382399251082742733633,"Running macro TOTAL_SALES",60,"<code>
SELECT * 
FROM total_sales(region => 'europe')
ORDER BY 3;
</code>",01-OCT-20 09.00.49.031616000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.21.50.751203000 AM,"KEITH.LAKER@ORACLE.COM"
234739324643848013529917726348985138936,234738183173691734382399251082742733633,"Results from macro TOTAL_SALES",70,"<p>The output should look something like this....
</p>
<code>
Turkey Europe 7837
Poland Europe 8447
Denmark Europe 1977765
Spain  Europe 2090863
France Europe 3776270
Italy Europe 4854505
United Kingdom Europe 6393763
Germany Europe 9210129              
</code>",01-OCT-20 09.02.30.956081000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.12.39.811735000 AM,"KEITH.LAKER@ORACLE.COM"
234741550225629451014601962801035141745,234738183173691734382399251082742733633,"What happens if no parameters are used?",90,"<p>
what happens if we don't pass any region name or country name into the macro? Will it simply return an error or return a list of all regions and their counties?
</p>
<code>
SELECT 
 region,
 name,
 total_sales,
 TRUNC(RATIO_TO_REPORT(total_sales) OVER (PARTITION BY region), 4) contribution
from (SELECT * 
FROM total_sales()
ORDER BY 1, 3 desc);
</code>
<p>This should return a list of all regions with their corresponding countries which means our SQL macro can deal with the times when the user doesn't provide any parameters.
</p>",01-OCT-20 09.37.44.565951000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.45.21.659217000 AM,"KEITH.LAKER@ORACLE.COM"
160293427297618920507272436693737306078,160204041387679825048250629187623908505,"Reset",25,"<code>
DECLARE
    CURSOR gettable IS
      SELECT table_name
      FROM   user_xml_tables
      WHERE  table_name IN ( 'PURCHASEORDER' );
BEGIN
    FOR t IN gettable() LOOP
        EXECUTE IMMEDIATE 'DROP TABLE ""'|| t.table_name|| '"" PURGE';
    END LOOP;
END;

/  
</code>",19-OCT-18 03.38.02.171392000 PM,"HARICHANDAN.ROY@ORACLE.COM",19-OCT-18 10.24.46.590358000 PM,"HARICHANDAN.ROY@ORACLE.COM"
182322864846515978141390703058687239188,182241645422959190146194127811898911783,"SQL%BULK_EXCEPTIONS",110,"<p>
If you want the PL/SQL engine to execute as many of the DML statements as possible, even if errors are raised along the way, add the SAVE EXCEPTIONS clause to the FORALL header. Then, if the SQL engine raises an error, the PL/SQL engine will save that information in a pseudo-collection** named SQL%BULK_EXCEPTIONS, and continue executing statements. When all statements have been attempted, PL/SQL then raises the ORA-24381 error.
</p><p>
You canâ€”and shouldâ€”trap that error in the exception section and then iterate through the contents of SQL%BULK_EXCEPTIONS to find out which errors have occurred. You can then write error information to a log table and/or attempt recovery of the DML statement.
</p>
<p>
It is a collection of records, each of which has two fields: ERROR_INDEX and ERROR_CODE. 
</p>
<p>
The index field contains a sequentially generated integer, incremented with each statement execution in the SQL engine. For sequentially filled collections, this integer matches the index value of the variable in the bind array. For sparsely-filled collections (see the modules on INDICES OF and VALUES OF for more details), you will have to write special-purpose code to ""link back"" the nth statement to its index value in the bind array.
<p>
</p>
<p>ERROR_CODE is the value returned by SQLCODE at the time the error occurred. Note that this collection does <i>not</i> include the error message. 
</p>
<p>Let's go back to the same block used to show the effects of SAVE EXCEPTIONS, but now also take advantage of SQL%BULK_EXCEPTIONS.
</p>
<code>DECLARE  
   TYPE namelist_t IS TABLE OF VARCHAR2 (5000);  
  
   enames_with_errors   namelist_t  
      := namelist_t ('ABC',  
                     'DEF',  
                     RPAD ('BIGBIGGERBIGGEST', 1000, 'ABC'),  
                     'LITTLE',  
                     RPAD ('BIGBIGGERBIGGEST', 3000, 'ABC'),  
                     'SMITHIE');  
BEGIN  
   FORALL indx IN 1 .. enames_with_errors.COUNT SAVE EXCEPTIONS  
      UPDATE employees  
         SET first_name = enames_with_errors (indx);  
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line (  
         'Updated ' || SQL%ROWCOUNT || ' rows.');  
      DBMS_OUTPUT.put_line (SQLERRM); 

      FOR indx IN 1 .. SQL%BULK_EXCEPTIONS.COUNT
      LOOP
         DBMS_OUTPUT.put_line (
               'Error '
            || indx
            || ' occurred on index '
            || SQL%BULK_EXCEPTIONS (indx).ERROR_INDEX
            || ' attempting to update name to ""'
            || enames_with_errors (
                  SQL%BULK_EXCEPTIONS (indx).ERROR_INDEX)
            || '""');
         DBMS_OUTPUT.put_line (
               'Oracle error is '
            || SQLERRM (
                  -1 * SQL%BULK_EXCEPTIONS (indx).ERROR_CODE));
      END LOOP;
 
      ROLLBACK;  
END;
</code>
<p>
Notice that I multiply the error code in SQL%BULK_EXCEPTIONS by -1 before passing it to SQLERRM. Here's why I do that:
</p>
<ul>
<li>SQLERRM not only returns the current error message. If you pass it an error code, it returns the error message for <i>that</i> code.
</li><li>
But it expects a negative value for all but two error codes - 0 (no error) and 100 (the ANSI-standard NO_DATA_FOUND error code).
</li><li>
For reasons that may never be known, the value for the error code stored in the pseudo-collection is not negative.
</li>
</ul>

<p>
** What's a ""pseudo-collection""? It's a data structure created implicitly by PL/SQL for us. The ""pseudo"" indicates that it doesn't have <i>all</i> the features of a ""normal"" collection. 
</p>
<p>
In the case of SQL%BULK_EXCEPTIONS, it has just one method defined on it: COUNT. It is always filled sequentially from index value 1. The number of elements in this pseudo-collection matches the number of times the SQL engine encountered an error.
</p>

<p><strong> Exercise 11:</strong></p>
<p>
Change the code you wrote in Exercise 10 as follows:</p>
<ol>
<li>Expand the number of employee IDs to update to 5.</li>
<li>Change the array of salary values so that at least 2 of the updates will fail.</li>
<li>Change the exception handler to show which of the attempts to update failed, the error code, and the error message associated with that code.</li>
</ol>",18-MAY-19 01.36.31.173805000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-19 01.45.13.230867000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182322864846530485251226078608783713300,182241645422959190146194127811898911783,"Binding Sparse Arrays with INDICES OF ",120,"<p>
The INDICES OF clause allows you to use sparsely-filled bind arrays in FORALL. This is necessary because with the ""usual"" FORALL syntax of:
</p>
<pre>
FORALL indx IN low_value .. high_value
</pre>
<p>
The PL/SQL engine assumes that every index value between the low and high values are defined. If any of them are ""missing"" (not assigned a value), then you get an error. Not the NO_DATA_FOUND you might expect (which you get when you try to ""read"" an element at an undefined index value), but the ORA-22160 
 error, as you will see when you run the code below. 
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.department_id%TYPE;
   l_ids ids_t := ids_t (10, 50, 100);
BEGIN
   /* Now I remove the ""in between"" element */
   l_ids.DELETE (2);

   FORALL l_index IN 1 .. l_ids.COUNT
      DELETE FROM employees
            WHERE department_id = l_ids (l_index);

   DBMS_OUTPUT.put_line ('Rows modified = ' || SQL%ROWCOUNT);
EXCEPTION
   WHEN OTHERS 
   THEN       
      DBMS_OUTPUT.put_line ('Rows modified = ' || SQL%ROWCOUNT);
      DBMS_OUTPUT.put_line ('Error = ' || SQLERRM);
      ROLLBACK;
END;
</code>
<p>
How do you get around this problem? Certainly, you could ""densify"" your collection before binding it to FORALL. And prior to 10.2 you would have <i>had</i> to do this. But as of Oracle Database 10g Release 2, you can use INDICES OF (or VALUES OF - covered in the next module).
</p>
<p>
With the INDICES OF clause, you say, in essence: ""Just bind the elements at the index values that are defined."" Or to put it more simply: ""Please skip over any undefined index values."" Note that the collection referenced in the INDICES OF clause must be indexed by PLS_INTEGER; you cannot use string-indexed collections. But the datatype of the elements in the collection can be anything - they are not referenced at all with INDICES OF.
</p>
<p>
Let's start with the simplest usage of INDICES OF: the indexing collection is the same as the bind collection.
In the block below, I populate the l_employees associative array using ndex values 1, 100, 500 - rather than, say, 1-3. That's not densely filled!
</p><p>
I then use INDICES OF to tell the PL/SQL engine to only use the defined indices of l_employees, and all is good. This is the simplest form of INDICES OF: using the bind array in the INDICES OF clause.
</p>
<code>DECLARE  
   TYPE employee_aat IS TABLE OF employees.employee_id%TYPE  
                           INDEX BY PLS_INTEGER;  
  
   l_employees   employee_aat;  
BEGIN  
   l_employees (1) := 137;  
   l_employees (100) := 126;  
   l_employees (500) := 147;  
  
   FORALL l_index IN INDICES OF l_employees  
      UPDATE employees  
         SET salary = 10000  
       WHERE employee_id = l_employees (l_index);  
  
   DBMS_OUTPUT.put_line ('Rows modified = ' || SQL%ROWCOUNT); 
   ROLLBACK; 
END; 
</code>
<p>
Here is a more interesting use of INDICES OF: I populate a <i>second</i> collection to serve as an ""index"" into the bind array. I also include the BETWEEN clause, which can be used to further restrict the set of elements in the indexing collection that will be used to bind values into the DML statement. It doesn't matter one bit what values are assigned to the elements in the indexing array - just which index values are defined.
</p>
<code>DECLARE
   TYPE employee_aat IS TABLE OF employees.employee_id%TYPE
      INDEX BY PLS_INTEGER;

   l_employees          employee_aat;

   TYPE boolean_aat IS TABLE OF BOOLEAN
      INDEX BY PLS_INTEGER;

   l_employee_indices   boolean_aat;
BEGIN
   l_employees (1) := 137;
   l_employees (100) := 126;
   l_employees (500) := 147;
   --
   l_employee_indices (1) := FALSE;
   l_employee_indices (500) := TRUE;
   l_employee_indices (799) := NULL;

   FORALL l_index IN INDICES OF l_employee_indices BETWEEN 1 AND 600
      UPDATE employees
         SET salary = 10001
       WHERE employee_id = l_employees (l_index);

   DBMS_OUTPUT.put_line ('Rows modified = ' || SQL%ROWCOUNT);

   FOR rec IN (SELECT employee_id
                 FROM employees
                WHERE salary = 10001)
   LOOP
      DBMS_OUTPUT.put_line (rec.employee_id);
   END LOOP;

   ROLLBACK;
END;
</code>
<p>
You might be thinking to yourself: ""Won't INDICES OF work just fine with densely-filled collections as well? why not use it all time?""
</p>
<p>
Yes, INDICES OF works just fine with dense or sparse collections, and there doesn't seem to be any performance difference. So why not use it all the time? One thing to consider is that using INDICES OF might <i>hide</i> a bug. What if your collection really <i>should</i> be dense but something went wrong and now it has undefined elements? In this case, your code should raise an error but it will not.</p>
<p>Not good. And that's why I suggest you use the syntax that is appropriate to your program and context. </p>

<p>
A common and most excellent use case for INDICES OF is to help manage bulk transactions that involve more than one DML statement. For example, you do a FORALL-UPDATE, then two FORALL-INSERTs. If any of the attempted updates fail, you need to <i>not</i> perform the associated inserts. One way to do this is to delete elements from the FORALL-INSERT bind arrays for failed updates. You will then have a sparse collection and will want to switch to INDICES OF.
</p>

<p>
We do not cover this complex scenario in the tutorial, but this LiveSQL script shows you the basic idea: Converting from Row-by-Row to Bulk Processing, http://bit.ly/bulkconv.
</p>

<p><strong> Exercise 12:</strong></p>
<p>
Change the procedure below so that no assumptions are made about the contents of the ids_in array and the procedure completes without raising an exception. Assumptions you <i>could</i> make, but should not, include:
</p>
<ul>
<li>The collection is never empty.</li>
<li>The first index defined in the collection is 1.</li>
<li>The collection is dense.</li>
<li>The collection is sparse.</li>
</ul>
<code>CREATE OR REPLACE TYPE numbers_t IS TABLE OF NUMBER
/

CREATE OR REPLACE PROCEDURE update_emps (ids_in IN numbers_t)
IS
BEGIN
   FORALL l_index IN 1 .. ids_in.COUNT
      UPDATE employees
         SET hire_date = SYSDATE
       WHERE employee_id = ids_in (l_index);

   ROLLBACK;
END;
/
</code>

<p><strong> Exercise 13 (advanced):</strong></p>
<p>
Got some time on your hands? Ready to explore all that INDICES OF can do? OK, let's go! Write a block of code that does the following:
</p>
<ul>
<li>Populates a collection using BULK COLLECT with all the rows and columns of the employees table, ordered by employee ID: the bind array.</li>
<li>Declare an associative array that can be used with INDICES OF.</li>
<li>Populate one collection of that type with index values from the bind array for all employees with salary = 2600: index array 1.</li>
<li>Populate a second collections of that type with index values from the bind array of those employees who were hired in 2004: index array 2.</li>
<li>Write a FORALL statements that uses index array 1 to set the salaries of those employees to 2626.</li>
<li>Write a FORALL statements that uses index array 2 to set the hire dates of those employees to January 1 2014.</li>
<li>Include code to verify that the changes were made properly.</li>
</ul>",18-MAY-19 01.36.52.122909000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-19 02.46.27.258850000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
152783915636882203396654426069746380888,152769908351014758555746151630340122318,"Review AV Schema",10,"<p>Before getting started, review data the in the AV sample schema that will be used by this tutorial.  The AV schema includes several dimension tables and a fact table containing sales data.</p>
<p>The TIME_DIM table.</p>
<code>SELECT * FROM av.time_dim;</code>
<p>The PRODUCT_DIM table.</p>
<code>SELECT * FROM av.product_dim;</code>
<p>The GEOGRAPHY_DIM table.</p>
<code>SELECT * FROM av.geography_dim;</code>
<p>The SALES_FACT table.</p>
<code>SELECT * FROM av.sales_fact WHERE rownum < 50;</code>",08-AUG-18 05.58.41.571141000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-AUG-18 06.01.21.639903000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
152783915638861214963363574028740391000,152769908351014758555746151630340122318,"Aggregating Data From Tables",20,"<p>When selecting data from tables, data is first filtered using the WHERE clause and aggregated using GROUP BY (or some variant of GROUP BY such as GROUPING SETS).</p>

<p>The following query select data for years.</p>

<code>
SELECT
  t.year_name,
  SUM(f.sales)
FROM
  av.time_dim t,
  av.sales_fact f
WHERE
  t.month_id = f.month_id
GROUP BY
  t.year_name
ORDER BY
  t.year_name;
</code>

<p>The next query also selects data for years, but also includes a filter rows in the TIME_DIM table to only those months that are in the first and second quarters of the year. (That is, January through June.  As a result, the SUM of sales for years includes only the data first the first half of the year.</p>

<code>
SELECT t.year_name,
  SUM(f.sales)
FROM av.time_dim t,
  av.sales_fact f
WHERE
  t.month_id = f.month_id
  AND TO_CHAR(t.month_end_date,'Q') in (1,2)
GROUP BY t.year_name
ORDER BY year_name;
</code>

<p>The next example also returns Regions and filters to countries Mexico and Canada.  The values of North American are the totals of Mexico and Canada.</p>

<code>
SELECT t.year_name,
  g.region_name,
  SUM(f.sales)
FROM av.time_dim t,
  av.geography_dim g,
  av.sales_fact f
WHERE t.month_id = f.month_id
  AND g.state_province_id = f.state_province_id
  AND TO_CHAR(t.month_end_date,'Q') in (1,2)
  AND g.country_name IN ('Mexico','Canada')
GROUP BY t.year_name,
  g.region_name
ORDER BY g.region_name,
  t.year_name;
</code>

<p>The important concept to keep in mind is that aggregation occurs after filtering.</p>",08-AUG-18 06.30.08.773217000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-AUG-18 08.16.58.079919000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
182247975188924486764837704382066336143,182241645422959190146194127811898911783,"RETURNING and BULK COLLECT",60,"<p>
The RETURNING clause is a wonderful thing. If you are inserting, updating or deleting data, and you need to get some information back after the statement completes (such as the primary key of the newly-inserted row), RETURNING is the thing for you! Here's an example:
</p>
<code>CREATE TABLE t (
   id NUMBER GENERATED ALWAYS AS IDENTITY,
   n NUMBER)
/

DECLARE
   l_id t.id%TYPE;
BEGIN
   INSERT INTO t (n) VALUES (100)
      RETURNING id INTO l_id;
   DBMS_OUTPUT.PUT_LINE (l_id);
END;
/
</code>
<p>
Suppose, however, that I am changing more than one row. Can I use RETURNING then? Let's see....
</p>
<code>DECLARE
   l_id employees.employee_id%TYPE;
BEGIN
   UPDATE employees
      SET last_name = UPPER (last_name)
      RETURNING employee_id INTO l_id;
   ROLLBACK;
END;
/
</code>
<p>
Oh no!
</p>
<pre>ORA-01422: exact fetch returns more than requested number of rows</pre>
<p>
But wait, that's the sort of error you can with a SELECT-INTO that returns more than one row. Why is it showing up here?
</p><p>
Because the RETURNING clause is essentially translated <i>into</i> a SELECT-INTO: get one value and stuff it into l_id. But in this case, the UPDATE statement is returning <i>many</i> IDs. How do we get this work?
</p><p>
BULK COLLECT to the rescue! I need to take multiple values and put them into something. What could that be? How about a collection? So, yes, if you are changing one <i>or more</i> rows, change INTO to BULK COLLECT INTO, and provide a collection to hold the values.
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;
   l_ids ids_t;
BEGIN
   UPDATE employees
      SET last_name = UPPER (last_name)
    WHERE department_id = 50
      RETURNING employee_id BULK COLLECT INTO l_ids;

   FOR indx IN 1 .. l_ids.COUNT 
   LOOP
      DBMS_OUTPUT.PUT_LINE (l_ids (indx));
   END LOOP;

   ROLLBACK;
END;
/
</code>
<p>
Now RETURNING works like a charm. Thanks, BULK COLLECT!
</p>

<p>
<strong>Fill in the Blanks</strong></p>
<p>
In the block below replace the #FINISH# tag with code so that ""Deleted = 3"" is displayed after execution.
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;
   l_ids ids_t;
BEGIN
   DELETE FROM employees
    WHERE salary > 15000
    #FINISH#

   DBMS_OUTPUT.PUT_LINE ('Deleted = ' || l_ids.COUNT);

   ROLLBACK;
END;
/
</code>

<p><strong> Exercise 4:</strong></p>
<p>
Write an anonymous block that deletes all the rows in the employees table for department 50 and returns all the employee IDs and the last names of deleted rows. Then display those values using DBMS_OUTPUT.PUT_LINE. Finally, you might want to rollback. That will make it easier to test your code - and continue on with the tutorial. 
</p>",17-MAY-19 08.00.46.655449000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",05-JUN-19 03.11.52.260398000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247975188987350907457665099151057295,182241645422959190146194127811898911783,"Errors and FORALL",100,"<p>
So many errors can occur when you are trying to change rows in a table. Constraint violations, values too large for columns....now add to that the fact that with FORALL you are executing that DML statement many times over. Managing errors with FORALL is a tricky and important thing to do!
</p>
<p>
Before diving into the error-related features of FORALL, let's review some important points about transactions, errors and exceptions in the world of PL/SQL.
</p>
<ul>
<li>
If the SQL engine raises an error back to the PL/SQL engine, that does not cause an automatic rollback of previously successful DML statements. They are still waiting to be committed or rolled back.
</li><li>
If an exception goes unhandled out of PL/SQL (back to, say, SQL*Plus or SQL Developer or your APEX app), then a rollback will always be performed.
</li><li>
Each SQL statement is atomic (""all or nothing""), unless you use the LOG ERRORS feature. In other words, if your update statement finds 100 rows to change, and as it is changing the 100th of them, it hits an error, the changes to all 100 rows are reversed.
</li><li>
LOG ERRORS is a SQL feature that allows you to suppress errors at the <i>row level</i>, avoiding the ""all or nothing"" scenario. We are not exploring LOG ERRORS in this tutorial. I just wanted to make sure you are aware. :-) Oh and search LiveSQL for ""log errors"" to see a script or two on that feature.
</li>
</ul>
<p>
What does this for FORALL? That, by default, the first time the SQL engine encounters an error processing the DML statement passed to it from FORALL, it stops and passes the error back to the PL/SQL engine. No further processing is done, but also any statements completed successfully by the FORALL are still waiting to be committed or rolled back.
</p>
<p>
You can see this behavior in the code below (note the constraint on the size of numbers in the n column: 1 and 10 are OK, 100 is too large).
</p>
<code>CREATE TABLE mynums (n NUMBER (2))
/

DECLARE
   TYPE numbers_t IS TABLE OF NUMBER;
   l_numbers   numbers_t := numbers_t (1, 10, 100);
BEGIN
   FORALL indx IN 1 .. l_numbers.COUNT
      INSERT INTO mynums (n)
         VALUES (l_numbers (indx));
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT || ' rows.');
      DBMS_OUTPUT.put_line (SQLERRM);
      ROLLBACK;
END;
/

DECLARE
   TYPE numbers_t IS TABLE OF NUMBER;
   l_numbers   numbers_t := numbers_t (100, 10, 1);
BEGIN
   FORALL indx IN 1 .. l_numbers.COUNT
      INSERT INTO mynums (n)
         VALUES (l_numbers (indx));
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT || ' rows.');
      DBMS_OUTPUT.put_line (SQLERRM);
      ROLLBACK;
END;
/

DROP TABLE mynums
/
</code>
<p>
The first block updated 2 rows, the second block updated 0 rows. You see here the ""shortcutting"" that the SQL engine does with your FORALL DML statement and the array of bind variables.
</p>
<p>
This is entirely consistent with non-bulk behavior. By which I mean: if you have a block that executes an insert, then another insert, then a delete and finally an update, the PL/SQL and SQL engines will keep on going just as long as they can, stop when there is an error, and leave it to you, the developer, to decide what to do about it.
</p>
<p>
But there is a difference between a non-bulk multi-DML set of statements and FORALL: with the non-bulk approach, you can tell the PL/SQL engine to continue past a failed SQL statement by putting it inside a nested block, as in:
</p>
<pre>
BEGIN
   BEGIN
      INSERT INTO ...
   EXCEPTION 
      WHEN OTHERS 
      THEN
          log_error ...
   END;

   BEGIN
      UPDATE ...
   EXCEPTION 
      WHEN OTHERS 
      THEN
          log_error ...
   END;
END;
</pre> 
<p>
Note: I am <i>not</i> recommending that you <i>do</i> surround each non-query DML statement in a nested table, trapping any exception, and then continuing. Not at all. I am saying: it is possible, and you can choose to do this if you'd like. What about with FORALL?
</p>
<p>
With FORALL, you are <i>choosing</i> to execute the same statement, many times over. It could be that you <i>do</i> want to stop your FORALL as soon as any statement fails. In which case, you are done. If, however, you want to keep on going, even if there is a SQL error for a particular set of bind variable values, you need to take advantage of the SAVE EXCEPTIONS clause.
</p>

<p><strong> Exercise 9:</strong></p>
<p>
Complete the block below as follows: use FORALL to update the salaries of employees whose IDs are in l_ids with the corresponding salary in l_salaries. Display the total number of employees modified. First, use values for salaries that will allow all statements to complete successfully (salary is defined as NUMBER (8,2). Then change the salary values to explore the different ways that FORALL deals with errors.
</p>
<pre>
DECLARE
   TYPE numbers_nt IS TABLE OF NUMBER;
   l_ids numbers_nt := numbers_nt (101, 111, 131);
   l_salaries numbers_nt := numbers_nt (#FINISH#);
BEGIN
   #FINISH#
END;
</pre>",17-MAY-19 08.02.15.076686000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",30-MAY-19 09.01.36.900751000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
160212489185927298911910490435643691556,160204041387679825048250629187623908505,"Updating XML Content",3,"<p>You can update XML content, replacing either the entire contents of a document or only particular parts of a document. In a single operation, you can make multiple changes to a document. Each change uses an XQuery expression to identify a node to be updated, and specifies the new value for that node.</p>

<p>Let's first check the current state of the document:</p>

<code>
SELECT xmlquery(
         '<POSummary lineItemCount=""{count($XML/PurchaseOrder/LineItems/LineItem)}"">{
            $XML/PurchaseOrder/User,
            $XML/PurchaseOrder/Requestor,
            $XML/PurchaseOrder/LineItems/LineItem[2]
          }
          </POSummary>'
         passing object_value AS ""XML""
         returning content 
       ).getclobval() initial_state 
FROM   PURCHASEORDER
WHERE  xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value AS ""XML"",
                  'ACABRIO-100PDT' AS ""REF""
       )
/
</code>

<p>UQ1. Modifying the content of existing nodes using XQuery update:</p>

<code>
UPDATE PURCHASEORDER
SET    object_value = XMLQuery
                      (
                        'copy $NEWXML := $XML modify (
                           for $PO in $NEWXML/PurchaseOrder return (
                                replace value of node $PO/User with $USERID,
                                replace value of node $PO/Requestor with $FULLNAME,
                                replace value of node $PO/LineItems/LineItem/Part[@Description=$OLDTITLE]/@Description with $NEWTITLE 
                               )
                         )
                        return $NEWXML'
                        passing object_value as ""XML"",
                        'KCHUNG' as ""USERID"",
                        'Kelly Chung' as ""FULLNAME"",
                        'Runaway' as ""OLDTITLE"",
                        'Runaway[Updated]' as ""NEWTITLE""
                        returning content
                      )
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]/LineItems/LineItem/Part[@Description=$OLDTITLE]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF"",
                  'Runaway' as ""OLDTITLE""
       )
/
</code>

<p>Checking updated document:</p>

<code>
SELECT XMLQUERY(
         '<POSummary lineItemCount=""{count($XML/PurchaseOrder/LineItems/LineItem)}"">{
            $XML/PurchaseOrder/User,
            $XML/PurchaseOrder/Requestor,
            $XML/PurchaseOrder/LineItems/LineItem[2]
          }
          </POSummary>'
         passing object_value as ""XML""
         returning CONTENT
       ).getclobval() UPDATED_NODES
FROM  PURCHASEORDER
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>UQ2. Deleting a node using XQuery update:</p>

<code>
UPDATE PURCHASEORDER
SET    object_value = XMLQuery(
                        'copy $NEWXML := $XML modify (
                                                delete nodes $NEWXML/PurchaseOrder/LineItems/LineItem[@ItemNumber=$ITEMNO]
                                               )
                         return $NEWXML'
                        passing object_value as ""XML"", 2 as ITEMNO
                        returning CONTENT
                      )
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>Checking updated document:</p>

<code>
SELECT XMLQUERY(
         '<POSummary lineItemCount=""{count($XML/PurchaseOrder/LineItems/LineItem)}"">{
            $XML/PurchaseOrder/LineItems/LineItem[2]
          }
          </POSummary>'
         passing object_value as ""XML""
         returning CONTENT
       ).getclobval() DELETED_NODE
FROM  PURCHASEORDER
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>UQ3. Inserting a node using XQuery update:</p>

<code>
UPDATE PURCHASEORDER
SET    object_value = XMLQuery(
                        'copy $NEWXML := $XML modify (
                                                for $TARGET in $NEWXML/PurchaseOrder/LineItems/LineItem[@ItemNumber=""3""]
                                                  return insert node $LINEITEM after $TARGET
                                         )
                          return $NEWXML'
                        passing object_value as ""XML"",
                                xmlType(
                                  '<LineItem ItemNumber=""4"">
                                     <Part Description=""Rififi"" UnitPrice=""29.95"">37429155622</Part>
                                     <Quantity>2</Quantity>
                                   </LineItem>'
                                ) as ""LINEITEM""
                        returning CONTENT
                     )
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>Checking updated document:</p>

<code>
SELECT XMLQUERY(
         '<POSummary lineItemCount=""{count($XML/PurchaseOrder/LineItems/LineItem)}"">{
            $XML/PurchaseOrder/LineItems/LineItem[3]
          }
          </POSummary>'
         passing object_value as ""XML""
         returning CONTENT
       ).getclobval() INSERTED_NODE
FROM  PURCHASEORDER
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>UQ4. Undo all the above changes using XQuery update: </p>

<code>
UPDATE PURCHASEORDER
SET    object_value = XMLQuery(
                        ' copy $NEWXML := $XML modify (
                             for $PO in $NEWXML/PurchaseOrder return (
                                  replace value of node $PO/User with $USERID,
                                  replace value of node $PO/Requestor with $FULLNAME,
                                  replace node $PO/LineItems with $LINEITEMS
                            )
                         )
                         return $NEWXML'
                         passing object_value as ""XML"",
                                 'ACABRIO-100' as ""USERID"",
                                 'A. Cabrio 100' as ""FULLNAME"",
                                 xmlType(
                                   '<LineItems>
                                      <LineItem ItemNumber=""1"">
                                        <Part Description=""Face to Face: First Seven Years"" UnitPrice=""19.95"">100</Part>
                                        <Quantity>100</Quantity>
                                      </LineItem>
                                      <LineItem ItemNumber=""2"">
                                        <Part Description=""Runaway"" UnitPrice=""27.95"">100</Part>
                                        <Quantity>100</Quantity>
                                      </LineItem>
                                      <LineItem ItemNumber=""3"">
                                        <Part Description=""Founding Fathers: Men Who Shaped"" UnitPrice=""19.95"">100</Part>
                                        <Quantity>100</Quantity>
                                      </LineItem>
                                    </LineItems>'
                                 ) as ""LINEITEMS""
                                 returning content
                     )
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"", 
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>

<p>Checking updated document:</p>

<code>
SELECT XMLQUERY(
         '<POSummary lineItemCount=""{count($XML/PurchaseOrder/LineItems/LineItem)}"">{
            $XML/PurchaseOrder/User,
            $XML/PurchaseOrder/Requestor,
            $XML/PurchaseOrder/LineItems/LineItem[2]
          }
          </POSummary>'
         passing object_value as ""XML""
         returning CONTENT
       ).getclobval() FINAL_STATE
FROM  PURCHASEORDER
WHERE xmlExists(
         '$XML/PurchaseOrder[Reference=$REF]'
          passing object_value as ""XML"",
                  'ACABRIO-100PDT' as ""REF""
       )
/
</code>",18-OCT-18 09.04.39.599859000 PM,"HARICHANDAN.ROY@ORACLE.COM",19-OCT-18 10.42.43.122873000 PM,"HARICHANDAN.ROY@ORACLE.COM"
188704818652766247145126617041078988536,188704173216096418598025228629326881042,"Overview",10,"<p>
Oracle Spatial and Graph offers the industryâ€™s most comprehensive, advanced database for enterprise spatial applications and high performance, secure graph databases. With Oracle Database 19c, in the cloud and on premises, Spatial and Graph powers applications from GIS and location services to fraud detection, social networks, linked data and knowledge management. 
<p>
The spatial features of Spatial and Graph include both basic spatial data management and analysis and additional high-end functionality including: spatial aggregates, 3D, LiDAR, geospatial imagery, geocoding, routing, and linear referencing.
<p>
Oracle Spatial and Graph is included at no extra cost with all Oracle Database cloud services and on-prem editions. Not all Spatial features are enabled on Autonomous Database as described <a href=""https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/user/experienced-database-users.html"" target=""_blank""> here </a> under the section Restrictions for Database Features.
<p>",18-JUL-19 03.37.18.093348000 PM,"DAVID.LAPP@ORACLE.COM",25-MAR-20 05.45.43.201038000 PM,"DAVID.LAPP@ORACLE.COM"
188704818654073095956130031178936364792,188704173216096418598025228629326881042,"Create tables and spatial metadata",30,"<p>
We will now create tables and spatial metadata for CUSTOMERS and WAREHOUSES. 
<p>
We first create the CUSTOMERS and WAREHOUSES tables. Notice that each has a column of type SDO_GEOMETRY to store location.
<p>
<code>CREATE TABLE CUSTOMERS  
(
  CUSTOMER_ID NUMBER(6, 0) 
, CUST_FIRST_NAME VARCHAR2(20 CHAR)
, CUST_LAST_NAME VARCHAR2(20 CHAR) 
, GENDER VARCHAR2(1 CHAR)
, CUST_GEO_LOCATION SDO_GEOMETRY 
, ACCOUNT_MGR_ID NUMBER(6, 0) 
);

CREATE TABLE WAREHOUSES 
( 
WAREHOUSE_ID	NUMBER(3,0) 
, WAREHOUSE_NAME	VARCHAR2(35 CHAR) 
, LOCATION_ID	NUMBER(4,0) 
, WH_GEO_LOCATION	SDO_GEOMETRY 
);
</code>

<p>
Next we add Spatial metadata for the CUSTOMERS and WAREHOUSES tables to the USER_SDO_GEOM_METADATA view. Each SDO_GEOMETRY column is registered with a row in USER_SDO_GEOM_METADATA. This is normally a simple INSERT statement, and a GUI in SQL Developer. However due to the proxy user configuration of LiveSQL we must use a procedure that gets the actual database username:
<p>
<code>EXECUTE SDO_UTIL.INSERT_SDO_GEOM_METADATA (sys_context('userenv','current_user'), -
 'CUSTOMERS', 'CUST_GEO_LOCATION', -
  SDO_DIM_ARRAY(SDO_DIM_ELEMENT('X',-180, 180, 0.05), -
                SDO_DIM_ELEMENT('Y', -90, 90, 0.05)),-
  4326);
  
EXECUTE SDO_UTIL.INSERT_SDO_GEOM_METADATA (sys_context('userenv','current_user'), -
 'WAREHOUSES', 'WH_GEO_LOCATION', -
  SDO_DIM_ARRAY(SDO_DIM_ELEMENT('X',-180, 180, 0.05), -
                SDO_DIM_ELEMENT('Y', -90, 90, 0.05)),-
  4326);
</code>
<p>
Here is a description of the items that were  entered:
<ul>
<li>TABLE_NAME: Name of the table which contains the spatial data.</li>
<li>COLUMN_NAME: Name of the SDO_GEOMETRY column which stores the spatial data</li>
<li>MDSYS.SDO_DIM_ARRAY: Constructor which holds the MDSYS.SDO_DIM_ELEMENT object, which in turn stores the extents of the spatial data in each dimension (-180.0, 180.0), and a tolerance value (0.05). The tolerance is a round-off error value used by Oracle Spatial, and is in meters for longitude and latitude data. In this example, the tolerance is 5 mm.</li>
<li>4326: Spatial reference system id (SRID): a foreign key to an Oracle dictionary table (MDSYS.CS_SRS) that contains all the supported coordinate systems. It is important to associate your customer's location to a coordinate system. In this example, 4326 corresponds to ""Longitude / Latitude (WGS 84).""</li>
</ul>
<p>",18-JUL-19 03.47.31.159748000 PM,"DAVID.LAPP@ORACLE.COM",06-AUG-19 01.18.47.854597000 PM,"DAVID.LAPP@ORACLE.COM"
234739428250702078620660744926297492727,234738183173691734382399251082742733633,"Using analytic functions - calculating contribution",80,"<p>Table macros are just like any ordinary database table so we can extend our SQL statement to include analytic functions such as RATIO_TO_REPORT to add more value to our SQL statement as follows:
</p>
<code>
SELECT 
 region, 
 name,
 total_sales,
 TRUNC(RATIO_TO_REPORT(total_sales) OVER (PARTITION BY region), 4) contribution
FROM (select * 
FROM total_sales(region => 'europe') 
ORDER BY 3 desc);
</code>",01-OCT-20 09.16.41.908063000 AM,"KEITH.LAKER@ORACLE.COM",01-OCT-20 09.21.29.479976000 AM,"KEITH.LAKER@ORACLE.COM"
182652206532416699442649736144665328043,182241645422959190146194127811898911783,"Making the Most of this Tutorial",5,"<p><strong>What You Should Know</strong></p>
<p>
Before taking this tutorial, you should be proficient with the basics of SQL and PL/SQL programming. For example, you know how to create procedures and functions. You know how to write DML statements. In addition, it'd be helpful to have some experience with collections, which are a requirement for use with bulk processing. You might find this blog post helpful: https://stevenfeuersteinonplsql.blogspot.com/2018/08/the-plsql-collection-resource-center.html
</p>
<p>
Be sure to run the Prerequisite SQL (red button above the list of modules) before trying to execute code in the modules. It creates a local copy of the employees table for you to work with.
</p>
<p>
If when you run code, you see this error...
</p>
<pre>
ORA-00942: table or view does not exist
</pre>
<p>
...that almost certainly means you skipped the setup step!
</p>
<p>
Run that same code again at any time if you'd like to reset the employees table to its ""original"" state.
</p>
<p>
Many of the blocks finish with a ROLLBACK statement to avoid changing the table's contents. Please don't conclude from these scripts that <i>you</i> should include a ROLLBACK in your exception handlers. That should be done only with great care and consideration.
</p>
<p>
You can run each of the code examples as is, to see the expected behavior. You can then modify that code as you like to play around with these fantastic features.
</p>
<p>
But you want to write some code yourself, right? That's how you learn best! So at the end of most of the modules, you will find:</p>
<ul>
<li>Fill in the Blanks: partially-written code that you need to finish up, that reinforces the content of that module.</li>
<li>Exercises: You do all the coding to solve the stated requirement (be on the lookout for copy/paste opportunities from the module to speed things up).</li>
</ul>
<p>
The very last module of the tutorial (""Solutions to Exercises"") contains my solutions to these exercises. I bet you could figure that out all by yourselves. :-)
</p>
<p>VERY IMPORTANT: if you find any mistakes or have suggestions to improve this tutorial, you can submit feedback through LiveSQL or sent an email directly: steven dot feuerstein at oracle dot com (so there, bots! :-)).
</p>",21-MAY-19 08.11.41.592129000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",30-OCT-19 06.14.32.747903000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
152139165451867071747989892650291204195,152139165451368994310308665430312259683,"Conditional Multi-table Insert",80,"<p>When doing a multi-table insert, you may want choose which tables to add rows to at runtime. You can do this by adding a when clause before ""into table"". The database will only add rows to this table if the condition is true.</p>

<p>Conditional multi-table inserts come in two forms: all and first.</p>

<pre>insert [ all | first ]
  when test1 then 
    into t1 ...
  when test2 then 
    into t2 ...
    into t3 ...
  else
    into t4 ...
  select * from query;</pre>

<h3>All</h3>

<p>When you specify all, the database evaluates every condition for each row the query returns. Each row goes in every table where the condition is true.</p>

<p>You can also add an else clause. This inserts rows which match no other conditions.</p>

<h3>First</h3>

<p>Insert first evaluates the conditions from top to bottom. If a row matches many conditions, it only adds the row to the highest one that is true. It skips all remaining tests.</p>

<p>As with insert all, you can provide an else clause to catch rows which match no other conditions.</p>

<p>You can see the difference between all and first in code below. The row for toy_id 11 has the name Cuteasaurus and colour blue. So it matches both of these tests:</p>

<pre>  when colour = 'blue' then
    ..
  when toy_name = 'Cuteasaurus' then</pre>

<p>So insert all adds two copies of this row into bricks. But insert first stops processing when identifies the colour as blue. So it only inserts one copy:</p>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 11, 'Cuteasaurus', 'blue' );
insert into toys ( toy_id, toy_name, colour ) 
  values ( 12, 'Sir Stripypants', 'blue' );
insert into toys ( toy_id, toy_name, colour ) 
  values ( 13, 'White Rabbit', 'white' );

exec savepoint post_toys;

insert all
  when colour = 'blue' then
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  when toy_name = 'Cuteasaurus' then
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  else
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  select * from toys
  where  toy_id >= 11;
  
select * from bricks
where  brick_id >= 11;

rollback to savepoint post_toys;

insert first
  when colour = 'blue' then
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  when toy_name = 'Cuteasaurus' then
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  else
    into bricks ( brick_id, colour ) 
    values ( toy_id, colour )
  select * from toys
  where  toy_id >= 11;
  
select * from bricks
where  brick_id >= 11;

rollback;</code>

<p>In both statements, the row for toy_id 13 matches no criteria. So it falls into the else block.</p>",02-AUG-18 01.55.48.106016000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.13.29.827130000 PM,"CHRIS.SAXON@ORACLE.COM"
152139165452964776392199975940924412003,152139165451368994310308665430312259683,"Performance: Single Row vs. Multi-row",42,"<p>In general it's better to combine SQL commands into as few statements as possible. This will give the best performance. </p>

<p>For example, the following compares the performance of single-row vs multi-row inserts. Both add 50,000 rows. Single-row adds one row 50,000 times. Multi-row adds 50,000 rows in one go. The multi-row version will be 10x to 100x faster!</p>

<code>declare
  start_time   pls_integer;
  insert_count pls_integer := 50000;
begin
  start_time := dbms_utility.get_time ();
  
  for i in 1 .. insert_count loop
    insert into bricks 
    values ( i, 'red', 'cube' );
  end loop;
  
  dbms_output.put_line ( 'Single-row duration = ' || ( dbms_utility.get_time () - start_time) );
  
  rollback;
  
  start_time := dbms_utility.get_time ();
  
  insert into bricks
    select level, 'red', 'cube' from dual
    connect by level <= insert_count;
  
  dbms_output.put_line ( 'Multi-row duration = ' || ( dbms_utility.get_time () - start_time) );
  
  rollback;
end;
/</code>

<p>So use multi-row inserts where you can!</p>",02-AUG-18 02.20.51.305632000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.03.40.339526000 PM,"CHRIS.SAXON@ORACLE.COM"
152139165451771566608240336945489416291,152139165451368994310308665430312259683,"Multi-row Insert",40,"<p>You can add many rows in one statement. To do this, you insert the output of a query.</p>

<p>For example, the insert below gets all the toy_ids in the toys table. Then inserts them into the bricks table:</p>

<code>select * from toys;

insert into bricks ( brick_id )
  select toy_id
  from   toys;

select * from bricks;</code>

<p>Like single-row inserts, the number of columns in the query and target table must match. The data types of the columns in each position must also be compatible. </p>

<p>The toys table has three columns. But the query only returns one column. So the following statement will fail: </p>

<code>insert into bricks ( brick_id )
  select *
  from   toys;</code>

<p>So, as with standalone queries, it's a good idea to list the columns in your select clause.</p>",02-AUG-18 01.53.35.812488000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.02.49.662200000 PM,"CHRIS.SAXON@ORACLE.COM"
156293833500577138176930945573078207494,152848903946926018441986284250590889613,"Conceptual Model",30,"<p>The conceptual model is a high-level overview of the information the database will store. It defines key entities in the application. Entities are real-world things the database will store details of.</p>

<p>For the booking system, the key entities are:</p>

<ul>
<li>The patient - the person seeking treatment</li>
<li>The consultant - the person who will diagnose the patient's condition & prescribe treatment</li>
<li>The appointment - the place and time of the consultation</li>
</ul>",11-SEP-18 08.37.50.697427000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 09.16.18.934910000 AM,"CHRIS.SAXON@ORACLE.COM"
156293833500693195055613949973850000390,152848903946926018441986284250590889613,"Logical Model",50,"<p>The logical model fleshes out the details in the conceptual model. It documents which aspects of the entities the system will store. These are the attributes of the entity. You represent this in an Entity-Relationship Diagram (ERD).</p>

<p>At this point, we've identified the following information to store:</p>

<h3>Patient</h3>

<ul>
<li>Their name</li>
</ul>

<h3>Consultant</h3>

<ul>
<li>Their name</li>
</ul>

<h3>Appointment</h3>

<ul>
<li>The date & time</li>
<li>The clinic name & address</li>
<li>The patient and consultant attending</li>
</ul>

<p>At this point you may identify new entities. For example, the appointment stores the clinic details. Every appointment at a clinic will be at the same place. Storing the address on each appointment duplicates these details, making data errors likely.</p>

<p>E.g., both the appointments below are for the PHYSIO clinic. But there's a different address for each!</p>

<pre><b>APPOINTMENT_DATETIME CLINIC_NAME CLINIC_ADDRESS</b>
1 SEP 2018 10:00     PHYSIO      1 Hospital Way
2 SEP 2018 12:00     PHYSIO      3 Hospital Street</pre>

<p>To avoid this, create a new entity, clinic. This stores the details of each clinic:</p>

<h3>Clinic</h3>

<ul>
<li>Clinic name</li>
<li>Its address</li>
</ul>

<p>The appointment will now store only the clinic's name:</p>

<h3>Appointment</h3>

<ul>
<li>The date & time</li>
<li>The clinic name</li>
<li>The patient and consultant attending</li>
</ul>

<p>Finding dependencies like this and splitting the tables is a process called normalization. This removes redundant information, ensuring you store each fact once.</p>

<p>The logical model will also define the data types of each attribute. And any constraints that apply. For example, when booking an appointment, the date must be in the future.</p>

<p>You should also define the attributes that uniquely identify each instance of an entity. These will form the primary and unique keys for the tables.</p>",11-SEP-18 08.41.12.397111000 AM,"CHRIS.SAXON@ORACLE.COM",30-JAN-23 08.41.50.479735000 PM,"SHARON.KENNEDY@ORACLE.COM"
156293833501527353871148044104397261830,152848903946926018441986284250590889613,"Physical Model",70,"<p>Once you've built your logical model, it's time to translate this to the physical model. The output of this is the create table statements to build the database.</p>

<p>At this point you should take into account non-functional requirements, such as performance. This will influence which type of table you create. For example, whether to partition it or build an index-organized table.</p>

<p>This leads to the following tables:</p>

<code>create table consultants (
  consultant_id   integer,
  consultant_name varchar2(100)
);

create table patients (
  patient_id   integer,
  patient_name varchar2(100)
);

create table clinics (
  clinic_name varchar2(10),
  address     varchar2(1000)
);

create table appointments (
  appointment_id       integer,
  appointment_datetime date,
  clinic_name          varchar2(30),
  consultant_id        integer,
  patient_id           integer
);</code>",11-SEP-18 08.50.52.848256000 AM,"CHRIS.SAXON@ORACLE.COM",11-SEP-18 09.13.10.827367000 AM,"CHRIS.SAXON@ORACLE.COM"
182247975187584996956704695256491893135,182241645422959190146194127811898911783,"The Row by Row Problem",10,"<p>
Almost every program Oracle Database developers write includes both PL/SQL and SQL statements. PL/SQL statements are run by the PL/SQL statement executor; SQL statements are run by the SQL statement executor. When the PL/SQL runtime engine encounters a SQL statement, it stops and passes the SQL statement over to the SQL engine. The SQL engine executes the SQL statement and returns information back to the PL/SQL engine. This transfer of control is called a context switch, and each one of these switches incurs overhead that slows down the overall performance of your programs.
</p>
<p>
Letâ€™s look at a concrete example to explore context switches more thoroughly and identify the reason that FORALL and BULK COLLECT can have such a dramatic impact on performance.
</p>
<p>
Suppose my manager asked me to write a procedure that accepts a department ID and a salary percentage increase and gives everyone in that department a raise by the specified percentage. Taking advantage of PL/SQLâ€™s elegant cursor FOR loop and the ability to call SQL statements natively in PL/SQL, I can implement this requirement easily:
</p>
<code>CREATE OR REPLACE PROCEDURE increase_salary (
   department_id_in   IN employees.department_id%TYPE,
   increase_pct_in    IN NUMBER)
IS
BEGIN
   FOR employee_rec
      IN (SELECT employee_id
            FROM employees
           WHERE department_id =
                    increase_salary.department_id_in)
   LOOP
      UPDATE employees emp
         SET emp.salary = emp.salary + 
             emp.salary * increase_salary.increase_pct_in
       WHERE emp.employee_id = employee_rec.employee_id;
      DBMS_OUTPUT.PUT_LINE ('Updated ' || SQL%ROWCOUNT);
   END LOOP;
END increase_salary;
</code>
<p>
Suppose there are 10000 employees in department 15. When I execute this block....
</p>
<code>BEGIN
   increase_salary (50, .10);
   ROLLBACK; -- to leave the table in its original state
END;
</code>
<p>
....the PL/SQL engine will â€œswitchâ€ over to the SQL engine 10000 times, once for each row being updated. Tom Kyte of AskTom fame (https://asktom.oracle.com) refers to row-by-row switching like this as â€œslow-by-slow processing,â€ and it is definitely something to be avoided.
</p>",17-MAY-19 07.58.14.726959000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",28-OCT-19 08.42.24.047708000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247975187889646263247581808517849487,182241645422959190146194127811898911783,"Introduction to BULK COLLECT",30,"<p>
To take advantage of bulk processing for queries, you simply put BULK COLLECT before the INTO keyword of your fetch operation, and then provide one or more collections after the INTO keyword. 
</p><p>
Here are some things to know about how BULK COLLECT works:
</p>
<ul>
<li>
    It can be used with all three types of collections: associative arrays, nested tables, and VARRAYs.
</li><li>
    You can fetch into individual collections (one for each expression in the SELECT list) or a single collection of records.
</li><li>
    The collection is always populated densely, starting from index value 1.
</li><li>
    If no rows are fetched, then the collection is emptied of all elements.
</li></ul>
<p>
You can use BULK COLLECT in all these forms:
</p>
<pre>
SELECT column(s) BULK COLLECT INTO collection(s)
FETCH cursor BULK COLLECT INTO collection(s)
EXECUTE IMMEDIATE query_string BULK COLLECT INTO collection(s)
</pre>
<p>
Here's a block of code that fetches all rows in the employees table with a single context switch, and loads the data into a collection of records that are based on the table.
</p>
<code>DECLARE
   TYPE employee_info_t IS TABLE OF employees%ROWTYPE;
   l_employees   employee_info_t;
BEGIN
   SELECT *
     BULK COLLECT INTO l_employees
     FROM employees
    WHERE department_id = 50;
   DBMS_OUTPUT.PUT_LINE (l_employees.COUNT);
END;
</code>
<p>
If you do not want to retrieve all the columns in a table, create your own user-defined record type and use that to define your collection. All you have to do is make sure the list of expressions in the SELECT match the record type's fields.
</p>
<code>DECLARE
   TYPE two_cols_rt IS RECORD (
      employee_id   employees.employee_id%TYPE,
      salary        employees.salary%TYPE
   );

   TYPE employee_info_t IS TABLE OF two_cols_rt;

   l_employees   employee_info_t;
BEGIN
   SELECT employee_id, salary
     BULK COLLECT INTO l_employees
     FROM employees
    WHERE department_id = 50;
   DBMS_OUTPUT.PUT_LINE (l_employees.COUNT);
END;
</code>
<p>
<strong>Quick Tip</strong></p>
<p><p>
You can avoid the nuisance of declaring a record type to serve as the type for the collection through the use of a ""template cursor."" This cursor should have the same select list as the BULK COLLECT query. You can, however, leave off the WHERE clause and anything else after the FROM clause, because it will never be used for anything but a %ROWTYPE declaration. Here's an example:
</p>
<pre>
DECLARE
   CURSOR employee_info_c IS
      SELECT employee_id, salary 
        FROM employees ;

   TYPE employee_info_t IS TABLE OF employee_info_c%ROWTYPE;

   l_employees   employee_info_t;
BEGIN
   SELECT employee_id, salary
     BULK COLLECT INTO l_employees
     FROM employees
    WHERE department_id = 10;
END;
</pre>
<p>
<strong>Fill in the Blanks</strong></p>
<p>
In the block below replace the #FINISH# tags with code so that the last names of all employees in department 50 are displayed.
</p>
<code>DECLARE
   #FINISH#
   l_names   names_t;
BEGIN
   SELECT last_name
     #FINISH#
     FROM employees
    WHERE department_id = 50
    ORDER BY last_name;
END;
</code>
<p>
<strong>Exercise 1</strong></p>
<p>Write a stored procedure that accepts a department ID, uses BULK COLLECT to retrieve all employees in that department, and displays their first name and salary. Then write an anonymous block to run that procedure for department 100.
</p>",17-MAY-19 07.59.04.462828000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",12-JUN-19 01.52.32.724749000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247975188912397506641558090319274383,182241645422959190146194127811898911783,"Managing PGA Memory with the LIMIT Clause",40,"<p>
As with almost all other types of variables and constants you use in your code, collections consume PGA (process global area) memory. If your collection gets too large, your users might encounter an error. To see this happen, run the code below (note: varchar2a is a collection type of strings defined in the DBMS_SQL package).
</p>
<code>DECLARE
   l_strings   DBMS_SQL.varchar2a;
BEGIN
   FOR indx IN 1 .. 2 ** 31 - 1
   LOOP
      l_strings (indx) := RPAD ('abc', 32767, 'def');
   END LOOP;
END;
</code>
<p>
When using BULK COLLECT, you could attempt to retrieve too many rows in one context switch and run out of PGA memory. To help you avoid such errors, Oracle Database offers a LIMIT clause for BULK COLLECT. Indeed, when using BULK COLLECT we recommend that you never or at least rarely use an ""unlimited"" BULK COLLECT which is what you get with a SELECT BULK COLLECT INTO (an implicit query) - and you saw in the previous module.
</p>
<p>
Instead, declare a cursor (or a cursor variable), open that cursor, and then in a loop, retrieve N number of rows with each fetch.
</p>
<p>
In the block below, I set my fetch limit to just 10 rows to demonstrate how this feature works. You will likely never want to set the limit to less than 100 - this topic is explored further below.
</p>
<code>DECLARE
   c_limit PLS_INTEGER := 10;

   CURSOR employees_cur
   IS
      SELECT employee_id
        FROM employees
       WHERE department_id = 50;

   TYPE employee_ids_t IS TABLE OF 
      employees.employee_id%TYPE;

   l_employee_ids   employee_ids_t;
BEGIN
   OPEN employees_cur;

   LOOP
      FETCH employees_cur
      BULK COLLECT INTO l_employee_ids
      LIMIT c_limit;

      DBMS_OUTPUT.PUT_LINE (l_employee_ids.COUNT || ' fetched');

      EXIT WHEN l_employee_ids.COUNT = 0;
   END LOOP;
END;
</code>
<p>
One thing to watch out for when switching to LIMIT with BULK COLLECT (in a loop) is following the same pattern for single-row fetching in a loop. I demonstrate this issue below, but first, a reminder: there are 107 rows in the employees table.
</p>
<code>SELECT COUNT(*) FROM employees</code>
<p>
Here's the common way to terminate a loop in which you fetch row-by-row from an explicit cursor:
</p>
<code>DECLARE
   CURSOR emps_c IS SELECT * FROM employees;
   l_emp   emps_c%ROWTYPE;
   l_count INTEGER := 0;
BEGIN
   OPEN emps_c;

   LOOP
      FETCH emps_c INTO l_emp;
      EXIT WHEN emps_c%NOTFOUND;
      DBMS_OUTPUT.put_line (l_emp.employee_id);
      l_count := l_count + 1;
   END LOOP;
   DBMS_OUTPUT.put_line ('Total rows fetched: ' || l_count);
END;
</code>
<p>
In other words: fetch a row, stop if the cursor has retrieved all rows. Now let's switch to using BULK COLLECT and LIMIT, fetching 10 rows at a time, using the same approach to exiting the loop.
</p>
<code>DECLARE
   CURSOR emps_c IS SELECT * FROM employees;
   TYPE emps_t IS TABLE OF emps_c%ROwTYPE;
   l_emps   emps_t;
   l_count INTEGER := 0;
BEGIN
   OPEN emps_c;

   LOOP
      FETCH emps_c BULK COLLECT INTO l_emps LIMIT 10;
      EXIT WHEN emps_c%NOTFOUND;
      DBMS_OUTPUT.put_line (l_emps.COUNT);
      l_count := l_count + l_emps.COUNT;
   END LOOP;
   DBMS_OUTPUT.put_line ('Total rows fetched: ' || l_count);
END;
</code>
<p>Wait, what? Is that right? Do I see ""Total rows fetched: 100""? Yes, I do. And therein lies the trap. You cannot continue to use the same EXIT WHEN statement in the same place in your loop when you switch to BULK COLLECT with LIMIT.</p>
<p>
The very last fetch performed retrieved the last 7 rows, but also exhausted the cursor. So the %NOTFOUND returns TRUE, while the collection has those 7 elements in it.</p>
<p>
To terminate a loop using BULK COLLECT with LIMIT, you should either:</p>
<ul>
<li>Move the EXIT WHEN to the bottom of the loop body, or</li>
<li>Ignore the cursor and check the collection. When empty, terminate.</li>
</ul>
<p>These two approaches are shown below.</p>

<code>DECLARE
   CURSOR emps_c IS SELECT * FROM employees;
   TYPE emps_t IS TABLE OF emps_c%ROwTYPE;
   l_emps   emps_t;
   l_count INTEGER := 0;
BEGIN
   OPEN emps_c;
   LOOP
      FETCH emps_c BULK COLLECT INTO l_emps LIMIT 10;
      l_count := l_count + l_emps.COUNT;
      EXIT WHEN emps_c%NOTFOUND;
   END LOOP;
   DBMS_OUTPUT.put_line ('Total rows fetched: ' || l_count);
END;
</code>

<code>DECLARE
   CURSOR emps_c IS SELECT * FROM employees;
   TYPE emps_t IS TABLE OF emps_c%ROwTYPE;
   l_emps   emps_t;
   l_count INTEGER := 0;
BEGIN
   OPEN emps_c;

   LOOP
      FETCH emps_c BULK COLLECT INTO l_emps LIMIT 10;
      EXIT WHEN l_emps.COUNT = 0;
      l_count := l_count + l_emps.COUNT;
   END LOOP;
   DBMS_OUTPUT.put_line ('Total rows fetched: ' || l_count);

   CLOSE emps_c;
END;
</code>

<p>
<strong>Fill in the Blanks</strong></p>
<p>
The employees table has 107 rows in it. In the block below replace the #FINISH# tags with code so that when the block is executed, the following text is displayed:
</p>
<pre>
Rows fetched 25
Rows fetched 25
Rows fetched 25
Rows fetched 25
Rows fetched 7
Rows fetched 0
</pre>
<code>DECLARE
   CURSOR ids_c IS SELECT employee_id FROM employees;
   #FINISH#
   l_count INTEGER;
BEGIN
   OPEN ids_c;

   LOOP
      FETCH ids_c #FINISH#;
      DBMS_OUTPUT.put_line ('Rows fetched: ' || l_count);
      EXIT WHEN #FINISH#;
   END LOOP;

   CLOSE ids_c;
END;
</code>

<p><strong>Exercise 2</strong></p>
<p>
Write an anonymous block that fetches (using BULK COLLECT) only the last name and salary from the employees table 5 rows at a time, and then displays that information. Make sure 107 names and salaries are shown!
</p>",17-MAY-19 08.00.02.077900000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",26-JUN-19 04.30.58.087859000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
182247975188952292058688840853084578191,182241645422959190146194127811898911783,"Dynamic SQL and BULK COLLECT",70,"<p>
You can also use BULK COLLECT with native dynamic SQL queries that might return more than one row. As with SELECT-INTO and FETCH-INTO, just stick that ""BULK COLLECT"" before the INTO and provide a collection (or multiple) to hold the multiple values returned.
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;

   l_ids   ids_t;
BEGIN
   EXECUTE IMMEDIATE
      'SELECT employee_id FROM employees WHERE department_id = :dept_id'
      BULK COLLECT INTO l_ids
      USING 50;

   FOR indx IN 1 .. l_ids.COUNT
   LOOP
      DBMS_OUTPUT.put_line (l_ids (indx));
   END LOOP;
END;
</code>
<p>
You can even get ""fancy"" and use BULK COLLECT in the RETURNING clause of a dynamic DML statement:
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;
   l_ids ids_t;
BEGIN
   EXECUTE IMMEDIATE
      'UPDATE employees SET last_name = UPPER (last_name)
          WHERE department_id = 100
          RETURNING employee_id INTO :ids'
   RETURNING BULK COLLECT INTO l_ids;
   
   FOR indx IN 1 .. l_ids.COUNT 
   LOOP
      DBMS_OUTPUT.PUT_LINE (l_ids (indx));
   END LOOP;
END;
</code>

<p><strong> Exercise 5:</strong></p>
<p>
Write the rest of the procedure whose signature is shown below. Use BULK COLLECT to fetch all the last names from employees identified by that WHERE clause and return the collection. Then write an anonymous block to test your procedure: pass different WHERE clauses, and display names retrieved.
</p>
<pre>
PROCEDURE get_names (
   where_in IN VARCHAR2,
   names_out OUT DBMS_SQL.VARCHAR2_TABLE)
</pre>",17-MAY-19 08.01.32.740749000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",05-JUN-19 03.23.35.150615000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
156294991118184812155579449132289109335,152848903946926018441986284250590889613,"Overview",15,"<p>The basic steps for designing a database for a new application are:</p>

<ul>
<li>Capturing requirements</li>
<li>Build the conceptual model</li>
<li>Design the logical model</li>
<li>Create the physical model</li>
</ul>

<p>Building a database is an iterative process. As you go through the steps, consult with the users regularly to check it meets their needs. Often you'll need to adjust the model as you hone in on what the system must do.</p>",11-SEP-18 08.32.55.078674000 AM,"CHRIS.SAXON@ORACLE.COM",14-SEP-18 12.10.00.531547000 PM,"CHRIS.SAXON@ORACLE.COM"
155038910857885306830335222389754593178,152848903946926018441986284250590889613,"Introduction",10,"<p>Data modelling is the process of taking user requirements and translating them into a table design. This is a large topic, worthy of a full course in its own right! This tutorial gives a brief overview of the process.</p>",30-AUG-18 08.09.09.237483000 AM,"CHRIS.SAXON@ORACLE.COM",14-SEP-18 03.00.09.612340000 PM,"CHRIS.SAXON@ORACLE.COM"
155038910857933663863119807556742840218,152848903946926018441986284250590889613,"Capturing Requirements",20,"<p>The first step in building a database is to find out what information you need to store. To complete this process, speak with people who will use the system. This could be potential customers or in-house staff in the business.</p>

<p>For example, say you're building a system to manage appointments for hospital clinics. This will allow patients to:</p>

<ul><li>View available appointment times & select one to attend</li>
<li>View details of upcoming appointments they've booked</li></ul>

<p>It must store the following information:</p>

<ul><li>The location, date, and time of the appointment</li>
<li>The name of the patient</li>
<li>The name of the consultant who will see the patient</li></ul>

<p>These are the functional requirements. i.e. what the system must do to serve its purpose. At this stage you should also capture non-functional requirements. These define how the application works.</p>

<p>For example, all pages in the application must load in under two seconds. These can influence how you build your tables.</p>",30-AUG-18 08.09.33.314072000 AM,"CHRIS.SAXON@ORACLE.COM",04-SEP-18 08.23.01.249367000 AM,"CHRIS.SAXON@ORACLE.COM"
155038910858159732991387743212412895130,152848903946926018441986284250590889613,"Relational vs. Document Storage",80,"<p>The process above may take a while to complete. To save time, you may be tempted to go straight from the requirements to storing each appointment as a document in on table, like so:</p>

<code>create table appointments (
  appointment_doc varchar2(4000)
);</code>

<p>There are various document formats such as JSON (JavaScript Object Notation) or XML that allow you to do this.</p>

<p>For example, for each appointment you could store a JSON document like the following:</p>

<pre>{
  appointmentDatetime: ""2018-09-01 10:00"",
  location: {
    name: ""PHYSIO"",
    address: ""1 Hospital Way""
  },
  consultant: {
    name: ""Doctor Awesome""
  }
  patient: {
    name: ""Miss Sick""
  }
}</pre>

<p>But this has many drawbacks. You need to look at the document to know the attribute names. Which makes it harder to query your data.</p>

<p>And it duplicates many details. Such as the clinic address and the consultant and patient names. This can lead to data errors.</p>

<p>For example, this document is for another appointment with Doctor Awesome:</p>

<pre>{
  appointmentDatetime: ""2018-09-01 11:00"",
  location: {
    name: ""PHYSIO"",
    address: ""3 Hospital Street""
  },
  consultant: {
    name: ""Doctor J Awesome""
  }
  patient: {
    name: ""Mr. Hypochondriac""
  }
}</pre>

<p>But this repeats the address error described before. And it stores the doctor's name with their initial, J. This is different to the first appointment. Errors like this can lead to confusion among staff and patients using the system. The time you saved in up-front design is often lost in the ongoing maintenance of the application.</p>

<p>In some cases, storing everything in a single document is the way to go. But taking time to build and create a relational model will make your system easier to use and more flexible to change in the long run.</p>",30-AUG-18 08.09.58.892098000 AM,"CHRIS.SAXON@ORACLE.COM",14-SEP-18 03.06.34.902237000 PM,"CHRIS.SAXON@ORACLE.COM"
219176609268093631311362933120559642568,219172783004222394396035270452714317015,"Finding When Predicted Sales Exceed Stock Level",30,"<p>We can now combine the hourly actual and predicted sales figures. We want to include actual figures up to the time of the last recorded sale. After this, the query should use the expected figures.</p>

<p>This query returns the real or projected sales figures. Then computes the running total of sales of this combined figure:</p>

<code>alter session set nls_date_format='YYYY-MM-DD HH24:MI:SS';

with shop as (
  select s.shopid
       , s.containers * 250 startnem
  from   fw_store s
), budget as (
  select db.shopid
       , db.budgetdate + numtodsinterval(hb.hour,'hour') budgethour
       , db.budgetnem * hb.percent / 100 budgetnem
  from   fw_daybudget db
  cross  join fw_hourbudget hb
), nem as (
  select budget.shopid
       , shop.startnem
       , budget.budgethour hour
       , case
           when budget.budgethour < to_date('2011-12-29 12:00:00','YYYY-MM-DD HH24:MI:SS')
           then 'S'
           else 'B'
         end salesbudget
       , case
           when budget.budgethour < to_date('2011-12-29 12:00:00','YYYY-MM-DD HH24:MI:SS')
           then nvl(sales.salesnem,0)
           else budget.budgetnem
         end qtynem
  from   shop
  join   budget
  on     budget.shopid = shop.shopid
  left outer join fw_sales sales
  on     sales.shopid = budget.shopid
  and    sales.saleshour = budget.budgethour
)
select nem.shopid
     , nem.hour
     , nem.salesbudget
     , nem.qtynem
     , sum(nem.qtynem) over (
         partition by nem.shopid
         order by nem.hour
         rows between unbounded preceding and current row
       ) sumnem
     , greatest(nem.startnem - nvl(
         sum(nem.qtynem) over (
           partition by nem.shopid
           order by nem.hour
           rows between unbounded preceding and 1 preceding
         )
       ,0),0) stocknem
  from nem
 order by shopid, hour;</code>

<p>It also predicts the remaining stock at each time by subtracting the running total of hourly sales (real or predicted) from the starting stock. Notice that this has this window clause:</p>

<pre>rows between unbounded preceding and 1 preceding</pre>

<p>This means include all the previous rows and exclude the current row. This is because we want to return the expected stock at the start of the hour. Including the current row returns the expected level at the end of the hour.</p>",05-MAY-20 09.36.49.655359000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.44.19.291080000 PM,"CHRIS.SAXON@ORACLE.COM"
221711771249831442953506711601073013597,210638425667855918502469137992560342521,"Check for Side Effects",12,"<p>Unlike queries, other processes can affect writes. Two common culprits are:</p>
<ul>
<li>Triggers</li>
<li>Blocking sessions</li>
</ul>

<h3>Triggers</h3>

<p>Triggers run extra code when processing your insert, update or delete. You can check if there are any triggers on your tables by querying the *_TRIGGERS views:</p>

<code>select * from user_triggers;</code> 

<p>If there are, review the code in the trigger. It may be the case the write is slow because of the work done in the trigger. If this is the case, see if you can change rewrite the process - ideally without using triggers!</p>

<h3>Blocking Locks</h3>

<p>Unlike SELECTS, uncommitted changes in other sessions can stop a write completing. Common causes are:</p>

<ul>
<li>Clashes on primary or unique keys for inserts</li>
<li>Another session updating or deleting the same rows</li>
</ul>

<p>It can be hard to completely avoid this problem. For example, if two users need to update the same rows at the same time, the second must wait until the first to end their transaction. To minimize its effect, ensure you commit or rollback transactions as soon as possible. If these continue to be an issue, you may need to redesign how you process data changes.</p>",29-MAY-20 04.43.01.904284000 PM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.43.27.481129000 PM,"CHRIS.SAXON@ORACLE.COM"
221495323642379379919960493235953628400,210638425667855918502469137992560342521,"Single Row Inserts vs Batch Inserts",15,"<p>This code inserts 10,000 rows in the bricks table:</p>

<code>begin
  delete bricks;
  for i in 1 .. 10000 loop 
    insert into bricks  
      values (  
        i,  
        case mod ( i, 3 )  
          when 0 then 'red' 
          when 1 then 'blue' 
          when 2 then 'green' 
        end, 
        case mod ( i, 2 )  
          when 0 then 'cube' 
          when 1 then 'cylinder' 
        end, 
        round ( dbms_random.value ( 2, 10 ) ) 
       ); 
  end loop; 
end;
/</code>

<p>This means the database has to run the INSERT statement 10,000 times. Although each individual execution is fast, these add up quickly, making the overall process slow. This particularly problematic if the SQL comes from the mid-tier application, as this makes a roundtrip to the database for each row.</p>

<p>If you have a commit inside the loop, the process will be even <strong>slower</strong>!</p>

<p>You can make this process faster using bulk processing.</p>

<p>To use this, load an array with data. Then use the FORALL statement. This looks like a FOR LOOP. But unlike a loop, it only processes the DML statement inside once:</p>

<code>delete bricks;
select count(*) from bricks;

declare
  type bricks_rec is record (
    brick_id integer, colour varchar2(10),
    shape    varchar2(10), weight integer
  );
  type bricks_array is table of bricks_rec
    index by pls_integer;
    
  brick_values bricks_array;
begin 
  for i in 1 .. 10000 loop
    brick_values ( i ) := bricks_rec (
      brick_id => i + 20000, 
      colour => case mod ( i, 3 )  
        when 0 then 'red' 
        when 1 then 'blue' 
        when 2 then 'green' 
      end, 
      shape => case mod ( i, 2 )  
        when 0 then 'cube' 
        when 1 then 'cylinder' 
      end, 
      weight => round ( dbms_random.value ( 2, 10 ) ) 
    );
  end loop;
  
  forall rws in 1 .. brick_values.count
    insert into bricks  
    values brick_values ( rws );
    
end; 
/

select count(*) from bricks;</code>

<p>You can also execute bulk or batch DML statements from the middle tier in most programming languages. Check the documentation for your technology to find out how to do this.</p>",27-MAY-20 02.19.33.007470000 PM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.45.09.819525000 PM,"CHRIS.SAXON@ORACLE.COM"
151174670266518265980301101511597639352,117463246204528657365867867361809943103,"Try It!",25,"<p>Complete the following statement to create the toys table with a column called toy_name:</p>

<code>create table toys (
  /* TODO */ varchar2(10)
);

select column_name, data_type, data_length
from   user_tab_columns
where  table_name = 'TOYS';</code>

<p>The query after should return the following row:</p>

<pre><b>COLUMN_NAME   DATA_TYPE   DATA_LENGTH   </b>
TOY_NAME      VARCHAR2               10</pre>",24-JUL-18 08.37.42.544483000 AM,"CHRIS.SAXON@ORACLE.COM",24-JUL-18 08.40.23.343459000 AM,"CHRIS.SAXON@ORACLE.COM"
214381017126485929061197948138673564893,210638425667855918502469137992560342521,"Many One Row Deletes vs. One Many Row Delete",27,"<p>Finally, DELETE. As with the other DML statements, running a single DELETE removing all the rows is faster than lots of statements removing one at a time.</p>

<p>This removes all the rows with BRICK_IDs between 1 and 1,000 one at a time:</p>

<code>declare
  num_rows pls_integer := 10000;
begin 
  ins_rows ( num_rows );

  for rw in 1 .. 1000 loop 
   
    delete bricks b
    where  b.brick_id = rw; 
     
  end loop; 

end;
/</code>

<p>It's much better to run a single DELETE with a WHERE clause that removes desired rows:</p>

<code>exec ins_rows ( 10000 );
   
delete bricks b
where  b.brick_id between 1 and 1000;</code>

<p>As with UPDATE and INSERT, you can use bulk/batch processing to process many rows in one call.</p>


<code>exec   ins_rows ( 10000 );

select count(*) from bricks;

declare
  rws dbms_sql.number_table;

begin 

  for i in 1 .. 1000 loop
    rws (i) := i;
  end loop;
   
  forall i in 1 .. rws.count 
    delete bricks 
    where  brick_id = rws (i); 

end;
/

select count(*) from bricks;</code>",20-MAR-20 11.32.12.191127000 AM,"CHRIS.SAXON@ORACLE.COM",21-MAY-20 03.23.19.463035000 PM,"CHRIS.SAXON@ORACLE.COM"
216473263046127764725201661277349009542,210638425667855918502469137992560342521,"Try It!",29,"<p>This code loops through all the rows with a weight less than six and sets their weight to one. Run it a few times to get a feel for how long it takes to execute:</p>
<code>declare
  num_rows pls_integer := 100000;
begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 

  for rws in (select * from bricks where weight <= 5) loop 
   
    update bricks 
    set    weight = 1
    where  brick_id = rws.brick_id; 
     
  end loop; 

  timing_pkg.calc_runtime ( 'Update-loop' );
end;
/</code>

<p>Replace /* TODO */ in the code below to turn the update above into one statement:</p>

<code>declare
  num_rows pls_integer := 100000;
begin
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  update bricks 
  set    weight = 1
  where  /* TODO */ ;
  
  timing_pkg.calc_runtime ( 'Update-all' ); 
end;
/</code>

<p>How does this affect the runtime of the process?</p>",09-APR-20 12.23.46.782198000 PM,"CHRIS.SAXON@ORACLE.COM",09-APR-20 03.24.44.135084000 PM,"CHRIS.SAXON@ORACLE.COM"
211969973560192972273079995992742982541,210639799513641285543570732327153347316,"Viewing Table Stats",5,"<p>You can query dictionary views to see the current stats for a table. Key statistics are:</p>
<ul>
<li>The total number of rows in a table</li>
<li>The number of distinct (different) values in each column</li>
<li>The high and low values for each column</li>
</ul>
<p>Use this query to get these values for all the columns in the current schema:</p>
<code>select ut.table_name, ut.num_rows, 
       utcs.column_name, utcs.num_distinct, 
       case utc.data_type
         when 'VARCHAR2' then
           utl_raw.cast_to_varchar2 ( utcs.low_value ) 
        when 'NUMBER' then
           to_char ( utl_raw.cast_to_number ( utcs.low_value ) )
       end low_val, 
       case utc.data_type
         when 'VARCHAR2' then
           utl_raw.cast_to_varchar2 ( utcs.high_value ) 
         when 'NUMBER' then
           to_char ( utl_raw.cast_to_number ( utcs.high_value ) )
       end high_val
from   user_tables ut
join   user_tab_cols utc
on     ut.table_name = utc.table_name
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
and    utc.column_name = utcs.column_name
order  by ut.table_name, utcs.column_name;</code>

<p><em><strong>Note</strong>: the high and low value columns are in binary format. You need to convert them to view the values</em>.</p>

<p>You can check to see how accurate these figures are by querying the tables:</p>

<code>select count ( distinct b.colour_rgb_value ) , count (*)
from   bricks b;

select count ( distinct c.colour_rgb_value ) , count (*)
from   colours c;</code>

<p>The values in the table vary greatly from the database stats! This makes it hard for the optimizer to find the best plan.</p>",26-FEB-20 09.37.39.269675000 AM,"CHRIS.SAXON@ORACLE.COM",28-MAY-21 12.24.45.243421000 PM,"CHRIS.SAXON@ORACLE.COM"
224391182782077287523318124377585367807,210639799513782729864465643940593969908,"Indexing Nulls",64,"<p>Change the COLOUR and WEIGHT columns so they allow null values:</p>

<code>alter table bricks
  modify ( colour null, weight null );
</code> 

<p>The table may now store rows where these both of columns are NULL. If you insert such a row, it won't be in the BRICK_COLOUR_WEIGHT_I or BRICK_COLOUR_I indexes. So the query selecting just COLOUR and WEIGHT can no longer use an index:</p>

<code>select /*+ gather_plan_statistics nulls */colour, weight
from   bricks
fetch  first 10 rows only;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>This also means queries searching for null-valued colours must use a full table scan:</p>

<code>select /*+ gather_plan_statistics null colour */ *
from   bricks
where  colour is null;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>To allow IS NULL searches to use an index, there must be at least one non-null expression in the index. So you can enable the database to use BRICK_COLOUR_WEIGHT_I by making WEIGHT mandatory:</p>

<code>alter table bricks
  modify weight not null;

select /*+ gather_plan_statistics */ *
from   bricks
where  colour is null;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code> 

<p>The optimizer can use BRICK_COLOUR_WEIGHT_I to find null-valued colours because every row is guaranteed to have a weight. For example, a row with a null colour and weight of one will have this entry in the index:</p>

<pre>null, 1</pre>

<p>Make COLOUR mandatory again before continuing:</p>

<code>alter table bricks
  modify colour not null;
</code>",24-JUN-20 08.01.33.013897000 AM,"CHRIS.SAXON@ORACLE.COM",24-JUN-20 08.06.22.230159000 AM,"CHRIS.SAXON@ORACLE.COM"
224397718775585866369967899435825021766,210639799513881861781674043532919876340,"Try It Challenge!",100,"<p>Using what you've learned in this course and the results of the PL/SQL profile captured above, change the tables and start_school procedure to make it faster:</p>

<code>create or replace procedure drop_off_students (
  school integer
) as
  att_count pls_integer;
begin
  for studs in ( 
    select student_id
    from   students
    where  school_id = school
  ) loop
   
    select count (*) into att_count
    from   school_attendance
    where  student_id = studs.student_id
    and    school_id = school;
  
    if att_count = 0 then 
      insert into school_attendance ( student_id, school_id )
        values ( studs.student_id, school );
    end if;
      
  end loop;
end drop_off_students;
/

create or replace procedure start_school as
begin
  for s in (
    select school_id from schools
  ) loop
    drop_off_students ( s.school_id );
  end loop;
end start_school;
/</code>

<p>Once you've made your changes, profile start_school again to see what difference they make:</p>

<code>alter session set statistics_level = all;

declare
  run pls_integer;
  analysis_id pls_integer;
begin
  dbms_hprof.create_tables ( true ); 
  execute immediate 
    'truncate table school_attendance';

  run := dbms_hprof.start_profiling ();
  
  start_school ();

  dbms_hprof.stop_profiling; 
  dbms_output.put_line ( run );

  analysis_id := dbms_hprof.analyze ( run, run_comment => 'Test run' );
end;
/

select unit, function_elapsed_time, calls, sql_text 
from   execution_stats
order  by function_elapsed_time desc;</code> 

<p><em>You may hit space limits (ORA-01536: space quota exceeded) when running this. If you do, run the profiler a second time. This should resolve the issue.</em></p>

<p>How fast can you get this procedure to execute?</p>",24-JUN-20 09.20.12.176857000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-20 09.28.42.676300000 AM,"CHRIS.SAXON@ORACLE.COM"
217013311795228310299216160425769623167,210638425667855918502469137992560342521,"Tuning Updates Using DDL Performance Comparison",33,"<p>This compares the run time of an update changing the shape and colour of every row to doing the ""update"" using CREATE-TABLE-AS-SELECT and swapping the tables over:</p>

<code>declare
  num_rows pls_integer := 100000;
begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  update bricks 
  set    colour = 'yellow',  
         shape = 'prism'; 
     
  timing_pkg.calc_runtime ( 'Update-all' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  execute immediate q'! 
  create table bricks_update ( 
    brick_id primary key, 
    colour, shape, weight 
  ) as  
    select brick_id,  
           cast ( 'yellow'as varchar2(10) ) colour, 
           cast ( 'prism' as varchar2(10) ) shape,  
           weight 
    from   bricks!'; 
     
  execute immediate 'drop table bricks purge'; 
   
  execute immediate 'rename bricks_update to bricks'; 
   
  timing_pkg.calc_runtime ( 'Update-ctas' ); 
end; 
/
</code>
<p>The speed gains of using a ""DDL update"" are small in this case. Given the extra complexity and risks of using this, it's only worth considering for when changing millions of rows or more. In most cases UPDATE is safer, easier, and fast enough.</p>",14-APR-20 04.28.47.309444000 PM,"CHRIS.SAXON@ORACLE.COM",15-APR-20 03.19.46.829421000 PM,"CHRIS.SAXON@ORACLE.COM"
217012782882105342648185117779692247954,210638425667855918502469137992560342521,"Delete Performance Comparison",28,"<p>This compares:</p>
<ul>
<li>100k DELETE statements removing one row</li>
<li>One DELETE removing all 100k rows</li>
<li>One FORALL DELETE wiping all 100k rows</li>
</ul>
<code>declare
  num_rows pls_integer := 100000;
  rws dbms_sql.number_table;

begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 

  for i in 1 .. num_rows loop 
   
    delete bricks 
    where  brick_id = i; 
     
  end loop; 
   
  timing_pkg.calc_runtime ( 'Delete-loop' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  delete bricks; 

  timing_pkg.calc_runtime ( 'Delete-all' ); 
  ins_rows ( num_rows );
  for i in 1 .. num_rows loop
    rws (i) := i;
  end loop;
  timing_pkg.set_start_time; 
   
  forall i in 1 .. rws.count 
    delete bricks 
    where  brick_id = rws (i); 

  timing_pkg.calc_runtime ( 'Delete-forall' ); 
end;
/</code>

<p>Again, the single statement is considerably faster. This time ~2-3x faster than deleting all 100k rows one-by-one. The runtime for bulk deletion typically falls somewhere between the two other methods.</p>",14-APR-20 04.16.57.557620000 PM,"CHRIS.SAXON@ORACLE.COM",28-MAY-20 04.52.04.698323000 PM,"CHRIS.SAXON@ORACLE.COM"
219176463836227620612959530707749307265,219172783004222394396035270452714317015,"Predicting Exact Time of Zero Stock",40,"<p>The previous query returned every hour and the expected stock level. To find the time stock is expected to run out, find the last date where the remaining stock is greater than zero:</p>

<code>alter session set nls_date_format='YYYY-MM-DD HH24:MI:SS';

with shop as (
  select s.shopid
       , s.containers * 250 startnem
    from fw_store s
), budget as (
  select db.shopid
       , db.budgetdate + numtodsinterval(hb.hour,'hour') budgethour
       , db.budgetnem * hb.percent / 100 budgetnem
    from fw_daybudget db
   cross join fw_hourbudget hb
), nem as (
  select budget.shopid
       , shop.startnem
       , budget.budgethour hour
       , case
           when budget.budgethour < to_date('2011-12-29 12:00:00','YYYY-MM-DD HH24:MI:SS')
           then 'S'
           else 'B'
         end salesbudget
       , case
           when budget.budgethour < to_date('2011-12-29 12:00:00','YYYY-MM-DD HH24:MI:SS')
           then nvl(sales.salesnem,0)
           else budget.budgetnem
         end qtynem
  from   shop
  join   budget
  on     budget.shopid = shop.shopid
  left outer join fw_sales sales
  on     sales.shopid = budget.shopid
  and    sales.saleshour = budget.budgethour
)
select shopid
     , max(hour)
     + numtodsinterval(
         max(stocknem) keep (dense_rank last order by hour)
         / max(qtynem) keep (dense_rank last order by hour)
         ,'hour'
       ) zerohour
  from (
  select nem.shopid
       , nem.hour
       , nem.salesbudget
       , nem.qtynem
       , sum(nem.qtynem) over (
           partition by nem.shopid
           order by nem.hour
           rows between unbounded preceding and current row
         ) sumnem
       , greatest(nem.startnem - nvl(
           sum(nem.qtynem) over (
             partition by nem.shopid
             order by nem.hour
             rows between unbounded preceding and 1 preceding
           )
         ,0),0) stocknem
    from nem
)
where stocknem > 0
group by shopid
order by shopid;</code>

<p>To estimate the exact time stock will run out, we can take the ratio of expected sales to remaining stock for the hour it's due to run out. The query does this with these functions:</p>

<pre>     , max(hour)
     + numtodsinterval(
         max(stocknem) keep (dense_rank last order by hour)
         / max(qtynem) keep (dense_rank last order by hour)
         ,'hour'
       ) zerohour</pre>

<p>This uses the KEEP clause to return the value for stock and quantity for the maximum HOUR. This is necessary because STOCKNEM & QTYNUM are not in the GROUP BY or in an aggregate function.</p>

<p>This technique of calculating the <em>previous</em> running total up to some limit has many other applications. These include:</p>
<ul>
<li>Stock picking algorithms</li>
<li>Calculating SLA breach times for support tickets</li>
</ul>
<p>We'll look at how you can use this to do stock picking algorithms next</p>",05-MAY-20 09.37.53.845888000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.45.52.841538000 PM,"CHRIS.SAXON@ORACLE.COM"
214298062988503579623338397997954937363,210638425667855918502469137992560342521,"Many One Row Inserts vs. One Many Row Insert",20,"<p>You can also use INSERT to copy rows from one table to another. This code simulates loading 10,000 from one table to another using single row inserts inside a cursor-for loop:</p>

<code>begin
  delete bricks;
  for rw in ( 
    with rws as ( 
      select level x from dual 
      connect by level <= 10000
    ) 
    select rownum id, 
           case mod ( rownum, 3 )  
             when 0 then 'red' 
             when 1 then 'blue' 
             when 2 then 'green' 
           end colour, 
           case mod ( rownum, 2 )  
             when 0 then 'cube' 
             when 1 then 'cylinder' 
           end shape, 
           round ( dbms_random.value ( 2, 10 ) ) weight
    from   rws
  ) loop 
    insert into bricks  
    values (  
      rw.id,  
      rw.colour, 
      rw.shape,
      rw.weight
    ); 
  end loop; 
end;
/</code>

<p>As with the previous example, this means the database has to run the INSERT statement 10,000 times.</p>

<p>When changing data, it's faster to run one statement that changes all the rows. Putting DML inside a loop that changes one row on each iteration is a sure way to slow SQL!</p>

<p>Instead, have one SQL statement that inserts all the rows. This uses INSERT-AS-SELECT to add all 10,000 rows in one go:</p>

<code>delete bricks;
insert into bricks  
  with rws as ( 
    select level x from dual 
    connect by level <= 10000
  ) 
    select rownum, 
           case mod ( rownum, 3 )  
             when 0 then 'red' 
             when 1 then 'blue' 
             when 2 then 'green' 
           end, 
           case mod ( rownum, 2 )  
             when 0 then 'cube' 
             when 1 then 'cylinder' 
           end, 
           round ( dbms_random.value ( 2, 10 ) ) 
    from   rws; </code>

<p>You can also use bulk processing to copy data. To use this, BULK COLLECT the query into an array. Then use a FORALL INSERT:</p>

<code>declare
  type bricks_rec is record (
    brick_id integer, colour varchar2(10),
    shape    varchar2(10), weight integer
  );
  type bricks_array is table of bricks_rec
    index by pls_integer;
    
  brick_values bricks_array;
  num_rows pls_integer := 10000;
begin 
  delete bricks;
  with rws as ( 
    select level x from dual 
    connect by level <= num_rows 
  ) 
    select rownum, 
           case mod ( rownum, 3 )  
             when 0 then 'red' 
             when 1 then 'blue' 
             when 2 then 'green' 
           end, 
           case mod ( rownum, 2 )  
             when 0 then 'cube' 
             when 1 then 'cylinder' 
           end, 
           round ( dbms_random.value ( 2, 10 ) ) 
    bulk collect
    into   brick_values
    from   rws; 
  
  forall rws in 1 .. brick_values.count
    insert into bricks  
    values brick_values ( rws );
    
end; 
/</code>

<p>In most cases it's quicker and easier to use INSERT-SELECT. Reserve BULK COLLECT ... FORALL for cases where you need to do procedural (non-SQL) processing of the data before loading it into the target table.</p>",19-MAR-20 04.42.20.446061000 PM,"CHRIS.SAXON@ORACLE.COM",28-MAY-20 04.43.16.135630000 PM,"CHRIS.SAXON@ORACLE.COM"
216376074608579113584299986848402014496,210638425667855918502469137992560342521,"Tuning Delete Using DDL: Removing Many Rows",50,"<p>Partitioning a table splits it up into smaller sub-tables. You can do operations that affect all the rows in a partition, leaving the other partitions unaffected. For example, dropping a partition.</p>

<p>This syntax (added in 12.2) changes the table to be partitioned into batches of 10,000 rows:</p>

<code>alter table bricks 
  modify partition by range ( brick_id ) 
  interval ( 10000 ) ( 
    partition p0 values less than ( 10001 ),
    partition p1 values less than ( 20001 ) 
  ) online;</code>

<p>You can now remove all the rows in a partition by dropping or truncating it. This truncates the first partition:</p>

<code>exec ins_rows ( 30000 );

select count(*) from bricks;

alter table bricks 
  truncate partition p1;

select count(*) from bricks;</code>

<p>Partitioning a table on insert date is a common strategy. This makes it fast and easy to remove all the rows added before a certain date.</p>

<p>Remember partitioning a table impacts all statements you run against it. Operations that read many partitions may be slower compared to the equivalent non-partitioned table. Ensure you test whole application workload before diving in with partitioning!</p>",08-APR-20 02.40.25.153293000 PM,"CHRIS.SAXON@ORACLE.COM",28-MAY-20 04.55.57.185913000 PM,"CHRIS.SAXON@ORACLE.COM"
216148263350381939317775408058507876755,210640540520278905448644863263597314383,"Try It Challenge!",100,"<p>Complete the MV definition below, so the database will use it in both the queries below:</p>

<code>drop materialized view bricks_challenge_mv;

create materialized view bricks_challenge_mv
enable query rewrite
as
  select trunc ( insert_date, 'mm' ) mth/* TODO */;
  
select /*+ gather_plan_statistics */trunc ( insert_date, 'mm' ), sum ( weight )
from   bricks
where  trunc ( insert_date, 'mm' ) = date'2020-06-01'
group  by trunc ( insert_date, 'mm' );

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));

select /*+ gather_plan_statistics */ trunc ( insert_date, 'mm' ), weight, count (*)
from   bricks
where  trunc ( insert_date, 'mm' ) = date'2020-12-01'
group  by trunc ( insert_date, 'mm' ), weight;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));
</code>

<p><em>Remember: the MV doesn't need a where clause. As long as expressions in a WHERE clause appear in an MV, the database can use it for rewrite.</em></p>

<p>You can make queries using MVs faster still by creating indexes on the MV! Can you create any indexes on BRICKS_CHALLENGE_MV which these queries will use?</p>",06-APR-20 09.10.57.197845000 AM,"CHRIS.SAXON@ORACLE.COM",20-MAY-20 03.15.46.786001000 PM,"CHRIS.SAXON@ORACLE.COM"
221495736715301286620646244935898133721,210638425667855918502469137992560342521,"Single Inserts vs Batch Performance Comparison",18,"<p>This compares methods for inserting 100,000 rows:</p>
<ol>
<li>A for loop with 100,000 iterations, each adding one row</li>
<li>One FORALL INSERT, adding all 100k rows</li>
</ol>

<p><em>Due to the shared nature of Live SQL, there can be a lot of variation in run times for the processes in this tutorial. Run the comparisons <strong>3-4 times</strong> to get a feel for the relative performance of the methods shown.</em></p>

<code>declare
  num_rows pls_integer := 100000;
  type bricks_rec is record (
    brick_id integer, colour varchar2(10),
    shape    varchar2(10), weight integer
  );
  type bricks_array is table of bricks_rec
    index by pls_integer;
    
  brick_values bricks_array;
begin 
  delete bricks;
  commit;
  timing_pkg.set_start_time; 
  for i in 1 .. num_rows loop 
    insert into bricks  
      values (  
        i,  
        case mod ( i, 3 )  
          when 0 then 'red' 
          when 1 then 'blue' 
          when 2 then 'green' 
        end, 
        case mod ( i, 2 )  
          when 0 then 'cube' 
          when 1 then 'cylinder' 
        end, 
        round ( dbms_random.value ( 2, 10 ) ) 
       ); 
  end loop; 
  timing_pkg.calc_runtime ( 'Insert-loop' ); 
   
  rollback; 

  timing_pkg.set_start_time; 
  for i in 1 .. num_rows loop
    brick_values ( i ) := bricks_rec (
      brick_id => i, 
      colour => case mod ( i, 3 )  
        when 0 then 'red' 
        when 1 then 'blue' 
        when 2 then 'green' 
      end, 
      shape => case mod ( i, 2 )  
        when 0 then 'cube' 
        when 1 then 'cylinder' 
      end, 
      weight => round ( dbms_random.value ( 2, 10 ) ) 
    );
  end loop;

  forall rws in 1 .. brick_values.count
    insert into bricks  
    values brick_values ( rws );

  timing_pkg.calc_runtime ( 'Insert-forall' ); 

end; 
/</code>

<p>The difference between these methods is large. The FORALL method completes in a few tenths of a second. Single-row inserts in a loop often takes 10x longer!</p>",27-MAY-20 02.21.53.108661000 PM,"CHRIS.SAXON@ORACLE.COM",07-JAN-21 04.29.46.558162000 PM,"CHRIS.SAXON@ORACLE.COM"
151370441068219210393945937582251371088,151382938526402044313447081869310725033,"Outer Joins",40,"<p>An outer join returns all the rows from one table along with the matching rows from the other. Rows without a matching entry in the outer table return null for the outer table's columns.</p>

<p>An outer join can either be left or right. The direction defines which side of the join the database preserves the rows for.</p>

<p>The following left joins toys and bricks on their IDs. Toys is on the left side of the join. So the database returns all its rows. You also get the rows from bricks which have the same id as these:</p>

<code>select * 
from   toys
left   outer join bricks
on     toy_id = brick_id;</code>

<p>To outer join with Oracle syntax use the (+) operator. This goes after the columns of the table you want to optionally include. So the following is the same as the previous query:</p>

<code>select * 
from   toys, bricks
where  toy_id = brick_id (+);</code>

<p>To return all the rows from bricks, you can switch from a left join to a right. Or change the order of the tables in your query:</p>

<code>select * 
from   toys
right  join bricks
on     toy_id = brick_id;

select * 
from   bricks
left   join toys
on     toy_id = brick_id;</code>

<p>To do the same with Oracle syntax, move the plus to the columns of the toys table:</p>

<code>select * 
from   toys, bricks
where  toy_id (+) = brick_id;</code>

<p>An outer join will always return all the rows from the preserved table. So the number of rows returned varies from the size of the preserved table to the Cartesian product of the tables.</p>",26-JUL-18 08.50.24.635963000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.35.31.317877000 PM,"CHRIS.SAXON@ORACLE.COM"
151370441068242179984518615536570788432,151382938526402044313447081869310725033,"Filtering Joins",50,"<p>You can filter the rows returned by a join in your where clause. For example, the following joins the tables on their id. Then filters the result to only show those combinations that have a green brick:</p>

<code>select * 
from   toys
join   bricks
on     toy_id = brick_id
where  brick_colour = 'green';</code>

<p>But you must take care when doing this with outer joins!</p>

<p>Say you want to return all the toy rows. And any brick rows with a matching id and the colour green. So you expect your results to include the three rows from toys. But only brick details for brick_id = 3:</p>

<pre><b>TOY_ID   TOY_NAME        TOY_COLOUR   BRICK_ID   BRICK_COLOUR   BRICK_SHAPE   </b>
       3 Baby Turtle     green                 3 green          cube          
       1 Miss Snuggles   pink             &lt;null&gt; &lt;null&gt;         &lt;null&gt;        
       2 Cuteasaurus     blue             &lt;null&gt; &lt;null&gt;         &lt;null&gt;</pre>

<p>If you filter the outer joined table in your where clause, you'll convert the query to an inner join. And only get one row:</p>

<code>select * 
from   toys
left   join bricks
on     toy_id = brick_id
where  brick_colour = 'green';

select * 
from   toys, bricks
where  toy_id (+) = brick_id
and    brick_colour = 'green';</code>

<p>To fix this, you must place the filtering criteria for the outer joined table in the join clause. And in Oracle style, add (+) to all the outer joined table's columns:</p>

<code>select * 
from   toys
left   join bricks
on     toy_id = brick_id
and    brick_colour = 'green';

select * 
from   toys, bricks
where  toy_id = brick_id (+)
and    brick_colour (+) = 'green';</code>",26-JUL-18 08.50.56.812346000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.36.23.358381000 PM,"CHRIS.SAXON@ORACLE.COM"
151370441068451324151311946383794956880,151382938526402044313447081869310725033,"Try It!",55,"<p>Complete the query below to return:</p>

<ul><li>All the rows from bricks</li>
<li>Any rows in toys with toy_id equal to the brick_id and the toy_colour is blue</li></ul>

<code>select *
from   bricks
left   join toys
/* TODO */</code>

<p>The query should return the following rows:</p>
<pre><b>BRICK_ID   BRICK_COLOUR   BRICK_SHAPE   TOY_ID   TOY_NAME      TOY_COLOUR   </b>
         2 blue           cube                 2 Cuteasaurus   blue         
         3 green          cube            &lt;null&gt; &lt;null&gt;        &lt;null&gt;       
         4 blue           pyramid         &lt;null&gt; &lt;null&gt;        &lt;null&gt;       
   </pre>",26-JUL-18 08.52.56.241156000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 01.37.22.600186000 PM,"CHRIS.SAXON@ORACLE.COM"
184092370859812510316703189793040385880,184084400229208125137976820412046812596,"Part 2 - More Queries",20,"<h3>More Queries</h3>

<p>We will be looking at Oracle Text queries in a bit more detail.

If you've just completed Part 1, jump to ""Jump Here"" below. Otherwise, we'll need to recreate the table, data and index:

<code>create table quickstart ( id number primary key, country varchar2(2), full_name varchar2(40) );

insert into quickstart values (1, 'US', 'John Doe');
insert into quickstart values (2, 'GB', 'John Smith');
insert into quickstart values (3, 'NZ', 'Peter Smith-Smith');

create index full_name_index on quickstart( full_name ) indextype is ctxsys.context;
</code>

<h3>Jump Here</h3>

Start here if you've come straight from Part 1:

<p>We mentioned before that Oracle Text CONTAINS queries look for <i>whole</i> words.  This means that a search for ""smit"" would not succeed, since ""smit"" does not occur as a whole word in any of our text.  If we want to do partial word matches, we must use <i>wildcards</i>. As with the SQL ""LIKE"" operator, we can use underscore (""_"") to mean ""any unknown single character"" and percent (""%"") to mean ""any number of unknown characters - including none"".</p>

<p>So to find words starting with ""smit"" we would do:</p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, 'smi%', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         7          3 Peter Smith-Smith
         4          2 John Smith</pre></p>

<p>Again we see that the first record scores higher than the second as there are two matches for ""smit%"" in the first.</p>

<p>Note that a search for ""%mit%"" will work, but may be slow.  If you need to use leading wildcards (that is, a wild card at the beginning of the word) you should consider using the SUBSTRING_INDEX option - <a href=""http://docs.oracle.com/database/121/CCREF/cdatadic.htm#CCREF0225"">look it up</a>if you need it.  But remember that Oracle Text is primarily a word-based index, it's not really designed, at least with its default settings, for finding substrings anywhere in the text.</p>

<p>So what if  we want to search for a phrase - two or more words together, in order?  That's easy, we just use the phrase in the search:</p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, 'peter smith', 99) > 0 order by score(99) desc;</code></p>

<p>Note that unlike some search engines such as Google, the CONTAINS search is quite precise.  If you look for the phrase ""peter smith"", it will <i>not </i>find documents containing ""Smith Peter"" or ""Peter John Smith"".  If you wanted to find containing combinations of those words, you could use an AND or an OR query. Let's look at one of those now:</p>

<p> </p>

<p>
<code>select score(99), id, full_name from quickstart where contains ( full_name, 'john OR smith', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         7          3 Peter Smith-Smith
         4          1 John Doe
         4          2 John Smith</pre></p>

<p> </p>

<p>We'll take an aside to look at the scoring here - skip to the next paragraph if you're not interested.  The first row scores higher because ""smith"" occurs twice. OK, but why doesn't the third one score higher, as it has two hit terms. ""john"" and ""smith""?  The answer is that the OR operator scores the <i>higher</i> of the two expressions it joins.  Since ""john"" and ""smith"" both score similar low scores, the result is just the one low score. In contrast, the AND operator scores the <i>lower</i> of two expressions.  This might seem unobvious, but it makes sense if you consider what happens when one of the search terms doesn't exist - and therefore scores zero.  The OR operator scores the higher of (zero and something), so always returns a score when one term is present. The AND operator scores the lower of (zero and something) - which is zero - so always returns a zero score unless both terms are present.</p>

<p> </p>

<p>Let's see a few more simple queries and their results. First the AND operator we've already mention - both terms must be present.</p>

<p>
<code>select score(99), id, full_name from quickstart where contains ( full_name, 'john AND smith', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         4          2 John Smith</pre></p>

<p>Then there's the NOT operator - if the second term is present at all then the query fails: </p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, 'john NOT smith', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         4          1 John Doe</pre></p>

<p> </p>

<p>And the ACCUM operator - similar to OR but instead of scoring the lower of the two expressions, it adds them together (or <i>accumulates</i> them)</p>

<p> </p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, 'john ACCUM smith', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
        52          2 John Smith
         4          3 Peter Smith-Smith
         2          1 John Doe</pre></p>

<p> And a glimpse at one of the more powereful capabilities of the query language - the FUZZY operator: do an ""inexact"" search for words which are spelled or sound similar:</p>

<p> </p>

<p><code> select score(99), id, full_name from quickstart where contains ( full_name, 'fuzzy((smythe))', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         7          3 Peter Smith-Smith
         4          2 John Smith</pre></p>

<p> </p>

<p> </p>

<p>You can add these operators together in arbitrarily complex expressions, using parenthesis to establish precedent if required:</p>

<p> </p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, '((john ACCUM peter) OR smith) NOT peter', 99) > 0 order by score(99) desc;</code></p>

<p><pre> SCORE(99)         ID FULL_NAME
---------- ---------- ------------------------------
         4          2 John Smith
         2          1 John Doe</pre></p>

<p>A few last things:</p>

<p>Oracle Text has a large number of reserved words - AND, OR, ACCUM and FUZZY all being in that list. If we want to search for any of those reserved words, we must enclose them in braces to negate their special meaning: </p>

<p> </p>

<p><code>select score(99), id, full_name from quickstart where contains ( full_name, '{fuzzy} OR {accum}', 99) > 0 order by score(99) desc;</code></p>

<p>Case sensitivity: CONTEXT indexes are case-<i>in</i>sensitive by default in most environments. ""smith"" will match ""Smith"", etc. However, for certain languages (notably German) this is switched and the indexes are case-sensitive by default. The default language will be based on the normal NLS settings for your database.  Case sensitivity for an index can be chosen as a setting when the index is created.</p>

<p>This is only a glimpse at the capabilities of the Oracle Text CONTAINS query capabilities.  For a more detailed look at the operators available, see <a href=""http://docs.oracle.com/database/121/CCREF/cqoper.htm#CCREF0300"">the documentation</a>.</p>",04-JUN-19 01.23.18.926988000 PM,"ROGER.FORD@ORACLE.COM",04-JUN-19 02.52.15.410375000 PM,"ROGER.FORD@ORACLE.COM"
213641258027992444676641617131862680201,210639799513782729864465643940593969908,"Try It!",30,"<p>Replace /* TODO */ in the index definition below, so the query will use it:</p>
<code>create index brick_shape_i 
  on bricks ( /* TODO */ );
  
select /*+ gather_plan_statistics */ count(*) from bricks
where  shape = 'cylinder';

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>If you want to try creating the index with different columns, you must re-create it. Do this by dropping it first:</p>

<code>drop index brick_shape_i;</code>",13-MAR-20 09.37.23.160224000 AM,"CHRIS.SAXON@ORACLE.COM",13-MAY-20 01.33.15.343282000 PM,"CHRIS.SAXON@ORACLE.COM"
216380454168661138825032648562655764495,210638425667855918502469137992560342521,"Tuning Delete Using DDL: Removing All Rows",35,"<p>You can also make deletion processes faster by changing DML to DDL. The easiest case is when you remove all the rows from a table.</p>

<p>Instead of a DELETE without a WHERE clause, use TRUNCATE. This is a meta-data only operation, so is ""instant"":</p>

<code>truncate table bricks;</code>

<p>This gives big performance gains, as this test shows: </p>

<code>declare
  num_rows pls_integer := 100000;
begin 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  delete bricks; 
   
  timing_pkg.calc_runtime ( 'Delete-all' ); 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
   
  execute immediate 'truncate table bricks';
   
  timing_pkg.calc_runtime ( 'Truncate' ); 
end;
/
</code>

<p>But TRUNCATE commits, so you can't roll it back. And it has some restrictions that don't apply to DELETE. So this is not always appropriate.</p>

<p>And it's rare you want to remove <strong>all</strong> the rows in a table. Even when archiving most of the data, usually you want to keep some rows. There are several other DDL tricks you can use to do this.</p>",08-APR-20 02.37.11.070149000 PM,"CHRIS.SAXON@ORACLE.COM",14-APR-20 04.38.17.394395000 PM,"CHRIS.SAXON@ORACLE.COM"
219154046205809591879263075152119387675,219172783004222394396035270452714317015,"Calculating Running Totals",10,"<p>By adding the OVER clause to SUM, you can calculate running totals. This has three clauses:</p>

<ul>
<li>Partition by - split the data set up into separate groups</li>
<li>Order by - sort the data, defining the order totals are calculated</li>
<li>Window clause - which rows to include in the total</li>
</ul>

<p>This returns the cumulative sales for each shop by date:</p>

<code>alter session set nls_date_format='YYYY-MM-DD HH24:MI:SS';

select sales.shopid
     , sales.salesnem
     , sales.saleshour
     , sum(sales.salesnem) over (
         partition by sales.shopid
         order by sales.saleshour
         rows between unbounded preceding and current row
       ) salesnemacc
  from fw_sales sales
order by sales.shopid, sales.saleshour;
</code>

<p>The clauses in this SUM work as follows:</p>

<ul>
<li><pre>partition by sales.shopid</pre>calculates the running total for each shop separately</li>
<li><pre>order by sales.saleshour</pre>sorts the rows by hour, so this gives the cumulative sales by date</li>
<li>
<pre>rows between unbounded preceding and current row</pre>sums the sales for this row and all previous rows</li>
</ul>",05-MAY-20 08.42.07.916948000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 08.29.40.617891000 AM,"CHRIS.SAXON@ORACLE.COM"
219154046210095233909796935576452781595,219172783004222394396035270452714317015,"Predicting Stock Shortages",5,"<p>To start we'll build a prediction engine, estimating when shops will run out of fireworks to sell. This will use data from tables storing the shops, order received, and daily and hourly sales predictions:</p>

<code>select * from fw_store;
select * from fw_sales;
select * from fw_daybudget;
select * from fw_hourbudget;
</code>",05-MAY-20 09.32.55.722374000 AM,"CHRIS.SAXON@ORACLE.COM",05-MAY-20 04.41.39.581288000 PM,"CHRIS.SAXON@ORACLE.COM"
182650138855946276015103313053670339080,182241645422959190146194127811898911783,"Solutions to Exercises",160,"<p>
Note that your solution might not match mine exactly, and that's just fine - as long as you use the bulk processing features correctly. :-)
</p>
<p><strong>Exercise 1:</strong></p>
Write a store procedure that accepts a department ID, uses BULK COLLECT to retrieve all employees in that department, and displays their first name and salary. Then write an anonymous block to run that procedure for department 100.
</p>
<p><strong>Solution to 1:</strong></p>
<code>CREATE OR REPLACE PROCEDURE show_emps_in_dept (
   department_id_in IN employees.department_id%TYPE)
IS
   TYPE two_cols_rt IS RECORD (
      first_name   employees.first_name%TYPE,
      salary       employees.salary%TYPE
   );

   TYPE employee_info_t IS TABLE OF two_cols_rt;

   l_employees   employee_info_t;
BEGIN
   SELECT first_name, salary
     BULK COLLECT INTO l_employees
     FROM employees
    WHERE department_id = department_id_in;

   FOR indx IN 1 .. l_employees.COUNT
   LOOP
      DBMS_OUTPUT.PUT_LINE (l_employees(indx).first_name || ' makes ' ||
                            l_employees(indx).salary);
   END LOOP;
END;
/

BEGIN
   show_emps_in_dept (100);
END;
/
</code>

<p><strong>Exercise 2</strong></p>
<p>
Write an anonymous block that fetches (using BULK COLLECT) only the last name and salary from the employees table 5 rows at a time, and then displays that information. Make sure 107 names and salaries are shown!
</p>
<p><strong> Solution to 2:</strong></p>
<code>DECLARE
   c_limit       PLS_INTEGER := 5;

   CURSOR employees_cur
   IS
      SELECT last_name, salary
        FROM employees;

   TYPE employee_ids_t IS TABLE OF employees_cur%ROWTYPE;

   l_employees   employee_ids_t;
BEGIN
   OPEN employees_cur;

   LOOP
      FETCH employees_cur BULK COLLECT INTO l_employees LIMIT c_limit;

      EXIT WHEN l_employees.COUNT = 0;

      FOR indx IN 1 .. l_employees.COUNT
      LOOP
         DBMS_OUTPUT.put_line (
            l_employees (indx).last_name || ' - ' || l_employees (indx).salary);
      END LOOP;
   END LOOP;
END;
</code>

<p><strong>Exercise 3</strong></p>
<p>
This exercise has two parts (and for this exercise assume that the employees table has 1M rows with data distributed equally amongst departments: (1) Write an anonymous block that contains a cursor FOR loop that does <i>not</i> need to be converted to using BULK COLLECT. (2) Write an anonymous block that contains a cursor FOR loop that <i>does</i> need to use BULK COLLECT (assume it cannot be rewritten in ""pure"" SQL).
</p>
<p><strong> Solution to 3:</strong></p>
<code>BEGIN
   /* There is no need to convert to bulk collect because the PL/SQL
      compiler will automatically optimize the CFL to run ""like"" a
      BULK COLLECT with limit set to 100. */
   FOR rec IN (SELECT *
                 FROM employees
                WHERE department_id = 50)
   LOOP
      DBMS_OUTPUT.put_line (rec.last_name || ' - ' || rec.salary);
   END LOOP;
END;
/

BEGIN
   /* This loop now has a non-query DML statement inside it. And lots
      of employees in each department. So this row by row update is
      going to be slow. The compiler will still optimize the cursor FOR
      loop to be like a BULK COLLECT, but the row-by-row update will
      still be slow. 

      Bottom line: loop with non-query DML must be converted to
      ""pure"" SQL or a BULK COLLECT - FORALL combo! */
   FOR rec IN (SELECT employee_id
                 FROM employees
                WHERE department_id = 50)
   LOOP
      UPDATE employees
         SET salary = salary * 1.1
       WHERE employee_id = rec.employee_id;
   END LOOP;
END;
/
</code>

<p><strong> Exercise 4:</strong></p>
<p>
Write an anonymous block that deletes all the rows in the employees table for department 50 and returns all the employee IDs and the last names of deleted rows. Then display those values using DBMS_OUTPUT.PUT_LINE. Finally, you might want to rollback. That will make it easier to test your code - and continue on with the tutorial. 
</p>
<p><strong> Solution to 4:</strong></p>
<p>
I offer two solutions below. One has two separate collections of scalar 
values to hold the data coming back from the RETURNING clause. The second uses a collection of records. Both work just fine!
</p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;

   l_ids     ids_t;

   TYPE names_t IS TABLE OF employees.last_name%TYPE;

   l_names   names_t;
BEGIN
   DELETE FROM employees
         WHERE department_id = 50
     RETURNING employee_id, last_name
          BULK COLLECT INTO l_ids, l_names;

   FOR indx IN 1 .. l_ids.COUNT
   LOOP
      DBMS_OUTPUT.put_line (l_ids (indx) || ' - ' || l_names (indx));
   END LOOP;

   ROLLBACK;
END;
/

DECLARE
   TYPE two_cols_rt IS RECORD
   (
      employee_id   employees.employee_id%TYPE,
      last_name     employees.last_name%TYPE
   );

   TYPE employee_info_t IS TABLE OF two_cols_rt;

   l_emps   employee_info_t;
BEGIN
   DELETE FROM employees
         WHERE department_id = 50
     RETURNING employee_id, last_name
          BULK COLLECT INTO l_emps;

   FOR indx IN 1 .. l_emps.COUNT
   LOOP
      DBMS_OUTPUT.put_line (
         l_emps (indx).employee_id || ' - ' || l_emps (indx).last_name);
   END LOOP;

   ROLLBACK;
END;
/
</code>

<p><strong> Exercise 5:</strong></p>
<p>
Write the rest of the procedure whose signature is shown below. Use BULK COLLECT to fetch all the last names from employees identified by that WHERE clause and return the collection. Then write an anonymous block to test your procedure: pass different WHERE clauses, and display names retrieved.
</p>
<pre>
PROCEDURE get_names (
   where_in IN VARCHAR2,
   names_out OUT DBMS_SQL.VARCHAR2_TABLE)
</pre>
<p><strong> Solution to 5:</strong></p>
<code>CREATE OR REPLACE PROCEDURE get_names (
   where_in    IN     VARCHAR2,
   names_out      OUT DBMS_SQL.varchar2_table)
IS
BEGIN
   EXECUTE IMMEDIATE '
   SELECT last_name
     FROM employees WHERE ' || where_in
      BULK COLLECT INTO names_out;
END;
/

DECLARE
   l_names   DBMS_SQL.varchar2_table;
BEGIN
   get_names ('department_id = 100', l_names);
   DBMS_OUTPUT.put_line ('For department_id = 100');

   FOR indx IN 1 .. l_names.COUNT
   LOOP
      DBMS_OUTPUT.put_line (l_names (indx));
   END LOOP;

   get_names ('salary > 15000', l_names);
   DBMS_OUTPUT.put_line ('For salary > 15000');

   FOR indx IN 1 .. l_names.COUNT
   LOOP
      DBMS_OUTPUT.put_line (l_names (indx));
   END LOOP;
END;
/
</code>
<p><strong> Exercise 6:</strong></p>
<p>
Write an anonymous block that uses BULK COLLECT to populate two collections, one with the employee Id and the other with the employee's last name. Use those collections in a FORALL statement to update the employee's last name to the UPPER of the name in the name collection, for each employee ID. Note: clearly, you do not need BULK COLLECT and FORALL to do this. Please pretend. :-)
</p>
<p><strong> Solution to 6:</strong></p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;

   l_ids     ids_t;

   TYPE names_t IS TABLE OF employees.last_name%TYPE;

   l_names   names_t;
BEGIN
   SELECT employee_id, last_name
     BULK COLLECT INTO l_ids, l_names
     FROM employees;

   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET last_name = UPPER (l_names (indx))
       WHERE employee_id = l_ids (indx);

   ROLLBACK;
END;
</code>

<p><strong> Exercise 7:</strong></p>
<p>
Write an anonymous block that uses BULK COLLECT to populate two collections, one with the employee Id and the other with the employee's last name. Use those collections in a FORALL statement to update the employee's last name to the UPPER of the name in the name collection, for each employee ID. Note: clearly, you do not need BULK COLLECT and FORALL to do this. Please pretend. :-)
</p>
<p><strong> Solution to 7:</strong></p>
<p>Again, in two forms: one using two separate collections, the other using a collection of records. </p>
<code>DECLARE
   TYPE ids_t IS TABLE OF employees.employee_id%TYPE;

   l_ids     ids_t;

   TYPE names_t IS TABLE OF employees.last_name%TYPE;

   l_names   names_t;
BEGIN
   SELECT employee_id, last_name
     BULK COLLECT INTO l_ids, l_names
     FROM employees;

   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET last_name = UPPER (l_names (indx))
       WHERE employee_id = l_ids (indx);

   ROLLBACK;
END;
/

DECLARE
   TYPE two_cols_rt IS RECORD
   (
      employee_id   employees.employee_id%TYPE,
      last_name     employees.last_name%TYPE
   );

   TYPE employee_info_t IS TABLE OF two_cols_rt;

   l_emps   employee_info_t;
BEGIN
   SELECT employee_id, last_name
     BULK COLLECT INTO l_emps
     FROM employees;

   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET last_name = UPPER (l_emps (indx).last_name)
       WHERE employee_id = l_emps (indx).employee_id;

   ROLLBACK;
END;
/
</code>

<p><strong> Exercise 8:</strong></p>
<p>
Write the rest of the procedure whose signature is shown below. The procedure uses FORALL to update the salaries of all employees in each department to the corresponding value in the salaries array. Afterwards, display the total number of rows modified. Raise the PROGRAM_ERROR exception if any of the update statements (for a specific department ID) change less than 2 rows.
</p>
<pre>
PROCEDURE update_salaries (
   department_ids_in IN DBMS_SQL.NUMBER_TABLE,
   salaries_in IN DBMS_SQL.NUMBER_TABLE)
</pre>
<p><strong> Solution to 8:</strong></p>

<p><strong> Exercise 9:</strong></p>
<p>
Complete the block below as follows: use FORALL to update the salaries of employees whose IDs are in l_ids with the corresponding salary in l_salaries. Display the total number of employees modified. First, use values for salaries that will allow all statements to complete successfully (salary is defined as NUMBER (8,2). Then change the salary values to explore the different ways that FORALL deals with errors.
</p>
<pre>
DECLARE
   TYPE numbers_nt IS TABLE OF NUMBER;
   l_ids numbers_nt := numbers_nt (101, 111, 131);
   l_salaries numbers_nt := numbers_nt (#FINISH#);
BEGIN
   #FINISH#
END;
</pre>
<p><strong> Solution to 9:</strong></p>
<code>DECLARE
   /* All valid, three rows should be updated. */
   TYPE numbers_nt IS TABLE OF NUMBER;

   l_ids        numbers_nt := numbers_nt (101, 111, 131);
   l_salaries   numbers_nt := numbers_nt (10000, 11000, 12000);
BEGIN
   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
   ROLLBACK;
END;
/

DECLARE
   /* First one too big so nothing updated. */
   TYPE numbers_nt IS TABLE OF NUMBER;

   l_ids        numbers_nt := numbers_nt (101, 111, 131);
   l_salaries   numbers_nt := numbers_nt (1000000, 11000, 12000);
BEGIN
   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
   ROLLBACK;
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line (SQLERRM);
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
      ROLLBACK;
END;
/

DECLARE
   /* Last one too big so two rows updated. */
   TYPE numbers_nt IS TABLE OF NUMBER;

   l_ids        numbers_nt := numbers_nt (101, 111, 131);
   l_salaries   numbers_nt := numbers_nt (10000, 11000, 1200000);
BEGIN
   FORALL indx IN 1 .. l_ids.COUNT
      UPDATE employees
         SET salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
   ROLLBACK;
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line (SQLERRM);
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
      ROLLBACK;
END;
/
</code>

<p><strong> Exercise 10:</strong></p>
<p>
Change the code you wrote in Exercise 9 so that the SQL engine executes the DML statement as many times as there are elements in the binding array. Notice how the error information changes.
</p>
<p><strong> Solution to 10:</strong></p>
<code>DECLARE
   /* Second value is too big but since I added SAVE EXCEPTIONS,
      we still see two rows updated - and a generic error message
      for the ""generic"" ORA-24381. */
   TYPE numbers_nt IS TABLE OF NUMBER;

   l_ids        numbers_nt := numbers_nt (101, 111, 131);
   l_salaries   numbers_nt := numbers_nt (10000, 1100000, 12000);
BEGIN
   FORALL indx IN 1 .. l_ids.COUNT SAVE EXCEPTIONS
      UPDATE employees
         SET salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
   ROLLBACK;
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line (SQLERRM);
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
      ROLLBACK;
END;
/
</code>

<p><strong> Exercise 11:</strong></p>
<p>
Change the code you wrote in Exercise 10 as follows:</p>
<ol>
<li>Expand the number of employee IDs to update to 5.</li>
<li>Change the array of salary values so that at least  2 of the updates will fail.</li>
<li>Change the exception handler to show which of the attempts to update failed, the error code, and the error message associated with that code.</li>
</ol>
<p><strong> Solution to 11:</strong></p>
<code>DECLARE
   /* Second value is too big but since I added SAVE EXCEPTIONS,
      we still see two rows updated - and a generic error message. */
   TYPE numbers_nt IS TABLE OF NUMBER;

   l_ids        numbers_nt := numbers_nt (117, 101, 111, 131, 143);
   l_salaries   numbers_nt := numbers_nt (10000, 1100000, 12000, 6669999, 5000);
BEGIN
   FORALL indx IN 1 .. l_ids.COUNT SAVE EXCEPTIONS
      UPDATE employees
         SET salary = l_salaries (indx)
       WHERE employee_id = l_ids (indx);

   DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT);
   ROLLBACK;
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line ('Updated ' || SQL%ROWCOUNT || ' rows.');  
      DBMS_OUTPUT.put_line (SQLERRM); 

      FOR indx IN 1 .. SQL%BULK_EXCEPTIONS.COUNT
      LOOP
         DBMS_OUTPUT.put_line (
               'Error '
            || indx
            || ' - array index '
            || SQL%BULK_EXCEPTIONS (indx).ERROR_INDEX
            || ' attempting to update salary to '
            || l_salaries (
                  SQL%BULK_EXCEPTIONS (indx).ERROR_INDEX));
         DBMS_OUTPUT.put_line (
               'Oracle error is '
            || SQLERRM (
                  -1 * SQL%BULK_EXCEPTIONS (indx).ERROR_CODE));
      END LOOP;
 
      ROLLBACK; 
END;
</code>

<p><strong> Exercise 12:</strong></p>
<p>
Change the procedure below so that no assumptions are made about the contents of the ids_in array and the procedure completes without raising an exception. Assumptions you <i>could</i> make, but should not, include:
</p>
<ul>
<li>The collection is never empty.</li>
<li>The first index defined in the collection is 1.</li>
<li>The collection is dense.</li>
<li>The collection is sparse.</li>
</ul>
<code>
CREATE OR REPLACE TYPE numbers_t IS TABLE OF NUMBER
/

CREATE OR REPLACE PROCEDURE update_emps (ids_in IN numbers_t)
IS
BEGIN
   FORALL l_index IN 1 .. ids_in.COUNT
      UPDATE employees
         SET hire_date = SYSDATE
       WHERE employee_id = ids_in (l_index);

   ROLLBACK;
END;
/
</code>
</p>
<p><strong> Solution to 12:</strong></p>
<code>CREATE OR REPLACE TYPE numbers_t IS TABLE OF NUMBER
/

/* This version doesn't assume the first defined index value is 1,
   and it handles empty collections just fine, but it does assume the 
   collection is densely filled. */

CREATE OR REPLACE PROCEDURE update_emps (ids_in IN numbers_t)
IS
BEGIN
   FORALL l_index IN ids_in.FIRST .. ids_in.LAST
      UPDATE employees
         SET hire_date = SYSDATE
       WHERE employee_id = ids_in (l_index);

   ROLLBACK;
END;
/

CREATE OR REPLACE PROCEDURE test_update_emps
IS
   l_ids   numbers_t := numbers_t (111, 121, 132);
BEGIN
   BEGIN
      /* This is OK */
      update_emps (l_ids);
      DBMS_OUTPUT.put_line ('Success Dense');
   EXCEPTION
      WHEN OTHERS
      THEN
         DBMS_OUTPUT.put_line (SQLERRM);
   END;

   BEGIN
      /* But not when it is sparse. */
      l_ids := numbers_t (111, 121, 132);
      l_ids.delete (2);
      update_emps (l_ids);
      DBMS_OUTPUT.put_line ('Success Sparse');
   EXCEPTION
      WHEN OTHERS
      THEN
         DBMS_OUTPUT.put_line ('Sparse ' || SQLERRM);
   END;

   BEGIN
      /* And not when it is empty. */
      l_ids.delete;
      update_emps (l_ids);
      DBMS_OUTPUT.put_line ('Success Empty');
   EXCEPTION
      WHEN OTHERS
      THEN
         DBMS_OUTPUT.put_line ('Empty ' || SQLERRM);
   END;
END;
/

BEGIN
   test_update_emps;
END;
/

/* Switch to INDICES OF and all is good. */

CREATE OR REPLACE PROCEDURE update_emps (ids_in IN numbers_t)
IS
BEGIN
   FORALL l_index IN INDICES OF ids_in
      UPDATE employees
         SET hire_date = SYSDATE
       WHERE employee_id = ids_in (l_index);

   ROLLBACK;
END;
/

BEGIN
   test_update_emps;
END;
/
</code>

<p><strong> Exercise 13 (advanced):</strong></p>
<p>
Got some time on your hands? Ready to explore all that INDICES OF can do? OK, let's go! Write a block of code that does the following:
</p>
<ul>
<li>Populates a collection using BULK COLLECT with all the rows and columns of the employees table, ordered by employee ID: the bind array.</li>
<li>Declare an associative array that can be used with INDICES OF.</li>
<li>Populate one collection of that type with index values from the bind array for all employees with salary = 2600: index array 1.</li>
<li>Populate a second collections of that type with index values from the bind array of those employees who were hired in 2004: index array 2.</li>
<li>Write a FORALL statements that uses index array 1 to set the salaries of those employees to 2626.</li>
<li>Write a FORALL statements that uses index array 2 to set the hire dates of those employees to January 1 2014.</li>
<li>Include code to verify that the changes were made properly.</li>
</ul>
<p><strong> Solution to 13:</strong></p>
<code>DECLARE
   TYPE employee_aat IS TABLE OF employees%ROWTYPE
      INDEX BY PLS_INTEGER;

   l_employees   employee_aat;

   TYPE index_aat IS TABLE OF BOOLEAN
      INDEX BY PLS_INTEGER;

   l_indices1    index_aat;
   l_indices2    index_aat;
BEGIN
     SELECT *
       BULK COLLECT INTO l_employees
       FROM employees
   ORDER BY employee_id;

   DBMS_OUTPUT.put_line ('*** Before changes:');

   FOR indx IN 1 .. l_employees.COUNT
   LOOP
      IF l_employees (indx).salary = 2600
      THEN
         l_indices1 (indx) := NULL;
         DBMS_OUTPUT.put_line (
            'ID ' || l_employees (indx).employee_id || ' has salary 2600');
      END IF;

      IF TO_CHAR (l_employees (indx).hire_date, 'YYYY') = '2004'
      THEN
         l_indices2 (indx) := NULL;
         DBMS_OUTPUT.put_line (
            'ID ' || l_employees (indx).employee_id || ' hired in 2004');
      END IF;
   END LOOP;

   FORALL indx IN INDICES OF l_indices1
      UPDATE employees
         SET salary = 2626
       WHERE employee_id = l_employees (indx).employee_id;

   FORALL indx IN INDICES OF l_indices2
      UPDATE employees
         SET hire_date = DATE '2014-01-01'
       WHERE employee_id = l_employees (indx).employee_id;

   DBMS_OUTPUT.put_line ('*** After changes:');

   FOR rec IN (  SELECT *
                   FROM employees
               ORDER BY employee_id)
   LOOP
      IF rec.salary = 2626
      THEN
         DBMS_OUTPUT.put_line ('ID ' || rec.employee_id || ' has salary 2626');
      END IF;

      IF rec.hire_date = DATE '2014-01-01'
      THEN
         DBMS_OUTPUT.put_line ('ID ' || rec.employee_id || ' hired 2014-01-01');
      END IF;
   END LOOP;

   ROLLBACK;
END;
</code>",21-MAY-19 04.26.28.289707000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",20-FEB-20 12.12.04.533885000 AM,"STEVEN.FEUERSTEIN@ORACLE.COM"
219177997902882078969677522496509363314,219172783004222394396035270452714317015,"Creating Hourly Sales Predictions",20,"<p>The budget figures are daily. To combine these with hourly sales, we need to convert them to hourly figures.</p>

<p>This query does this by cross joining the day and hour budget tables. Then multiplying the daily budget by the hourly percentage to give the expected sales for that hour: </p>
<code>alter session set nls_date_format='YYYY-MM-DD HH24:MI:SS';

with shop as (
  select s.shopid
       , s.containers * 250 startnem
    from fw_store s
), budget as (
  select db.shopid
       , db.budgetdate + numtodsinterval(hb.hour,'hour') budgethour
       , db.budgetnem * hb.percent / 100 budgetnem
  from   fw_daybudget db
  cross  join fw_hourbudget hb
)
  select budget.shopid
       , shop.startnem
       , budget.budgethour hour
       , budget.budgetnem qtynem
  from   shop
  join   budget
  on     budget.shopid = shop.shopid;
</code>",05-MAY-20 09.35.30.865910000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 08.30.42.190085000 AM,"CHRIS.SAXON@ORACLE.COM"
151370441068295372720581659220257860176,151382938526402044313447081869310725033,"Full Outer Joins",60,"<p>Sometimes you may want to join two tables to find the matching rows. But also include any unmatched rows from both tables. I.e. a ""double outer"" join. This is known as a full (outer) join.</p>

<p>You do this using ANSI syntax like so:</p>

<code>select * 
from   toys
full   join bricks
on     toy_id = brick_id;</code>

<p>Writing a full outer join in Oracle syntax is a little clumsy. You need two outer join queries. One for each table. Then to combine the results of these using union all:</p>

<code>select * 
from   toys, bricks
where  toy_id = brick_id (+)
union all
select * 
from   toys, bricks
where  toy_id (+) = brick_id
and    toy_id is null;</code>

<p>Like cross joins, it's rare to use this. But it's useful to have in your SQL toolkit!</p>",26-JUL-18 08.51.30.221385000 AM,"CHRIS.SAXON@ORACLE.COM",29-AUG-18 09.04.27.241440000 AM,"CHRIS.SAXON@ORACLE.COM"
188697879653888639634414722972616684536,188704173216096418598025228629326881042,"Load data",40,"<p>
First we load CUSTOMERS by copying from the table OE.CUSTOMERS.  Note that we are using two spatial functions in this step: 1) we use sdo_cs.transform() to convert to our desired coordinate system SRID of 4326, and 2) we use sdo_geom.validate_geometry() to insert only valid geometries. 
<code>INSERT INTO CUSTOMERS 
SELECT CUSTOMER_ID, CUST_FIRST_NAME, CUST_LAST_NAME , GENDER, sdo_cs.transform(CUST_GEO_LOCATION,4326), ACCOUNT_MGR_ID
FROM oe.customers
WHERE sdo_geom.validate_geometry(CUST_GEO_LOCATION,0.05)='TRUE';
commit;
</code>
<p>
Next WAREHOUSES manually load warehouses using teh SDO_GEOMETRY constructor. 
<code>INSERT INTO WAREHOUSES values (1,'Southlake, TX',1400,
  SDO_GEOMETRY(2001, 4326, MDSYS.SDO_POINT_TYPE(-103.00195, 36.500374, NULL), NULL, NULL));
INSERT INTO WAREHOUSES values (2,'San Francisco, CA',1500,
  SDO_GEOMETRY(2001, 4326, MDSYS.SDO_POINT_TYPE(-124.21014, 41.998016, NULL), NULL, NULL));
INSERT INTO WAREHOUSES values (3,'Sussex, NJ',1600,
  SDO_GEOMETRY(2001, 4326, MDSYS.SDO_POINT_TYPE(-74.695305, 41.35733, NULL), NULL, NULL));
INSERT INTO WAREHOUSES values (4,'Seattle, WA',1700,
  SDO_GEOMETRY(2001, 4326, MDSYS.SDO_POINT_TYPE(-123.61526, 46.257458, NULL), NULL, NULL));
COMMIT;
</code>
<p>
The elements of the constructor are:
<ul>
<li>2001: SDO_GTYPE attribute and it is set to 2001 when storing a two-dimensional single point such as a customer's location.</li>
<li>4326:  This is the spatial reference system ID (SRID): a foreign key to an Oracle dictionary table (MDSYS.CS_SRS) that contains all the supported coordinate systems. It is important to associate your customer's location to a coordinate system. In this example, 4326 corresponds to ""Longitude / Latitude (WGS 84).""</li>
<li>MDSYS.SDO_POINT_TYPE:  This is where you store your longitude and latitude values within the SDO_GEOMETRY constructor. Note that you can store a third value also, but for these tutorials, all the customer data is two-dimensional.</li>
<li>NULL, NULL:  The last two null values are for storing linestrings,  polygons, and geometry collections.  For more information on all the fields of the SDO_GEOMETRY object, please refer to the Oracle Spatial Developer's Guide. For this tutorial with point data, these last two fields should be set to NULL.</li>
</ul>
<p>",18-JUL-19 03.55.52.468729000 PM,"DAVID.LAPP@ORACLE.COM",06-AUG-19 01.27.53.566923000 PM,"DAVID.LAPP@ORACLE.COM"
217202053342133066566478383917598122975,210639799513881861781674043532919876340,"Introduction",5,"<p>The tutorials in this series so far have focussed on tuning a single SQL statement. But often you need to do analysis to find what the slowest SQL is!</p>

<p>To do this, you should profile the code that is running when ""the database is slow"". This captures SQL execution details for the session that runs the statements the customer is complaining about.</p>

<p>This tutorial introduces the tools in Oracle Database to help you profile SQL and PL/SQL execution.</p>

<p><em>Live SQL has restricted capabilities. Call to run SQL traces in this tutorial will fail. You can copy/paste these into your own environment to see how they work.</em></p>",16-APR-20 02.48.43.492366000 PM,"CHRIS.SAXON@ORACLE.COM",25-JUN-20 08.53.31.112238000 AM,"CHRIS.SAXON@ORACLE.COM"
184104672860698006996950504175919886105,184084400229208125137976820412046812596,"Part 3 - Index Maintenance",30,"<h3>Part 3: Index Maintenance</h3>

<p>In this module we look at keeping indexes up to date and in good order<p>

If you've just completed Part 2, jump to ""Jump Here"" below. Otherwise, we'll need to recreate the table, data and index:

<code>create table quickstart ( id number primary key, country varchar2(2), full_name varchar2(40) );

insert into quickstart values (1, 'US', 'John Doe');
insert into quickstart values (2, 'GB', 'John Smith');
insert into quickstart values (3, 'NZ', 'Peter Smith-Smith');

create index full_name_index on quickstart( full_name ) indextype is ctxsys.context;
</code>

(<i><b>note:</b></i> to restart the this module, manually enter ""drop table quickstart"" into the SQL Worksheet and run it, then start from above)

<h3>Jump Here</h3>

<h3>Start here if you've come straight from Part 2:</h3>

<p>This post follows on from <a href=""https://blogs.oracle.com/searchtech/entry/getting_started_with_oracle_text"" title=""Getting Started Part 1"">Part 1</a> and <a href=""https://blogs.oracle.com/searchtech/entry/getting_started_part_2_more"" title=""Getting Started Part 2"">Part 2</a>, and uses the same example ""quickstart"" table.

<p>One thing that surprised new users is that Oracle Text CONTEXT indexes are not <i>synchronous</i>. That is, updates to the table are not immediately reflected in the index. Instead, the index must be <i>synchronized. </i>We can do this manually with a call to <i>CTX_DDL.SYNC_INDEX, </i>or we can arrange to have it done automatically for us.

<p>Let's show the manual method first:. We'll insert a new run then search for it. We won't find it because the index is not up-to-date. Then we'll call SYNC_INDEX, giving it the name of the index, and search again:

<p><code>insert into quickstart values (4, 'GB', 'Michael Smith');

select score(99), id, full_name from quickstart where contains ( full_name, 'michael', 99) > 0 order by score(99) desc;</code>

Nothing is found because the update has not been sync'd. So sync the index:

<code>execute ctx_ddl.sync_index('full_name_index')</code>

and search again...

<code>select score(99), id, full_name from quickstart where contains ( full_name, 'michael', 99) > 0 order by score(99) desc;</code>

This time we find our new record.

<p>If we don't want to bother with manual SYNC calls,  we can use the parameter string ""SYNC (ON COMMIT)"" when we create the index. That means that immediately after updates are committed to the table, a SYNC operation will be run to get the index into step with the table. We need to add a <i>PARAMETERS</i> clause to the create index statement for this:

<code>drop index full_name_index;

create index full_name_index on quickstart( full_name )
indextype is ctxsys.context
parameters ('sync (on commit)');</code>

We have a new index. Now insert yet another row - and immediately search for it:

<code>insert into quickstart values (5, 'US', 'Scott Peters');

select score(99), id, full_name from quickstart where contains ( full_name, 'scott', 99) > 0 order by score(99) desc;
</code>

We didn't find anything from that search because the insert wasn't committed yet. But LiveSQL commits after every batch of statements, so if we try the query again:

<code>select score(99), id, full_name from quickstart where contains ( full_name, 'scott', 99) > 0 order by score(99) desc;
</code>

This time it finds it.
<p>

<h3>Time-scheduled Sync</h3>

SYNC(ON COMMIT) is fine if you don't make too many updates to your table, but is not ideal if you have a very large table and lots of changes take place on it.

<p>
Because of the way CONTEXT indexes are built, frequent changes will cause fragmentation of the index, decreasing performance over time. So a common way is to arrange for indexes to be synchronized at certain fixed intervals.  We can do this manually, using the database scheduler, or we can have it done automatically with a ""SYNC (EVERY ... )"" string in the parameters clause.  The time clause in that uses scheduler syntax, and can be a little convoluted. Every 1 minute can be represented in days as 1/24/60"".

<p>
Unfortunately we can't try that here, as LiveSQL doesn't allow us to create jobs. But if you want to try this on your own database, the syntax would be:
<pre>
create index full_name_index on quickstart( full_name )
indextype is ctxsys.context 
parameters ('sync ( every SYSDATE+1/24/60 )');
</pre>

Back in Part 1 we mentioned that it was a good idea to grant the ""CREATE JOB"" privilege to a text user - this is why.  If the user doesn't have CREATE JOB privilege, then we will get an ""insufficient privileges"" error if we try to create the index with ""SYNC (EVERY ...)"".

<p>Finally, we should talk about index optimization. CONTEXT indexes, over time, get less efficient as they get updated.  This inefficiency takes two forms:

<ol>
	<li>The index gets filled with garbage - it contains references to deleted or updated documents which are no longer current</li>
	<li>The ""postings lists"" get fragmented. The pointers to which documents contain each word get broken up into small chunks instead of the idea long strings.</li>
</ol>

We can fix this by running index optimization.  As with the manual call to sync indexes, this is done with a call to the PL/SQL package CTX_DDL:

<code>execute ctx_ddl.optimize_index('full_name_index', 'FULL')
</code>

The ""FULL"" parameter there is the <i>mode</i> of optimization. You can choose from the following:

<ul>
	<li>FAST: Only posting fragmentation is fixed - no garbage collection occurs</li>
	<li>FULL: Both fragmentation and garbage collection is dealt with</li>
	<li>REBUILD: The entire index is copied to a fresh table (requires more disk space but produces the best results).</li>
</ul>

The more frequently you run SYNC, the more fragmented your index will become, and the more often you will need to optimize.

<p>A typical ""maintenance regime"" at many customers is to run SYNC every five minutes, then run OPTIMIZE in FULL mode nightly, and OPTIMIZE in REBUILD mode once a week.",04-JUN-19 02.50.44.551534000 PM,"ROGER.FORD@ORACLE.COM",04-JUN-19 03.41.04.058824000 PM,"ROGER.FORD@ORACLE.COM"
219176342808554233808004916486221777410,219172783004222394396035270452714317015,"Try Different Stock Picking Algorithms",70,"<p>Like the previous problem, the SQL needs to keep adding rows to the result until the running total for the <em>previous</em> row is greater than the ordered quantity.</p>

<p>This starts the SQL to find all the needed locations. You can implement different stock picking algorithms by changing the ORDER BY for the running total. Experiment by replacing /* TODO */ with different columns to see what effect this has on the locations chosen:</p>
<code>select s.*
     , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
from (
 select o.item
      , o.qty ord_qty
      , i.loc
      , i.purch
      , i.qty loc_qty
      , nvl(sum(i.qty) over (
          partition by i.item
          order by /* TODO */
          rows between unbounded preceding and 1 preceding
        ),0) sum_prv_qty
  from  orderline o
  join  inventory i
  on    i.item  = o.item
  where o.ordno = 42
) s
where s.sum_prv_qty < s.ord_qty
order by s.item, s.purch, s.loc;</code>",05-MAY-20 09.41.27.899163000 AM,"CHRIS.SAXON@ORACLE.COM",05-MAY-20 04.23.49.416623000 PM,"CHRIS.SAXON@ORACLE.COM"
220671297147129808585568523510981756538,210640540520278905448644863263597314383,"Real-Time Materialized Views",90,"<p>A real-time MV allows a query to use its stale data. At query run time, the database applies any changes in the MV logs to get current results. To do this, the MV must be:</p>
<ul>
<li>FAST REFRESH ON DEMAND</li>
<li>Use the ENABLE ON QUERY COMPUTATION clause</li>
</ul>

<p>These statements change the MV do to this:</p>

<code>alter materialized view brick_colours_mv
  refresh fast on demand;
  
alter materialized view brick_colours_mv
  enable on query computation;
</code>

<p>Now when you make DML changes, the MV remains unchanged. So it is stale:</p>

<code>insert into bricks values ( -3, 'red', 'cube', 100, sysdate, default );
commit;

select *
from   brick_colours_mv;

select mview_name, staleness 
from   user_mviews;</code>

<p>But running the query within the MV uses it as a starting point, applying the changes in the log. This leads to a scary looking execution plan!</p>

<code>select /*+ gather_plan_statistics on query computation */colour, count(*) 
from   bricks
group  by colour;
  
select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>Provided you refresh the MV regularly so there is always few rows in the log, this is efficient. But leave it too long between refreshes and there will be lots of changes to apply. At some point this is slower than running the query itself!</p>

<p>The optimizer takes this into account when deciding whether to do a rewrite using a real-time MVs. If it thinks applying the changes is too much work, it'll read raw table data.</p>

<p>To ensure the best performance for real-time MVs, keep the time between refreshes small.</p>",19-MAY-20 04.55.02.203461000 PM,"CHRIS.SAXON@ORACLE.COM",17-JUL-20 04.45.41.501450000 PM,"CHRIS.SAXON@ORACLE.COM"
149117521792643576625295666920335132890,149075222712493902096666428584818910723,"Read Committed",50,"<p>This is the default mode in Oracle Database. Using this you have statement-level consistency. Each command can view all the changes saved in the database at the time it starts.</p>

<p>The following code starts a read committed transaction. The nested autonomous transaction set the price of all the rows to 1.61. Then adds a row for Baby Turtle. The two queries either side of the commit at the end can see these changes. So they return the same rows:</p>

<code>set transaction isolation level read committed;
select * from toys;

declare 
  pragma autonomous_transaction;
begin
  update toys set price = 1.61;
  insert into toys values ( 'Baby Turtle', 19.99 );  
  commit;
end;
/

select * from toys;
commit;
select * from toys;</code>

<p>Note that the changes must be committed before the query starts. The database hides any changes saved by another transaction while the query runs. This means none of the read phenomena apply to a single query.</p>",04-JUL-18 04.33.04.079429000 PM,"CHRIS.SAXON@ORACLE.COM",06-JUL-18 12.40.11.081233000 PM,"CHRIS.SAXON@ORACLE.COM"
219179164259547692187677246845621023325,219172783004222394396035270452714317015,"Row Numbering Methods",80,"<p>Whichever algorithm you decide to use to select stock, it may choose a poor route for the stock picker to walk around the warehouse. This can lead to them walking back down an aisle, when it would be better to continue up to the top of the aisle. Then walk back down the next one.</p>

<p>One way to do this is to number the aisles needed. Then walk up the odds and back down the evens. This means locations in the same aisle need the same aisle number. We want to assign these numbers!</p>

<p>Oracle Database has three row numbering functions:</p>

<ul>
<li>Rank - An Olympic ranking system. Rows with same sort key have the same rank. After ties there is a gap in the ranks. After ties the numbering starts from the row's position in the results.</li>
<li>Dense_Rank - Like RANK, this sets the rank to be the same for rows with the same sort key. But this has no gaps in the sequence</li>
<li>Row_Number - This gives unique consecutive values</li>
</ul>

<p>This compares the different ranking functions:</p>

<code>with inv_locations as ( 
  select s.loc
       , to_number(substr(s.loc,1,1)) warehouse
       , substr(s.loc,3,1) aisle
       , to_number(substr(s.loc,5,2)) position
  from   inventory s
)
  select loc
       , warehouse
       , aisle
       , row_number () over (
           order by warehouse, aisle
         ) rn
       , rank () over (
           order by warehouse, aisle
         ) rk
       , dense_rank () over (
           order by warehouse, aisle
         ) dr
  from   inv_locations;</code>",05-MAY-20 09.42.11.403790000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.49.16.026960000 PM,"CHRIS.SAXON@ORACLE.COM"
220608081358015009529359397056194171565,210639799513881861781674043532919876340,"Viewing PL/SQL Profiles",85,"<p>Querying this view returns the execution details:</p>

<code>select * from execution_stats;</code>

<p>The key columns to review are:</p>
<ul>
<li>SUBTREE_ELAPSED_TIME - total time for this procedure, including the child procedures it calls</li>
<li>FUNCTION_ELAPSED_TIME - the duration of this procedure, excluding its children</li>
<li>CALLS - how many times this procedure executed</li>
</ul>

<p>If the code is a SQL statement, the profiler will populate the SQL_ID and SQL_TEXT columns. You can also use the NAMESPACE to filter either SQL or PL/SQL execution stats.</p>

<p>To find the parts that run the longest, sort by the FUNCTION_ELAPSED_TIME descending:</p>

<code>select unit, function_elapsed_time, calls, sql_text 
from   execution_stats
order  by function_elapsed_time desc;</code>

<p>Once you've found the slowest parts of the program, you can find ways to make it faster. In this example, the INSERT and SELECT COUNT(*) take the longest, accounting for 90% of the program's runtime. These statements are executed thousands of times each, a sign that you can make gains by reducing the number of calls to these statements.</p>",19-MAY-20 10.15.06.531122000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-20 09.19.55.297914000 AM,"CHRIS.SAXON@ORACLE.COM"
219179169681906404846132244134979340529,219172783004222394396035270452714317015,"Finding Consecutive Rows - Pattern Matching",110,"<p>Added in Oracle Database 12c, the row pattern matching clause, MATCH_RECOGNIZE, offers another way to solve this problem.</p>

<p>To do this, you need to define pattern variables: criteria for rows to meet. Then create a regular expression using these variables.</p>

<p>To find consecutive dates, you need to look for rows where the current date equals the previous date plus one. This pattern variable does this:</p>

<pre>consecutive as run_date = prev ( run_date ) + 1</pre>

<p>To search for a series of consecutive rows, use this pattern:</p>

<pre>pattern ( init consecutive* ) </pre>

<p>This matches one instance of INIT followed by any number of CONSECUTIVE.</p>

<p>But what's this INIT variable? It has no definition!</p>

<p>Undefined variables are ""always true"". This matches any row. This enables the pattern to match the first row in the data set. Without this CONSECUTIVE will always be false, because the previous RUN_DATE will always be null.</p>

<p>This query returns the first date and number of rows in each group:</p>

<code>select *   
from   running_log   
match_recognize (  
  order by run_date  
  measures   
    first ( run_date ) as start_date,  
    count (*) as days  
  pattern ( init consecutive* )  
  define   
    consecutive as run_date = prev ( run_date ) + 1
);</code>

<p>By default MATCH_RECOGNIZE returns one row per group. To make it easier to see what's going on, this query adds the clause:</p>

<pre>all rows per match</pre>

<p>This returns all the matched rows. In the MEASURES clause it also adds the CLASSIFIER function. This returns the name of the pattern variable this row matches:</p>

<code>select *   
from   running_log   
match_recognize (  
  order by run_date  
  measures   
    classifier () as var,
    first ( run_date ) as start_date,  
    count (*) as days  
  all rows per match
  pattern ( init consecutive* )  
  define   
    consecutive as run_date = prev ( run_date ) + 1
);</code>",05-MAY-20 09.44.07.849554000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.41.42.526811000 PM,"CHRIS.SAXON@ORACLE.COM"
219176637560044200622833148363406994197,219172783004222394396035270452714317015,"Counting Number of Child Nodes in a Tree",120,"<p>To finish, we'll build an organization tree using the classic EMP table:</p>

<code>select empno
     , lpad(' ', (level-1)*2) || ename as ename
from   emp
start with mgr is null
connect by mgr = prior empno
order siblings by empno;</code>

<p>We want to augment this by adding the total number of reports each person has. I.e. count the number of nodes in the tree below this one.</p> 

<p>You can do this with the following query:</p>

<code>select empno
     , lpad(' ', (level-1)*2) || ename as ename
     , (
         select count(*)
           from emp sub
          start with sub.mgr = emp.empno
          connect by sub.mgr = prior sub.empno
       ) subs
from   emp
start with mgr is null
connect by mgr = prior empno
order siblings by empno;</code>

<p>The subquery calculates the hierarchy for every row in the table. So it queries EMP once for each row in the table. This leads to a huge amount of extra work. You can see this by getting its execution plan:</p>

<code>select /*+ gather_plan_statistics */empno
     , lpad(' ', (level-1)*2) || ename as ename
     , (
         select count(*)
           from emp sub
          start with sub.mgr = emp.empno
          connect by sub.mgr = prior sub.empno
       ) subs
from   emp
start with mgr is null
connect by mgr = prior empno
order siblings by empno;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>Note the FULL TABLE SCAN at line three happens 14 times. Reading a total of 196 rows. As you add more rows to EMP, this query will scale terribly.</p>",05-MAY-20 09.44.47.740824000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.55.52.858067000 PM,"CHRIS.SAXON@ORACLE.COM"
214380374397976606491367437882249361283,210638425667855918502469137992560342521,"Many One Row Updates vs. One Many Row Update",23,"<p>This runs a cursor-for loop to get all the red rows. For each row fetched, it updates its weight to one:</p>

<code>declare
  num_rows pls_integer := 10000;
begin 
  ins_rows ( num_rows );

  for rws in (select * from bricks where colour = 'red') loop 
   
    update bricks b
    set    weight = 1
    where  b.brick_id = rws.brick_id; 
     
  end loop; 

end;
/</code>

<p>But the loop is unnecessary! Adding a WHERE clause to UPDATE changes all the rows where the conditions are true. Instead of a loop, search for rows where the colour is red:</p>

<code>update bricks 
set    weight = 1
where  colour = 'red' ;
</code>

<p>As with INSERT, you can also use FORALL or other batch processing methods to change many rows in one call.</p> 

<p>This uses BULK COLLECT to populate an array with BRICK_IDs for the red rows:</p>

<code>declare
  num_rows pls_integer := 10000;
  rws dbms_sql.number_table;

begin 
  ins_rows ( num_rows );

  select brick_id
  bulk collect into rws
  from  bricks
  where colour = 'red';
  
  forall i in 1 .. rws.count 
    update bricks 
    set    weight = 1
    where  brick_id = rws (i); 
end;
/</code>

<p>Typically you only need to bulk updates when either:</p>
<ul>
<li>You receive arrays with different new column values for each row from the application</li>
<li>You use the array to do other procedural (non-SQL) processing</li>
</ul>",20-MAR-20 11.31.43.960842000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 03.47.15.309863000 PM,"CHRIS.SAXON@ORACLE.COM"
213643325217603996377423173493933648751,210639799513782729864465643940593969908,"Avoiding Function-based Indexes",100,"<p>Function-based indexes are powerful. But it's best to avoid creating them!</p>

<p>It's better to create indexes on the base columns. Then change queries so they don't include indexes on the columns in the query.</p>

<p>The DATE data type in Oracle Database includes a time component. So the previous query used TRUNC() to set the time to midnight for every date to find all the rows inserted on 1 Jan.</p>

<p>Instead of using TRUNC(), you can rewrite the query to search for all rows with a date greater than or equal to 1 Jan 2020. And strictly less than 2 Jan 2020. This allows you to create a regular index on INSERT_DATETIME:</p>

<code>create index bricks_insert_date_i 
  on bricks ( insert_datetime );
  
select /*+ gather_plan_statistics  1st Jan weights */weight, count ( * ) 
from   bricks
where  insert_datetime >= date'2020-01-01'
and    insert_datetime < date'2020-01-02'
group  by weight;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));
</code>

<p>This is more flexible than creating an index on TRUNC ( INSERT_DATETIME ). The index BRICKS_INSERT_DATE_I stores the full datetime values. So if you write another query search for rows with specific times, the database can use this index to read only these values:</p>

<code>select /*+ gather_plan_statistics  1st Jan weights */weight, count ( * ) 
from   bricks
where  insert_datetime >= date'2020-01-01'
and    insert_datetime < date'2020-01-01' + interval '12' hour
group  by weight;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));
</code>

<p>If you create a function-based TRUNC() index, the times are no longer part of the index. So while the optimizer may be able to use it, it'll be less effective than the non-function-based index.</p>",13-MAR-20 10.05.59.453842000 AM,"CHRIS.SAXON@ORACLE.COM",24-JUN-20 08.11.30.262052000 AM,"CHRIS.SAXON@ORACLE.COM"
219905691677373616463004679467669580093,210639799513782729864465643940593969908,"Introduction",5,"<p>The two most common data access methods in Oracle Database are:</p>
<ul>
<li>A full table scan</li>
<li>An index access method</li>
</ul>
<p>This tutorial compares these two methods and shows you how to create indexes. It uses the BRICKS table for the examples:</p>
<code>select * from bricks
fetch first 100 rows only;</code>",12-MAY-20 08.52.38.101106000 AM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 09.18.59.881010000 AM,"CHRIS.SAXON@ORACLE.COM"
182652206531848504307430860432553425323,182241645422959190146194127811898911783,"Continuing Past Errors with SAVE EXCEPTIONS",105,"<p>
Add the SAVE EXCEPTIONS clause to your FORALL statement when you want the PL/SQL runtime engine to execute all DML statements generated by the FORALL, even if one or more than fail with an error. 
</P><p>
In the code below (which is, I admit, kind of silly, since it updates all the employee rows each time), I am going to get errors. 
You can't update a first_name to a string of 1000 or 3000 bytes. But without SAVE EXCEPTIONS we never get past the third element in the bind array. The employees table has 107 rows. How many were updated?
</p>
<code>DECLARE  
   TYPE namelist_t IS TABLE OF VARCHAR2 (5000);  
  
   enames_with_errors   namelist_t  
      := namelist_t ('ABC',  
                     'DEF',  
                     RPAD ('BIGBIGGERBIGGEST', 1000, 'ABC'),  
                     'LITTLE',  
                     RPAD ('BIGBIGGERBIGGEST', 3000, 'ABC'),  
                     'SMITHIE');  
BEGIN  
   FORALL indx IN 1 .. enames_with_errors.COUNT  
      UPDATE employees  
         SET first_name = enames_with_errors (indx);    
EXCEPTION  
   WHEN OTHERS  
   THEN  
      DBMS_OUTPUT.put_line (  
         'Updated ' || SQL%ROWCOUNT || ' rows.');  
      DBMS_OUTPUT.put_line (SQLERRM);  
      ROLLBACK;  
END; 
</code>
<p>
Let's try that again with SAVE EXCEPTIONS. Below I ask that PL/SQL execute every generated statement no matter how of them fail, please! Now how many rows were updated? Notice that with SAVE EXCEPTIONS in place, 
</p>
<code>DECLARE  
   TYPE namelist_t IS TABLE OF VARCHAR2 (5000);  
  
   enames_with_errors   namelist_t  
      := namelist_t ('ABC',  
                     'DEF',  
                     RPAD ('BIGBIGGERBIGGEST', 1000, 'ABC'),  
                     'LITTLE',  
                     RPAD ('BIGBIGGERBIGGEST', 3000, 'ABC'),  
                     'SMITHIE');  
BEGIN  
   FORALL indx IN 1 .. enames_with_errors.COUNT SAVE EXCEPTIONS  
      UPDATE employees  
         SET first_name = enames_with_errors (indx);  
EXCEPTION
   WHEN OTHERS
   THEN
      DBMS_OUTPUT.put_line (  
         'Updated ' || SQL%ROWCOUNT || ' rows.');  
      DBMS_OUTPUT.put_line (SQLERRM);  
      ROLLBACK;  
END;
</code>

<p>
<strong>Fill in the Blanks</strong></p>
<p>
In the block below replace the #FINISH# tags with code so that after execution, we see ""Updated 2"" and ""ORA-24381: error(s) in array DML"".
</p>
<code>DECLARE  
   TYPE numbers_t IS TABLE OF NUMBER;  
  
   l_salaries   numbers_t  
      := numbers_t (100, 10000000, -900, 222888999);  
   l_ids         numbers_t  
      := numbers_t (107, 175, 133, 192); 
BEGIN
   #FINISH#  
      UPDATE employees  
         SET salary = l_salaries (indx)
         #FINISH#;  
EXCEPTION  
   WHEN OTHERS  
   THEN  
      DBMS_OUTPUT.put_line (  
         'Updated ' || SQL%ROWCOUNT || ' rows.');  
      DBMS_OUTPUT.put_line (SQLERRM);  
      ROLLBACK;  
END; 
</code>

<p><strong> Exercise 10:</strong></p>
<p>
Change the code you wrote in Exercise 9 so that the SQL engine executes the DML statement as many times as there are elements in the binding array. Notice how the error information changes.
</p>",21-MAY-19 07.49.56.290208000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM",05-JUN-19 07.35.20.643778000 PM,"STEVEN.FEUERSTEIN@ORACLE.COM"
212077512874769380690331378710441776760,210639799513641285543570732327153347316,"Disabling Histograms",75,"<p>In most cases histograms help the optimizer find better plans in columns with data skew. But sometimes they may lead to unstable plans. In which case you may want to disable histograms on the columns in problematic queries.</p>

<p>The SIZE option for the gather method determines what histograms the optimizer will create. By default, it uses:</p>

<pre>for all columns size auto</pre>

<p>This creates histograms based on the column usage and data skew rules described in the module Gathering Histograms.</p>

<p>To disable histograms for a column, set SIZE to one. Do this with the METHOD_OPT parameter of the gather stats routines. You can specify a specific column or ALL to do this for. This removes all the histograms on BRICKS:</p>

<code>exec dbms_stats.gather_table_stats ( null, 'bricks', method_opt => 'for all columns size 1' ) ;

select utcs.column_name, utcs.histogram, utcs.num_buckets
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'BRICKS';</code>

<p>If you want to ensure histograms remain disabled, it's best to do this by setting table preferences:</p>

<code>begin 
  dbms_stats.set_table_prefs ( 
    null, 'bricks', 
    'method_opt', 'for all columns size 1' 
  ); 
  dbms_stats.gather_table_stats ( null, 'bricks' ) ;
end;
/

select utcs.column_name, utcs.histogram
from   user_tables ut
join   user_tab_col_statistics utcs
on     ut.table_name = utcs.table_name
where  ut.table_name = 'BRICKS';</code>",27-FEB-20 10.19.58.431346000 AM,"CHRIS.SAXON@ORACLE.COM",30-APR-20 04.53.22.917073000 PM,"CHRIS.SAXON@ORACLE.COM"
219167227021919749012700990682583191156,219172783004222394396035270452714317015,"Finding Consecutive Rows - Tabibitosan",100,"<p>For the third problem we're searching for consecutive dates in this running log:</p>

<code>select * from running_log;</code>

<p>The goal is to split these rows into groups of consecutive dates. For each group, return the start date and number of days in it.</p>

<p>There's a trick you can use to do this:</p>

<ol>
<li>Assign unique, consecutive numbers sorted by date to each row</li>
<li>Subtract this row number from the date</li>
</ol>

<p>After applying this method, consecutive dates will have the same value:</p>

<code>with grps as (  
  select run_date
       , row_number () over ( order by run_date ) rn 
       , run_date - row_number () over ( order by run_date ) grp  
  from   running_log  
)  
  select *
  from   grps
  order  by run_date;</code>

<p>You can then summarise these data by grouping by the expression above and returning min, max, and counts to find start, end, and numbers of rows:</p>

<code>with grps as (  
  select run_date, row_number () over ( order by run_date ) rn ,  
         run_date - row_number () over ( order by run_date ) grp  
  from   running_log  
)  
  select min ( run_date ) first_run, count(*) runs  
  from   grps  
  group  by grp  
  order  by first_run;</code>

<p>This technique is referred to at the Tabibitosan method.</p>",05-MAY-20 09.43.37.711756000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.53.37.500765000 PM,"CHRIS.SAXON@ORACLE.COM"
213554788645537495461309999154230632072,210639064323015416218443333736849885877,"Measuring Work Done",10,"<p>To make SQL statements faster, you need to reduce the amount of work they do. The two key resources are CPU and I/O.</p>

<p>I/O is usually the limiting factor in databases. All other things being equal, the less I/O a statement does, the better it will scale.</p>

<p>Oracle Database stores rows in blocks. To access a row, it must first read the block the row is stored in. Each time the database reads a new block, this is another logical I/O operation.</p>

<p>So one of the key metrics to measure in Oracle Database are the logical I/Os for a query. This has two components:</p>
<ul>
<li>Consistent (read) gets</li>
<li>Current mode gets</li>
</ul>

<h3>Consistent gets</h3>

<p>These access blocks as they existed at some point in the past. This is usually the time the query started. This ensures that the query only returns values that existed when the query started. Any changes made after this are ignored.</p>

<p>You'll see these gets when running queries or to process the where clause of UPDATEs and DELETEs.</p>

<h3>Current mode gets</h3>

<p>This gets the block as it exists right now. You'll see these gets when changing data in INSERT, UPDATE, and DELETE statements.</p>

<p>When reading a block - whether in consistent or current mode - if it's not cached in memory, the database will also do a physical read. This fetches the block from disk.</p>

<p>You can view the I/O for a statement with the IOSTATS format in DBMS_XPlan. This adds a Buffers column to the plan. This sums the consistent and current mode gets for the operations:</p>

<code>select /*+ gather_plan_statistics */count(*) from bricks;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>",12-MAR-20 01.50.12.133857000 PM,"CHRIS.SAXON@ORACLE.COM",15-JUL-20 03.47.39.527213000 PM,"CHRIS.SAXON@ORACLE.COM"
214361139248917532636419446356040117276,210639799513881861781674043532919876340,"Profiling PL/SQL Code",70,"<p>SQL is just part of each process. There will also be some non-SQL code to call the statement, process its results, etc. In some cases the application code may take longer than the SQL!</p>

<p>So when performance tuning, it's important you profile all the code. Exactly how you do this depends on the programming language you're using. For PL/SQL code, you can use the hierarchical profiler.</p>

<p>This uses DBMS_HProf to capture execution details. Do this by calling:</p>
<ul>
<li>dbms_hprof.start_profiling to start the profile</li>
<li>the PL/SQL procedure you want to profile</li>
<li>dbms_hprof.stop_profiling to end the run</li>
</ul>

<p>This snippet outlines how to do this:</p>

<code>truncate table school_attendance;

exec dbms_hprof.create_tables ( true ); 
declare
  run pls_integer;
begin
  
  run := dbms_hprof.start_profiling ();
  
  start_school ();

  dbms_hprof.stop_profiling; 
  dbms_output.put_line ( run );
end;
/</code>",20-MAR-20 08.35.25.378336000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 04.04.52.774612000 PM,"CHRIS.SAXON@ORACLE.COM"
214088681964640443178231343765198123863,210640540520278905448644863263597314383,"Aggregating Millions of Rows",10,"<p>It's common for reporting queries to aggregate many rows down to a handful. Examples include:</p>
<ul>
<li>Counts of sales per week, month or year</li>
<li>Total value of orders per customer</li>
<li>Counting employees per department</li>
</ul>
<p>These queries may process millions or billions of rows, but only return a few. This is particularly common when aggregating by date.</p>
<p>This query counts the total rows per colour in the table:</p>
<code>select /*+ gather_plan_statistics */colour, count(*) 
from   bricks
group  by colour;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));
</code>

<p>This reads 10,000 rows. But returns just 4. You can speed up this search by creating a summary table with the count per colour. Then querying that:</p>

<code>create table colour_summary as 
  select colour, count(*)  c
  from   bricks
  group  by colour;

select /*+ gather_plan_statistics */*
from   colour_summary ;

select * 
from   table(dbms_xplan.display_cursor( :LIVESQL_LAST_SQL_ID, null, 'IOSTATS LAST'));</code>

<p>But for COLOUR_SUMMARY to be correct, you need to update it every time sometime changes rows in BRICKS. Coding this is a lot of effort. Luckily Oracle Database has an in-built option to do this automatically: materialized views (MVs).</p>",17-MAR-20 04.17.18.640306000 PM,"CHRIS.SAXON@ORACLE.COM",12-MAY-20 04.03.10.720765000 PM,"CHRIS.SAXON@ORACLE.COM"
214369197108048531715125675425297739791,210639799513881861781674043532919876340,"Tracing SQL Execution",10,"<p>To capture SQL execution details for all statements in a session, you can run a SQL trace. Do this with DBMS_monitor.session_trace_enable:</p>

<code>exec sys.dbms_monitor.session_trace_enable ( waits => true, binds => true );

exec start_school ();
  
exec sys.dbms_monitor.session_trace_disable ();
</code> 

<p><em><strong>Note:</strong> Live SQL does not support running a SQL trace, so the calls to dbms_monitor will error</em></p>

<p>This creates a trace file on the database server. You can also use this to capture execution details for other sessions. </p>",20-MAR-20 08.35.00.103417000 AM,"CHRIS.SAXON@ORACLE.COM",24-JUN-20 09.12.32.508099000 AM,"CHRIS.SAXON@ORACLE.COM"
222179806043892033188345143935636225871,210639064323015416218443333736849885877,"Reducing I/O: Filter vs Access Predicates",105,"<p>Indexes can also help the database find rows it's looking for faster. With a full table scan, the database reads every row and checks if it meets the WHERE clause.</p>

<p>This means the database may read more rows than your query returns, leading to wasted I/O. You can verify this by looking at the ""Predicate Information"" section of an execution plan:</p>

<code>select /*+ gather_plan_statistics */
       count(*) 
from   bricks
where  colour = 'red';

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>For this query, the predicate is a filter operation:</p>

<pre>Predicate Information (identified by operation id):
---------------------------------------------------
 
   2 - filter(""COLOUR""='red')</pre>

<p>Whereas with an index on COLOUR, the database can use this to only read red rows:</p>

<code>select /*+ gather_plan_statistics */  
       count(*) 
from   bricks_indexed
where  colour = 'red';

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));</code>

<p>The predicate is now an ACCESS:</p>

<pre>Predicate Information (identified by operation id):
---------------------------------------------------
 
   2 - access(""COLOUR""='red')</pre>

<p>This means the database only reads rows that match the search criteria. In this case it lead to a big reduction in work; just three I/Os to read the rows with an index compared to 40+ for a full table scan.</p>

<p>The next two modules will teach you how to create indexes and get the best out of them in Oracle Database.</p>",03-JUN-20 07.32.47.952001000 AM,"CHRIS.SAXON@ORACLE.COM",03-JUN-20 07.48.37.597192000 AM,"CHRIS.SAXON@ORACLE.COM"
219179447687505842611369255729113999472,219172783004222394396035270452714317015,"Change Stock Picking Route",90,"<p>To improve the routing algorithm, we want to give locations row numbers. Locations on the same aisle must have the same rank. We can then alternate the route up and down the aisles by sorting:</p>
<ul>
<li>By ascending position for odd aisles</li>
<li>By descending position for even aisles</li>
</ul>

<p>To do this locations in the same aisle must have the same rank. And there must be no gaps in the ranks. Thus DENSE_RANK is the correct function to use here. This sorts by warehouse number, then aisle:</p>

<pre>      , dense_rank() over (
           order by to_number(substr(s.loc,1,1))     -- warehouse
                  , substr(s.loc,3,1)                -- aisle
        ) aisle_no</pre>

<p>To alternate ascending/descending sorts, take this rank modulus two. Return the position when it's one and negate the position when it's zero:</p>

<pre>      , case
           when mod(s2.aisle_no,2) = 1 then s2.position
           else                            -s2.position
        end;</pre>

<p>The complete query for this is:</p>

<code>select s2.warehouse, s2.aisle, s2.aisle_no, s2.position
     , s2.loc, s2.item, s2.pick_qty
from (
  select to_number(substr(s.loc,1,1)) warehouse
      , substr(s.loc,3,1) aisle
      , dense_rank() over (
           order by to_number(substr(s.loc,1,1))     -- warehouse
                  , substr(s.loc,3,1)                -- aisle
        ) aisle_no
      , to_number(substr(s.loc,5,2)) position
      , s.loc, s.item
      , least(s.loc_qty, s.ord_qty - s.sum_prv_qty) pick_qty
  from (
    select o.item, o.qty ord_qty, i.loc, i.purch, i.qty loc_qty
         , nvl(sum(i.qty) over (
             partition by i.item
             order by i.qty, i.loc
             rows between unbounded preceding and 1 preceding
           ),0) sum_prv_qty
    from   orderline o
    join   inventory i
    on     i.item  = o.item
    where  o.ordno = 42
  ) s
  where s.sum_prv_qty < s.ord_qty
) s2
order by s2.warehouse
      , s2.aisle_no
      , case
           when mod(s2.aisle_no,2) = 1 then s2.position
           else                            -s2.position
        end;</code>",05-MAY-20 09.42.43.432054000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.52.33.652031000 PM,"CHRIS.SAXON@ORACLE.COM"
219179447687854013247418268931429378160,219172783004222394396035270452714317015,"Counting Child Nodes - Pattern Matching",130,"<p>You can overcome the performance issues for the previous query with this algorithm:</p>

<ul>
<li>Create the hierarchy returning the rows using depth first search.</li>
<li>Add a row number to the results showing which order this returns rows</li>
<li>Walk through the tree in this order</li>
<li>For each row, count the number of rows after it which are at a lower depth (the LEVEL is greater)</li>
</ul>

<p>This uses the fact that when using depth-first search, all the children of a node will be at a lower depth. The next node that is the same depth or higher as the current is not a child.</p>

<p>You can implement this in MATCH_RECOGNIZE with these clauses:</p>

<pre>      pattern ( strt higher* )
   define
      higher as higher.lvl > strt.lvl</pre>

<p>Like searching for consecutive rows, this starts with an always true variable. Then looks for zero or more rows which are at a greater depth.</p>

<p>A key difference is this clause:</p>

<pre>   after match skip to next row</pre>

<p>After matching the pattern for the first row, this instructs the database to repeat the process for the second row in the data set. Then the third, fourth, etc.</p>

<p>This contrasts with the default behaviour for MATCH_RECOGNIZE: after completing a pattern, continue the search from the last matched row. Because all rows are children of the root, the default would match every row. Then have nothing left to match! So it only returns the count of the child for KING!</p>

<p>All together this gives:</p>

<code>with hierarchy as (
  select lvl, empno, ename, rownum as rn
  from (
    select level as lvl, empno, ename
    from   emp
    start  with mgr is null
    connect by mgr = prior empno
    order siblings by empno
  )
)
select /*+ gather_plan_statistics */empno
     , lpad(' ', (lvl-1)*2) || ename as ename
     , subs
from hierarchy
  match_recognize (
   order by rn
   measures
      strt.rn as rn
    , strt.lvl as lvl
    , strt.empno as empno
    , strt.ename as ename
    , count(higher.lvl) as subs
   one row per match
   after match skip to next row
   pattern ( strt higher* )
   define
      higher as higher.lvl > strt.lvl
)
order by rn;

select * 
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'ROWSTATS LAST'));</code>

<p>Notice that now the pattern only reads EMP once, for a total of 14 rows read. This scales significantly better than using a subquery!</p>",05-MAY-20 09.45.11.688472000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.57.34.092741000 PM,"CHRIS.SAXON@ORACLE.COM"
214359925393836982117123723852081940542,210639799513881861781674043532919876340,"Saving PL/SQL Profile Details",80,"<p>To analyze the profile and persist it to database tables, run:</p>

<code>declare
  analysis_id pls_integer;
begin
  analysis_id := dbms_hprof.analyze ( 1, run_comment => 'Test run' );
end;
/</code>

<p>This stores the execution breakdown in these tables:</p>

<code>select * from dbmshp_function_info;
select * from dbmshp_parent_child_info;</code>

<p>To view the hierarchy, you need to use a hierarchical query! This view saves this query and adds formatting to make the results easier to understand:</p>

<code>create or replace view execution_stats as
with execution_stats as (
  select fi.runid,
         fi.symbolid,
         pci.parentsymid,
         rtrim(fi.module || '.' || nullif(fi.function,fi.module), '.') as unit,
         nvl(pci.subtree_elapsed_time, fi.subtree_elapsed_time) as subtree_elapsed_time,
         nvl(pci.function_elapsed_time, fi.function_elapsed_time) as function_elapsed_time,
         fi.line#,
         nvl(pci.calls, fi.calls) as calls,
         namespace,
         sql_id, 
         sql_text
  from   dbmshp_function_info fi
  left join dbmshp_parent_child_info pci 
  on     fi.runid = pci.runid 
  and    fi.symbolid = pci.childsymid
  where  fi.module != 'DBMS_HPROF'
), execution_tree as (
  select runid, 
         rpad(' ', (level-1)*2, ' ') || unit as unit,
         line#,
         subtree_elapsed_time,
         function_elapsed_time,
         calls,
         namespace,
         sql_id, 
         sql_text
  from   execution_stats
  start with parentsymid is null
  connect by parentsymid = prior symbolid
  and runid = prior runid
)
  select * from execution_tree;</code>",20-MAR-20 08.46.57.587488000 AM,"CHRIS.SAXON@ORACLE.COM",21-JUL-20 04.04.30.253637000 PM,"CHRIS.SAXON@ORACLE.COM"
219158891722255985200342144164202959339,219172783004222394396035270452714317015,"Get the Cumulative Quantity of Stock Picked",60,"<p>The algorithm needs to keep selecting locations until the total quantity picked is greater than the ordered quantity. As with the previous problem, this is possible with a running SUM.</p>

<p>This query gets the cumulative selected quantity for the current row and the previous quantity:</p>

<code>select o.item
     , o.qty ord_qty
     , i.loc
     , i.purch
     , i.qty loc_qty
     , sum(i.qty) over (
         partition by i.item
         order by i.purch, i.loc
         rows between unbounded preceding and current row
       ) sum_qty
     , sum(i.qty) over (
         partition by i.item
         order by i.purch, i.loc
         rows between unbounded preceding and 1 preceding
       ) sum_prv_qty
from   orderline o
join   inventory i
on     i.item  = o.item
where  o.ordno = 42
order by o.item, i.purch, i.loc;</code>

<p>To filter this to those locations needed to fulfil the order, we need all the rows where the previous picked quantity is less than the ordered quantity.</p>",05-MAY-20 09.39.34.077935000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 08.36.15.322263000 AM,"CHRIS.SAXON@ORACLE.COM"
152139165451798162976271858787332952163,152139165451368994310308665430312259683,"Undoing DML",60,"<p>Between inserting a row and committing it, your code may throw an exception. So you may wish to undo the change. You can do this with rollback. Rollback reverts all the changes since your last commit.</p>

<p>Say you added a row for Pink Rabbit. But realize you made a mistake, so want to remove it. The following does that:</p>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 7, 'Pink Rabbit', 'pink' );

select * from toys
where  toy_id = 7;

rollback;

select * from toys
where  toy_id = 7;</code>

<p>When you issue a rollback, the database will undo all changes you made since your last commit. If you commit between insert and rollback, rollback does nothing. And you need to run a delete to remove the row.</p>

<p><b>Note</b>: Please ensure the SQL Worksheet includes all insert and rollback statements when you click Run. Each time you press run, Live SQL runs an implicit commit at the end of your statements so if you run each statement one at a time, you will not be able to rollback.</p>

<p>For example, the following commits the change before the rollback - so the row for Pink Rabbit remains in the table:</p>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 7, 'Pink Rabbit', 'pink' );

select * from toys
where  toy_id = 7;

commit;
rollback;

select * from toys
where  toy_id = 7;</code>

<p>Clicking one button in your application may fire many DML statements. You should defer committing until after processing them all. This allows you to rollback the changes if an error happens part way through. This ensures all other users of the application either see all the changes or none of them.</p>",02-AUG-18 01.54.17.302488000 PM,"CHRIS.SAXON@ORACLE.COM",22-JAN-20 02.08.03.442505000 PM,"SHARON.KENNEDY@ORACLE.COM"
152139165452733871560653581768555532387,152139165451368994310308665430312259683,"Try It!",66,"<p>Complete the following code, placing commits, savepoints, and rollbacks in the /* TODO */ sections, so that you:</p>

<ul><li>Commit the insert of toy_id 8</li>
<li>Add rows for toy_id 9 & 10, but then remove the row for 10</li>
<li>Then undo the insert for toy_id 9</li></ul>

<code>insert into toys ( toy_id, toy_name, colour ) 
  values ( 8, 'Red Rabbit', 'red' );

/* TODO */

insert into toys ( toy_id, toy_name, colour ) 
  values ( 9, 'Purple Ninja', 'purple' );

exec /* TODO */

insert into toys ( toy_id, toy_name, colour ) 
  values ( 10, 'Blue Dinosaur', 'blue' );

/* TODO */

select * from toys
where  toy_id in ( 8, 9, 10 );

/* TODO */

select * from toys
where  toy_id in ( 8, 9, 10 );
</code>

<p>The first query should return the following rows:</p>

<pre><b>TOY_ID   TOY_NAME       COLOUR   </b>
       8 Red Rabbit     red      
       9 Purple Ninja   purple</pre>

<p>And the second this rows:</p>

<pre><b>TOY_ID   TOY_NAME     COLOUR   </b>
       8 Red Rabbit   red</pre>
<p><i>If you make a mistake and need to reset the table, click ""Execute Prerequisite SQL"" again</i></p>",02-AUG-18 02.10.28.639775000 PM,"CHRIS.SAXON@ORACLE.COM",05-SEP-18 02.23.02.320364000 PM,"CHRIS.SAXON@ORACLE.COM"
221498117967990582387211869132040091227,210638425667855918502469137992560342521,"Try It Challenge!",70,"<p>This process starts with 50,000 rows in BRICKS. It the loops through data to:</p>
<ul>
<li>Insert 20,000 more rows</li>
<li>Update the weight of blue bricks</li>
<li>Delete all the green bricks</li>
</ul>
<code>declare
  num_rows pls_integer := 50000;
begin
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
  
  for rws in ( 
    select level id from dual 
    connect by level <= 20000
  ) loop
  
    insert into bricks ( brick_id, colour, shape, weight )
    values ( rws.id + num_rows, 'red', 'cube', 1 );
  
  end loop;
  
  for rws in ( 
    select brick_id from bricks where colour = 'blue'
  ) loop
  
    update bricks b
    set    weight = 2
    where  b.brick_id = rws.brick_id;
  
  end loop;
  
  for rws in ( 
    select brick_id from bricks where colour = 'green'
  ) loop
  
    delete bricks b
    where  b.brick_id = rws.brick_id;
  
  end loop;
  
  timing_pkg.calc_runtime ( 'Looping' );

end; 
/
</code> 

<p>Run it a few times to see how long it takes. Then rewrite the process using the template below to make this faster:</p>

<code>declare
  num_rows pls_integer := 50000;
begin
 
  ins_rows ( num_rows );
  timing_pkg.set_start_time; 
  
  insert into bricks /* TODO */;
  
  update bricks b /* TODO */;
  
  delete bricks b /* TODO */;
  
  timing_pkg.calc_runtime ( 'Optimized' );
  
end; 
/</code> 

<p>How fast can you get the process to complete?</p>",27-MAY-20 02.50.15.377614000 PM,"CHRIS.SAXON@ORACLE.COM",27-MAY-20 02.50.15.377630000 PM,"CHRIS.SAXON@ORACLE.COM"
214369742431064075981401123307001641566,210639799513881861781674043532919876340,"Accessing SQL Trace Files",20,"<p>Once you've traced a session, you need to get the log file. This is on the database server. But it's rare you'll have access to this!</p>

<p>To overcome this, you can query a trace file using V$DIAG_TRACE_FILE_CONTENTS. This enables you to read the file using SQL. Export the table to a local file using spool or similar so you can analyse the trace:</p>

<pre>set serveroutput off
set pagesize 0
set echo off 
set feedback off 
set trimspool on 
set heading off
set tab off

spool c:\temp\trace-file.trc
select payload 
from   v$diag_trace_file_contents
where  trace_filename = (
    select substr (
           value,
           instr ( value, '/', -1 ) + 1
         ) filename
  from   v$diag_info
  where  name = 'Default Trace File'
)
order  by line_number;
spool off
</pre>",20-MAR-20 08.35.11.754763000 AM,"CHRIS.SAXON@ORACLE.COM",25-JUN-20 08.53.42.191282000 AM,"CHRIS.SAXON@ORACLE.COM"
213639088388810966196300410162550277189,210639799513782729864465643940593969908,"Full Table Scans",10,"<p>There are no indexes on the bricks table. So all queries against it will do a full table scan:</p>
<code>select /*+ gather_plan_statistics */count(*) from bricks;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

select /*+ gather_plan_statistics */count(*) from bricks
where  colour = 'red';

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));

select /*+ gather_plan_statistics */count(*) from bricks
where  brick_id = 1;

select *  
from   table(dbms_xplan.display_cursor(:LIVESQL_LAST_SQL_ID, format => 'IOSTATS LAST'));
</code>

<p>This means the database reads every single row in the table and every block (<a href=""https://www.oracle.com/pls/topic/lookup?ctx=dblatest&id=GUID-EA237A0C-5C39-4797-89F9-2F38566B10D8"">up to the high water mark</a>). Then applies the where clause. And returns those rows where this is true. For searches that return few rows, inspecting every single row is clearly a huge waste of effort.</p>

<p>Creating an index on columns in the WHERE clause of a query enables the database to only read rows matching the search criteria. This may reduce the amount of work needed to execute the query.</p>",13-MAR-20 09.32.05.523633000 AM,"CHRIS.SAXON@ORACLE.COM",12-AUG-20 02.53.36.889786000 PM,"CHRIS.SAXON@ORACLE.COM"
219179102786039679519409611372854159010,219172783004222394396035270452714317015,"Stock Picking Routines",50,"<p>Next we'll look at how to use SQL to find which stock location to get inventory from to fulfil orders. This will use these tables:</p>

<code>select * from orderline;
select * from inventory;</code>

<p>The algorithm needs to list all the locations stock pickers need to choose from. There must be enough locations for the sum of quantities in stock reaches the ordered quantity of that product.</p>",05-MAY-20 09.38.19.617967000 AM,"CHRIS.SAXON@ORACLE.COM",06-MAY-20 02.47.03.985960000 PM,"CHRIS.SAXON@ORACLE.COM"
213958468787149950265842890954288811464,210639064323027505476639480028596947637,"Try It Challenge!",100,"<p>Experiment with replacing /* TODO */ with different sets of columns from BRICKS. Which combination of columns gives the lowest overall clustering factor? Which gives the highest?</p>

<code>alter table bricks
  drop clustering ;
  
alter table bricks
  add clustering 
  by interleaved order ( /* TODO */ );
  
alter table bricks
  move online;
  
exec dbms_stats.gather_table_stats ( null, 'bricks' ) ;

select index_name, sum ( clustering_factor )
from   user_indexes
where  table_name = 'BRICKS'
group  by rollup ( index_name );
</code>

<p>But it's not the total clustering factor for your indexes that matters. You need to assess the performance needs of your queries. It may be that queries by weight or colour are part of the application search page. These are constantly executed and must be as fast as possible.</p>

<p>Whereas searches by date only happen once per day as part of a background job. So this query can take a little longer. Once you've found these requirements you can choose the appropriate columns to cluster on. In this example WEIGHT and COLOUR.</p>",16-MAR-20 10.10.17.931380000 AM,"CHRIS.SAXON@ORACLE.COM",14-MAY-20 01.14.59.059900000 PM,"CHRIS.SAXON@ORACLE.COM"
230282812958329308208162808955138646069,92046253613416673541249082253875246934,"Inner Select and WITH Clause Examples",110,"<p>Just like any other query, an AV query can be used as within an inner select or WITH clause.  As you have probably noticed by now, most queries selecting from an analytic view use very similar templates.  You can easily re-use these templates within inner selects.</p>

<p>Rank geographies by sales with the analytic view query in an inner select.</p>

<code>
SELECT
    product,
    geography,
    RANK() OVER(
        PARTITION BY product
        ORDER BY
            sales DESC
    ) AS sales_geog_rank,
    sales
FROM
    (
        SELECT
            time_hier.member_name        AS time,
            product_hier.member_name     AS product,
            geography_hier.member_name   AS geography,
            sales
        FROM
            sales_av HIERARCHIES (
                time_hier,
                product_hier,
                geography_hier
            )
        WHERE
            time_hier.member_name = 'CY2014'
            AND product_hier.level_name = 'DEPARTMENT'
            AND geography_hier.level_name = 'REGION'
    )
ORDER BY
    1,
    3;
</code>

<p>Rank geographies by sales with the analytic view query in a WITH clause.</p>

<code>
WITH my_query AS (
    SELECT
        time_hier.member_name        AS time,
        product_hier.member_name     AS product,
        geography_hier.member_name   AS geography,
        sales
    FROM
        sales_av HIERARCHIES (
            time_hier,
            product_hier,
            geography_hier
        )
    WHERE
        time_hier.member_name = 'CY2014'
        AND product_hier.level_name = 'DEPARTMENT'
        AND geography_hier.level_name = 'REGION'
)
SELECT
    product,
    geography,
    RANK() OVER(
        PARTITION BY product
        ORDER BY
            sales DESC
    ) AS sales_geog_rank,
    sales
FROM
    my_query
ORDER BY
    1,
    3;
</code>

<p>Pivot years to columns with the analytic view query as an inner select.</p>

<code>
SELECT
    *
FROM
    (
        SELECT
            time_hier.member_name        AS time,
            product_hier.member_name     AS product,
            geography_hier.member_name   AS geography,
            sales
        FROM
            sales_av HIERARCHIES (
                time_hier,
                product_hier,
                geography_hier
            )
        WHERE
            time_hier.level_name = 'YEAR'
            AND product_hier.level_name = 'DEPARTMENT'
            AND geography_hier.level_name = 'REGION'
    ) PIVOT (
        SUM ( sales )
        FOR time
        IN ( 'CY2011' AS cy2011, 'CY2012' AS cy2012, 'CY2013' AS cy2013, 'CY2014' AS cy2014, 'CY2015' AS cy2015 )
    )
ORDER BY
    product,
    geography;
</code>

<p>Pivot years to columns with the analytic view query in a WITH clause.</p>

<code>
WITH my_table AS (
    SELECT
        time_hier.member_name        AS time,
        product_hier.member_name     AS product,
        geography_hier.member_name   AS geography,
        sales
    FROM
        sales_av HIERARCHIES (
            time_hier,
            product_hier,
            geography_hier
        )
    WHERE
        time_hier.level_name = 'YEAR'
        AND product_hier.level_name = 'DEPARTMENT'
        AND geography_hier.level_name = 'REGION'
)
SELECT
    *
FROM
    my_table PIVOT (
        SUM ( sales )
        FOR time
        IN ( 'CY2011' AS cy2011, 'CY2012' AS cy2012, 'CY2013' AS cy2013, 'CY2014' AS cy2014, 'CY2015' AS cy2015 )
    )
ORDER BY
    product,
    geography;
</code>",19-AUG-20 05.03.30.937086000 PM,"WILLIAM.ENDRESS@ORACLE.COM",20-AUG-20 01.07.05.117054000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325417080764412197566268879610455307357,325337177180857205374378153253835695768,"Selecting Fact and Predefined Calculated Measures",30,"<p>Calculated measures defined in the analytic view can be selected as columns using this form of the FROM clause. This query selects MEMBER_CAPTION columns, fact measures and a calculated measure.</p>

<code>
  SELECT
    time_hier.member_caption,
    product_hier.member_caption,
    geography_hier.member_caption,
    sales,
    units,
    sales_prior_period
FROM
    sales_av
        HIERARCHIES (
            time_hier,
            product_hier,
            geography_hier
        )
WHERE
    time_hier.level_name = 'YEAR'
    AND product_hier.level_name = 'CATEGORY'
    AND geography_hier.level_name = 'COUNTRY'
ORDER BY
    product_hier.hier_order,
    geography_hier.hier_order,
    time_hier.hier_order;
</code>",16-FEB-23 12.19.47.184211000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 06.18.26.406749000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328895370482650832988438656292833237650,328774488191673081606718197428836641462,"Create the Analytic View",50,"<p>The analytic view can now be created.</p>

<p><code>
CREATE OR REPLACE ANALYTIC VIEW sales_av
USING av.sales_fact
DIMENSION BY
  (time_attr_dim
    KEY month_id REFERENCES month_name
    HIERARCHIES (
      time_hier DEFAULT),
   product_attr_dim
    KEY category_id REFERENCES category_id
    HIERARCHIES (
      product_hier DEFAULT),
   geography_attr_dim
    KEY state_province_id 
    REFERENCES state_province_id
    HIERARCHIES (
      geography_hier DEFAULT)
   )
MEASURES
 (sales FACT sales,
  units FACT units,
  sales_prior_period AS (LAG(SALES) OVER (HIERARCHY time_hier OFFSET 1))
  );
</code></p>

<p>Notes:</p>

<ul>

<li>The MONTH_NAME and MONTH_ID columns contain the same data. Therefore, either can be used for the join.  MONTH_NAME was used as the level KEY, so it is used in the join in the analytic view.</li>

<li>The SALES_PRIOR_PERIOD measure is not required and could be expressed in a query later, but it is included as an example.</li>

</ul>",21-MAR-23 07.23.26.056315000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 12.19.54.972304000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14812655490669191573419465663150328652,14810007204900148430867509211759172961,"Install Sample Data",40,"<p>The following script creates tables and loads data from an Oracle Object Store using DBMS_CLOUD.</p>

<code>CREATE TABLE time_dim (
    ""MONTH_ID""           VARCHAR2(30),
    ""MONTH_NAME""         VARCHAR2(40),
    ""MONTH_LONG_NAME""    VARCHAR2(30),
    ""MONTH_END_DATE""     DATE,
    ""MONTH_OF_QUARTER""   NUMBER,
    ""MONTH_OF_YEAR""      INTEGER,
    ""QUARTER_ID""         VARCHAR2(30),
    ""QUARTER_NAME""       VARCHAR2(40),
    ""QUARTER_END_DATE""   DATE,
    ""QUARTER_OF_YEAR""    NUMBER,
    ""SEASON""             VARCHAR2(10),
    ""SEASON_ORDER""       NUMBER,
    ""YEAR_ID""            VARCHAR2(30),
    ""YEAR_NAME""          VARCHAR2(40),
    ""YEAR_END_DATE""      DATE);

BEGIN
  DBMS_CLOUD.COPY_DATA (
  table_name => 'TIME_DIM',
    file_uri_list => 'https://objectstorage.uk-london-1.oraclecloud.com/p/tTVLBYgRx4xVM6DD74MWIiRnMO3UHgWA1ibqleMJqz7TTnehMw19_ZEFBG8wY5ky/n/adwc4pm/b/data_library/o/d1945/table=TIME_DIM/partition=all_rows/*.csv',
  format => '{""delimiter"":"","",""recorddelimiter"":""newline"",""skipheaders"":""1"",""quote"":""\\\"""",""rejectlimit"":""1000"",""trimspaces"":""rtrim"",""ignoreblanklines"":""false"",""ignoremissingcolumns"":""true"",""dateformat"":""DD-MON-YYYY HH24:MI:SS""}'
  );
END;
/

CREATE TABLE product_dim (
    ""CATEGORY_ID""       INTEGER,
    ""CATEGORY_NAME""     VARCHAR2(100),
    ""DEPARTMENT_ID""     INTEGER,
    ""DEPARTMENT_NAME""   VARCHAR2(100)
);

BEGIN
  DBMS_CLOUD.COPY_DATA (
  table_name => 'PRODUCT_DIM',
    file_uri_list => 'https://objectstorage.uk-london-1.oraclecloud.com/p/tTVLBYgRx4xVM6DD74MWIiRnMO3UHgWA1ibqleMJqz7TTnehMw19_ZEFBG8wY5ky/n/adwc4pm/b/data_library/o/d1945/table=PRODUCT_DIM/partition=all_rows/*.csv',
  format => '{""delimiter"":"","",""recorddelimiter"":""newline"",""skipheaders"":""1"",""quote"":""\\\"""",""rejectlimit"":""1000"",""trimspaces"":""rtrim"",""ignoreblanklines"":""false"",""ignoremissingcolumns"":""true"",""dateformat"":""DD-MON-YYYY HH24:MI:SS""}'
  );
END;
/

CREATE TABLE geography_dim (
    ""REGION_ID""             VARCHAR2(120),
    ""REGION_NAME""           VARCHAR2(100),
    ""COUNTRY_ID""            VARCHAR2(2),
    ""COUNTRY_NAME""          VARCHAR2(120),
    ""STATE_PROVINCE_ID""     VARCHAR2(120),
    ""STATE_PROVINCE_NAME""   VARCHAR2(400)
);

BEGIN
  DBMS_CLOUD.COPY_DATA (
  table_name => 'GEOGRAPHY_DIM',
    file_uri_list => 'https://objectstorage.uk-london-1.oraclecloud.com/p/tTVLBYgRx4xVM6DD74MWIiRnMO3UHgWA1ibqleMJqz7TTnehMw19_ZEFBG8wY5ky/n/adwc4pm/b/data_library/o/d1945/table=GEOGRAPHY_DIM/partition=all_rows/*.csv',
  format => '{""delimiter"":"","",""recorddelimiter"":""newline"",""skipheaders"":""1"",""quote"":""\\\"""",""rejectlimit"":""1000"",""trimspaces"":""rtrim"",""ignoreblanklines"":""false"",""ignoremissingcolumns"":""true"",""dateformat"":""DD-MON-YYYY HH24:MI:SS""}'
  );
END;
/

CREATE TABLE sales_fact (
    ""MONTH_ID""            VARCHAR2(10),
    ""CATEGORY_ID""         NUMBER,
    ""STATE_PROVINCE_ID""   VARCHAR2(120),
    ""UNITS""               NUMBER,
    ""SALES""               NUMBER
);

BEGIN
  DBMS_CLOUD.COPY_DATA (
  table_name => 'SALES_FACT',
    file_uri_list => 'https://objectstorage.uk-london-1.oraclecloud.com/p/tTVLBYgRx4xVM6DD74MWIiRnMO3UHgWA1ibqleMJqz7TTnehMw19_ZEFBG8wY5ky/n/adwc4pm/b/data_library/o/d1945/table=SALES_FACT/partition=all_rows/*.csv',
  format => '{""delimiter"":"","",""recorddelimiter"":""newline"",""skipheaders"":""1"",""quote"":""\\\"""",""rejectlimit"":""1000"",""trimspaces"":""rtrim"",""ignoreblanklines"":""false"",""ignoremissingcolumns"":""true"",""dateformat"":""DD-MON-YYYY HH24:MI:SS""}'
  );
END;
/</code>",27-NOV-23 03.22.57.791719000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.29.08.022279000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14833194710624787620353509032124744982,14810007204900148430867509211759172961,"Query the Relational Representation",100,"<p>Before continuing with this module, be sure to run the last CREATE VIEW statement in the previous module.</p>

<p>A relational representation of the analytic view is queried using the SELECT .. SUM .. FROM .. GROUP BY pattern. Like the hierarchical representation of the analytic view, the relational representation is a flattened (joined) view of the dimension and fact table tables. No joins are required. Unlike the hierarchical representation, the view returns only detailed rows. Use GROUP BY to return aggregate rows.</p>

<p>Calculated measures in an analytic view are not understood by GROUP BY. For example, GROUP BY does not understand how the Sales Percent Change Year Ago (SALES_PCT_CHG_YEAR_AGO) measure is calculated and cannot aggregate it. (In the analytic view, data is first aggregated and then the percent change year ago is calculated).</p>

<p>Since GROUP BY does not understand how a calculated measure is processed, calculated measures are selected using the AV_AGGREGATE aggregation operator. AV_AGGREGATE is a pass-through function that allows the calculated value to pass through GROUP BY unchanged.</p>

<p>The following query selects from the relational representation of the SALES_AV analytic view.</p>

<code>SELECT
    year
  , department
  , region
  , AV_AGGREGATE(sales)                  AS sales
  , AV_AGGREGATE(sales_year_ago)         AS sales_year_ago
  , AV_AGGREGATE(sales_chg_year_ago)     AS sales_chg_year_ago
  , AV_AGGREGATE(sales_pct_chg_year_ago) AS sales_pct_chg_year_ago
FROM
    sales_av_view
GROUP BY
    year
  , department
  , region;</code>

<p>If would like to view the SQL plan, you can use the tool of your choice (for example, Oracle SQL Developer) or run the following commands. Note that the plan shows that the SALES_AV analytic view is accessed.</p>

<p>If you wish to run commands, you will need a PLAN_TABLE.</p>

<code>CREATE TABLE PLAN_TABLE ( 
        statement_id       varchar2(30), 
        plan_id            number, 
        timestamp          date, 
        remarks            varchar2(4000), 
        operation          varchar2(30), 
        options            varchar2(255), 
        object_node        varchar2(128), 
        object_owner       varchar2(30), 
        object_name        varchar2(30), 
        object_alias       varchar2(65), 
        object_instance    numeric, 
        object_type        varchar2(30), 
        optimizer          varchar2(255), 
        search_columns     number, 
        id                 numeric, 
        parent_id          numeric, 
        depth              numeric, 
        position           numeric, 
        cost               numeric, 
        cardinality        numeric, 
        bytes              numeric, 
        other_tag          varchar2(255), 
        partition_start    varchar2(255), 
        partition_stop     varchar2(255), 
        partition_id       numeric, 
        other              long, 
        distribution       varchar2(30), 
        cpu_cost           numeric, 
        io_cost            numeric, 
        temp_space         numeric, 
        access_predicates  varchar2(4000), 
        filter_predicates  varchar2(4000), 
        projection         varchar2(4000), 
        time               numeric, 
        qblock_name        varchar2(30), 
        other_xml          clob)
NOCOMPRESS;</code>

<p>Now you can run EXPLAIN PLAN.</p>

<code>TRUNCATE TABLE plan_table;
 EXPLAIN PLAN
SET STATEMENT_ID = '1' INTO plan_table FOR
SELECT
    year
  , department
  , region
  , AV_AGGREGATE(sales)                  AS sales
  , AV_AGGREGATE(sales_year_ago)         AS sales_year_ago
  , AV_AGGREGATE(sales_chg_year_ago)     AS sales_chg_year_ago
  , AV_AGGREGATE(sales_pct_chg_year_ago) AS sales_pct_chg_year_ago
FROM
    sales_av_view
GROUP BY
    year
  , department
  , region;</code>

  <code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
START WITH id       = 0
 AND statement_id      = '1'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '1';</code>
 
<p>The next example selects data at the quarter, category, and country levels.</p>

<code>SELECT
    quarter
  , category
  , country
  , AV_AGGREGATE(sales)                  AS sales
  , AV_AGGREGATE(sales_year_ago)         AS sales_year_ago
  , AV_AGGREGATE(sales_chg_year_ago)     AS sales_chg_year_ago
  , AV_AGGREGATE(sales_pct_chg_year_ago) AS sales_pct_chg_year_ago
FROM
    sales_av_view
GROUP BY
    quarter
  , category
  , country;</code>",27-NOV-23 08.11.31.470548000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-JAN-24 06.00.38.291109000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19217921170978641460067455167488916001,14810007204900148430867509211759172961,"Pass Through Functions in Business Intelligence Tools",120,"<p>The use of AV_AGGREGATE requires the ability to use a pass-through function in a business intelligence tool.  For example, in Tableau, that is the RAWSQL function.</p>",08-JAN-24 07.34.57.081309000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.44.10.482966000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19221202468130745390443419692414552132,14810007204900148430867509211759172961,"Relational and Dimensional Semantics Compared",140,"<p>The hierarchical representation is consistent with OLAP cube semantics.  In the hierarchical representation of the analytic view, each row has a deterministic value independent of the WHERE clause. Data are aggregated first and the WHERE clause filters the resulting row set.</p>

<p>The relational representation of the analytic view is consistent with relational query semantics of tables. In the relational representation of the analytic view, the WHERE clause is applied prior to aggregation, and as a result, the value of any given row is determined in part by the WHERE clause.</p>

<p>(The hierarchical representation of the analytic view can actually blend relational and hierarchical semantics using FILTER FACT clause in a SELECT statement. For the purpose of comparing the hierarchical and relational representations, this tutorial focuses on the WHERE clause.  See the <a ref=""https://livesql.oracle.com/apex/livesql/file/tutorial_G28JKNOYZQNBIFOP1VDMHN5LS.html"">Using FILTER FACT to Extend Analytic View Aggregation in SELECT</a> for details.)</p>

<p>Consider the following queries that select sales for the year CY2015.</p>

<p>This hierarchical query selects sales for CY2015, and Europe and Africa. Rows are returned for each geography.</p>

<code>SELECT
    time_hier.member_name      AS time
  , geography_hier.member_name AS geography
  , sales
FROM
    sales_av HIERARCHIES (
        time_hier
    , geography_hier
    )
WHERE
        time_hier.member_name = 'CY2015'
    AND geography_hier.level_name = 'REGION'
    AND geography_hier.member_name IN ( 'Europe', 'Africa' )
ORDER BY
    2;</code>

<p>The same rows and same measure values are returned with the following relational query.</p>

<code>SELECT
    year
  , region
  , AV_AGGREGATE(sales) AS sales
FROM
    sales_av_view
WHERE
        year = 'CY2015'
    AND region IN ( 'Europe', 'Africa' )
GROUP BY
    year
  , region
ORDER BY
    2;</code>

<p>If geography is eliminated from the select list and HIERARCHIES clause of the hierarchical query, no rows are returned. (When a hierarchy is not listed in the HIERARCHY clause, the level defaults to ALL. Europe and Africa are not members of the ALL level.)</p>

<code>SELECT
    time_hier.member_name      AS time
  , sales
FROM
    sales_av HIERARCHIES (
        time_hier
    )
WHERE
        time_hier.member_name = 'CY2015'
    AND geography_hier.member_name IN ( 'Europe', 'Africa' );</code>

<p>If geography is eliminated from the select list and GROUP BY of the relational query, the sales for CY2015 is the aggregate of Europe and Africa.</p>

<code>SELECT
    year
  , AV_AGGREGATE(sales) AS sales
FROM
    sales_av_view
WHERE
        year = 'CY2015'
    AND region IN ( 'Europe', 'Africa' )
GROUP BY
    year;</code>

<p>Eliminate the REGION filter and note that the value for CY2015 changes to the aggregate of all geographies.</p>

<code>SELECT
    year
  , AV_AGGREGATE(sales) AS sales
FROM
    sales_av_view
WHERE
    year = 'CY2015'
GROUP BY
    year;</code>

<p>This is equivalent to dropping the GEOGRAPHY_HIER hierarchy from the hierarchical query.</p>

<code>SELECT
    time_hier.member_name      AS time
  , sales
FROM
    sales_av HIERARCHIES (
        time_hier
    )
WHERE
    time_hier.member_name = 'CY2015'
ORDER BY
    1;</code>

<p>The next set of examples illustrates this effect on calculated measures.</p>

<p>Consider the following examples which return sales and sales year ago by year. The first year in this data set is CY2011, so it will always return NULL for SALES_YEAR_AGO. CY2011 is the prior year to CY2012, so SALES_YEAR_AGO for CY2012 can return a non NULL value.</p>

<p>The hierarchical query returns a non NULL value for CY2011. The analytic view calculates the value and the filter is applied to the returning row set. This is consistent with OLAP cube semantics.</p>

<code>SELECT
    year_name
    , sales
    , sales_year_ago
FROM
    sales_av HIERARCHIES (time_hier)
WHERE
    time_hier.level_name = 'YEAR'
    AND year_name != 'CY2011'
ORDER BY
    year_name;</code>

<p>The relational query returns null for SALES_YEAR_AGO for CY2012 because the filter is applied prior to GROUP BY. This is consistent with relational semantics.</p>

<code>SELECT
    year
  , AV_AGGREGATE(sales)          AS sales
  , AV_AGGREGATE(sales_year_ago) AS sales_year_ago
FROM
    sales_av_view
WHERE
    year != 'CY2011'
GROUP BY
    year
ORDER BY
    year;</code>

<p>With the filter eliminated, the relational query returns a non NULL values for SALES_YEAR_AGO in CY2012.</p>

<code>SELECT
    year
    , AV_AGGREGATE(sales)          AS sales
    , AV_AGGREGATE(sales_year_ago) AS sales_year_ago
FROM
    sales_av_view
GROUP BY
    year
ORDER BY
    year;</code>

<p>When querying the relational representation, the HAVING clause can be used to exclude CY2011 from the query result while returning Sales Year Ago for CY2012.</p>

<code>SELECT
    year
  , AV_AGGREGATE(sales)          AS sales
  , AV_AGGREGATE(sales_year_ago) AS sales_year_ago
FROM
    sales_av_view
GROUP BY
    year
HAVING
    year != 'CY2011'
ORDER BY
    year;</code>",08-JAN-24 08.35.30.768669000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 02.21.44.713115000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19212832365237972382853041891963867594,14810007204900148430867509211759172961,"Important:  Required Oracle Versions for This Tutorial",20,"<p>The features that enable the relational representation of the analytic view are available in Oracle 23c and the Oracle Autonomous Database.</p>

<p>As of today, Oracle Live SQL runs Oracle 19c.  If you would like to run the code in this tutorial, copy and paste into a SQL worksheet tool that is connected to Oracle 23c or the Oracle Autonomous Database (including Oracle Autonomous Database on the Oracle Cloud Free Tier).</p>",08-JAN-24 06.26.05.454766000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 08.30.13.335482000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325441162490464114030445859479239012139,325337177180857205374378153253835695768,"Add a Calculated Measures to the Query",50,"<p>To add a calculated measure to the query, use the ADD MEASURES clause. Any measure that can be defined in analytic view DDL can be included in ADD MEASURES.</p>

<p>Several LAG parallel period calculations are added to the query.</p>

<code>
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units, 
     sales_quarter_ago,
    sales_chg_quarter_ago,
    ROUND(sales_pct_chg_quarter_ago,2) AS sales_pct_chg_quarter_ago
FROM ANALYTIC VIEW ( 
    USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        )
    ADD MEASURES (
        -- Sales 1 quarter ago
        sales_quarter_ago AS
            (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter)),
        -- Change in sales from 1 quarter ago
        sales_chg_quarter_ago AS
            (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter)),
        -- Percent change in sales one quarter ago
        sales_pct_chg_quarter_ago AS
            (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL quarter))
    )
) 
WHERE 
    time_hier.level_name = 'MONTH' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY' 
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order;
</code>",16-FEB-23 05.59.30.601796000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 07.49.31.371069000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14815354548123799472543049964276705054,14810007204900148430867509211759172961,"Examine the Sample Data",60,"<p>Become familiar with the sample data.</p>

<p>Hierarchies and analytic views access data from tables or views, typically a star schema or a 'flat' fact table (or some blend). This tutorial uses dimension tables from a star schema with sales data that varies by time, product and geography. In each dimension table there are _ID columns that are used as keys and _NAME columns that are used as textual descriptors. (In the attribute dimension objects, note the KEY and MEMBER_NAME properties of levels.) Other columns might be used for purposes such as sorting.</p>

<p>Select columns of the TIME_DIM table that are used by the analytic view.</p>

<code>SELECT
    year_name
    , quarter_name
    , month_name
    , month_id
    , month_end_date
FROM
    time_dim
ORDER BY
    month_end_date;</code>

<p>View columns of the PRODUCT_DIM table that are used by the analytic view.</p>

<code>SELECT
    department_name
    , category_id
    , category_name
FROM
    product_dim
ORDER BY
    department_name
    , category_name;</code>

<p>View columns of the GEOGRAPHY_DIM table that are used by the analytic view.</p>

<code>SELECT
    region_name
    , country_name
    , state_province_id
    , state_province_name
FROM
    geography_dim
ORDER BY
    region_name
    , country_name
    , state_province_name;</code>

<p>View columns of the SALES_FACT table.</p>

<code>SELECT * FROM sales_fact WHERE rownum < 20;</code>

<p>The following query returns data aggregated by year, department and country. This uses the familiar SELECT .. SUM .. FROM .. GROUP BY pattern that is used by most business intelligence tools. Note that the dimension tables join the fact tables using the _ID columns.</p>

<code>SELECT t.year_name,
  p.department_name,
  g.country_name,
  SUM(f.sales)
FROM time_dim t,
  product_dim p,
  geography_dim g,
  sales_fact f
WHERE t.month_id        = f.month_id
AND p.category_id       = f.category_id
AND g.state_province_id = f.state_province_id
GROUP BY t.year_name,
  p.department_name,
  g.country_name
ORDER BY t.year_name,
  p.department_name,
  g.country_name;</code>",27-NOV-23 03.58.44.043065000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 01.05.34.416535000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325455420250041220166796878286891799707,325337177180857205374378153253835695768,"Sample APEX Application using ADD MEASURE",70,"<p>You can view a sample APEX (Oracle Application Expression) application that uses ADD MEASURES that allows users to apply time series transformation. The application shows the generated SQL.</p>

<a href=""https://xcsvu3mx9d9bvet-adppm.adb.uk-london-1.oraclecloudapps.com/ords/r/av_demo/massachusetts-well-drilling136/""target=""_blank"" rel=""noopener"">Wells Drilled in the State of Massachusetts</a>",16-FEB-23 08.58.55.443215000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 09.05.22.643772000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328985821924214312870545519023029769260,328974142671334092196222097220687633361,"Review Sample Data and Analytic View Objects",20,"<p>Take a minute and review the analytic view objects.</p>

<p>List the analytic views, hierarchies and levels.</p>

<p><code>
SELECT
    analytic_view_name
    , hier_alias AS hierarchy
    , level_name
    , order_num
FROM
    user_analytic_view_levels
ORDER BY
    hier_alias,
    order_num;
</code></p>

<p>List the base (fact) measures. These are the measure that come from the fact table.</p>

<p><code>
SELECT
    analytic_view_name
    , measure_name
    , table_alias AS table_name
    , column_name
FROM
    user_analytic_view_base_meas;
</code></p>

<p>List the calculated measures of the analytic view.</p>

<p><code>
SELECT
    analytic_view_name
    , measure_name
    , meas_expression
FROM
    user_analytic_view_calc_meas;
</code></p>",22-MAR-23 04.13.45.332581000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 04.30.26.112121000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328990172869424006895036525175153683052,328974142671334092196222097220687633361,"Dimension Table and Hierarchy View Queries",30,"<p>Review the sample data using queries that select from the dimension lookup tables and hierarchy views.</p>

<p>A subset of columns in the tables was used in the analytic view.  The queries will select those columns.</p>

<p>Query the TIME_DIM table.</p>

<p><code>
SELECT 
    year_name
    , quarter_name
    , month_name
FROM
    time_dim
ORDER BY
    month_end_date;
</code></p>

<p>The following query selects the same columns and rows from the TIME_HIER hierarchy view.</p>

<p><code>
SELECT
    year_name
    , quarter_name
    , month_name
FROM
    time_hier
WHERE
    level_name = &#39;MONTH&#39;
ORDER BY
    hier_order;
</code></p>

<p>The hierarchy view can return rows for all levels of aggregation. Filtering to the lowest level causes the hierarchy view to return the same rows as the table.</p>

<p>The previous query selects <em>attribute</em> columns. Attribute columns reflect the data in the same columns of the dimension table.</p>

<p>Hierarchy views also include <em>hierarchical</em> columns.  Hierarchical columns are automatically created in every hierarchy view. Hierarchical columns are very useful for navigating hierarchies.  The names and roles are the same for every hierarchy view and analytic view, so they are easily queried with repeatable patterns.</p>

<p>The next query selects hierarchical columns for data at the Year level.</p>

<p><code>
SELECT
     member_name
     , member_unique_name
     , level_name
     , hier_order
FROM
    time_hier
WHERE
    level_name = &#39;YEAR&#39;
ORDER BY
    hier_order;
</code></p>

<p>The MEMBER_UNIQUE_NAME column is the primary key of the hierarchy view.  Values will always be unique hierarchy members unique across levels. This solves the problem where the same value is at more than one level in the source tables. For example &#39;New York&#39; in a CITY column and &#39;New York&#39; in a STATE column.</p>

<p>To query Quarter level data just change the LEVEL_NAME filter to &#39;QUARTER&#39;.</p>

<p><code>
SELECT
     member_name
     , member_unique_name
     , level_name
     , hier_order
FROM
    time_hier
WHERE
    level_name = &#39;QUARTER&#39;
ORDER BY
    hier_order;
</code></p>

<p>The hierarchy view can query data at multiple levels. Note that the values are sorted by HIER_ORDER, which sorts children within parents and ancestors.</p>

<p><code>
SELECT
     member_name
     , member_unique_name
     , level_name
     , hier_order
FROM
    time_hier
WHERE
    level_name IN (&#39;YEAR&#39;,&#39;QUARTER&#39;)
ORDER BY
    hier_order;
</code></p>

<p>Review data in the other dimension tables and hierarchy views.</p>

<p>This query selects from the PRODUCT_DIM table.</p>

<p><code>
SELECT
    department_name
    , category_name
FROM
    product_dim
ORDER BY
    department_name
    , category_name;
</code></p>

<p>This query selects from the PRODUCT_HIER hierarchy, returning the same rows and columns.</p>

<p><code>
SELECT
    department_name
    , category_name
FROM
    product_hier
WHERE
    level_name = &#39;CATEGORY&#39;
ORDER BY
    hier_order;
</code></p>

<p>This query selects from the GEOGRAPHY_DIM table.</p>

<p><code>
SELECT
    region_name
    , country_name
    , state_province_name
FROM
    geography_dim
ORDER BY
    region_name
    , country_name
    , state_province_name;
</code></p>

<p>This query selected from the GEOGRAPHY_HIER hierarchy, returning the same rows and columns.</p>

<p><code>
SELECT
    region_name
    , country_name
    , state_province_name
FROM
    geography_hier
WHERE
    level_name = &#39;STATE_PROVINCE&#39;
ORDER BY
    hier_order;
</code></p>

<p>The hierarchical columns work well in query templates because they have a consistent set of names. Application developers will find them easy to use in SQL generators.</p>

<p>Select all of the hierarchical columns from the GEOGRAPHY_HIER hierarchy view.</p>

<p><code>
SELECT
      member_name
    , member_unique_name
    , parent_unique_name
    , level_name
    , hier_order
    , depth
    , is_leaf
FROM
    geography_hier
ORDER BY
    hier_order;
</code></p>

<p>The following query selecting from the GEOGRAPHY_DIM table could be used to replicate the hierarchical columns of the hierarchy view.</p>

<code>
WITH foo AS (
    SELECT DISTINCT
        region_name AS member_name
      , '[REGION].&['
          || region_name
          || ']'      AS member_unique_name
      , NULL        AS parent_unique_name
      , 'REGION'    AS level_name
      , 0           AS depth
      , 0           AS is_leaf
    FROM
        geography_dim
    UNION
    SELECT DISTINCT
        country_name AS member_name
      , '[COUNTRY].&['
          || country_name
          || ']'       AS member_unique_name
      , '[REGION].&['
          || region_name
          || ']'       AS parent_unique_name
      , 'COUNTRY'    AS level_name
      , 1            AS depth
      , 0            AS is_leaf
    FROM
        geography_dim
    UNION
    SELECT DISTINCT
        state_province_name AS member_name
      , '[STATE_PROVINCE].&['
          || state_province_name
          || ']'              AS member_unique_name
      , '[COUNTRY].&['
          || country_name
          || ']'              AS parent_unique_name
      , 'STATE_PROVINCE'    AS level_name
      , 2                   AS depth
      , 1                   AS is_leaf
    FROM
        geography_dim
)
SELECT
    member_name,
    member_unique_name,
    parent_unique_name,
    level_name,
    depth,
    is_leaf
FROM 
    foo;
</code>

<p>Business intelligence applications often let users drill down on an aggregate value to reveal more detail.</p>

<p>The following query could be used to select children of Asia, selecting from the GEOGRPAHY_DIM table.</p>

<code>
SELECT
    country_name
FROM
    geography_dim
WHERE
    region_name = 'Asia';
</code>

<p>The following query could be used to select children of Asia, selecting from the GEOGRAPHY_HIER hierarchy view.</p>

<code>
SELECT
    country_name
FROM
    geography_hier
WHERE
    region_name = 'Asia'
    AND level_name = 'COUNTRY';
</code>

<p>This query also selects the children of Asia.  Because it filters on PARENT_UNIQUE_NAME, the LEVEL_NAME filter is not required.</p>.

<p><code>
SELECT
      member_name
    , member_unique_name
    , parent_unique_name
    , level_name
    , hier_order
    , depth
    , is_leaf
FROM
    geography_hier
WHERE
    parent_unique_name = &#39;[REGION].&amp;[Asia]&#39;
ORDER BY
    hier_order;
</code></p>

<p>This query template is reusable.  It  can also be used to select the children of Japan.</p>

<p><code>
SELECT
      member_name
    , member_unique_name
    , parent_unique_name
    , level_name
    , hier_order
    , depth
    , is_leaf
FROM
    geography_hier
WHERE
    parent_unique_name = &#39;[COUNTRY].&amp;[Japan]&#39;
ORDER BY
    hier_order;
</code></p>

<p>Attribute columns (the original columns from the table) and hierarchical columns can be used in the same query. Use the columns in any way you find useful. In this example, the attribute columns are returned while the hierarchical columns are used in the WHERE and ORDER BY clauses.</p>

<p><code>SELECT
    region_name
    , country_name
    , state_province_name
FROM
    geography_hier
WHERE
    parent_unique_name = '[COUNTRY].&[Japan]'
ORDER BY
    hier_order;</code></p>",22-MAR-23 05.16.27.358514000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-23 08.39.29.719172000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19222820545268541122856907316123402799,14810007204900148430867509211759172961,"Dimension Views and Star Queries",150,"<p>The examples so far have queried a single, flattened view of the analytic view. You might want to add dimension views to use as lookup tables or your application might want to map to dimension views and join them to the fact table. The relational representation of the analytic view supports this star style model.

You might be tempted to join the dimension table to the relational representation of the analytic view as in this query.</p>

<code>SELECT
    t.year_name
  , AV_AGGREGATE(f.sales)
FROM
    time_dim      t
  , sales_av_view f
WHERE
    t.month_name = f.month
GROUP BY
    t.year_name;</code>

<p>This will result in:</p>

<p>ORA-18388: Invalid usage of analytic view aggregation function.</p>

<p>If you query a fact column, nulls are returned.</p>

<code>SELECT
    t.year_name
  , SUM(f.sales)
FROM
    time_dim      t
  , sales_av_view f
WHERE
    t.month_name = f.month
GROUP BY
    t.year_name;</code>

<p>An analytic view can be joined to tables, but not within the context of a FACT ROWS view. A fact rows can can be joined to a dimension view created from the analytic view using the STAR ROWS DIMENSION keywords.</p>

<code>CREATE OR REPLACE VIEW time_dim_view AS
    SELECT
        time_hier.year_name
      , time_hier.quarter_name
      , time_hier.month_name
    FROM
        sales_av
    STAR ROWS DIMENSION time_attr_dim;</code>

<p>This view can be joined to the FACT ROWS view.</p>

<code>SELECT
    t.year_name
  , AV_AGGREGATE(f.sales)
  , SUM(sales_fact)
FROM
    time_dim_view      t
  , sales_av_view f
WHERE
    t.month_name = f.month
GROUP BY
    t.year_name;</code>

<p>With both the dimension view and the view of the analytic fact as STAR ROWS and FACT ROWS views, the Database understands the full context of the query and can best optimize the generated SQL.</p>",08-JAN-24 08.53.50.114175000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 02.27.54.143258000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325341389041059857821376011424761957851,325337177180857205374378153253835695768,"Introduction",10,"<p>An Analytic View is a type of view in the Oracle Database that allows users to perform complex queries and calculations on data stored in one or more tables. These views provide a higher level of abstraction over the underlying data, allowing users to access and analyze the data in a more meaningful way. They are typically used in business intelligence and data warehousing applications, and can be based on a single table or multiple tables joined together.</p>

<p>Analytic views include fact measures, which are accessed or derived from columns in a fact table, and calculated measures, which are measures created using analytic view expressions and SQL single-row functions.  Common examples of calculated measures include lead, lags, same period year ago, ranking, and shares. To get started with calculated measures, review the following tutorials:</p>
<ul>
<li><a href=""https://livesql.oracle.com/apex/livesql/file/tutorial_EDVE861H8YZ72TOBQQ4KS6EH0.html"">Creating Analytic Views - Getting Started</a></li>
<li><a href=""https://livesql.oracle.com/apex/livesql/file/tutorial_EDVE861IAM5PW9HEEWEQPIRB8.html"">Creating Time Series Calculations in Analytic Views</a></li>
</ul>
<p>Creating calculated measures within a query provides application developers the ability to provide large numbers of calculations without overloading the analytic view with calculations that satisfy every possible requirement. By allowing users of the application to choose a calculation type and set parameters for those calculation, the application can easily support hundreds or even thousands of calculations.</p>

<p>Consider a modest application with just a few fact measures such as Sales, Units Sold, Units Returns, and Dollar Value of Units Returned.  Let's say there are requirements for the base measures plus:</p>

<ul>
<li>Sum of each measure quarter to date.</i>
<li>Sum of each measure year to date</li>
</ul>

<p>So far, there are 8 measures (four base * two calculations).   Additional calculations might be required for each of the 8 measures:</p>

<ul>
<li>Prior period</li>
<li>Change from prior period</li>
<li>Percent change from prior period</li>
<li>Same period one year ago</li>
<li>Change from same period one year ago</li>
<li>Percent change from same period one year ago</li>
</ul>

<p>There now 48 measures, and this is a very small model. If we started out with 20 base measures, 240 predefined measures would be required in this scenario.  And every time new fact measures are added, new calculated measures would also need to be added.</p>

<p>As another example, what if there was a requirement for user defined moving aggregates (for example, a 6 month moving average)  where the use could control the number of trailing and leading periods and the aggregation operators?  The only way to satisfy that requirement would be on-the-fly calculations.</p>

<p>Because Oracle Application Express (APEX) developer provide SQL used in APEX application, APEX is well suited for defining calculations on-the-fly.</p>",15-FEB-23 06.52.05.676302000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 09.21.49.844175000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328999657451201401502112048199099234778,328974142671334092196222097220687633361,"Aggregate Sales",40,"<p>A comparison of queries access the fact table will begin with a simple aggregation of Sales to the Year, Department, and Year levels.</p>

<p>The table query is relatively straightforward, with JOINS and GROUP BY.</p>

<p><code>
SELECT
    t.year_name
  , p.department_name
  , g.region_name
  , SUM(sales)
FROM
    time_dim      t
  , product_dim   p
  , geography_dim g
  , sales_fact    f
WHERE
        t.month_name = f.month_id
    AND p.category_id = f.category_id
    AND g.state_province_id = f.state_province_id
GROUP BY
    t.year_name
  , p.department_name
  , g.region_name
ORDER BY
    t.year_name
  , p.department_name
  , g.region_name;
</code></p>

<p>There are two basic forms of this query using the analytic view.  The first selects from attribute columns and most closely mimics the table query.</p>

<p><code>
SELECT
    year_name
  , department_name
  , region_name
  , sales
FROM
    sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>

<p>The HIERARCHIES clause replaces joins and GROUP BY.</p>

<p>It would probably be fair to characterize the complexity of these queries as being about the same.</p>

<p>The AV query could also be written to select hierarchical columns as in the following example.</p>

<p><code>
SELECT
    time_hier.member_name AS time
  , product_hier.member_name AS product
  , geography_hier.member_name AS geography
  , sales
FROM
    sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    time_hier.hier_order
  , product_hier.hier_order
  , geography_hier.hier_order;
 </code></p>

<p>One of the advantages of using hierarchical columns is that they do not need to change when selecting data at different levels of aggregation.  Only the value of a LEVEL_NAME filter needs to change to select different aggregate levels.</p>

<p>Stable column names can also be an advantage when mapping the query to visualizations in your application. Those mappings typically need to remain stable. Keeping this in mind, the following queries select data at the Quarter level and Category levels. </p>

<p>The select list, GROUP BY, and ORDER BY need to change.  Aliases are added for name stability.</p>

<p><code>
SELECT
    t.quarter_name AS time
  , p.category_name AS product
  , g.region_name AS geography
  , SUM(sales) AS sales
FROM
    time_dim      t
  , product_dim   p
  , geography_dim g
  , sales_fact    f
WHERE
        t.month_name = f.month_id
    AND p.category_id = f.category_id
    AND g.state_province_id = f.state_province_id
GROUP BY
    t.quarter_name
  , p.category_name
  , g.region_name
ORDER BY
    t.quarter_name
  , p.category_name
  , g.region_name;
</code></p>

<p>In the query selecting from the analytic view, only the level filters need to change.  The query template is simply reused.</p>

<p><code>
SELECT
    time_hier.member_name AS time
  , product_hier.member_name AS product
  , geography_hier.member_name AS geography
  , sales
FROM
    sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
WHERE
    time_hier.level_name = &#39;QUARTER&#39;
    AND product_hier.level_name = &#39;CATEGORY&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    time_hier.hier_order
  , product_hier.hier_order
  , geography_hier.hier_order;
 </code></p>

<p> If you are generating queries against tables metadata that defines the meaning of each column might be needed. If you are querying analytic views, queries can be hard-coded to hierarchical column names.  And, if you need metadata, it is in the Oracle data dictionary.</p>

<p>It is easy to query multiple levels of aggregation from the analytic view. In this example, data is selected at both the Year and Quarter levels.</p>

<p><code>
SELECT
    time_hier.member_name AS time
  , product_hier.member_name AS product
  , geography_hier.member_name AS geography
  , sales
FROM
    sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
WHERE
    time_hier.level_name IN (&#39;YEAR&#39;,&#39;QUARTER&#39;)
    AND product_hier.level_name = &#39;CATEGORY&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
  product_hier.hier_order
  , geography_hier.hier_order
  , time_hier.hier_order;
 </code></p>",22-MAR-23 07.20.25.393973000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-23 08.19.55.002003000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328879168260287723270187754724058512849,328774488191673081606718197428836641462,"Sample Data Set",20,"<p>This tutorial uses data in the AV schema. The AV schema contains a simple star schema. It is the same schema used by the Creating Analytic Views - Getting Started tutorial.</p>

<p>Start with a sample query using data the AV schema.  The analytic view that will be designed will support queries such as this.</p>

<p><code>
SELECT
    t.year_name
  , p.department_name
  , g.region_name
  , SUM(f.units)
  , SUM(f.sales)
FROM
    av.time_dim      t
  , av.product_dim   p
  , av.geography_dim g
  , av.sales_fact    f
WHERE
        t.month_name = f.month_id
    AND p.category_id = f.category_id
    AND g.state_province_id = f.state_province_id
GROUP BY
    t.year_name
  , p.department_name
  , g.region_name
ORDER BY
    t.year_name
  , p.department_name
  , g.region_name;
</code></p>

<p>The analytic view will use a subset of columns from tables in the AV schema.</p>

<p>Start by reviewing the coluns in the SALES_FACT table.  We see that the table includes key columns for time, product and geography, and two fact columns. This table will need to be joined to dimension tables.</p 

<p><code> 
SELECT * FROM av.sales_fact; 
</code></p>

<p>Review the TIME_DIM table.</p>

<p><code>
SELECT * FROM av.time_dim;
</code></p>

<p>The following subset of columns will be used to create a time hierarchy.</p>

<p><code>
SELECT
    year_name
  , quarter_name
  , month_name
FROM
    av.time_dim
ORDER BY
    month_end_date;
</code></p>

<p>Review the PRODUCT_DIM table.</p>

<p><code>
SELECT * FROM av.product_dim;
</code></p>

<p>The following subset of columns will be used to create a product hierarchy.</p>

<p><code>
SELECT
    department_name
  , category_name
  , category_id
FROM
    av.product_dim
ORDER BY
    department_name
  , category_name;
</code></p>

<p>Review the GEOGRPAHY_DIM table.</p>

<p><code>
SELECT * FROM av.geography_dim;
</code></p>

<p>The following subset of columns will be used to create a geography hierarchy.</p>

<p><code>
SELECT
    region_name
  , country_name
  , state_province_name
  , state_province_id
FROM
    av.geography_dim
ORDER BY
    region_name
  , country_name
  , state_province_id;
 </code></p>
<p>Data from the fact table </p>

<p>All of the columns that will be used are combined in the following query.  Using a query to flatten a collection of dimension tables and the fact table can be a good way to begin thinking about the analytic view design.  The analytic view that will be created will be an improved version of this query.</p>

<p><code>
SELECT
    t.year_name
  , t.quarter_name
  , t.month_name
  , p.department_name
  , p.category_name
  , g.region_name
  , g.country_name
  , g.state_province_name
  , f.units
  , f.sales
FROM
    av.time_dim      t
  , av.product_dim   p
  , av.geography_dim g
  , av.sales_fact    f
WHERE
        t.month_name = f.month_id
    AND p.category_id = f.category_id
    AND g.state_province_id = f.state_province_id
ORDER BY
    t.month_end_date
  , p.department_name
  , p.category_name
  , g.region_name
  , g.country_name;
 </code></p>",21-MAR-23 03.54.21.520485000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 11.38.11.421801000 AM,"WILLIAM.ENDRESS@ORACLE.COM"
19311663134600053796630491469050819739,92046253613535148271571315912996452182,"Notes About Query Rewrite to the Materialized Cache",65,"<p>A few important notes about rewrite to the materialized view cache:</p>

<ul>
<li>The analytic view uses text match to rewrite to the materialized view.</li>
<li>Neither table constraints or SQL dimension objects are required for rewrite from the analytic view to the materialized view.</li>
<li>The defining query of the materialized view much match the query generated by DBMS_HIERARCHY.GET_MV_SQL_FOR_AV_CACHE.</li>
<li>ALTER SESSION SET query_rewrite_integrity = stale_tolerated is supported, allowing you to choose whether or not a materialized view used by the materialized cache is allowed to access a stale materialized view.",09-JAN-24 05.07.06.343497000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 05.10.38.803566000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
331926794232432792259523163623188893472,328974142671334092196222097220687633361,"Get Familiar with the Schema",15,"<p>Before getting started with querying analytic view objects, take a quick look at the tables in the AV schema on Live SQL.  The AV schema contains a simple star schema with three dimension tables and a fact table.</p>

<p>The TIME_DIM table.</p>

<code>SELECT * FROM time_dim;</code>

<p>The PRODUCT_DIM table.</p>

<code>SELECT * FROM product_dim;</code>

<p>The GEOGRAPHY_DIM table.</p>

<code>SELECT * FROM geography_dim;</code>

<p>And the SALES_FACT table.</p>

<code>SELECT * FROM sales_fact;</code>

That is the raw material that we have to work with.",19-APR-23 08.10.59.715531000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-23 08.16.53.851816000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19224183692405914378387150542810954579,14810007204900148430867509211759172961,"Summary",170,"<p>In this tutorial you learned that the analytic view can be queried with both hierarchical and relational GROUP BY style queries, with each providing full access the the data of the analytic view. Relational GROUP BY style queries allow any tool that includes support for aggregation pass-through functions to access measures calculated by the analytic view, and any tool can query columns of the fact table using relational aggregation operators.</p>

<p>There are many advantages to using analytic view as compared to accessing tables directly. The analytic view:</p>

<ul>
<li>Provides the owner of a data set to provide an in-database semantic model that presents data to many application as a 'single version of the truth'.</li>
<li>Can extend a data set with rich calculations, easing the burden on users to define calculations in business intelligence tools.</li>
<li>Provides a flattened presentation of data, simplifying the definition of data sets in business intelligence tools and query generation in custom applications.</li>
<li>Provides an simple and efficient method of creating and using aggregate table to improve query performance.</li>
</ul>",08-JAN-24 09.03.44.761942000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 09.03.44.761966000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14930621746151306766876371121984248507,14810007204900148430867509211759172961,"KEY, ALTERNATE KEY, and GROUP BY",125,"<p>Aggregate values are produced by the analytic view using the values in the level KEY attribute.  That is, the SQL generated by the analytic view uses the column of the level KEY in GROUP BY.</p>

<p>There are use cases where the level KEY is not the value that a user wishes to view in a business intelligence tool. For example, when the level KEY is an integer.  In that case, the user would more likely  want to view a different column with text value.</p>

<p>The attribute dimension for product uses the CATEGORY_ID column a the level KEY because it joins to the fact able and CATEGORY_NAME as an ALTERNATE KEY.  ALTERNATE KEY attributes can be used in GROUP BY.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING product_dim
ATTRIBUTES (
    department_name
    , category_name
    , category_id)
LEVEL department
    KEY department_name
LEVEL category
    KEY category_id
    ALTERNATE KEY category_name
    MEMBER NAME category_name
    DETERMINES (department_name);</code>

<p>There must be a 1:1 relationship between the level KEY and the ALTERNATE KEY in order for the GROUP BY of the level KEY and the GROUP BY of the ALTERNATE KEY to return the same data values. 
 The DBMS_HIERARCHY.VALIDATE_HIERARCHY and DBMS_HIERARCHY.VALIDATE_ANALYTIC_VIEW functions can be used to test that 1:1 relationships between the level KEY and ALTERNATE KEYs exist as expected</p>

<p>The following statement creates the product attribute dimension without CATEGORY_NAME as an ALTERNATE KEY.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING product_dim
ATTRIBUTES (
    department_name
    , category_name
    , category_id)
LEVEL department
    KEY department_name
LEVEL category
    KEY category_id
--    ALTERNATE KEY category_name
     MEMBER NAME category_name
    DETERMINES (department_name);</code>

<p>If CATEGORY_NAME is used in GROUP BY, an error is returned by the query.  Remember that in the SALES_AV_VIEW view, YEAR is a alias for YEAR_NAME and CATEGORY is an alias for CATEGORY_NAME.</p>

<code>SELECT
    year
  , category
  , AV_AGGREGATE(sales_pct_chg_year_ago) AS sales_pct_chg_year_ago
FROM
    sales_av_view
GROUP BY
    year
  , category;</code>

<p>Correct the attribute dimension by adding back CATEGORY_NAME as an ALTERNATE KEY.</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING product_dim
ATTRIBUTES (
    department_name
    , category_name
    , category_id)
LEVEL department
    KEY department_name
LEVEL category
    KEY category_id
    ALTERNATE KEY category_name
     MEMBER NAME category_name
    DETERMINES (department_name);</code>",28-NOV-23 06.28.38.847446000 PM,"WILLIAM.ENDRESS@ORACLE.COM",09-JAN-24 04.35.06.481118000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14931727625376877116252602465026863701,14810007204900148430867509211759172961,"Create Analytic View Objects",50,"<p>The next series of statements create analytic view objects.</p>

<code>-- Time dimension

CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING time_dim
ATTRIBUTES (
    year_name
    , quarter_name
    , month_name
    , month_end_date)
LEVEL year
    KEY year_name
LEVEL quarter
    KEY quarter_name
    DETERMINES (year_name)
LEVEL month
    KEY month_name
    ORDER BY month_end_date
    DETERMINES (month_end_date,quarter_name);

-- Time hierarchy

CREATE OR REPLACE HIERARCHY time_hier
USING time_attr_dim (
    month CHILD OF
    quarter CHILD OF
    year);

-- Product dimension

CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING product_dim
ATTRIBUTES (
    department_name
    , category_name
    , category_id)
LEVEL department
    KEY department_name
LEVEL category
    KEY category_id
    ALTERNATE KEY category_name
    MEMBER NAME category_name
    DETERMINES (department_name);

-- Product Hierarchy

CREATE OR REPLACE HIERARCHY product_hier
USING product_attr_dim (
    category CHILD OF
    department);

-- Geography dimension

CREATE OR REPLACE ATTRIBUTE DIMENSION geography_attr_dim
USING geography_dim
ATTRIBUTES (
    region_name
    , country_name
    , state_province_name
    , state_province_id)
LEVEL region
    KEY region_name
LEVEL country
    KEY country_name
    DETERMINES (region_name)
LEVEL state_province
    KEY state_province_id
    ALTERNATE KEY state_province_name
    MEMBER NAME state_province_name
    DETERMINES (country_name);

-- Geography hierarchy

CREATE OR REPLACE HIERARCHY geography_hier
USING geography_attr_dim (
    state_province CHILD OF
    country CHILD OF
    region );

-- Analytic view

CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY (
    time_attr_dim
        KEY month_id REFERENCES month_name
        HIERARCHIES (time_hier DEFAULT )
    , product_attr_dim
        KEY category_id REFERENCES category_id
        HIERARCHIES (product_hier DEFAULT)
    , geography_attr_dim
        KEY state_province_id REFERENCES state_province_id
        HIERARCHIES (geography_hier DEFAULT)
    )
MEASURES (
    sales FACT sales AGGREGATE BY SUM
    -- Sales prior period
    , sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales change from prior period
    , sales_chg_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales percent change from prior period
    , sales_pct_chg_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales year ago
    , sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales change from year ago
    , sales_chg_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales percent change from year ago.
    , sales_pct_chg_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales share of product within level (all members)
    , sales_shr_of_product AS (SHARE_OF(sales HIERARCHY product_hier MEMBER ALL))
    );</code>",28-NOV-23 06.40.15.749074000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.29.08.023716000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328985366220585297899463612774811185132,328974142671334092196222097220687633361,"Introduction",10,"<p>In the Update Tables with Simple Analytic Views - Part 1: Creating a Simple AV, analytic view objects were created using relatively simple DDL. This DDL includes only the elements necessary to create the structure needed for aggregation, hierarchical navigation, and calculation. It relied on defaults wherever possible and omitted descriptive metadata.  This analytic view is well-suited for many applications and is easy to create.</p>

<p>Part 2 will demonstrate how easy it is to generate SQL to query this analytic view. Queries will be generated that select from both tables and the analytic view.  You will see that the SQL required for tables grows increasingly complex as calculations are added and that the SQL required for the analytic view remains relatively simple.</p>

<p>Although these examples query the analytic view created with simple DDL, they are applicable to any analytic view. The analytic view is created by the Prerequisite SQL for this lab.</p>",22-MAR-23 04.05.40.223207000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 04.05.40.223230000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328970396923689763364431115177810738595,328774488191673081606718197428836641462,"Summary",70,"<p>The hierarchy views and analytic view created in this tutorial contain all the structure needed to support aggregations, hierarchical navigation, and calculations.  The elements in the DDL statements those that are required or recommended for best practices with this particular data set.  This DDL is excludes optional elements such as classifications that are used to provide additional descriptive metadata.</p>

<p>This style of analytic view would be appropriate for custom applications such as those developed using Oracle Application Expression (APEX).   Application development is simplified because analytic view can be queried using simple, re-usable query templates and calculations expressions. </p>

<p>Part 2 of this tutorial focuses on SQL generation. It will compare queries written against tables with queries written against this analytic view.</p>",22-MAR-23 12.40.42.064640000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 12.40.42.064664000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
14811886726907715414169505068996362807,14810007204900148430867509211759172961,"Introduction",10,"<p>Business intelligence tools such as Microsoft Power BI and Tableau query relational tables using the familiar SELECT .. SUM .. FROM .. GROUP BY pattern.  These tools can query Oracle analytic views using the same pattern.</p>

<p>Oracle always presents a hierarchical representation of hierarchies and the analytic view.  These are the views found in the USER_HIERARCHIES and USER_ANALYTIC_VIEWS dictionary views.  These views are queried with hierarchical semantics using the SELECT .. FROM AV(HIERARCHIES..) pattern.  ""AV aware"" applications such as Oracle Analytic Cloud use this pattern for ease of SQL generation and to use advanced features of the analytic view.</p>

<p>SELECT statements using the GROUP BY pattern select from an alternate relational representation of hierarchies and analytic views.  These SELECT statements can access all data in analytic views, including calculated measures, allowing you to query an analytic view with almost any business intelligence tool.  Different tools work best with specific relational representations of hierarchies and analytic views.  Therefore, you create that representation by creating a new view that selects from the hierarchy or analytic view.</p>

<p>This tutorial introduces the relational representation of the analytic view.  It uses the LiveSQL AV schema data, which many other Live SQL tutorials use.  You can easily compare hierarchical and relational style SQL by running this and other LiveSQL tutorials.</p>",27-NOV-23 03.10.10.933068000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 06.20.45.651243000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328968069453618840814240472734160190893,328774488191673081606718197428836641462,"Analytic View DDL - The Basics",30,"<p>Here is a very brief introduction to the elements of analytic view DDL.  A complete discussion is available in the tutorial <a href=""https://livesql.oracle.com/apex/livesql/file/tutorial_EDVE861IID1QUD1NIUPU5ALEW.html"">Creating Analytic Views - Getting Started Deep Dive</a>.</p></p>

<p>Just skim the following to become aquatinted with the terminology.  Do not worry about the details.</p>

<h3 id=""attribute-dimensions"">Attribute Dimensions</h3>

<p>An attribute dimension is a database object that contains most of the metadata of the dimension and hierarchies.  Attributes dimensions are reusable.  More than one hierarchy can reference an attribute dimension.</p>

<p>Every attribute dimension contains the following elements.  There are additional elements that are not discussed in this tutorial.</p>

<ul>

<li><p>The USING clause references a table or view.</p></li>

<li><p>The ATTRIBUTES list contains a list of columns of the form table used by the attribute dimension.  For this tutorial, you may think of attributes and columns as synonymous.</p>
</li>

<li><p>A LEVEL represents a level of aggregation in a hierarchy.</p>
</li>

<li><p>A level KEY contains the attributes (column) that is the primary key of the level.  As the primary key, it should have unique values of the level.</p>
</li>

<li><p>The ORDER BY property sorts values in the level, just like ORDER BY in a query.  ORDER BY uses an attribute from the ATTRIBUTES list.</p>
</li>

<li><p>MEMBER NAME, MEMBER CAPTION, MEMBER DESCRIPTION properties list attributes that are used as descriptive properties of the level key.   The properties are optional, but at least one is usually used when the level key is not an end user-friendly value.</p>
</li>

<li><p>The DETERMINES clause acts as a constraint.  It contains a list of one or more attributes.  The DETERMINES clause tells the hierarchy and analytic view that there is one and only one value of the <em>determined</em> attribute for each value of the key attribute. </p>
</li>

</ul>

<h2 id=""hierarchies"">Hierarchies</h2>

<p>A hierarchy is a view.  You can select from it with a SELECT statements.</p>

<ul>

<li><p>A hierarchy is simply a list of levels arranged as child levels and parent levels, where the parent level is a higher level of aggregation.</p>

</li>
<li><p>Hierarchies are reusable.  They may be referenced by more than one analytic view.</p>
</li>

<li><p>The USING clause references an attribute dimension.</p>
</li>

</ul>

<h3 id=""analytic-views"">Analytic Views</h3>

<p>The analytic view is the main event!  It brings together attribute dimensions, hierarchies, and the FACT table.</p>

<ul>
<li><p>The USING clause references the fact table.</p>
</li>

<li><p>The DIMENSIONS clauses list the dimensions and hierarchies the analytic views use.</p>
</li>

<li><p>The REFERENCES clause defines joins between the analytic view (and thus the fact table) to the attribute dimension (and therefore the dimension table).</p>
</li>

<li><p>The HIERARCHIES list contains the hierarchies used by the analytic view.</p>
</li>

<li><p>The MEASURES list contains a list of measures.  FACT measures come from the fact table.</p>
</li>

</ul>",22-MAR-23 12.16.29.121717000 PM,"WILLIAM.ENDRESS@ORACLE.COM",25-APR-23 11.21.55.223387000 AM,"WILLIAM.ENDRESS@ORACLE.COM"
328968069453918654417504900769487322541,328774488191673081606718197428836641462,"Create Attribute Dimensions and Hierarchies",40,"<h3 id=""introduction"">Introduction</h3>

<p>The analytic view objects will reference only the selected subset of columns that are used by the required elements of attribute dimensions, hierarchies, and the analytic view. Optional elements will be set to default values.</p>

<h3 id=""time-attribute-dimension-and-hierarchy"">Time Attribute Dimension and Hierarchy</h3>

<p>The time attribute dimension and hierarchy will include levels Month, Quarter, and Year.</p>

<p>Create the time attribute dimension.</p>

<p><code>
CREATE OR REPLACE ATTRIBUTE DIMENSION time_attr_dim
USING av.time_dim
ATTRIBUTES
 (year_name,
  quarter_name,
  month_name,
  month_end_date)
LEVEL year
  KEY year_name
LEVEL quarter
  KEY quarter_name
  DETERMINES (
    year_name)
LEVEL month
  KEY month_name
  ORDER BY month_end_date
  DETERMINES (
    month_end_date,
    quarter_name);
</code></p>

<p>Create the time hierarchy view.</p>

<p><code>
CREATE OR REPLACE HIERARCHY time_hier
USING time_attr_dim
 (month CHILD OF
  quarter CHILD OF
  year);
</code></p>
<p>Notes:</p>
<ul>

<li>Strictly speaking, the parent level key attribute does not need to be included in the DETERMINES clause.   Including the parent level key attribute in the DETERMINES clause enables certain query optimizations.</li>

<li>ORDER BY is included on the MONTH level because the values of MONTH_NAME do not sort in chronological order.  The values of QUARTER_NAME and YEAR_NAME will sort chronologically, so they do not need ORDER BY. ORDER BY will default to the key value.</li>

<li>All of the other properties of the level will default to using the level KEY value.</li>

</ul>

<p>Select from the TIME_HIER hierarchy.</p>

<p><code>
SELECT * FROM time_hier;
</code></p>

<h3 id=""product-attribute-dimension-and-hierarchy"">Product Attribute Dimension and Hierarchy</h3>

<p>Create the product attribute dimension.</p>

<p><code>
CREATE OR REPLACE ATTRIBUTE DIMENSION product_attr_dim
USING av.product_dim
ATTRIBUTES
 (department_name,
  category_name,
  category_id)
LEVEL department
  KEY department_name
LEVEL category
  KEY category_id
  MEMBER NAME category_name
  DETERMINES (
    department_name);
</code></p>

<p>Create the product hierarchy view.</p>

<p><code>
CREATE OR REPLACE HIERARCHY product_hier
USING product_attr_dim
 (category CHILD OF
  department);
 </code></p>

<p>Select from the PRODUCT_HIER hierarchy.</p>

<p><code>
SELECT * FROM product_hier;
</code> </p>

<p>Notes:</p>

<ul>

<li>CATEGORY_ID is used as the level key because PRODUCT_DIM table joins the the SALES_FACT table using CATEGORY_ID.</li>

<li>MEMBER NAME is used at the CATEGORY level to provide the descriptive value for the data.</li>

</ul>

<h3 id=""geography-attribute-dimension-and-hierarchy"">Geography Attribute Dimension and Hierarchy</h3>

<p>Create the geography attribute dimension.</p>

<p><code>
CREATE OR REPLACE ATTRIBUTE DIMENSION geography_attr_dim
USING av.geography_dim
ATTRIBUTES
 (region_name
  , country_name
  , state_province_name
  , state_province_id)
LEVEL region
  KEY region_name
LEVEL country
  KEY country_name
  DETERMINES (
    region_name)
LEVEL state_province
  KEY state_province_id
  MEMBER NAME state_province_name
  DETERMINES (
    country_name);
</code></p>

<p>Create the geography hierarchy view.</p>

<p><code>
CREATE OR REPLACE HIERARCHY geography_hier
USING geography_attr_dim
 (state_province CHILD OF
  country CHILD OF
  region);
 </code></p>

<p>Select from the GEOGRAPHY_HIER hierarchy.</p>

<p><code>
SELECT * FROM geography_hier;
</code> </p>

<p>Notes:</p>
<ul>

<li>STATE_PROVINCE_ID is used as the level key because GEOGRAPHY_DIM table joins the the SALES_FACT table using STATE_PROVINCE_ID.</li>

<li>MEMBER NAME is used at the STATE_PROVINCE level to provide the descriptive value for the data.</li>

</ul>",22-MAR-23 12.17.34.282166000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 12.17.54.851494000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325417079993383157877477416303163645836,325337177180857205374378153253835695768,"The USING form of the FROM clause",40,"<p>The USING form of the FROM clause allows the use of the ADD MEASURES and FILTER FACT clauses in the SELECT statement. ADD MEASURES extends the content of the analytic view with calculated measures.  To learn about FILTER FACT, see 
<a href=""https://livesql.oracle.com/apex/livesql/file/tutorial_EDVE861H8YZ72TOBQQ4KS6EH0.html"">Using FILTER FACT to Extend Analytic View Aggregation in SELECT.</a>

<p>The USING form of the FROM clause. It is structured as:</p> 

<code>FROM ANALYTIC VIEW USING (analytic view name(HIERARCHIES(hier1, hier2))</code>

<p>The following query uses the USING form of the FROM clause.</p>.

<code>
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units, 
    sales_prior_period 
FROM ANALYTIC VIEW ( 
    USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        ) 
) 
WHERE 
    time_hier.level_name = 'YEAR' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY' 
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order
</code>

<p>This form of the query can always be used.  With the exception of ADD MEASURES and FILTER FACT it is functionally equivalent the non-USING form.</p>

<p>This query can also be written as:</p>

<code>
WITH my_av ANALYTIC VIEW AS (
     USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        ) 
) 
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units, 
    sales_prior_period 
FROM
     my_av
WHERE 
    time_hier.level_name = 'YEAR' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY'
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order;
</code>",16-FEB-23 12.36.22.001114000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 08.44.13.200150000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
329115277962559400523971189361778687762,328974142671334092196222097220687633361,"Additional Calculation Examples",60,"<p>The comparisons between queries selecting from tables and analytic views show that table queries become increasingly complex as calculations are added, that analytic views can be queried with relatively simple, reusable templates, and that calculations are easily added to analytic view queries.</p>
<p>Analytic view expressions reference elements of the semantic model to make it easy to define calculations. Calculations can use both analytic view expressions and SQL single-row functions.</p>
<p>A few more calculation examples are provided in this section.</p>
<p>Shares calculate the ratio of the current member and the parent, ancestor, or the grand total of the hierarchy.</p>
<p>The following query includes:</p>
<ul>
<li>SHARE to ancestor at the region level.</li>
<li>SHARE to PARENT, which is the ratio of the current member to the member&#39;s parent in the hierarchy.</li>
<li>SHARE to ALL, which is the ratio of the current member to the grand total.</li>
</ul>
<p><code>
SELECT
    year_name
  , region_name
  , sales
  , ROUND(sales_share_of_region,2) AS sales_share_of_region
  , ROUND(sales_share_of_parent,2) as sales_share_of_parent
  , ROUND(sales_share_of_total,2) as sales_share_of_total
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , geography_hier
    )
    ADD MEASURES (
        sales_share_of_region AS (SHARE_OF(sales HIERARCHY geography_hier LEVEL region)),
        sales_share_of_parent AS (SHARE_OF(sales HIERARCHY geography_hier PARENT)),
        sales_share_of_total AS (SHARE_OF(sales HIERARCHY geography_hier MEMBER ALL))
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND geography_hier.level_name = &#39;STATE_PROVINCE&#39;
ORDER BY
    year_name
  , region_name;
</code></p>
<p>Rank expressions calculate the ranking of the current member within the parent, ancestor or grand total of the hierarchy.</p>
<p><code>
SELECT
    year_name
  , region_name
  , sales
  , sales_rank_geog_region
  , sales_rank_geog_parent
  , sales_rank_geog_level
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , geography_hier
    )
    ADD MEASURES (
    sales_rank_geog_region AS (RANK () OVER (HIERARCHY geography_hier ORDER BY sales DESC NULLS LAST WITHIN ANCESTOR AT LEVEL REGION)),
    sales_rank_geog_parent AS (RANK () OVER (HIERARCHY geography_hier ORDER BY sales DESC NULLS LAST WITHIN PARENT)),
    sales_rank_geog_level AS (RANK () OVER (HIERARCHY geography_hier ORDER BY sales DESC NULLS LAST WITHIN LEVEL))
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND geography_hier.level_name = &#39;STATE_PROVINCE&#39;
ORDER BY
    year_name
  , region_name;
</code></p>
<p>While this is not intended to be a calculation tutorial, it makes the point that this simple query template is easily extended with interesting calculations.</p>",23-MAR-23 09.54.09.518988000 PM,"WILLIAM.ENDRESS@ORACLE.COM",23-MAR-23 10.41.45.457290000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19224508808255673268939760381653480464,14810007204900148430867509211759172961,"Relational Representation and the Materialized Cache",160,"<p>The analytic view MATERIALIZED CACHE clause allows the analytic view to access data stored in aggregate tables. These aggregate tables are designed specifically for the analytic view, and are relatively small and efficient as compared to the typical materialized view used by a star schema. Materialized caches may be manually created at specific levels of aggregation or they may be system managed.</p>

<p>(A good starting strategy is to create a materialized cache at the first or second level from the top of a hierarchy. This will typically have the greatest benefit while also being small in size.)</p>

<p>Using the following statement, a materialized cache is added to the SALES_AV analytic view. Note the CACHE clause.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY (
    time_attr_dim
        KEY month_id REFERENCES month_name
        HIERARCHIES (time_hier DEFAULT )
    , product_attr_dim
        KEY category_id REFERENCES category_id
        HIERARCHIES (product_hier DEFAULT)
    , geography_attr_dim
        KEY state_province_id REFERENCES state_province_id
        HIERARCHIES (geography_hier DEFAULT)
    )
MEASURES (
    sales FACT sales AGGREGATE BY SUM
    , units FACT units AGGREGATE BY SUM
    , sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_chg_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_pct_chg_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_chg_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_pct_chg_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_shr_of_product AS (SHARE_OF(sales HIERARCHY product_hier MEMBER ALL))
    )
CACHE
    -- Cache specification for sales and units at the year, department, and country levels
    MEASURE GROUP (
        sales
        , units)
        LEVELS (
          time_hier.year,
          product_hier.department,
          geography_hier.country)
        MATERIALIZED
FACT COLUMNS (sales, units);</code>

<p>This PL/SQL code will create the aggregate table.</p>

<code>DECLARE
  mvsql CLOB;
BEGIN
  mvsql := dbms_hierarchy.get_mv_sql_for_av_cache('SALES_AV', 0);

  EXECUTE IMMEDIATE
    'CREATE TABLE sales_av_cache AS ' || mvsql;

END;
/</code>

<p>If you want to see the SQL, you run the following SELECT statement.</p>

<code>SELECT TO_CHAR(dbms_hierarchy.get_mv_sql_for_av_cache (
    analytic_view_name => 'SALES_AV'
    , cache_idx => 0 )) FROM dual;</code>

<p>The table contains aggregate data at the specified levels. The analytic view will access this table for any query at or above the levels of the materialized cache.</p>

<p>The materialized cache can be a materialized view or a table. In this tutorial it has been created as a table. The analytic view will be replaced, now with MATERIALIZED USING sales_av_cache. USING directs the database to always use the table when it is possible to do so. (Note that this rewrite is not using materialized view query rewrite.)</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY (
    time_attr_dim
        KEY month_id REFERENCES month_name
        HIERARCHIES (time_hier DEFAULT )
    , product_attr_dim
        KEY category_id REFERENCES category_id
        HIERARCHIES (product_hier DEFAULT)
    , geography_attr_dim
        KEY state_province_id REFERENCES state_province_id
        HIERARCHIES (geography_hier DEFAULT)
    )
MEASURES (
    sales FACT sales AGGREGATE BY SUM
    , units FACT units AGGREGATE BY SUM
    , sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_chg_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_pct_chg_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1))
    , sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_chg_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_pct_chg_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    , sales_shr_of_product AS (SHARE_OF(sales HIERARCHY product_hier MEMBER ALL))
    )
CACHE
    -- Cache specification for sales and units at the year, department, and country levels
    MEASURE GROUP (
        sales
        , units)
        LEVELS (
          time_hier.year,
          product_hier.department,
          geography_hier.country)
        MATERIALIZED USING sales_av_cache
FACT COLUMNS (sales, units);</code>

<p>The following query, at the year, department, and country levels, will use the materialized cache.</p>

<code>SELECT
    year
  , department
  , region
  , AV_AGGREGATE(sales)                  AS sales
  , SUM(sales_fact)                      AS sales_fact
  , SUM(units_fact)                      AS units_fact
FROM
    sales_av_view
WHERE
    region = 'Africa'
GROUP BY
    year
  , department
  , region;</code>

<p>Access to the SALES_AV_CACHE table can be seen in the execution plan.</p>

<code>TRUNCATE TABLE plan_table;
EXPLAIN PLAN
SET STATEMENT_ID = '2' INTO plan_table FOR
SELECT
    year
  , department
  , region
  , AV_AGGREGATE(sales)                  AS sales
  , SUM(sales_fact)                      AS sales_fact
  , SUM(units_fact)                      AS units_fact
FROM
    sales_av_view
WHERE
    region = 'Africa'
GROUP BY
    year
  , department
  , region;</code>

<p>You can view the plan with the following query.  Note that the analytic view is accessing the SALES_AV_CACHE table rather than the SALES_FACT table.</p>

<code>SELECT LPAD('............................',2*(LEVEL-1)) ||operation operation,
  OPTIONS,
  object_name,
  position
FROM plan_table
START WITH id       = 0
 AND statement_id      = '2'
 CONNECT BY PRIOR id = parent_id
 AND statement_id      = '2'; </code>

<p>So you now see an additional advantage of using the analytic view, transparent access to aggregate tables.</p>

<p>The materialized cache is a rich feature deserving of its own tutorial. That said, here are a few important points of information:</p>

<ul>
<li>The materialized cache can be manually managed, as in this example, or automatically managed.</li>
 <li>See the DBMS_AVTUNE package form more information on automatically managed materialized caches.</li>
<li>The materialized cache services both hierarchical and relational style queries.</li>
<li>The materialized cache pre-aggregates data. You can choose when and how to refresh it, and whether the analytic view is allowed to use a stale aggregate table.</li>
<li>The materialized cache may contain multiple level and measure groups, each with their own table, to optimize different workloads.</li>
</ul>",08-JAN-24 09.01.53.704321000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-JAN-24 04.52.13.921968000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19217934881073100008789513642895752199,14810007204900148430867509211759172961,"Fact Columns",130,"<p>All measures, including FACT measures, are calculated using operators and expressions of the measure. Aggregation operators of a FACT measure cannot be overridden in the query. For example, a measure defined as the SUM of SALES cannot be overriden as AVG(SALES) in a query using GROUP BY.</p>

<p>Columns in the fact table can, however, be selected through the analytic view in a GROUP BY type query. These columns are identified in the analytic view using the FACT COLUMNS clause. A column in the fact table can be used as a measure in the analytic view and as a FACT COLUMN.</p>

<p>For example, a SALES column can be used as a measure and be listed in FACT COLUMNS. A column that is not used as a measure can be used as a FACT COLUMN.</p>

<p>The following CREATE ANALYTIC VIEW statement defines SALES a measure (sales FACT sales) and includes the SALES column in FACT column. The UNITS column, which is not defined as a FACT measure, is also included in FACT COLUMNS.</p>

<code>CREATE OR REPLACE ANALYTIC VIEW sales_av
USING sales_fact
DIMENSION BY (
    time_attr_dim
        KEY month_id REFERENCES month_name
        HIERARCHIES (time_hier DEFAULT )
    , product_attr_dim
        KEY category_id REFERENCES category_id
        HIERARCHIES (product_hier DEFAULT)
    , geography_attr_dim
        KEY state_province_id REFERENCES state_province_id
        HIERARCHIES (geography_hier DEFAULT)
    )
MEASURES (
    sales FACT sales AGGREGATE BY SUM
    -- Sales prior period
    , sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales change from prior period
    , sales_chg_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales percent change from prior period
    , sales_pct_chg_prior_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1))
    -- Sales year ago
    , sales_year_ago AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales change from year ago
    , sales_chg_year_ago AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales percent change from year ago.
    , sales_pct_chg_year_ago AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1 ACROSS ANCESTOR AT LEVEL year))
    -- Sales share of product within level (all members)
    , sales_shr_of_product AS (SHARE_OF(sales HIERARCHY product_hier MEMBER ALL))
    )
FACT COLUMNS (sales, units);</code>

<p>FACT COLUMNS are included in the FACT ROWS view using FACT$. Note that when a column is used as both a measure and a FACT COLUMN, the column alias must be unique.</p>

<code>CREATE OR REPLACE VIEW sales_av_view AS
    SELECT
        time_hier.year_name                 AS year
      , time_hier.quarter_name              AS quarter
      , time_hier.month_name                AS month
      , time_hier.month_end_date            AS month_end_date
      , product_hier.department_name        AS department
      , product_hier.category_name          AS category
      , geography_hier.region_name          AS region
      , geography_hier.country_name         AS country
      , geography_hier.state_province_name  AS state_province
      -- These are fact columns
      , facts$.sales                         AS sales_fact
      , facts$.units                         AS units_fact
      -- These are analytic view measures
      , measures.sales                      AS sales
      , measures.sales_prior_period         AS sales_prior_period
      , measures.sales_chg_prior_period     AS sales_change_prior_period
      , measures.sales_pct_chg_prior_period AS sales_percent_change_prior_period
      , measures.sales_year_ago             AS sales_year_ago
      , measures.sales_chg_year_ago         AS sales_change_year_ago
      , measures.sales_pct_chg_year_ago     AS sales_percent_change_year_ago
      , measures.sales_shr_of_product       AS sales_share_of_product
    FROM
        sales_av fact ROWS;</code>

<p>Measures are selected with the AV_AGGREGATE operator and fact columns are selected using relational aggregation operators. The following query includes SALES as both a fact column and a measure, and UNITS as a fact column as both SUM and AVG.</p>

<code>SELECT
    year
  , department
  , region
  , AV_AGGREGATE(sales)                  AS sales
  , SUM(sales_fact)                      AS sales_fact
  , SUM(units_fact)                      AS units_fact
  , AVG(units_fact)                      AS avg_units_fact
FROM
    sales_av_view
GROUP BY
    year
  , department
  , region;</code>

<p>Querying fact columns through the analytic view allows the flexibility of applying different aggregation operators in the SELECT statement while offering the offering advantages of the analytic view such as eliminating the need for joins, hiding the complexity of underlying tables, and the ability to use a materialized cache (see later section in this document).</p>",08-JAN-24 07.38.28.264825000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.44.10.484387000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
19217921731682314158277060534812573091,14810007204900148430867509211759172961,"Some Advantages of Using an Analytic View",110,"<p>This is a good time to note some advantages of using the analytic view:</p>

<ul>
<li>Underlying tables are masked from the application.</li>
<li>Aggregation rules can be embedded in the analytic view.</li>
<li>Joins are not required.</li>
<li>Calculated measures are expressed in the analytic view and thus do not need to be expressed in the query.</li>
</ul>

<p>If this query were to select from tables:</p>
<ul>
<li>It would be a three pass query 1) aggregating data, 2) calculating year ago, and 3) calculated the percent change year ago.</li>
<li>The calculation expressions would need to change each time the GROUP BY clauses changed.</li>
<li>It would require joins.</li>
</ul>",08-JAN-24 07.32.43.550557000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.44.10.481933000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
329106944861368443140035273944289254545,328974142671334092196222097220687633361,"Prior Period Calculations",50,"<p>Time series calculations common to business intelligence applications.</p>
<h3 id=""assumption-that-data-is-dense-over-time"">Assumption that Data is Dense over Time</h3>
<p>When querying tables a prior period calculation is relatively easy if the assumption is made that the row set is <em>dense</em> over time.  Dense means that there are no gaps in the time periods within the selected row set.  In this example, it means that for each value of Department and Region, there are sales for every time period.  Then this is true, a simple LAG analytic window function can be used in the query of the tables.</p>
<p>The LAG function cannot be used when the query includes GROUP BY, so this is a two-pass query.  The first pass aggregates data and the second calculates the LAG.</p>
<p><code>
WITH sum_sales AS (
    SELECT
        t.year_name
      , p.department_name
      , g.region_name
      , SUM(sales) AS sales
    FROM
        time_dim      t
      , product_dim   p
      , geography_dim g
      , sales_fact    f
    WHERE
            t.month_name = f.month_id
        AND p.category_id = f.category_id
        AND g.state_province_id = f.state_province_id
    GROUP BY
        t.year_name
      , p.department_name
      , g.region_name
)
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , LAG(sales)
      OVER(PARTITION BY department_name, region_name
           ORDER BY
               year_name ASC
    ) AS sales_prior_period
FROM
    sum_sales
ORDER BY
    year_name
  , department_name
  , region_name;
</code></p>
<p>Calculations can be defined in the analytic view. The sample analytic view includes a SALES_PRIOR_PERIOD calculation.  The expression can be seen in the data dictionary.</p>
<p><code>
SELECT
    measure_name
  , meas_expression
FROM
    user_analytic_view_calc_meas;
</code></p>
<p>The calculation can be queried as a column of the analytic view. Note that this analytic view query differs from previous examples only by the addition of SALES_PRIOR_PERIOD in the select list.</p>
<p><code>
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , sales_prior_period
FROM
    sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>
<p>The following query is the same as the previous one except that it includes the USING form of the FROM clause.</p>
<p><code>
SELECT
    time_hier.member_name AS time
  , product_hier.member_name AS product
  , geography_hier.member_name AS geography
  , sales
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    time_hier.hier_order
  , product_hier.hier_order
  , geography_hier.hier_order;
 </code></p>
<p>The user form of the from clause can be used anytime, but it is required when using the ADD MEASURES clause. Calculated measures can be added using ADD MEASURES. In the following example, the Sales Prior Period measure is added in the query.</p>
<p><code>
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , sales_pp
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
    ADD MEASURES (
        sales_pp AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>
<p>Notice that the calculation is named SALES_PP.  The query modifies the definition of the analytic view for the duration of the query. Because the measure SALES_PRIOR_PERIOD is defined permanently in the analytic view a different name is needed for the on-the-fly calculation.</p>
<h3 id=""assumption-that-data-is-sparse-over-time"">Assumption that Data is Sparse over Time</h3>
<p>If it is assumed that data is dense over time and it is not, wrong data can be returned. For example, sales for 2015 could be returned as the prior period to 2017. It is important to understand your data when you make certain assumptions.</p>
<p>It would be expected that a time lookup table would have all time periods (no gaps).  To guard against sparse data, add a partitioned outer join between the aggregated row set and the TIME_DIM table.</p>
<p><code>
WITH sum_sales AS (
    SELECT
        t.year_name
      , p.department_name
      , g.region_name
      , SUM(sales) AS sales
    FROM
        time_dim      t
      , product_dim   p
      , geography_dim g
      , sales_fact    f
    WHERE
            t.month_name = f.month_id
        AND p.category_id = f.category_id
        AND g.state_province_id = f.state_province_id
    GROUP BY
        t.year_name
      , p.department_name
      , g.region_name
), year_dim AS (
    SELECT DISTINCT
        year_name
    FROM
        time_dim
)
SELECT
    b.year_name
  , a.department_name
  , a.region_name
  , a.sales
  , LAG(a.sales)
      OVER(PARTITION BY a.department_name, a.region_name
           ORDER BY
               b.year_name ASC
    ) AS sales_prior_period
FROM
    sum_sales a
    PARTITION BY ( a.department_name
                 , a.region_name ) RIGHT OUTER JOIN (
        SELECT DISTINCT
            b.year_name
        FROM
            year_dim b
    )         b ON ( a.year_name = b.year_name )
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>
<p> The analytic view automatically handles this sparse data case with time series calculations so the query selects from the analytic view is unchanged.</p>
<p> <code>
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , sales_pp
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
    ADD MEASURES (
        sales_pp AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>
<h3 id=""add-change-and-percent-change-prior-period-calculations"">Add Change and Percent Change Prior Period Calculations</h3>
<p> Change and Percent Change from Prior Periods are also common calculations.</p>
<p> These measures require a third pass in the query selecting from tables.</p>
<p> <code>
 WITH sum_sales AS (
    SELECT
        t.year_name
      , p.department_name
      , g.region_name
      , SUM(sales) AS sales
    FROM
        time_dim      t
      , product_dim   p
      , geography_dim g
      , sales_fact    f
    WHERE
            t.month_name = f.month_id
        AND p.category_id = f.category_id
        AND g.state_province_id = f.state_province_id
    GROUP BY
        t.year_name
      , p.department_name
      , g.region_name
), year_dim AS (
    SELECT DISTINCT
        year_name
    FROM
        time_dim
), sales_prior_period AS (
    SELECT
        b.year_name
      , a.department_name
      , a.region_name
      , a.sales
      , LAG(a.sales)
          OVER(PARTITION BY a.department_name, a.region_name
               ORDER BY
                   b.year_name ASC
        ) AS sales_prior_period
    FROM
        sum_sales a
        PARTITION BY ( a.department_name
                     , a.region_name ) RIGHT OUTER JOIN (
            SELECT DISTINCT
                b.year_name
            FROM
                year_dim b
        )         b ON ( a.year_name = b.year_name )
)
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , sales_prior_period
  , sales - sales_prior_period                          AS sales_change_prior_period
  , ( sales - sales_prior_period ) / sales_prior_period AS sales_pct_change_prior_period
FROM
    sales_prior_period
ORDER BY
    year_name
  , department_name
  , region_name;
</code></p>
<p>Two additional calculations can be added to the query selecting from the analytic view using the same query template.</p>
<p><code>
SELECT
    year_name
  , department_name
  , region_name
  , sales
  , sales_pp
  , sales_change_prior_period
  , sales_pct_change_period_period
FROM ANALYTIC VIEW (
    USING sales_av HIERARCHIES (
      time_hier
    , product_hier
    , geography_hier
    )
    ADD MEASURES (
        sales_pp AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1)),
        sales_change_prior_period AS (LAG_DIFF(sales) OVER (HIERARCHY time_hier OFFSET 1)),
        sales_pct_change_period_period AS (LAG_DIFF_PERCENT(sales) OVER (HIERARCHY time_hier OFFSET 1))
    )
  )
WHERE
    time_hier.level_name = &#39;YEAR&#39;
    AND product_hier.level_name = &#39;DEPARTMENT&#39;
    AND geography_hier.level_name = &#39;REGION&#39;
ORDER BY
    year_name
  , department_name
  , region_name;
 </code></p>",23-MAR-23 08.40.01.757666000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-23 08.20.34.886088000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328876007541456048049200928556184517445,328774488191673081606718197428836641462,"Introduction",10,"<p>This tutorial is the first of a two-part series.</p>

<p>In part one, you will design an analytic view using the simplest DDL statements possible for the sample data set.  This analytic view will provide all the necessary structure needed to aggregate data, navigate hierarchies, and support calculations. It will not include descriptive labels. It will rely on default values wherever possible.</p>

<p>In part two, you will walk through a series of queries of increasing complexity. Queries will be presented for both tables and the analytic view.  You will see that queries selecting from the analytic view use relatively simple, template-like SQL, even when more advanced calculations are required.</p>

<p>You will see that analytic views can be a great foundation for custom-built applications where the developer is responsible for generating queries.</p>

<p>Analytic views can be queried with simple, template-like queries that eliminate the need for joins, GROUP BY, and aggregation operators. Column names are consistent from one query and one AV to the next, often simplifying or eliminating the need for metadata used for query generation. And if metadata is needed, it is available in the Oracle data dictionary.</p>

<p>Queries selecting from analytic views can use analytic view calculation expressions to enhance applications with a variety of time series calculations, shares and hierarchical ratios, and rankings.</p>
<p>If you have not already done so, consider running the Creating Analytic Views - Getting Started tutorial. It will expose more features of the analytic view. You can also use it to compare a full featured analytic view with the simple analytic view presented in this tutorial.</p>",21-MAR-23 03.05.01.572772000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 01.14.36.523630000 AM,"WILLIAM.ENDRESS@ORACLE.COM"
19213581385429972152593124010085059960,14810007204900148430867509211759172961,"Required Permissions",30,"<p>Installing sample data requires the EXECUTE privilege on the DBMS_CLOUD package. Alternatively, CSV files can be downloaded and loaded into tables using an application such as Oracle SQL Developer.</p>

<p>Creating analytic view objects requires the following privileges:</p>

<ul>
 <li>CREATE ATTRIBUTE DIMENSION</li>
<li>CREATE HIERRACHY</li>
<li>CREATE ANALYTIC VIEW</li>
</ul>
<p>On Autonomous Database, these privileges are included in the DWROLE role.</p>",08-JAN-24 06.34.20.190935000 PM,"WILLIAM.ENDRESS@ORACLE.COM",08-JAN-24 07.29.08.020827000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325450619212532116952233847155987599702,325337177180857205374378153253835695768,"Usage Notes",60,"<p><b>Empty ADD MEASURES Clause</b></p>

<p>Applications that sometimes need define calculations can standardize on the USING form of the FROM clause to simplify SQL generation code.  Note that ADD MEASURES cannot be empty. There must be at least one measure in ADD MEASURES.</p>

<p>The following query will fail.</p>

<code>
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units
FROM ANALYTIC VIEW ( 
    USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        )
    ADD MEASURES ( )
) 
WHERE 
    time_hier.level_name = 'MONTH' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY' 
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order;
</code>

<p>Including one 'dummy' calculation in ADD MEASURES will solve this problem. It is not necessary to include the measure in the SELECT list.  For example:</p>

<code>
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units
FROM ANALYTIC VIEW ( 
    USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        )
    ADD MEASURES (
  dummy_measures AS (null)
  )
) 
WHERE 
    time_hier.level_name = 'MONTH' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY' 
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order;
</code>

<p><b>Duplicate Measures</b></p>

<p>Measure names must be unique within the analytic view.  A measure defined in ADD MEASURES is considered to be part of the analytic view for the duration of the query. In the following query SALES_CHG_PRIOR_PERIOD is defined in the analytic view and in the query. This query will fail with:</p>
<p>ORA-18345: Duplicate metadata object ""SALES_PRIOR_PERIOD"".</p>

<code>
SELECT 
    time_hier.member_caption, 
    product_hier.member_caption, 
    geography_hier.member_caption, 
    sales, 
    units
FROM ANALYTIC VIEW ( 
    USING sales_av 
        HIERARCHIES ( 
            time_hier, 
            product_hier, 
            geography_hier 
        )
    ADD MEASURES (
      sales_prior_period AS (LAG(sales) OVER (HIERARCHY time_hier OFFSET 1))
  )
) 
WHERE 
    time_hier.level_name = 'MONTH' 
    AND product_hier.level_name = 'CATEGORY' 
    AND geography_hier.level_name = 'COUNTRY' 
ORDER BY 
    product_hier.hier_order, 
    geography_hier.hier_order, 
    time_hier.hier_order;
</code>

<p><b>Unselected Calculations</b></p>

<p>All calculations that are included in ADD MEASURES are checked for correct syntax, adding a very small amount of parsing time.  Calculations are only executed if they are in the SELECT list.</p>",16-FEB-23 07.50.35.932486000 PM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 08.56.00.434235000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
325414938161147437535319074198266066164,325337177180857205374378153253835695768,"Sample Hierarchies and Analytic View",20,"<p>The prerequisite SQL for this tutorial created three hierarchies and a simple analytic view.  The following queries can be used to get aquatinted with these objects.</p>
<p>The TIME_HIER hierarchy has three levels:  Month, Quarter and Year.</p>

<code>
SELECT * FROM time_hier;
</code>

<code>
SELECT
    member_name,
    member_unique_name,
    level_name
FROM
    time_hier
ORDER BY
    hier_order;
</code>

<p>The GEOGRAPHY_HIER hierarchy also has three levels: State/Province, Country, and Region:</p>

<code>
SELECT * FROM geography_hier
</code>

<code>SELECT
    member_name,
    member_unique_name,
    level_name
FROM
    geography_hier
ORDER BY
    hier_order
</code>

<p>The PRODUCT_HIER hierarchy as two levels:  Category and Department.</p>

<code>
SELECT * FROM product_hier;
</code>

<code>
SELECT
    member_name,
    member_unique_name,
    level_name
FROM
    product_hier
ORDER BY
    hier_order
</code>

<p>The analytic view has two fact measures:</p>

<code>
SELECT * FROM user_analytic_view_base_meas;
</code>

<p>There are two calculated measures.</p>

<code>
SELECT *  FROM user_analytic_view_calc_meas
</code>",16-FEB-23 11.39.08.626563000 AM,"WILLIAM.ENDRESS@ORACLE.COM",16-FEB-23 08.40.23.559288000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
328892171483575719281910707513586679542,328774488191673081606718197428836641462,"Query the Analytic View",60,"<p>Analytic views contain <em>attribute columns</em> and <em>hierarchical columns</em>. Attribute columns replicate the data from the source tables (some transformations are possible, so the data might not always be an exact replication).  Hierarchical columns represent the different roles that the data plays and are automatically calculated by the hierarchy.</p>

<p>This example uses built-in hierarchical columns.</p>

<code>
SELECT
    time_hier.member_name AS time,
    product_hier.member_name AS product,
    geography_hier.member_name as geography,
    sales,
    sales_prior_period,
    units
FROM sales_av
    HIERARCHIES (
    	time_hier,
    	product_hier,
    	geography_hier)
WHERE
    time_hier.level_name = 'YEAR'
    AND product_hier.level_name = 'DEPARTMENT'
    AND geography_hier.level_name = 'REGION'
ORDER BY
    time_hier.hier_order,
    product_hier.hier_order,
    geography_hier.hier_order;
</code>

<p>Every hierarchy view and analytic view has the same set of hierarchical columns. Data from all levels of aggregation is presented in the same hierarchical columns.  This can dramatically simplify SQL generation.,  The following query selects from data at the Quarter, Country, and Department levels.</p>

<p>Note the the query does not include joins or GROUP BY.  Joins and aggregation rules are embedded in the analytic view and hierarchies. Listing the hierarchies replaces joins and GROUP BY in the query. Only the hierarchies used in the select list or filters needs to be included in the hierarchies list.</>

<code>
SELECT
    time_hier.member_name AS time,
    product_hier.member_name AS product,
    geography_hier.member_name as geography,
    sales,
    sales_prior_period,
    units
FROM sales_av
    HIERARCHIES (
    	time_hier,
    	product_hier,
    	geography_hier)
WHERE
    time_hier.level_name = 'QUARTER'
    AND product_hier.level_name = 'DEPARTMENT'
    AND geography_hier.level_name = 'COUNTRY'
ORDER BY
    time_hier.hier_order,
    product_hier.hier_order,
    geography_hier.hier_order;
</code>

<p>The following query uses attribute columns, which reflect the original columns from the tables.</p>

<code>
SELECT
    year_name,
    department_name,
    region_name,
    sales,
    sales_prior_period,
    units
FROM sales_av
    HIERARCHIES (
    	time_hier,
    	product_hier,
    	geography_hier)
WHERE
    time_hier.level_name = 'YEAR'
    AND product_hier.level_name = 'DEPARTMENT'
    AND geography_hier.level_name = 'REGION'
ORDER BY
    year_name,
    department_name,
    region_name;
</code>

<p>Queries can use any combination of attribute and hierarchical columns.</p>",21-MAR-23 07.31.18.906331000 PM,"WILLIAM.ENDRESS@ORACLE.COM",22-MAR-23 12.30.57.925192000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29031501982644120822754090784032565761,29025679842449363857149979830310169176,"Questions or Comments?",40,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.40.16.473342000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.41.09.156507000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29031501982673135042424841884225513985,14810007204900148430867509211759172961,"Questions or Comments?",180,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.40.53.658740000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.40.53.658765000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29031501982727536704307500197087291905,92046253613416673541249082253875246934,"Questions or Comments?",130,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.42.19.028047000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.42.19.028079000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29032730036794383633945771509108729435,328774488191673081606718197428836641462,"Questions or Comments?",80,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.43.16.300655000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.43.16.300681000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29032730037008363504017560873031722587,153496376830076594111863106943541076337,"Questions or Comments?",60,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.44.41.177190000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.44.41.177264000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29838797982085961262744012077915983533,29836425107181974316375148569777144134,"Create Attribute Dimension and Hierarchy",30,"<p>Create an attribute dimension and hierarchy using the TIME_DIM view.</p>

<p>Note that all levels can ORDER BY day_id.  The hierarchy will automatically ORDER BY MAX(day_id).</p>

<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_ad
USING time_dim
ATTRIBUTES (
  day_id
  , day
  , month
  , quarter
  , year)
LEVEL year
  KEY year
  MEMBER NAME year
  ORDER BY day_id
LEVEL quarter
  KEY quarter
  MEMBER NAME quarter
  ORDER BY day_id
  DETERMINES (year)
LEVEL month
  KEY month
  MEMBER NAME month
  ORDER BY day_id
  DETERMINES (quarter) 
LEVEL day
  KEY day_id
  MEMBER NAME day
  ORDER BY day_id
  DETERMINES (day, month);</code>

<p>Create the TIME_HIER hierarchy.</p>

<code>CREATE OR REPLACE HIERARCHY time_hier
USING time_ad (
  day CHILD OF
  MONTH CHILD OF
  QUARTER CHILD OF
  YEAR);</code>

<p>View data in the hierarchy.</p>

<code>SELECT * FROM time_hier;</code>",19-APR-24 11.57.57.777946000 AM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 01.10.58.881585000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29029975315639003315574108329959549637,29025679842449363857149979830310169176,"Create Attribute Dimension",30,"<p>To use multiple tables, list each table in the USING clause (comma separated) and add join paths. Joins paths join the tables as they would be joined in a query. Note that JOIN PATHs are not comma separated.</p>

<p>Be sure to alias attributes.</p>


<code>CREATE OR REPLACE ATTRIBUTE DIMENSION time_ad
USING time_month_dim AS m, time_quarter_dim AS q, time_year_dim AS y
  JOIN PATH m_to_q ON m.quarter_id = q.quarter_id
  JOIN PATH q_to_y ON q.year_id = y.year_id
ATTRIBUTES (
  m.month_id AS month_id
  , m.month_name AS month_name
  , m.month_end_date AS month_end_date
  , q.quarter_id AS quarter_id
  , q.quarter_name AS quarter_name
  , y.year_id AS year_id
  , y.year_name AS year_name
  )
LEVEL year
  KEY year_id
  MEMBER NAME year_name
  ORDER BY month_end_date
LEVEL quarter
  KEY quarter_id
  MEMBER NAME quarter_name
  ORDER BY month_end_date
  DETERMINES (year_id)
LEVEL month
  KEY month_id
  MEMBER NAME month_name
  ORDER BY month_end_date
  DETERMINES (quarter_id, month_end_date)
ALL 
  MEMBER NAME 'ALL'
  MEMBER CAPTION 'All'; 
</code>

<p>Now you can create the hierarchy.</p>

<code> CREATE OR REPLACE HIERARCHY time_hier
  USING time_ad
  (month
    CHILD OF quarter
    CHILD OF year);
</code>
 
<p>Query the hierarchy.</p>

<code>SELECT *
FROM time_hier
ORDER BY hier_order;</code>",11-APR-24 06.08.23.579052000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.51.44.573511000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29025165091529297699833865769846865047,29025679842449363857149979830310169176,"Introduction",10,"<p>Analytic view attribute dimensions metadata that maps to physical tables (or view) and defines levels.  Attribute dimensions may be mapped to a single star-style dimension table or multiple tables in a snowflake configuration.  This tutorial provides an example of a snowflake configuration using multiple tables.</p>

<p>Be sure to run the Perquisite SQL before continuing.</p>",11-APR-24 05.08.27.834249000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 05.08.27.834284000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29025165092730969964530807169504803991,29025679842449363857149979830310169176,"Examine Sample Data",20,"<p>The same data is contained in three tables with time data at the month, quarter, and year levels</p>

<code>SELECT *
FROM time_month_dim
ORDER BY month_end_date;
</code>

<code>SELECT *
FROM time_quarter_dim
ORDER BY quarter_end_date;
</code>

<code>SELECT *
FROM time_year_dim
ORDER BY year_name;
</code>

<p>The next query joins the month, quarter, and year tables.</p>

<code>SELECT
	y.year_id
	, y.year_name
    , q.quarter_id
	, q.quarter_name
	, m.month_id
	, m.month_name
	, m.month_end_date
FROM
	time_year_dim y
	, time_quarter_dim q
	, time_month_dim m
WHERE
	m.quarter_id = q.quarter_id
	AND q.year_id = y.year_id
ORDER BY
	m.month_end_date;</code>",11-APR-24 05.14.54.954331000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.52.24.637336000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29837541305539464252069731185228555245,29836425107181974316375148569777144134,"Introduction",10,"<p>Hierarchies typically contain multiple levels.  Data representing hierarchy members in the MEMBER_NAME or MEMBER_CAPTON columns might or might not be suitable for sorting hiearchy members. Often there is the need to sort hierachy members using an alternative value or to sort hierarchy members as nested values within parent and ancestors, as in a pivot table or tree control.</p>

<p>The HIER_ORDER column, which is present in every hierarchy, can be used to sort hierarchy members in nested order within parents and ancestors, independany of MEMBER_NAME, MEMBER_CAPTION or other attributes</p>",19-APR-24 11.49.11.626633000 AM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 11.49.11.626662000 AM,"WILLIAM.ENDRESS@ORACLE.COM"
29841639860177196478961413887398790355,29836425107181974316375148569777144134,"Using HIER_ORDER ** Best Practice **",50,"<p>As seen in previous queries, HIER_ORDER can be used in the ORDER BY clause of a SELECT statement without including HIER_ORDER in the SELECT list.</p>

<p>How HIER_ORDER is calculated in the SQL differs depending on whether it is included in the SELECT list.  This can have a significant impact on query performance, pending on the size of the hierarchy.</p>

<p>If HIER_ORDER is used in ORDER BY but not the SELECT list, HIER_ORDER is calculated after filters are applied.  This provides the expected sorted behavior and the best query performance.  For example.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name IN ('MONTH','QUARTER','YEAR')
   AND year = '2024'
  ORDER BY
    hier_order;</code>

<p><b>Best practice is to not include HIER_ORDER in the SELECT list.</b></p>

<p>If HIER_ORDER is included in the SELECT list, HIER_ORDER returns the absolute order within the hierarchy, This means the HIER_ORDER needs to be calculated for all hierarchy members, not just those members that are returned by the query</p>

<p>Note that in the following query, which is not filtered by year, HIER_ORDER begins at 1.</p>

<code>SELECT
    hier_order
    , member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name IN ('MONTH','QUARTER','YEAR')
  ORDER BY
    hier_order;</code>

<p>With the YEAR = '2024' filter, HIER_ORDER begins at 8792</p>

<code>SELECT
    hier_order
    , member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name IN ('MONTH','QUARTER','YEAR')
   AND year = '2024'
  ORDER BY
    hier_order;</code>

<p>While the difference in query performance is probably not noticeable with this small hierarchy, in the context of querying the hierarchy directly rather than through an anaytic view, including HIER_ORDER in the SELECT list can cause a noticeable different in query performance in other contexts.</p>",19-APR-24 12.35.24.550596000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 01.18.15.615202000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29033008720069390684968632177548866267,92046253613515805458457481846201153366,"Questions or Comments?",90,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.46.38.885125000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.46.38.885154000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29838815232703375783795862436917047370,29836425107181974316375148569777144134,"Examine Sample Data",20,"<p>The prerequisite (setup) SQL of this tutorial creates a view with 25 years of time data at the day, month, quarter and year levels.</p>

<p>View the sample data.</p>

<code>SELECT * FROM time_dim;</code>

<p>Note that data does not sort according to the calendar when ordering by DAY, MONTH or QUARTER.</p>

<p>Order by DAY.</p>

<code>SELECT
  day
  , month
  , quarter
  , year
FROM
  time_dim
ORDER BY
  day;</code>

<p>Order by MONTH.</p>

<code>SELECT
  DISTINCT
  month
  , quarter
  , year
FROM
  time_dim
ORDER BY
  month;</code>

<p>Order by QUARTER.</p>

<code>SELECT
  DISTINCT
  quarter
  , year
FROM
  time_dim
ORDER BY
  quarter;</code>

<p>Non-leaf columns can be sorted by DAY_ID, a date data type.</p>

<code>SELECT
  month
  , quarter
  , year
  , MAX(day_id)
FROM
  time_dim
GROUP BY
    month
  , quarter
  , year
ORDER BY
  MAX(day_id);</code>

<code>SELECT
  quarter
  , year
  , MAX(day_id)
FROM
  time_dim
GROUP BY
  quarter
  , year
ORDER BY
  MAX(day_id);</code>",19-APR-24 11.57.01.054888000 AM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 01.07.15.816570000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29033008147855450255611450784233841830,95114558141387237177774563848501259509,"Questions or Comments?",90,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.45.55.928357000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.45.55.928383000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29031452010201425644167446874218869077,325337177180857205374378153253835695768,"Questions or Comments?",80,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.41.49.977838000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.41.49.977869000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29031452010228022012198968716062404949,328974142671334092196222097220687633361,"Questions or Comments?",70,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.42.48.277235000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.42.48.277260000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29032771729647627205878783569203087496,152769908351014758555746151630340122318,"Questions or Comments?",40,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.44.00.872201000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.44.00.872226000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29033020639837193799921076064237958449,92046253613535148271571315912996452182,"Questions or Comments?",110,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.45.18.534674000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.45.18.534700000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29033020639909729349097953814720329009,92046253613504925126080950183628797782,"Questions or Comments?",90,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.46.18.666446000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.46.18.666470000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29844971033887458756865377260422574899,29836425107181974316375148569777144134,"Questions or Comments?",60,"<p>Get in touch at william.endress@oracle.com.</p>",19-APR-24 01.19.53.789672000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 01.19.53.789712000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29839076587697659350856777184791541008,29836425107181974316375148569777144134,"Sorting Hierarchy Members",40,"<p>Time hierarchies are typically sorted in calendar order. If multiple levels are selected, children or often positioned before or after parents and ancestors (as in a pivot table or tree control)</p>

<p>The DAY, MONTH, QUARTER, and YEAR columns were used as MEMBER NAME in each of the levels.  Each level used DAY_ID in ORDER BY.</p>

<p>SELECT and ORDER BY DAY.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'DAY'
  ORDER BY
    day;</code>

<p>Most likely, this is not what a user would expect.  They would expect days to be sorted in calendar order.  This can be done by sorting by HIER_ORDER, which uses DAY_ID, a date data type.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'DAY'
  ORDER BY
    hier_order;</code>

<p>The same can be seen at the month, quarter, and year levels</p>

<p>Select month level data ORDER BY month.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'MONTH'
  ORDER BY
    month;</code>

<p>Select month level data ORDER BY hier_order.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'MONTH'
  ORDER BY
    hier_order;</code>

<p>Select month level data ORDER BY month.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'QUARTER'
  ORDER BY
    quarter;</code>

<p>Select month level data ORDER BY hier_order.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name = 'QUARTER'
  ORDER BY
    hier_order;</code>

<p>The nested sorting can be seen by selecting multiple levels in a query.</p>

<code>SELECT
    member_name
    , day
    , month
    , quarter
    , year
  FROM
    time_hier
  WHERE
    level_name IN ('MONTH','QUARTER','YEAR')
  ORDER BY
    hier_order;</code>",19-APR-24 12.31.16.984062000 PM,"WILLIAM.ENDRESS@ORACLE.COM",19-APR-24 12.56.30.068515000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
29033063943805220397576253026817021443,92046253613429971725264843174797014870,"Questions or Comments?",130,"<p>Get in touch at william.endress@oracle.com.</p>",11-APR-24 06.47.00.431460000 PM,"WILLIAM.ENDRESS@ORACLE.COM",11-APR-24 06.47.00.431485000 PM,"WILLIAM.ENDRESS@ORACLE.COM"
